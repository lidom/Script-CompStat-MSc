<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Computational Statistics (M.Sc.) - 2&nbsp; The Bootstrap</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch3_MaximumLikelihood.html" rel="next">
<link href="./Ch1_Random_Variable_Generation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Bootstrap</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Statistics (M.Sc.)</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Organization of the Course</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_Random_Variable_Generation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_Bootstrap.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Bootstrap</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_MaximumLikelihood.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_EMAlgorithmus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Expectation Maximization (EM) Algorithm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_NonparametricRegression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Nonparametric Regression Analysis</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-empirical-distribution-function" id="toc-the-empirical-distribution-function" class="nav-link active" data-scroll-target="#the-empirical-distribution-function"><span class="toc-section-number">2.1</span>  The empirical distribution function</a></li>
  <li><a href="#basic-idea" id="toc-basic-idea" class="nav-link" data-scroll-target="#basic-idea"><span class="toc-section-number">2.2</span>  Basic idea</a></li>
  <li><a href="#the-nonparametric-standard-bootstrap" id="toc-the-nonparametric-standard-bootstrap" class="nav-link" data-scroll-target="#the-nonparametric-standard-bootstrap"><span class="toc-section-number">2.3</span>  The nonparametric (standard) bootstrap</a>
  <ul class="collapse">
  <li><a href="#the-bootstrap-algorithm" id="toc-the-bootstrap-algorithm" class="nav-link" data-scroll-target="#the-bootstrap-algorithm">The bootstrap algorithm</a></li>
  <li><a href="#example-inference-about-the-population-mean" id="toc-example-inference-about-the-population-mean" class="nav-link" data-scroll-target="#example-inference-about-the-population-mean"><span class="toc-section-number">2.3.1</span>  Example: Inference about the population mean</a></li>
  <li><a href="#example-inference-about-a-population-proportion" id="toc-example-inference-about-a-population-proportion" class="nav-link" data-scroll-target="#example-inference-about-a-population-proportion"><span class="toc-section-number">2.3.2</span>  Example: Inference about a population proportion</a></li>
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="toc-section-number">2.3.3</span>  Confidence intervals</a></li>
  </ul></li>
  <li><a href="#pivot-statistics-and-the-bootstrap-t-method" id="toc-pivot-statistics-and-the-bootstrap-t-method" class="nav-link" data-scroll-target="#pivot-statistics-and-the-bootstrap-t-method"><span class="toc-section-number">2.4</span>  Pivot statistics and the bootstrap-<span class="math inline">\(t\)</span> method</a>
  <ul class="collapse">
  <li><a href="#bootstrap-t-confidence-interval" id="toc-bootstrap-t-confidence-interval" class="nav-link" data-scroll-target="#bootstrap-t-confidence-interval"><span class="toc-section-number">2.4.1</span>  Bootstrap-t confidence interval</a></li>
  <li><a href="#accuracy-of-the-bootstrap-t-method" id="toc-accuracy-of-the-bootstrap-t-method" class="nav-link" data-scroll-target="#accuracy-of-the-bootstrap-t-method"><span class="toc-section-number">2.4.2</span>  Accuracy of the Bootstrap-<span class="math inline">\(t\)</span> method</a></li>
  </ul></li>
  <li><a href="#regression-analysis-bootstrapping-pairs" id="toc-regression-analysis-bootstrapping-pairs" class="nav-link" data-scroll-target="#regression-analysis-bootstrapping-pairs"><span class="toc-section-number">2.5</span>  Regression Analysis: Bootstrapping pairs</a>
  <ul class="collapse">
  <li><a href="#bootstrapping-pairs-bootstrap-under-random-design" id="toc-bootstrapping-pairs-bootstrap-under-random-design" class="nav-link" data-scroll-target="#bootstrapping-pairs-bootstrap-under-random-design"><span class="toc-section-number">2.5.1</span>  Bootstrapping pairs: Bootstrap under random design</a></li>
  </ul></li>
  <li><a href="#regression-analysis-residual-bootstrap" id="toc-regression-analysis-residual-bootstrap" class="nav-link" data-scroll-target="#regression-analysis-residual-bootstrap"><span class="toc-section-number">2.6</span>  Regression Analysis: Residual bootstrap</a>
  <ul class="collapse">
  <li><a href="#bootstrap-confidence-intervals-for-the-regression-coefficients" id="toc-bootstrap-confidence-intervals-for-the-regression-coefficients" class="nav-link" data-scroll-target="#bootstrap-confidence-intervals-for-the-regression-coefficients"><span class="toc-section-number">2.6.1</span>  Bootstrap confidence intervals for the regression coefficients</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Bootstrap</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading (asymptotic statistics), for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.</p>
<p>Some literature:</p>
<ul>
<li>Hall, P. (1992): The Bootstrap and Edgeworth Expansion; Springer Verlag</li>
<li>Shao, J. and Tu, D. (1995): The Jacknife and Bootstrap; Springer Verlag</li>
<li>Horowitz, J.L. (2001): The Bootstrap. In: Handbook of Econometrics, Volume 5; Elsevier Science B.V.</li>
<li>Davison, A.C. and Hinkley, D.V. (2005): Bootstrap Methods and their Applications; Cambridge University Press</li>
</ul>
<section id="the-empirical-distribution-function" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="the-empirical-distribution-function"><span class="header-section-number">2.1</span> The empirical distribution function</h2>
<p>The distribution of a real-valued random variable <span class="math inline">\(X\)</span> can be completely described by its distribution function <span class="math display">\[
F(x)=P(X\leq x)\quad \text{for all } x\in\mathbb{R}.
\]</span></p>
<p><strong>Data:</strong> i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span></p>
<p>For given data, the sample analogue of <span class="math inline">\(F\)</span> is the so-called <em>empirical distribution function</em>, which is an important tool of statistical inference.</p>
<p>Let <span class="math inline">\(I(\cdot)\)</span> denote the indicator function, i.e., <span class="math inline">\(I(x\leq t)=1\)</span> if <span class="math inline">\(x\leq t\)</span>, and <span class="math inline">\(I(x\leq t)=0\)</span> if <span class="math inline">\(x&gt;t.\)</span></p>
<div id="def-ecdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Empirical distribution function) </strong></span><span class="math display">\[
F_n(x)=\frac{1}{n} \sum_{i=1}^n I(X_i\leq x)
\]</span> i.e <span class="math inline">\(F_n(x)\)</span> is the proportion of observations with <span class="math inline">\(X_i\le x,\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
</div>
<p><strong>Properties:</strong></p>
<ul>
<li><span class="math inline">\(0\le F_n(x)\le 1\)</span></li>
<li><span class="math inline">\(F_n(x)=0\)</span>, if <span class="math inline">\(x&lt;X_{(1)}\)</span>, where <span class="math inline">\(X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}\)</span> denotes the <strong>order-statistic</strong>; i.e.&nbsp;<span class="math inline">\(X_{(1)}\)</span> is the smallest observation</li>
<li><span class="math inline">\(F(x)=1\)</span>, if <span class="math inline">\(x\ge X_{(n)}\)</span>, where <span class="math inline">\(X_{(n)}\)</span> is largest observation</li>
<li><span class="math inline">\(F_n\)</span> monotonically increasing step function</li>
<li>Structurally, <span class="math inline">\(F_n\)</span> itself is a distribution function; it is equivalent to the distribution function of a <strong>discrete random variable</strong> <span class="math inline">\(X^*\)</span> with possible values <span class="math inline">\(X^*\in\{X_1,\dots,X_n\}\)</span> and with <span class="math inline">\(P(X^*=X_i)=\frac{1}{n}\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></li>
</ul>
<div id="exm-ecdfexample" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Empirical distribution function) </strong></span><br></p>
<p>Some data:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.20</td>
</tr>
<tr class="even">
<td>2</td>
<td>4.80</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.40</td>
</tr>
<tr class="even">
<td>4</td>
<td>4.60</td>
</tr>
<tr class="odd">
<td>5</td>
<td>6.10</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.40</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.80</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
</tr>
</tbody>
</table>
<p>Corresponding empirical distribution function using <code>R</code>:</p>
<div class="cell" data-hash="Ch2_Bootstrap_cache/html/ecdfPlot_b6c302701bb5727d08d983268dc32c44">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">5.20</span>, <span class="fl">4.80</span>, <span class="fl">5.40</span>, <span class="fl">4.60</span>, </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">6.10</span>, <span class="fl">5.40</span>, <span class="fl">5.80</span>, <span class="fl">5.50</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>myecdf_fun     <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(observedSample)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myecdf_fun, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch2_Bootstrap_files/figure-html/ecdfPlot-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<p><span class="math inline">\(F_n(x)\)</span> depends on the observed sample and thus is <strong>random</strong>. We obtain</p>
<ul>
<li>For every <span class="math inline">\(x\in\mathbb{R}\)</span> <span class="math display">\[
nF_n(x)\sim B(n, p=F(x))
\]</span> I.e., <span class="math inline">\(nF_n(x)\)</span> has a binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p=F(x)\)</span>.</li>
<li><span class="math inline">\(E(F_n(x))=F(x)\)</span></li>
<li><span class="math inline">\(Var(F_n(x))=\frac{F(x)(1-F(x))}{n}\)</span></li>
</ul>
<div id="thm-Clivenk-Cantelli" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Theorem of Glivenko-Cantelli) </strong></span><span class="math display">\[
P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1
\]</span></p>
</div>
</section>
<section id="basic-idea" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="basic-idea"><span class="header-section-number">2.2</span> Basic idea</h2>
<p>The basic idea of the bootstrap is to replace random sampling from the true (unknown) population <span class="math inline">\(F\)</span> (infeasible Monte Carlo simulation) by random sampling from the empirical distribution <span class="math inline">\(F_n\)</span> (feasible Monte Carlo simulation).</p>
<ul>
<li>The random sample <span class="math inline">\(X_1,\dots,X_n\)</span> is generated by drawing observations independently and with replacement from the unknown population distribution function <span class="math inline">\(F\)</span>. That is, for each interval <span class="math inline">\([a,b]\)</span> the probability of drawing an observation in <span class="math inline">\([a,b]\)</span> is given by <span class="math display">\[
P(X\in [a,b])=F(b)-F(a).
\]</span></li>
<li>For large <span class="math inline">\(n\)</span>: The empirical distribution <span class="math inline">\(F_n\)</span> of the sample values is ‚Äúclose‚Äù to the unknown distribution <span class="math inline">\(F\)</span> (<a href="#thm-Clivenk-Cantelli">Theorem&nbsp;<span>2.1</span></a>). That is, for <span class="math inline">\(n\rightarrow\infty\)</span> the relative frequency of observations <span class="math inline">\(X_i\)</span> in <span class="math inline">\([a,b]\)</span> converges to <span class="math inline">\(P(X\in [a,b])\)</span> <span class="math display">\[
\begin{align*}
\frac{1}{n}\sum_{i=1}^nI(X_i\in[a,b])&amp;\to_p P(X\in [a,b])\\
\Leftrightarrow \qquad F_n(b)-F_n(a)&amp;\to_p F(b)-F(a)
\end{align*}
\]</span></li>
<li>The idea of the bootstrap consists in mimicking the data generating process: Instead of random sampling from the true unknown population distribution <span class="math inline">\(F\)</span>, the bootstrap uses random sampling from the known <span class="math inline">\(F_n\)</span>. This is justified by the insight that the empirical distribution <span class="math inline">\(F_n\)</span> of the observed data is ‚Äúsimilar‚Äù to the true distribution <span class="math inline">\(F\)</span> (<a href="#thm-Clivenk-Cantelli">Theorem&nbsp;<span>2.1</span></a>).</li>
</ul>
</section>
<section id="the-nonparametric-standard-bootstrap" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="the-nonparametric-standard-bootstrap"><span class="header-section-number">2.3</span> The nonparametric (standard) bootstrap</h2>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> from <span class="math inline">\(X\sim F\)</span>.
<ul>
<li>The distribution <span class="math inline">\(X_i\sim F,\)</span> <span class="math inline">\(i=1,\dots,n,\)</span> depends on an unknown parameter (vector) <span class="math inline">\(\theta.\)</span></li>
<li>The data <span class="math inline">\(X_1,\dots,X_n\)</span> is used to estimate an element <span class="math inline">\(\theta\)</span> of the parameter vector <span class="math inline">\(\theta.\)</span><br>
</li>
<li>Thus, the estimator is a function of the random sample <span class="math display">\[
\hat\theta\equiv \hat\theta(X_1,\dots,X_n).
\]</span></li>
</ul></li>
<li><strong>Inference:</strong> We are interested in evaluating the distribution of <span class="math inline">\(\hat\theta-\theta\)</span> in order to
<ul>
<li>provide standard errors</li>
<li>construct confidence intervals</li>
<li>perform tests of hypothesis.</li>
</ul></li>
</ul>
<section id="the-bootstrap-algorithm" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-bootstrap-algorithm">The bootstrap algorithm</h3>
<ol type="1">
<li><strong>Draw a bootstrap sample:</strong> Generate a new random sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> by drawing observations independently and with replacement from the available sample <span class="math inline">\(X_1,\dots,X_n.\)</span></li>
<li><strong>Compute bootstrap estimate:</strong> Compute the estimate <span class="math display">\[
\hat\theta^*\equiv \hat\theta(X_1^*,\dots,X_n^*)
\]</span></li>
<li><strong>Bootstrap replications:</strong> Repeat Steps 1 and 2 <span class="math inline">\(m\)</span> times (e.g.&nbsp;<span class="math inline">\(m=2000\)</span>) leading to <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*
\]</span></li>
<li>For large <span class="math inline">\(m\)</span>, the estimates <span class="math inline">\(\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*\)</span> allow to approximate the <strong>bootstrap distribution</strong> of <span class="math inline">\(\hat\theta^*-\hat\theta\)</span> arbitrarily well. The bootstrap distribution of <span class="math inline">\(\hat\theta^*-\hat\theta\)</span> is used to approximate the unknown distribution of <span class="math inline">\(\hat\theta-\theta\)</span>.</li>
</ol>
<p>The theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The bootstrap does not always work
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap does <strong>not always work</strong>. A necessary condition for the use of the bootstrap is the <strong>consistency of the bootstrap approximation</strong>.</p>
</div>
</div>
<p>The bootstrap is called <strong>consistent</strong>, if for large <span class="math inline">\(n\)</span>, the bootstrap distribution of <span class="math inline">\(\hat{\theta}^* -\hat{\theta}\)</span> is a good approximation of the underlying distribution of <span class="math inline">\(\hat{\theta}-\theta\)</span>, i.e. <span class="math display">\[
\text{distribution}(\hat{\theta}^* -\hat{\theta}\ |{\cal S}_n)\approx
\text{distribution}(\hat{\theta}-\theta).
\]</span> The following definition states this more precisely.</p>
<div id="def-BootstrapConsistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Bootstrap consistency) </strong></span><em>If for some <span class="math inline">\(\gamma&gt;0\)</span> (usually: <span class="math inline">\(\gamma=1/2\)</span>) we have <span class="math inline">\(n^\gamma(\hat{\theta}-\theta)\rightarrow_d Z\)</span> for some random variable <span class="math inline">\(Z\)</span> with a non-degenerate distribution, then the bootstrap is <strong>consistent</strong> if and only if <span class="math display">\[
\sup_\delta \left|
   P\left(n^\gamma(\hat\theta^*-\hat\theta)\le \delta|{\cal S}_n\right)
  -P\left(n^\gamma(\hat\theta      -\theta)\le \delta\right)
  \right|\rightarrow 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></em></p>
</div>
<p>Luckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are some crucial requirements:</p>
<ol type="1">
<li>Generation of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).</li>
<li>The distribution of the estimator <span class="math inline">\(\hat\theta\)</span> needs to be asymptotically normal.</li>
</ol>
<p>The standard bootstrap <strong>will usually fail</strong> if one of the above conditions 1 or 2 is violated. For instance,</p>
<ul>
<li>The bootstrap will not work if the i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(X_1,\dots,X_n\)</span> does not properly reflect the way how the <span class="math inline">\(X_1,\dots,X_n\)</span> are generated when <span class="math inline">\(X_1,\dots,X_n\)</span> is a time-series with auto-correlated data.</li>
<li>The distribution of the estimator <span class="math inline">\(\hat\theta\)</span> is not asymptotically normal. (E.g. in case of extreme value problems.)</li>
</ul>
<p><strong>Note:</strong> In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g.&nbsp;the block-bootstrap in case of time-series data).</p>
</section>
<section id="example-inference-about-the-population-mean" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="example-inference-about-the-population-mean"><span class="header-section-number">2.3.1</span> Example: Inference about the population mean</h3>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Population Model:</strong> Continuous random variable <span class="math inline">\(X\sim F\)</span> with unknown <span class="math inline">\(F\)</span> and thus unknown mean <span class="math inline">\(\mu\)</span></li>
<li><strong>Data:</strong> i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span> from <span class="math inline">\(X\)</span></li>
<li><strong>Problem:</strong> What is the distribution of <span class="math inline">\(\bar{X} -\mu\)</span>?</li>
</ul>
<p>Now assume that <span class="math inline">\(n=8\)</span> and that the <strong>observed sample</strong> is</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>-0.6</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.4</td>
</tr>
<tr class="even">
<td>4</td>
<td>-0.8</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.6</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td>7</td>
<td>-0.1</td>
</tr>
<tr class="even">
<td>8</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<div class="cell" data-hash="Ch2_Bootstrap_cache/html/unnamed-chunk-1_c0af08d7cdede2264fde9e0ad283a695">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.6</span>, <span class="fl">1.0</span>, <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.8</span>, <span class="fl">1.6</span>, <span class="fl">1.9</span>, <span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
So the sample mean is
<center>
<span class="math inline">\(\bar X =\)</span> <code>mean(observedSample)</code> <span class="math inline">\(=\)</span> 0.6375
</center>
<p><br></p>
<p><strong>Bootstrap:</strong></p>
<p>The observed sample <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span> is taken as underlying empirical ‚Äúpopulation‚Äù in order to generate <strong>‚Äúbootstrap data‚Äù</strong> <span class="math inline">\(X_1^*,\dots,X_n^*\)</span>:</p>
<ul>
<li>i.i.d. samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> are generated by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span>.</li>
</ul>
<div class="cell" data-hash="Ch2_Bootstrap_cache/html/unnamed-chunk-2_b9f9a36a302949c9a46cfe9578366030">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generating a bootstrap sample</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>bootSample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">size    =</span> <span class="fu">length</span>(observedSample), </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">replace =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>The distribution of <span class="math inline">\(\bar X -\mu\)</span> is approximated by the conditional distribution of <span class="math inline">\(\bar X^* -\bar X\)</span> given the original sample <span class="math inline">\({\cal S}_n\)</span> <span class="math display">\[
\underbrace{P\left(\bar{X}-\mu&lt;\delta\right)}_{\text{unknown}}\approx
\underbrace{P\left(\bar{X}^*-\bar{X}&lt;\delta|\mathcal{S}_n\right)}_{\text{approximable}}
\]</span></li>
</ul>
<p>For the given data with <span class="math inline">\(n=8\)</span> observations, there are <span class="math display">\[
n^n=8^8=16,777,216
\]</span> possible bootstrap samples which are all equally probable.</p>
<p>The conditional distribution function of <span class="math inline">\(\bar{X}^*-\bar{X}\)</span> given <span class="math inline">\(\mathcal{S}_n\)</span> <span class="math display">\[
P\left(\bar{X}^*-\bar{X}&lt;\delta|\mathcal{S}_n\right)
\]</span> can be approximated using a Monte-Carlo simulation. For this, we draw new data from <span class="math inline">\(F_n,\)</span> i.e., we sample with replacement data points from the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span></p>
<p>Using a large number <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10000\)</span>) of simulation runs allows us to generate bootstrap estimates <span class="math display">\[
\bar{X}^*_1,\bar{X}^*_2,\dots,\bar{X}^*_m
\]</span></p>
<!-- Simul. | $X_1^*$|  $X_2^*$| $X_3^*$| $X_4^*$|  $X_5^*$|  $X_6^*$|  $X_7^*$|   $X_8^*$ | $\bar X^*-\bar{X}$
----|----:|----:|----:|----:|----:|----:|----:|----:|:----:
1 | 1.9| -0.8| 1.9|  -0.6| 1.4| -0.1| -0.8| 1.0 | -0.15
2 | 0.7| -0.8| -0.8| 1.0 | 1.6| 1.0| -0.1| -0.8 | -0.4125  
3 | -0.1| 1.9| 0.7|  1.0| -0.1| 1.6| 1.0| -0.6 |  0.0375 
4 | 1.4| 1.0| 1.4|  -0.1| 1.9| -0.8| 1.9| 1.0 | 0.325 
5 | 1.0| 0.7| -0.1|  0.7| 1.4| -0.8| 1.0| 1.6 |  0.05
... ||||||||| -->
<p>These bootstrap estimates are then used to approximate <span class="math display">\[
\frac{1}{m}\sum_{k=1}^m
I( \bar X^*_k-\bar X\leq \delta) \approx \underbrace{P\left(\bar X^*-\bar X\leq \delta |{\cal S}_n\right)}_{\text{bootstrap distribution}},
\]</span> where the approximation will be arbitrarily precise as <span class="math inline">\(m\to\infty\)</span>.</p>
<!-- 

::: {.cell hash='Ch2_Bootstrap_cache/html/unnamed-chunk-3_5550063abeb9d45c116c082279c75192'}

```{.r .cell-code}
n                <- length(observedSample)
Xbar             <- mean(observedSample)
m                <- 2000
bootRealizations <- numeric(m)

for(l in seq_len(m)){
 bootSample          <- sample(x       = observedSample, 
                               size    = n, 
                               replace = TRUE)
 bootXbar            <- mean(bootSample)
 bootRealizations[l] <- bootXbar - Xbar
}

plot(ecdf(bootRealizations), main="")
```

::: {.cell-output-display}
![](Ch2_Bootstrap_files/figure-html/unnamed-chunk-3-1.png){width=672}
:::
:::

 -->
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation <span class="math inline">\(E^*(\cdot),\)</span> <span class="math inline">\(Var^*(\cdot),\)</span> and <span class="math inline">\(P^*(\cdot)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the bootstrap literature one also frequently finds the notation <span class="math inline">\(E^*(\cdot),\)</span> <span class="math inline">\(Var^*(\cdot),\)</span> or <span class="math inline">\(P^*(\cdot)\)</span> to denote <strong>conditional</strong> expectations <span class="math inline">\(E^*(\cdot)=E(\cdot|\mathcal{S}_n),\)</span>, variances <span class="math inline">\(Var^*(\cdot)=Var(\cdot|\mathcal{S}_n),\)</span>, or probabilities <span class="math inline">\(P^*(\cdot)=P(\cdot|\mathcal{S}_n),\)</span> given the sample <span class="math inline">\({\cal S}_n.\)</span></p>
</div>
</div>
<section id="the-bootstrap-distribution-of-bar-x" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-bootstrap-distribution-of-bar-x">The bootstrap distribution of <span class="math inline">\(\bar X^*\)</span></h4>
<p>The bootstrap focuses on the <strong>conditional</strong> distribution of <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> given the observed sample <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span> and the resulting conditional distribution of <span class="math display">\[
(\bar X^* -\bar X)|\mathcal{S}_n
\]</span> Often these conditional distributions are called <strong>bootstrap distributions</strong>.</p>
<p>ü§ü We can analyze the bootstrap distribution of <span class="math inline">\(\bar X^* -\bar X\)</span>, since <strong>we <em>know</em> the discrete distribution</strong> of the conditional random variables <span class="math display">\[
X_i^*|\mathcal{S}_n,
\]</span> even though, we do <strong>not know</strong> the distribution of <span class="math inline">\(X_i\sim F.\)</span></p>
<p>For each <span class="math inline">\(i=1,\dots,n\)</span>, the possible values of the discrete random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> are <span class="math display">\[
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\},
\]</span> and each of these values is equally probable <span class="math display">\[
\begin{align*}
P^*(X_i^*=X_1)&amp;= P(X_i^*=X_1|{\cal S}_n) = \frac{1}{n} \\
P^*(X_i^*=X_2)&amp;= P(X_i^*=X_2|{\cal S}_n) = \frac{1}{n} \\
&amp;\vdots\\
P^*(X_i^*=X_n)&amp;= P(X_i^*=X_n|{\cal S}_n) = \frac{1}{n}.
\end{align*}
\]</span></p>
<p>Thus, we know the whole distribution of the (conditional) discrete random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> and can compute, for instance, easily its conditional mean and its variance.</p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(X_i^*\)</span> is <span class="math display">\[
\begin{align*}
E^*(X_i^*)
&amp;=E(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_2\\
&amp;=\bar X
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(X_i^*\)</span> is <span class="math display">\[
\begin{align*}
Var^*(X_i^*)
&amp;=Var(X_i^*|{\cal S}_n)\\
&amp;=E((X_i^* - E(X_i^*|{\cal S}_n))^2|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2\\
&amp;=\hat\sigma^2
\end{align*}
\]</span></p></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
General case: Conditional moments of transformed <span class="math inline">\(g(X_i^*)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any measurable function <span class="math inline">\(g\)</span> we have <span class="math display">\[
E^*(g(X_i^*))=E(g(X_i^*)|\mathcal{S}_n)=\frac{1}{n}\sum_{i=1}^n g(X_i).
\]</span></p>
</div>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Conditioning on <span class="math inline">\(\mathcal{S}_n\)</span> in important!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Conditioning on the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> is very important. The <strong>marginal</strong> (non-conditional) distribution of <span class="math inline">\(X_i^*\)</span> is equal to the distribution of <span class="math inline">\(X_i.\)</span> This follows from the fact that for <span class="math display">\[
\begin{align*}
P(X_i^*\leq \delta)
&amp;= E\left(I\left(X_i^*\leq \delta\right)\right)\\
&amp;= E\left[E\left(I\left(X_i^*\leq \delta\right)|\mathcal{S}_n\right)\right]\\
&amp;= E\left[\frac{1}{n}\sum_{i=1}^nI\left(X_i\leq \delta\right)\right]\\
&amp;= E\left[I\left(X_i\leq \delta\right)\right]\\
&amp;= P\left(X_i\leq \delta\right)=F(\delta)
\end{align*}
\]</span> But we do not know <span class="math inline">\(F.\)</span></p>
</div>
</div>
<p><strong>Now consider the bootstrap distribution of <span class="math inline">\(\bar X^*\)</span></strong></p>
<p>Since we know the distribution of the i.i.d. sample <span class="math display">\[
X_1|\mathcal{S}_n, X_2|\mathcal{S}_n,\dots, X_n|\mathcal{S}_n
\]</span> it is straight forward to derive the asymptotic distribution of <span class="math inline">\(\bar X^*\)</span> using the central limit theorem.</p>
<p>Firstly, let us derive the conditional mean and variance of <span class="math display">\[
\bar X^* = \frac{1}{n}\sum_{i=1}^nX_i^*
\]</span></p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(\bar X^*\)</span> is <span class="math display">\[
\begin{align*}
E^*(\bar X^*)
&amp;=E(\bar X^*|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^nE(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^n \bar X\\
&amp;=\frac{n}{n}\bar X \\
&amp;=\bar X
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(\bar X^*\)</span> is <span class="math display">\[
\begin{align*}
Var^*(\bar X^*)
&amp;=Var(\bar X^*|{\cal S}_n)\\
&amp;=\frac{1}{n^2}\sum_{i=1}^n Var(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n^2}\sum_{i=1}^n \hat\sigma^2\\
&amp;=\frac{n}{n^2}\hat\sigma^2\\
&amp;=\frac{1}{n}\hat\sigma^2
\end{align*}
\]</span></p></li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
We can apply the CLT to <span class="math inline">\(\bar X^*|\mathcal{S}_n\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that, conditionally on <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span>,</p>
<ul>
<li>the random variables <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> are i.i.d.</li>
<li>with mean <span class="math inline">\(E^*(X_i^*)=\bar X\)</span></li>
<li>and variance <span class="math inline">\(Var^*(X^*)=\hat\sigma^2\)</span></li>
</ul>
<p>Thus, we can apply the <a href="https://www.statlect.com/asymptotic-theory/central-limit-theorem">central limit theorem (Lindeberg-L√©vy)</a> to the appropriately scaled sample mean conditionally on <span class="math inline">\({\cal S}_n\)</span> <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\bar X^* - \bar X)}{\hat\sigma}\right|{\cal S}_n\right)
\]</span></p>
</div>
</div>
<p>The central limit theorem (Lindeberg-L√©vy) implies that <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>Moreover, <span class="math inline">\(\hat\sigma^2\)</span> is a consistent estimator of <span class="math inline">\(\sigma^2,\)</span> and thus asymptotically <span class="math inline">\(\hat\sigma^2\)</span> may be replaced by <span class="math inline">\(\sigma\)</span>. Therefore, <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>On the other hand, we also have that <span class="math display">\[
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right)
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>This means that the bootstrap is <strong>consistent</strong>. The bootstrap distribution of <span class="math inline">\(\sqrt{n}(\bar X^* -\bar X)\)</span> asymptotically coincides with the distribution of <span class="math inline">\(\sqrt{n}(\bar X-\mu)\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. In other words, for large <span class="math inline">\(n\)</span>, <span class="math display">\[
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
\]</span></p>
<p>This bootstrap consistency result justifies using the bootstrap distribution <span class="math display">\[
P(\bar{X}^*-\bar{X}\leq \delta|\mathcal{S}_n),
\]</span> which we can approximate with arbitrary precision (as <span class="math inline">\(m\to\infty),\)</span> and which we thus can use as a tool for doing inference about <span class="math inline">\(\mu.\)</span></p>
</section>
</section>
<section id="example-inference-about-a-population-proportion" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="example-inference-about-a-population-proportion"><span class="header-section-number">2.3.2</span> Example: Inference about a population proportion</h3>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n,\)</span> where <span class="math inline">\(X_i\in\{0,1\}\)</span> is dichotomous and <span class="math inline">\(P(X_i=1)=p\)</span>, <span class="math inline">\(P(X_i=0)=1-p\)</span>.</li>
<li><strong>Estimator:</strong> Let <span class="math inline">\(S\)</span> denote the number of <span class="math inline">\(X_i\)</span> which are equal to <span class="math inline">\(1.\)</span> The maximum likelihood estimate of <span class="math inline">\(p\)</span> is <span class="math inline">\(\hat p=S/n.\)</span></li>
<li><strong>Problem:</strong> Inference about <span class="math inline">\(p\)</span>.</li>
</ul>
<p><strong>Recall:</strong></p>
<ul>
<li><span class="math inline">\(n\hat p=S\sim B(n,p)\)</span></li>
<li>As <span class="math inline">\(n\rightarrow\infty\)</span> the central limit theorem implies that <span class="math display">\[
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
\]</span></li>
<li>Thus for <span class="math inline">\(n\)</span> large, the distributions of <span class="math inline">\(\sqrt{n}(\hat p -p)\)</span> and <span class="math inline">\(\hat p -p\)</span> can be approximated by <span class="math inline">\(\mathcal{N}(0,p(1-p))\)</span> and <span class="math inline">\(\mathcal{N}(0,p(1-p)/n)\)</span>, respectively.</li>
</ul>
<p><strong>Bootstrap:</strong></p>
<ul>
<li>Random sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> generated by drawing observations independently and with replacement from <span class="math display">\[
{\cal S}_n:=\{X_1,\dots,X_n\}.
\]</span></li>
<li>Let <span class="math inline">\(S^*\)</span> denote the number of <span class="math inline">\(X_i^*\)</span> which are equal to <span class="math inline">\(1.\)</span></li>
<li>Bootstrap estimate of <span class="math inline">\(p\)</span>: <span class="math inline">\(\hat p^*=S^*/n\)</span></li>
</ul>
<p>The distribution of <span class="math inline">\(\hat p^*\)</span> depends on the observed sample <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span>. A different sample <span class="math inline">\({\cal S}_n\)</span> will lead to a different distribution. The bootstrap now tries to approximate the true distribution of <span class="math inline">\(\hat p - p\)</span> by the <strong>conditional</strong> distribution of <span class="math inline">\(\hat p^*-\hat p\)</span> given the observed sample <span class="math inline">\({\cal S}_n\)</span>.</p>
<p>The bootstrap is called <strong>consistent</strong> if asymptotically (i.e.&nbsp;for <span class="math inline">\(n\rightarrow \infty\)</span>) the conditional distribution of <span class="math inline">\(\hat p^*-\hat p\)</span> coincides with the true distribution of <span class="math inline">\(\hat p - p.\)</span> (Note: a proper scaling is required!)</p>
<p>We obtain <span class="math display">\[
\begin{align*}
&amp; P^*(X_i^*=1)=P(X_i^*=1|\ {\cal S}_n)=\hat p, \\  
&amp; P^*(X_i^*=0)=P(X_i^*=0|\ {\cal S}_n)=1-\hat p
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
&amp;   E^*(\hat p^*)=E(\hat p^*|\ {\cal S}_n)=\hat p, \\
&amp; Var^*(\hat p^*)=E[(\hat p^*-\hat p)^2|\ {\cal S}_n]=\frac{\hat p(1-\hat p)}{n}
\end{align*}
\]</span></p>
<p>The <strong>bootstrap distribution</strong> of <span class="math inline">\(n\hat p^*=S^*\)</span> given <span class="math inline">\({\cal S}_n\)</span> is equal to the binomial distribution <span class="math inline">\(B(n,\hat p).\)</span> That is, for large <span class="math inline">\(n\)</span>, the bootstrap distribution of <span class="math display">\[
\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}
\]</span> is approximately standard normal. In other words, <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>Moreover, <span class="math inline">\(\hat p\)</span> is a consistent estimator of <span class="math inline">\(p\)</span> <span class="math display">\[
\hat p(1-\hat p)\rightarrow_p p(1-p),\quad n\rightarrow\infty.
\]</span> This implies that asymptotically <span class="math inline">\(\hat p(1-\hat p)\)</span> may be replaced by <span class="math inline">\(p(1-p)\)</span>, and <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right)\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span> More precisely, as <span class="math inline">\(n\rightarrow\infty\)</span> <span class="math display">\[
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0,
\]</span> where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e.&nbsp;for large <span class="math inline">\(n\)</span> <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx
\text{distribution}(\sqrt{n}(\hat p -p))%\approx N(0,p(1-p))
\]</span> as well as <span class="math display">\[
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx
\text{distribution}(\hat p -p)%\approx N(0,p(1-p)/n)
\]</span></p>
</section>
<section id="confidence-intervals" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">2.3.3</span> Confidence intervals</h3>
<section id="the-traditional-non-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-traditional-non-bootstrap-approach">The traditional (non-bootstrap) approach</h4>
<p>Traditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if</p>
<ul>
<li><span class="math inline">\(\theta\in\mathbb{R}\)</span> and</li>
<li><span class="math inline">\(\sqrt{n}(\hat\theta-\theta)\rightarrow_d\mathcal{N}(0,v^2)\)</span> as <span class="math inline">\(n\to\infty,\)</span></li>
</ul>
<p>then one traditionally tries to determine an approximation <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span> (the standard error of <span class="math inline">\(\hat\theta\)</span>) from the data. An approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval is then given by <span class="math display">\[
\left[
\hat{\theta}-z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}},
\hat{\theta}+z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}}
\right]
\]</span></p>
<p>In some cases it is, however, very difficult to obtain approximations <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span>. Statistical inference is then usually based on the bootstrap.</p>
<p>In contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span> are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed <strong>more precise</strong> than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)</p>
</section>
<section id="the-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-bootstrap-approach">The bootstrap approach</h4>
<!-- General approach: Basic bootstrap $(1-\alpha)\times 100\%$ confidence interval -->
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. random sample <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> with <span class="math inline">\(X_i\sim F\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>, where the distribution <span class="math inline">\(F\)</span> depends on the unknown parameter (vector) <span class="math inline">\(\theta.\)</span></li>
<li><strong>Problem:</strong> Construct a confidence interval for <span class="math inline">\(\theta.\)</span></li>
</ul>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumption: Bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the following, we will assume that the bootstrap is consistent; i.e.&nbsp;that <span class="math display">\[
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^* -\hat{\theta})|{\cal S}_n)
&amp;\approx
\text{distribution}(\sqrt{n}(\hat{\theta}-\theta))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*-\hat{\theta})|{\cal S}_n
&amp;\overset{d}{\approx} \sqrt{n}(\hat{\theta}-\theta)
\end{align*}
\]</span> if <span class="math inline">\(n\)</span> is sufficiently large. This is not always the case and in cases of doubt one needs to show this property.</p>
</div>
</div>
<p><strong>Derivation of the nonparametric bootstrap confidence intervals:</strong></p>
<ul>
<li><p>We can generate <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_k^*\equiv\hat\theta(X_{1k}^*,\dots,X_{nk}^*),\quad k=1,\dots,m,
\]</span> by drawing bootstrap samples <span class="math inline">\(X_{1k}^*,\dots,X_{nk}^*\)</span> independently and with replacement from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span></p></li>
<li><p>The <span class="math inline">\(m\)</span> bootstrap estimates allow us to approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> quantile <span class="math inline">\(\hat t_\frac{\alpha}{2}\)</span> and the <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantile <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> of the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}.\)</span> This can be done with negligible approximation error (for <span class="math inline">\(m\)</span> large) using the empirical quantiles <span id="eq-empiricalQuantile"><span class="math display">\[
\hat t_{p}=\left\{
\begin{array}{ll}
\hat\theta^*_{(\lfloor mp\rfloor+1)},         &amp; mp \text{ not a whole number}\\
(\hat\theta^*_{(mp)}+\hat\theta^*_{(mp+1)})/2,&amp; mp \text{ a whole number}
\end{array}\right.
\tag{2.1}\]</span></span> for <span class="math inline">\(p=\frac{\alpha}{2}\)</span> or <span class="math inline">\(p=1-\frac{\alpha}{2},\)</span> where <span class="math inline">\(\hat\theta_{(i)}^*\)</span> denotes the order statistic <span class="math display">\[
\hat\theta_{(1)}^* \leq \hat\theta_{(2)}^*\leq \dots\leq \hat\theta_{(m)}^*,
\]</span> and <span class="math inline">\(\lfloor mp\rfloor\)</span> denotes the greatest whole number less than or equal to <span class="math inline">\(mp.\)</span></p></li>
</ul>
<p>Then <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat t_\frac{\alpha}{2} \leq \hat{\theta}^* \leq \hat t_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P^*\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}^*-\hat{\theta} \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P^*\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)
\approx 1-\alpha
\end{align*}
\]</span> Here, <span class="math inline">\(P^*\)</span> denotes probabilities with respect to the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span>.</p>
<p>Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> <span class="math display">\[
{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}-\theta)}.
\]</span> Therefore, for large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
&amp;P\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{blue}\sqrt{n}(\hat{\theta}-\theta)}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)\approx 1-\alpha\\
\Rightarrow &amp;P\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}-\theta \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\
\Rightarrow &amp;P\left(\hat{\theta}-(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\le \theta\le \hat{\theta}-
(\hat t_\frac{\alpha}{2}-\hat{\theta})\right)\approx 1-\alpha\\
\Rightarrow &amp;P\left(2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}\le \theta\le 2\hat{\theta}-
\hat t_\frac{\alpha}{2}\right)\approx 1-\alpha.
\end{align*}
\]</span></p>
<p>Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) bootstrap confidence interval is given by <span id="eq-NPBootCI"><span class="math display">\[
\left[2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}, 2\hat{\theta}-\hat t_\frac{\alpha}{2}\right],
\tag{2.2}\]</span></span> where <span class="math inline">\(\hat t_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> are the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles of the bootstrap distribution approximated by the empirical quantiles of the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math inline">\(\hat{\theta}^*_1, \hat{\theta}^*_2,\dots, \hat{\theta}^*_m.\)</span></p>
</section>
<section id="example-confidence-intervals-for-the-population-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-confidence-intervals-for-the-population-mean">Example: Confidence intervals for the population mean</h4>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> Let <span class="math inline">\(X_1,\dots,X_n\)</span> denote an i.i.d. random sample with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. <!-- * In the following $F$ will denote the corresponding distribution function; i.e., $X_i\sim F$ for all $i=1,\dots,n.$ --></li>
<li><strong>Estimator:</strong> <span class="math inline">\(\bar X=\frac{1}{n} \sum_{i=1}^n X_i\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span></li>
<li><strong>Inference Problem:</strong> Construct a confidence interval for <span class="math inline">\(\mu.\)</span></li>
</ul>
</section>
<section id="the-traditional-non-bootstrap-approach-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-traditional-non-bootstrap-approach-1">The traditional (non-bootstrap) approach</h4>
<p>Traditional, non-bootstrap approach for constructing a <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval:</p>
<ul>
<li>By the CLT: <span class="math inline">\(\bar X\overset{a}{\sim} \mathcal{N}(\mu,\frac{\sigma^2}{n})\)</span> for large <span class="math inline">\(n\)</span></li>
<li>Estimation of <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2\)</span></li>
<li>This implies: <span class="math inline">\(\sqrt{n}((\bar X -\mu)/S)\overset{a}{\sim} t_{n-1}\)</span>, and hence <span class="math display">\[
\begin{align*}
&amp;P\left(-t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\le \bar X -\mu\le t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\right)\approx 1-\alpha\\
\Rightarrow
&amp;P\left(\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\le \mu\le
      \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}
\right)\approx 1-\alpha
\end{align*}
\]</span></li>
<li><span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval: <span class="math display">\[
\left[\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}},
    \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\right]
\]</span></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This traditional construction relies on the assumption that <span class="math display">\[
\bar X \overset{a}{\sim}\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right).
\]</span> <span class="math inline">\(\bar X\)</span> is exactly normal distributed (also for small <span class="math inline">\(n\)</span>) if the random sample <span class="math inline">\(X_1,\dots,X_n\)</span> is i.i.d. normally distributed. If the underlying distribution is <strong>not normal</strong>, then this condition is <em>approximately</em> fulfilled if the sample size <span class="math inline">\(n\)</span> is sufficiently large (central limit theorem). In this case the constructed confidence interval is an <em>approximate</em> <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval.</p>
</div>
</div>
</section>
<section id="the-nonparametric-standard-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-nonparametric-standard-bootstrap-approach">The nonparametric (standard) bootstrap approach</h4>
<p>The bootstrap offers an alternative method for constructing approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals. We already know that the bootstrap is consistent in this situation.</p>
<p><strong>Construction of the nonparametric (standard) bootstrap confidence interval:</strong></p>
<ul>
<li>Draw <span class="math inline">\(m\)</span> bootstrap samples (e.g.&nbsp;<span class="math inline">\(m=10000\)</span>) and calculate the corresponding estimates <span class="math inline">\(\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m\)</span>.</li>
<li>Compute the empirical quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> from <span class="math inline">\(\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m\)</span></li>
<li>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) nonparametric bootstrap confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;<span>2.2</span></a>: <span class="math display">\[
\left[2\bar X-\hat t_{1-\frac{\alpha}{2}},
    2\bar X-\hat t_\frac{\alpha}{2}\right]
\]</span></li>
</ul>
</section>
</section>
</section>
<section id="pivot-statistics-and-the-bootstrap-t-method" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="pivot-statistics-and-the-bootstrap-t-method"><span class="header-section-number">2.4</span> Pivot statistics and the bootstrap-<span class="math inline">\(t\)</span> method</h2>
<p>In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-<span class="math inline">\(t\)</span> method (one also speaks of ‚Äústudentized bootstrap‚Äù). The construction relies on so-called pivotal statistics.</p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be an i.i.d. random sample and assume that the distribution of <span class="math inline">\(X\)</span> depends on an unknown parameter (or parameter vector) <span class="math inline">\(\theta\)</span>.</p>
<div id="def-pivotal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Asymptotically pivotal statistics) </strong></span><em>A statistic <span class="math display">\[
T_n\equiv T(X_1,\dots,X_n)
\]</span> is called (exact) <strong>pivotal</strong>, if the distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter. A statistic <span class="math inline">\(T_n\)</span> is called <strong>asymptotically pivotal</strong>, if the asymptotic distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter.</em></p>
</div>
<p>Exact pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an <em>asymptotically pivotal</em> statistic. Assume that an estimator <span class="math inline">\(\hat{\theta}\)</span> satisfies <span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta)\rightarrow_d\mathcal{N}(0,v^2),
\]</span> where <span class="math inline">\(v^2\)</span> denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator <span class="math display">\[
\hat v_n^2\equiv \hat v_n(X_1,\dots,X_n)^2
\]</span> of <span class="math inline">\(v\)</span> such that <span class="math display">\[
\hat v_n^2 \rightarrow_p v^2.
\]</span> Then, of course, also <span class="math inline">\(\hat v_n\rightarrow_p v\)</span>, and <span class="math display">\[
T_n:= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span> This means that <span class="math inline">\(T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}\)</span> is <em>asymptotically pivotal</em>.</p>
<section id="example-barx-is-asymptotically-pivotal" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-barx-is-asymptotically-pivotal">Example: <span class="math inline">\(\bar{X}\)</span> is asymptotically pivotal</h4>
<p>Let <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> be a i.i.d. random sample with <span class="math inline">\(X_i\sim X\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> with mean <span class="math inline">\(E(X)=\mu\)</span>, variance <span class="math inline">\(Var(X)=\sigma^2&gt;0\)</span>, and <span class="math inline">\(E(|X|^4)=\beta&lt;\infty\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is normally distributed, we obtain <span class="math display">\[
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{S}\sim t_{n-1}
\]</span> with <span class="math inline">\(S^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2\)</span>, where <span class="math inline">\(t_{n-1}\)</span> denotes the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. We can conclude that <span class="math inline">\(T_n\)</span> is pivotal.</p>
<p>If <span class="math inline">\(X\)</span> is <em>not</em> normally distributed, the central limit theorem implies that <span class="math display">\[
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{S}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span> In this case <span class="math inline">\(T_n\)</span> is an asymptotically pivotal statistics.</p>
</section>
<section id="bootstrap-t-consistency" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-t-consistency">Bootstrap-<span class="math inline">\(t\)</span> consistency</h4>
<p>The general idea of the bootstrap-<span class="math inline">\(t\)</span> method relies on approximating the unknown distribution of <span class="math display">\[
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
\]</span> by the approximable (via bootstrap resampling) conditional distribution of <span class="math display">\[
T_n^*=\sqrt{n}\frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*},
\]</span> given <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> where the variance estimate <span class="math display">\[
\hat v_n^*=v_n(X_1^*,\dots,X_n^*)
\]</span> is computed from the bootstrap sample <span class="math inline">\(X_1^*,\dots,X_n^*.\)</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bootstrap-<span class="math inline">\(t\)</span> consistency follows if the standard nonparametric bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the standard nonparametric bootstrap is consistent, i.e.&nbsp;if the conditional distribution of <span class="math inline">\(\sqrt{n}(\hat{\theta}^*-\hat{\theta})|\mathcal{S}_n\)</span>, given <span class="math inline">\(\mathcal{S}_n\)</span>, yields a consistent estimate of <span class="math inline">\(\mathcal{N}(0,v^2)\)</span>, then also the bootstrap-<span class="math inline">\(t\)</span> method is consistent. That is, then the conditional distribution of <span class="math inline">\(T_n^*|\mathcal{S}_n\)</span>, given <span class="math inline">\(\mathcal{S}_n\)</span>, provides a consistent estimate of the asymptotic distribution of <span class="math inline">\(T_n\rightarrow_d \mathcal{N}(0,1)\)</span> such that <span class="math display">\[
\sup_\delta \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0,
\]</span> where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution.</p>
</div>
</div>
</section>
<section id="bootstrap-t-confidence-interval" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="bootstrap-t-confidence-interval"><span class="header-section-number">2.4.1</span> Bootstrap-t confidence interval</h3>
<p>Let <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> be a i.i.d. random sample and let the distribution <span class="math inline">\(X_i\sim F\)</span>, <span class="math inline">\(i=1,\dots,n,\)</span> depend on the unknown parameter (vector) <span class="math inline">\(\theta\)</span>. Assume that bootstrap is consistent and that the estimator <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is asymptotically normal. Furthermore, suppose that a <strong>consistent</strong> estimator <span class="math display">\[
\hat v\equiv \hat v(X_1,\dots,X_n)
\]</span> of the asymptotic standard deviation <span class="math inline">\(v\)</span> is available.</p>
<p><strong>Derivation of the bootstrap-<span class="math inline">\(t\)</span> confidence interval:</strong></p>
<ul>
<li><p>Based on an i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> calculate the bootstrap estimates <span class="math inline">\(\hat{\theta}^*\)</span> and <span class="math inline">\(v^*\)</span> and the bootstrap statistic <span class="math inline">\(T^*=\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}.\)</span> Repeating this yields <span class="math inline">\(m\)</span> many bootstrap statistics <span class="math display">\[
T_1^*,T_2^*, \dots, T_m^*
\]</span> which allow us to approximate the bootstrap distribution of of <span class="math inline">\(T^*\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}.\)</span> arbitrarily precise as <span class="math inline">\(m\to\infty.\)</span></p></li>
<li><p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2}}\)</span> of the bootstrap distribution of <span class="math inline">\(T^*=\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}\)</span> using the empirical quantiles based on <span class="math inline">\(T_1^*,T_2^*, \dots, T_m^*.\)</span> (see <a href="#eq-empiricalQuantile">Equation&nbsp;<span>2.1</span></a>)</p></li>
</ul>
<p>This implies <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat \tau_\frac{\alpha}{2}\leq {\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha
\end{align*}
\]</span> Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> that <span class="math display">\[
{\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}}|\mathcal{S}_n\overset{d}{\approx}
{\color{blue}\frac{\hat{\theta}-\theta}{\hat v}}
\]</span> Therefore, for lage <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
&amp; P\left(\hat \tau_\frac{\alpha}{2}\leq {\color{blue}\frac{\hat{\theta}-\theta}{\hat v}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P\left(-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta-\hat{\theta} \leq -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P\left(\hat{\theta}-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta \leq \hat{\theta} -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha
\end{align*}
\]</span> Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) bootstrap-<span class="math inline">\(t\)</span> confidence interval is given by <span id="eq-Boot_tCI"><span class="math display">\[
\left[\hat{\theta}-\hat \tau_{1-\frac{\alpha}{2}}\hat v,
      \hat{\theta}-\hat \tau_{  \frac{\alpha}{2}}\hat v\right]
\tag{2.3}\]</span></span></p>
<section id="example-bootstrap-t-confidence-interval-for-the-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-bootstrap-t-confidence-interval-for-the-mean">Example: Bootstrap-<span class="math inline">\(t\)</span> confidence interval for the mean</h4>
<p><strong>Algorithm:</strong></p>
<ul>
<li>Draw i.i.d. random samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\({\cal S}_n\)</span> and calculate <span class="math inline">\(\bar X^*\)</span> as well as <span class="math inline">\(S^{*2}=\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*)^2\)</span>.</li>
<li>Determine <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2}}\)</span> of the bootstrap distribution of <span class="math inline">\(\frac{\bar X^*-\bar X}{S^*}\)</span></li>
<li>This yields the <span class="math inline">\(1-\alpha\)</span> confidence interval (using <a href="#eq-Boot_tCI">Equation&nbsp;<span>2.3</span></a>): <span class="math display">\[
\left[\bar X-\hat \tau_{1-\frac{\alpha}{2}}S,
    \bar X-\hat \tau_{\frac{\alpha}{2}}S\right]
\]</span></li>
</ul>
</section>
</section>
<section id="accuracy-of-the-bootstrap-t-method" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="accuracy-of-the-bootstrap-t-method"><span class="header-section-number">2.4.2</span> Accuracy of the Bootstrap-<span class="math inline">\(t\)</span> method</h3>
<p>Usually, the bootstrap-<span class="math inline">\(t\)</span> provides a <strong>gain in accuracy</strong> over the standard nonparametric bootstrap. The reason is that the approximation of the law of <span class="math inline">\(T_n\)</span> by the bootstrap law of <span class="math inline">\(T_n^*=\sqrt{n}(\hat{\theta}^*-\hat{\theta})/v^*_n\)</span> is more direct and hence more accurate (also <span class="math inline">\(v^*n\)</span> depends on the bootstrap sample) than the approximation of the law of <span class="math inline">\(\sqrt{n}(\hat{\theta}-\theta)\)</span> by the bootstrap law of <span class="math inline">\(\sqrt{n}(\hat{\theta}^*-\hat{\theta}).\)</span></p>
<p>The use of pivotal statistics and the corresponding construction of bootstrap-<span class="math inline">\(t\)</span> confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-<span class="math inline">\(t\)</span> methods are <strong>second order accurate</strong>.</p>
<p>Consider generally <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals of the form <span class="math inline">\([L_n,U_n]\)</span> of <span class="math inline">\(\theta\)</span>. The lower, <span class="math inline">\(L_n\)</span>, and upper bounds, <span class="math inline">\(U_n\)</span>, of such intervals are determined from the data and are thus random, <span class="math display">\[
L_n\equiv L(X_1,\dots,X_n)
\]</span> <span class="math display">\[
U_n\equiv U(X_1,\dots,X_n)
\]</span> and their accuracy depends on the particular procedure applied (e.g.&nbsp;nonparametric bootstrap vs.&nbsp;bootstrap-<span class="math inline">\(t\)</span>).</p>
<ul>
<li>(Symmetric) confidence intervals are said to be <strong>first-order accurate</strong> if there exist some constants <span class="math inline">\(c_1,c_2&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta&lt;L_n)-\frac{\alpha}{2}\right|\le \frac{c_1}{\sqrt{n}}\\
\left|P(\theta&gt;U_n)-\frac{\alpha}{2}\right|\le \frac{c_2}{\sqrt{n}}
\end{align*}
\]</span></li>
<li>(Symmetric) confidence intervals are said to be <strong>second-order accurate</strong> if there exist some constants <span class="math inline">\(c_3,c_4&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta&lt;L_n)-\frac{\alpha}{2}\right|\le \frac{c_3}{n}\\
\left|P(\theta&gt;U_n)-\frac{\alpha}{2}\right|\le \frac{c_4}{n}
\end{align*}
\]</span></li>
</ul>
<p>If the distribution of <span class="math inline">\(\hat\theta\)</span> is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that</p>
<ul>
<li>Standard confidence intervals based on asymptotic approximations are <strong>first-order</strong> accurate.</li>
<li>Nonparametric (standard) boostrap confidence intervals are <strong>first-order</strong> accurate.</li>
<li>Bootstrap-<span class="math inline">\(t\)</span> confidence intervals are <strong>second-order</strong> accurate.</li>
</ul>
<p>The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to <em>much</em> better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.</p>
</section>
</section>
<section id="regression-analysis-bootstrapping-pairs" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="regression-analysis-bootstrapping-pairs"><span class="header-section-number">2.5</span> Regression Analysis: Bootstrapping pairs</h2>
<p>Consider the linear regression model <span class="math display">\[
Y_i=X_i^T\beta+ \varepsilon_i,\quad  i=1,\dots,n,
\]</span> where <span class="math inline">\(Y_i\in\mathbb{R}\)</span> denotes the response (or ‚Äúdependent‚Äù) variable and <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
\]</span> denotes the vector of predictor variables. In the following, we differentiate between a <strong>random design</strong> and a <strong>fixed design</strong>.</p>
<div id="def-RandomFixedDesign" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Random and fixed design) </strong></span><br></p>
<p><em><strong>Random design:</strong> <span class="math display">\[
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
\]</span> are i.i.d. random variables and <span class="math inline">\(E(\varepsilon_i|X_i)=0\)</span> with either</em></p>
<ul>
<li><em><strong>homoscedastic</strong> errors: <span class="math inline">\(E(\varepsilon_i^2|X_i)=\sigma^2\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, for a fixed <span class="math inline">\(\sigma^2&lt;\infty\)</span> or</em></li>
<li><em><strong>heteroscedastic</strong> errors: <span class="math inline">\(E(\varepsilon_i^2|X_i)=\sigma^2(X_i)&lt;\infty\)</span>, <span class="math inline">\(i=1,\dots,n.\)</span></em></li>
</ul>
<p><em><strong>Fixed design:</strong> <span class="math display">\[
X_1, X_2, \dots, X_n
\]</span> are deterministic vectors in <span class="math inline">\(\mathbb{R}^p\)</span> and <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. random variables with zero mean <span class="math inline">\(E(\varepsilon_i)=0\)</span> and <strong>homoscedastic errors</strong> <span class="math inline">\(E(\varepsilon_i^2)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></em></p>
</div>
<p>We additionally assume that there exists a positive definite (thus invertible) matrix <span class="math inline">\(M\)</span> <span class="math display">\[
M=E(X_iX_i^T)
\]</span> and a positive semi-definite matrix <span class="math inline">\(Q\)</span> such that <span class="math display">\[
Q=E(\varepsilon_i^2X_iX_i^T)=E(\sigma^2(X_i)X_iX_i^T)
\]</span> <strong>Note:</strong> For homoscedastic errors we have <span class="math inline">\(Q=\sigma^2 M.\)</span></p>
<p>The least squares estimator <span class="math inline">\(\hat\beta\in\mathbb{R}^p\)</span> is given by <span class="math display">\[
\begin{align*}
\hat\beta
&amp;=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i\\
&amp;=\beta+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i.
\end{align*}
\]</span></p>
<p>The law of large numbers, the continuous mapping theorem, Slutsky‚Äôs theorem, and the central limit theorem (see your econometrics lecture) implies that <span class="math display">\[
\sqrt{n}(\hat\beta-\beta)\rightarrow_d\mathcal{N}(0,M^{-1}QM^{-1}),\quad n\to\infty.
\]</span></p>
<section id="bootstrapping-pairs-bootstrap-under-random-design" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="bootstrapping-pairs-bootstrap-under-random-design"><span class="header-section-number">2.5.1</span> Bootstrapping pairs: Bootstrap under random design</h3>
<p>Bootstrapping regression estimates <span class="math inline">\(\hat\beta\)</span> is straightforward under a <strong>random design</strong> (<a href="#def-RandomFixedDesign">Definition&nbsp;<span>2.4</span></a>). Assuming a random design, <span class="math inline">\((Y_i,X_i)\)</span> are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of <span class="math display">\[
\hat\beta-\beta.
\]</span> In the literature this procedure is usually called <strong>bootstrapping pairs</strong>, namely, <span class="math inline">\((Y_i, X_i)\)</span>-pairs.</p>
<ul>
<li>Original data: i.i.d. sample <span class="math inline">\({\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}\)</span></li>
<li>Generate bootstrap samples <span class="math display">\[
(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
\]</span> by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n.\)</span></li>
<li>Each bootstrap sample <span class="math inline">\((Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)\)</span> leads to a bootstrap realization of the least squares estimator <span class="math display">\[
\hat\beta^*=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
\]</span></li>
</ul>
<p>It can be shown that bootstrapping pairs is <strong>consistent</strong>; i.e.&nbsp;that for large <span class="math inline">\(n\)</span> <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)\approx\mathcal{N}(0,M^{-1}QM^{-1})
\]</span></p>
<p>This allows to construct basic bootstrap confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>:</p>
<ul>
<li><p>Determine the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2},j}\)</span><br>
of the conditional distribution of <span class="math inline">\(\hat\beta_j^*\)</span> given <span class="math inline">\({\cal S}_n.\)</span></p></li>
<li><p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;<span>2.2</span></a>: <span class="math display">\[
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j},
    2\hat\beta_j-\hat t_{\frac{\alpha}{2},j}\right]
\]</span></p></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are <strong>heteroscedastic</strong>. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages.</p>
</div>
</div>
</section>
</section>
<section id="regression-analysis-residual-bootstrap" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="regression-analysis-residual-bootstrap"><span class="header-section-number">2.6</span> Regression Analysis: Residual bootstrap</h2>
<p>If the sample <span class="math display">\[
(Y_1,X_1),\dots,(Y_n,X_n)
\]</span> is <strong>not</strong> an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for <strong>fixed designs</strong> and also generally not in time-series regression contexts. However, if error terms are <strong>homoscedastic</strong>, then it is possible to rely on the <strong>residual bootstrap</strong>.</p>
<p>In the following we will formally assume a regression model <span class="math display">\[
Y_i=X_i^T\beta+ \varepsilon_i, \quad i=1,\dots,n,
\]</span> under <strong>fixed design</strong> (<a href="#def-RandomFixedDesign">Definition&nbsp;<span>2.4</span></a>), i.e., where <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. with zero mean <span class="math inline">\(E(\varepsilon_i)=0\)</span> and <strong>homoscedastic</strong> errors <span class="math display">\[
E(\varepsilon_i^2)=\sigma^2.
\]</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated <span class="math inline">\(X\)</span>-variables (time-series). In these cases all arguments are meant conditionally on the given <span class="math inline">\(X_1,\dots,X_n\)</span>. The above assumptions on the error terms then of course have to be satisfied conditionally on <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
</div>
</div>
<p>The idea of the residual bootstrap is very simple: The model implies that the error terms <span class="math display">\[
\varepsilon_1,\dots,\varepsilon_n
\]</span> are i.i.d which suggests a bootstrap based on resampling the error terms.</p>
<p>These errors are, of course, unobserved, but they can be approximated by the corresponding residuals <span class="math display">\[
\hat \varepsilon_i:=Y_i-X_i^T\hat\beta, \quad i=1,\dots,n,
\]</span> where again <span class="math display">\[
\hat\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
\]</span> denotes the least squares estimator.</p>
<p>It is well known that <span class="math display">\[
\hat\sigma^2:= \frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^2
\]</span> provides an unbiased, consistent estimator of the error variance <span class="math inline">\(\sigma^2\)</span>. That is, <span class="math display">\[
E(\hat\sigma^2)=\sigma^2 \quad \text{and}\quad \hat\sigma^2\rightarrow_p \sigma^2.
\]</span></p>
<section id="the-residual-bootstrap-algorithm" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-residual-bootstrap-algorithm">The residual bootstrap algorithm</h4>
<p>Based on the original data <span class="math inline">\((Y_i,X_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, and the least squares estimate <span class="math inline">\(\hat\beta\)</span>, calculate the residuals <span class="math inline">\(\hat\varepsilon_1,\dots,\hat \varepsilon_n\)</span>.</p>
<ol type="1">
<li>Generate random bootstrap samples <span class="math inline">\(\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*\)</span> of residuals by drawing observations independently and with replacement from <span class="math display">\[
{\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}.
\]</span></li>
<li>Calculate new depend variables <span class="math display">\[
Y_i^*=X_i^T\hat\beta+\hat\varepsilon_i^*,\quad i=1,\dots,n
\]</span></li>
<li>Bootstrap estimators <span class="math inline">\(\hat\beta^*\)</span> are determined by least squares estimation from the data <span class="math inline">\((Y_1^*,X_1),\dots,(Y_n^*,X_n)\)</span>: <span class="math display">\[
\hat\beta^*=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
\]</span></li>
</ol>
<p>Repeating Steps 1-3 <span class="math inline">\(m\)</span> many times yields <span class="math inline">\(m\)</span> bootstrap estimators <span class="math display">\[
\hat\beta^*_1,\hat\beta^*_2,\dots,\hat\beta^*_m
\]</span> which allow us to approximate the bootstrap distribution <span class="math inline">\(\hat\beta^*-\hat\beta|\mathcal{S}_n.\)</span></p>
</section>
<section id="motivating-the-residual-bootstrap" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="motivating-the-residual-bootstrap">Motivating the residual bootstrap</h4>
<p>It is not difficult to understand why the residual bootstrap generally works for <em>homoscedastic</em> (!) errors. We have <span class="math display">\[
\hat\beta-\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
\]</span> and for large <span class="math inline">\(n\)</span> the distribution of <span class="math inline">\(\sqrt{n}(\hat\beta-\beta)\)</span> is approximately normal with mean 0 and covariance matrix <span class="math inline">\(\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\)</span> <span class="math display">\[
\sqrt{n}(\hat\beta-\beta)\to_d\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)
\]</span></p>
<p>On the other hand (the bootstrap world), we have construction <span class="math display">\[
\hat\beta^*-\hat\beta
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*
\]</span> Conditional on <span class="math inline">\({\cal S}_n,\)</span> the bootstrap error terms are i.i.d with <span class="math display">\[
E(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
\]</span> and <span class="math display">\[
Var(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2
\]</span> Thus, by the central limit (<a href="https://www.statlect.com/asymptotic-theory/central-limit-theorem">Lindeberg-L√©vy</a>) one obtains that <span class="math display">\[
\sqrt{n}(\hat\beta^*-\hat\beta)\to_d\mathcal{N}\left(0,\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 (\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\right),
\]</span> for <span class="math inline">\(n\to\infty.\)</span></p>
<p>Since <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2\rightarrow_P \sigma^2\)</span> as <span class="math inline">\(n\rightarrow\infty\)</span>, the bootstrap is consistent. That is, for large <span class="math inline">\(n\)</span>, we have approximately <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)
\approx\underbrace{\text{distribution}(\sqrt{n}(\hat\beta-\beta))}_{\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)}
\]</span></p>
</section>
<section id="bootstrap-confidence-intervals-for-the-regression-coefficients" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="bootstrap-confidence-intervals-for-the-regression-coefficients"><span class="header-section-number">2.6.1</span> Bootstrap confidence intervals for the regression coefficients</h3>
<section id="nonparametric-bootstrap-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="nonparametric-bootstrap-confidence-intervals">Nonparametric bootstrap confidence intervals</h4>
<p>Basic <strong>nonparametric bootstrap</strong> confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2},j}\)</span> of the conditional distribution of <span class="math inline">\(\hat\beta_j^*|\mathcal{S}_n\)</span> using the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles based on the <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\beta_1^*,\hat\beta_2^*, \dots, \hat\beta_m^*.
\]</span> <!-- This approximation step is with arbitrary accuracy as $m\to\infty.$ --></p>
<p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) nonparametric bootstrap confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;<span>2.2</span></a>: <span class="math display">\[
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j},
      2\hat\beta_j-\hat t_{ \frac{\alpha}{2},j }\right]
\]</span></p>
</section>
<section id="bootstrap-t-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-t-confidence-intervals">Bootstrap-<span class="math inline">\(t\)</span> confidence intervals</h4>
<p><strong>Bootstrap-<span class="math inline">\(t\)</span></strong> confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Let <span class="math inline">\(\gamma_{jj}\)</span> denote the <span class="math inline">\(j\)</span>-th diagonal element of the matrix <span class="math inline">\((\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\)</span>, <span class="math display">\[
\gamma_{jj}:=\left[(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\right]_{jj}.
\]</span> Then <span class="math display">\[
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}
\]</span> is an asymptotically pivotal statistics, since <span class="math display">\[
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>A bootstrap-<span class="math inline">\(t\)</span> interval for <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>, can thus be constructed as follows:</p>
<p>Additionally compute <span class="math display">\[
\hat\sigma^{*2}:=\frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^{*2},
\]</span> and approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2},j}\)</span> of the bootstrap distribution of <span class="math display">\[
T^*=\frac{\hat\beta_j^*-\hat\beta_j}{\hat\sigma^* \sqrt{\gamma_{jj}}}
\]</span> using the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles based on the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math display">\[
T^*_1,T_2^*,\dots, T_m^*.
\]</span></p>
<p>Compute the <span class="math inline">\((1-\alpha)\times 100\%\)</span> bootstrap-<span class="math inline">\(t\)</span> confidence interval as in <a href="#eq-Boot_tCI">Equation&nbsp;<span>2.3</span></a>: <span class="math display">\[
\left[
  \hat\beta_j-\hat \tau_{1-\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}},
  \hat\beta_j-\hat \tau_{\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}}
\right]
\]</span></p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch1_Random_Variable_Generation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Random Variable Generation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch3_MaximumLikelihood.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>