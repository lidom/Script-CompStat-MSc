<!-- LTeX: language=en-US -->
# Nonparametric Density Estimation

## Introduction

```{r}
#| echo: false
data_path <- "81b9ff859175053edfc3c2eb024a87b782aa47fa/data/"
data_path <- "data/"
# Libraries
suppressPackageStartupMessages(library("ggplot2"))    # Plotting 
suppressPackageStartupMessages(library("np"))         # Nonparametric Statistics
suppressPackageStartupMessages(library("KernSmooth")) #
suppressPackageStartupMessages(library("gridExtra"))  # To arrange multiple plots 
suppressPackageStartupMessages(library("gtable")) 
suppressPackageStartupMessages(library("scales"))     # Transparent colors 
```


### Example: Income Data Analysis


```{r}
#| echo: false
inc76 <- read.table(paste0(data_path,"inc76.txt"), col.names="income")
```

```{r}
c(inc76$income)[1:6]
```

Typical aim: Characterizing the income distribution (density function) $f$ given a random sample 
$$
\{X_1,\dots,X_n\}
$$
with 
$$
X_i\overset{\text{i.i.d.}}{\sim} f
$$


Traditional statistical key figures:

* Empirical mean 
* Empirical median
* Empirical variance 
* Empirical interquartile distance, etc. 

Only summarize single aspects of a distribution. 

Estimating the total density function $f$ can provide more detailed, more wholistic information. 


@fig-HistOrdinary shows a histogram (i.e., a very simple density estimator) of the income data.

```{r}
#| echo: true
#| fig-cap: Histrogram of the income data.
#| label: fig-HistOrdinary
hist(inc76$income, 
     freq = FALSE,
     xlab = "Income", 
     ylab = "Density", main = "")
```


Disadvantages of the histogram: 

* Choice of the bin width (and of the start point)
* Discontinuous, locally constant $\Rightarrow$ A histogram cannot be a very efficient estimator for a (continuous) density function $f(x).$


The choice of the bin width is crucial. @fig-HistBinWidths shows examples for a too small and a too large bin width choice.

```{r}
#| echo: false
#| fig-cap: Histrograms of income data for different choices of the bin width. 
#| label: fig-HistBinWidths
par(mfrow = c(1,2))
hist(inc76$income, 
     freq = FALSE,
     breaks = 25,
     xlab = "Income", 
     ylab = "Density", main = "")
hist(inc76$income, 
     freq = FALSE,
     breaks = 4,
     xlab = "Income", 
     ylab = "Density", main = "")
par(mfrow = c(1,1))
```


@fig-HistKDE shows a comparison of the histogram versus the density estimate from a kernel density estimator. 

```{r}
#| echo: false
#| fig-cap: Histogram versus a nonparametric kernel density estimation (green solid line).
#| label: fig-HistKDE
hist(inc76$income, 
     freq = FALSE,
     xlab = "Income", 
     ylab = "Density", main = "")
lines(density(inc76$income), col = "darkgreen")
```


Advantages of the kernel density estimator: 

* Choice of the bandwidth (similar to bin width) can be done using statistical theory
* Continuous estimate for a (continuous) density function $f.$


### From the Histogram to the Kernel Density Estimator


Consider a histogram with $J$ (e.g., $J=8$ as in @fig-HistKDE) bins all having the same bin-width $2h,$ defined by equidistant intervals
$$
(x_{j-1},x_j]
$$ 
with 
$$
x_j-x_{j-1}=2h\quad \text{for all} \quad j=1,\dots,J.
$$
The bin-height is determined **locally** at the $j$th interval mid-point 
$$
\bar{x}_j=(x_{j-1}+x_j)/2
$$
by the relative frequency of data points $X_1,\dots,X_n$ that fall within the $j$th interval $(x_{j-1},x_j],$


$$ 
\begin{align*}
\hat f_{hist}(\bar{x}_j)
& =\frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{2hn}
\end{align*}
$${#eq-fHist}
**Note:** The scaling by $2hn$ is necessary to guarantee that the area of each bin equals the relative frquency of data points $X_1,\dots,X_n$ that fall into the interval $(x_{j-1},x_j],$
$$
\begin{align*}
\text{Area of $j$th Bin} 
& = (\text{bin-width}) \cdot (\text{bin-height of $j$th bin})\\[2ex]
& = \qquad 2h\quad\, \cdot \frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{2hn}\\[2ex]
& = \frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{n}\\[2ex]
& = \frac{1}{n}\sum_{i=1}^n \; 1_{(X_i\in(x_{j-1},x_j])}, 
\end{align*}
$$
where $1_{(\cdot)}$ denotes the indicator function, i.e., 
$1_{(\text{TRUE})}=1$ and $1_{(\text{FALSE})}=0.$ Thus, the scaling guarantees that the bin areas of the histogram sum up to one
$$
\begin{align*}
\sum_{j=1}^J\text{Area of $j$th Bin}
&=\sum_{j=1}^J \frac{1}{n}\sum_{i=1}^n \; 1_{(X_i\in(x_{j-1},x_j])}\\[2ex]
&=\frac{1}{n} \sum_{j=1}^J \sum_{i=1}^n \; 1_{(X_i\in(x_{j-1},x_j])}\\[2ex]
&=\frac{n}{n}=1.
\end{align*}
$$


Doing some rearrangements of @eq-fHist, allows writing the histogram as a type of kernel density estimator:  
$$ 
\begin{align*}
\hat f_{hist}(\bar{x}_j)
& =\frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{2hn}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(x-h < X_i\leq x+h\right)}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(-1<\left(\frac{X_i-x}{h}\right)\leq 1\right)}\\[2ex]
& =\frac{1}{nh}\sum_{i=1}^n K\left(\frac{X_{i}-\bar{x}_j}{h}\right),
\end{align*}
$$
where $K(z)$ denotes the symmetric $(K(z)=K(-z))$ kernel function
$$
K(z)=\left\{
\begin{array}{ll}
1/2 & \hbox{ if } z\in (-1,1] \\
0   & \hbox{ else}.
\end{array}\right.
$$


A kernel density estimator generalizes the histogram $\hat f_{hist}(\bar{x}_j)$ by estimating the unknown density $f$ not only at the mid-points $\bar{x}_j,$ but at *every* $x,$ which yields the **moving histogram**
$$
\begin{align*}
{\hat{f}}_{nh}(x)=\frac{1}{nh}\sum_{i=1}^nK\left(
\frac{x-X_{i}}{h}\right)
\end{align*}
$${#eq-MovHist}
where 

* $K$ denotes the kernel function and  
* $h>0$ the bandwidth  


## A More Formal Motivation of the Kernel Density Estimator


Let 
$$
\{X_1, \ldots, X_n\}
$$
denote a random sample with 
$$
X_i\overset{\text{i.i.d.}}{\sim} f\quad\text{for}\quad i=1,\dots,n.
$$


**Aim:** Find a density estimator 
$$
\hat{f}
$$ 
for the true, unknown density $f$ **without** making a parametric assumption on $f$ such as assuming that $f$ is the density of a normal distribution with unknown mean and unknown variance. 


The only assumption on $f$ is the following **qualitative smoothness assumption:** The density function $f(x)$ is assumed to be sufficiently smooth---i.e., to be sufficiently often differentiable for all $x.$  


**Starting point:** We use the connection between density functions $f$ and distribution functions $F(x) = P(X \leq x);$ namely,  
$$
\begin{equation*}
  f(x) = \frac{d}{dx} F(x) = F'(x), \qquad x \in \mathbb{R}.
\end{equation*}
$$


**Idea:** Approximate the derivative of the distribution function using the difference quotient. For small $h > 0,$ we have that 
$$
\begin{align*}
f(x) =   F'(x) &\approx \frac{F(x+h) - F(x)}{h}%\\[2ex]
%  F'(x) &= \frac{F(x+h) - F(x)}{h} + O(h)
\end{align*}
$${#eq-DiffQ1}
or
$$
\begin{align*}
f(x) = F'(x) &\approx \frac{F(x) - F(x-h)}{h}.%\\[2ex]
%F'(x)      &= \frac{F(x) - F(x-h)}{h} + O(h),
\end{align*}
$${#eq-DiffQ2}
<!-- where the "Big-Oh" term $O(h)$ refers to a $h\to 0$ with $h>0$ asymptotic.  -->


In both the right-hand (@eq-DiffQ1) and the left-hand difference quotient (@eq-DiffQ2) have an approximation error of the order $O(h)$ for $h\to 0$ with $h>0.$ 

<!-- 
$$
\left|F'(x)-\frac{F(x) - F(x-h)}{h}\right|\to 0\quad\text{as}\quad h\to 0 
$$
such that
$$
\frac{\left|F'(x)-\frac{F(x) - F(x-h)}{h}\right|}{h}\to c,
$$
where $0\leq c < \infty.$
 -->


In his seminal work on the kernel density estimator, @Rosenblatt_1956 uses an even better derivative approximation based on the symmetric difference quotient
$$
\begin{equation*}
    f(x) = F'(x) = \frac{F(x+h) - F(x-h)}{2h} + O(h^2),
\end{equation*}
$${#eq-DiffQ3}
where the approximation error is of the order $O(h^2)$ for $h\to 0$ with $h>0.$ This means that the approximation error goes, in absolute values, to zero as fast as $h^2\to 0$ *or faster*; i.e.
$$
\begin{align*}
& O\big(h^2\big)=\\[2ex]
=&\left\{\text{Any function}\;g(h)\text{ such that }\frac{|g(h)|}{h^2}\to c\;\text{ as }\;h\to 0\text{, where }0\leq c<\infty\right\}.
\end{align*}
$$

::: {.callout-note collapse="true"}
## Deriving the approximation error in @eq-DiffQ3 
Let $F$ be three times continuously differentiable; i.e. $F^{(3)}(x)$ is a continuous function for all $x.$ 
$$
\begin{align*}
&\frac{F(x+h) - F(x-h)}{2h}=\\[2ex]
&[\text{Taylor polynomial approximations for $F(x+h)$ and $F(x-h)$ around $x\colon$}]\\[2ex]
=&\frac{\left(F(x)+F^{(1)}(x)h + \frac{1}{2}\,F^{(2)}(x) h^2 +\frac{1}{6}\,F^{(3)}(x)h^3 + o(h^3)\right)}{2h}-\\[2ex]
&\frac{\left(F(x)-F^{(1)}(x)h+ \frac{1}{2}\,F^{(2)}(x) h^2-\frac{1}{6}\,F^{(3)}(x)h^3+o(h^3)\right)}{2h}\\[2ex]
&[\text{Applying straight forward simplifications:}]\\[2ex]
=&\frac{2\,F^{(1)}(x)h+\frac{2}{6}\,F^{(3)}(x)h^3+o(h^3)}{2h}\\[2ex]
=&\frac{2\,F^{(1)}(x)h}{2h}+\frac{\frac{1}{3}\,F^{(3)}(x)h^3}{2h}+\frac{o(h^3)}{2h}\\[2ex]
&[\text{Using that $h^{-1}o(h^3)=o(h^2)\colon$}]\\[2ex]
=&F'(x)
+\frac{1}{6}\,F^{(3)}(x)h^2
+\frac{1}{2}\,o(h^2)\\[2ex]
&[\text{Using that $\texttt{constant}\times h^2=O(h^2)$ and that $\texttt{constant}\times o(h^2)=o(h^2)\colon$}]\\[2ex]
=&F'(x)+O(h^2) + o(h^2)\\[2ex]
&[\text{Using that $o(h^2)=O(h^2)$ and that $2O(h^2)=O(h^2)\colon$}]\\[2ex]
=&F'(x)+O(h^2)
\end{align*}
$$
:::

Using the definition of the distribution function $F(x)=P(X\leq x)$ we get
$$
\begin{align*}
    f(x) & = \frac{F(x+h) - F(x-h)}{2h} + O(h^2) \\[2ex]
    & = \frac{P(X \leq x+h) - P(X \leq x-h)}{2h} + O(h^2)\\[2ex]
    & =\frac{1}{2h} P(x-h < X \leq x+h) + O(h^2)\\[2ex]
\Rightarrow\quad        f(x) & \approx \frac{1}{2h} P(x-h < X \leq x+h) ,
\end{align*}
$$
where the approximation is very good for smallish $h>0.$

Estimating the unknown probability $P(x-h < X \leq x+h)$ by its empirical counterpart (relative frequency) yields the **moving histogram** estimator as in @eq-MovHist:
$$
\begin{align*}
\hat{f}_{nh}(x) 
&=\frac{1}{2h}\hat{P}\left(x-h < X \leq x+h\right)\\[2ex]
&=\frac{1}{2h}\frac{\text{Number of $X_i$ in $(x-h, x+h]$}}{n}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(x-h < X_i\leq x+h\right)}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(-1<\left(\frac{X_i-x}{h}\right)\leq 1\right)}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_{i}}{h}\right)
\end{align*}
$$
where $K(z)$ denotes the symmetric $(K(z)=K(-z))$ kernel function
$$
K(z)=\left\{
\begin{array}{ll}
1/2 & \hbox{ if } z\in (-1,1] \\
0   & \hbox{ else}.
\end{array}\right.
$$


```{r}
my_movhist <- function(x, data, h){
      n      <- length(data)
      dat_x  <- data[data > x-h & data <= x+h]
      result <- length(dat_x)/(n*2*h)
      return(result)
}
my_movhist <- Vectorize(my_movhist, "x")
```

```{r}
#| fig-cap: Income density estimate based on the moving histogram density estimator.
#| label: fig-MovHistInc
data  <- inc76$income
h     <- 12  # bandwidth
xx    <- seq(from=(min(data)), to=(max(data)), len=750)
yy    <- my_movhist(x = xx, data = data, h = h)                

plot(x = xx, y = yy, 
     ylab="Density", xlab="Income", 
     main = "Density Estimation using the Moving Histogram", type="l")
```

Density estimates of the moving histrogram estimator are discontinuous---even though, we aim to estimate a smooth (e.g. three times continuously differentiable) denstiy function $f.$

**Kernel Density Estimator:**

Replacing the symmetric, discontinuous rectengular kernel function of the naive moving histogram by a symmetric, continuous and differentiable function, such as

* $K(z) = \frac{3}{4}(1-z^2)$ for $z \in [-1,1]$ and $0$ else
* or the density function of the standard normal distribution:
$$
K(z) = \phi(z) = \frac{1}{\sqrt{2\pi}} \exp( -z^2 / 2 )
$$
yields to the **kernel density estimator** with kernel $K$ and bandwidth $h\colon$
$$
\begin{equation*}
  \hat{f}_{nh}(x)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{h} K\left(\frac{x - X_i}{h} \right),
  \quad\text{for}\quad x \in \mathbb{R}.
\end{equation*}
$$


The kernel density estimator yields a smooth density estimate. It is more efficient than the (moving) histrogram and provides a flexible adjustment to the data. 

* Bandwidth $h$ is also called **smoothing parameter**. The bandwidth (and the kernel $k$) need to be selected by the user (or autmatically by the computer). 
* New, compact notation for the scaled kernel function: $K_h(u) := K(u / h) / h$.
$$
\begin{align*}
\hat{f}_{nh}(x) 
&=  n^{-1} \sum_{i=1}^{n} \frac{1}{h}  K\left(\frac{x - X_i}{h}\right)\\[2ex]
&=  n^{-1} \sum_{i=1}^{n} K_h(x - X_i)
\end{align*}
$$


::: {.callout-note}
## Notation 

* $\hat{f}_{nh}:$ the index $nh$ means here that the estimator depends on the sample size $n$ and the bandwidth $h$. 
* $K_h:$ the index $h$ is here an abbreviation for $K( \cdot / h) / h$.
::::


## Properties of the Kernel Density Estimator

::: {.callout-tip}
## Reminder: Definition of a Density Function
We call a function $f$ a density function if it fulfilles the following properties: 

* Non-negative: $f(x)\geq 0$ for all $x\in\mathbb{R}$
* Normed: $\int f(x)dx = 1$
:::


It can be shown that $f_{nh}$ is a density function if the kernel function $K$ is 

* Non-negative: $K(x) \geq 0$ for all $x\in\mathbb{R}\quad \Rightarrow\quad \hat{f}_{nh}(x) \geq 0$  for all $x\in\mathbb{R}$
* Normed: $\int K(x)dx = 1\quad \Rightarrow\quad \int \hat{f}_{nh}(x)dx = 1$

That is, if $K$ is a density function, then also $f_{nh}$ is a density function. 

This inheritance of the properties of $K$ to the properties of $f_{nh}$ also holds for the smoothness properties: 

* Smoothness of $K$: If $K$ is continuously differentiable $\quad\Rightarrow\quad$ $\hat{f}_{nh}$ is continuously differentiable


### Theoretical Requirements on the Kernel Function

* **Normed:** 
$$
\int K(x)dx=1
$$
* **Symmetric around zero:** 
$$
K(x)=K(-x)\quad\text{such that}\quad \int xK(x)dx=0
$$ 


Typically, one uses smooth density functions that are symmetric around zero.  

**Examples:**


* The family of the **symmetric beta density** functions; for $p = 0,1,2,\ldots$
   $$
   K(u; p) = \mathrm{Const}_p \left(1 - u^2 \right)^p
   $$
   for $u \in [-1,1]$ and $0$ else, where the $p$-specific constant is chosen such that the kernel integrates to one. 
   * $p=0$ yields the **uniform kernel:** 
     $$
     K(u) = \frac{1}{2}
     $$
    * $p=1$ yields the (important) **Epanechnikov kernel:** 
     $$
     K(u) = \frac{3}{4}\,(1-u^2)
     $$
    * $p=2$ yields the **biweight kernel:** 
     $$
     K(u) = \frac{15}{16}\,(1-u^2)^2
     $$
    * $p=3$ yields the **triweight kernel:** 
     $$
     K(u) = \frac{35}{32}\,(1-u^2)^3
     $$
* **Normal (Gaussian) kernel:** 
  $$
  K(u) = \phi(u) = \frac{1}{\sqrt{2\pi}} \exp(-u^2 / 2)
  $$
  for $u \in\mathbb{R}.$
* Triangular kernel:
  $$
  K(u) = 1-|u|
  $$
  for $u\in[-1,1]$ and $0$ else.



### Choice of the Bandwidth Parameter


Using a too small bandwidth (here $h=$ `r round(bw.SJ(inc76$income) * .25,  2)`):


```{r}
#| fig-cap: Kernel density estimate with a too small bandwidth.
#| label: fig-KDEhsmall
data  <- inc76$income
h     <- bw.SJ(inc76$income) * .25  # too small bandwidth
KDE   <- density(data, bw = h, from = min(data), to = max(data))

plot(x = KDE$x, y = KDE$y, 
     ylab="Density", xlab="Income", 
     main = "Kernels Density Estimtion With a too Small Bandwidth", type="l")
```



Too a too large bandwidth (here $h=$ `r round(bw.SJ(inc76$income) * 5.5,  2)`):


```{r}
#| fig-cap: Kernel density estimate with a too large bandwidth.
#| label: fig-KDEhlarge
data  <- inc76$income
h     <- bw.SJ(inc76$income) * 2.5  # too large bandwidth
KDE   <- density(data, bw = h, from = min(data), to = max(data))

plot(x = KDE$x, y = KDE$y, 
     ylab="Density", xlab="Income", 
     main = "Kernels Density Estimtion With a Too Large Bandwidth", type="l")
```



<!-- Way too a too large bandwidth (here $h=$ `r round(bw.SJ(inc76$income) * 60,  2)`):


```{r}
#| fig-cap: Kernel density estimate with a way too large bandwidth.
#| label: fig-KDEhLARGE
data  <- inc76$income
h     <- bw.SJ(inc76$income) * 60  # way too large bandwidth
KDE   <- density(data, bw = h, from = min(data), to = max(data))

plot(x = KDE$x, y = KDE$y, 
     ylab="Density", xlab="Income", 
     main = "Kernels Density Estimtion With a VERY Large Bandwidth", type="l")
``` 
-->

Using an estimated bandwidth (here $h=$ `r round(bw.SJ(inc76$income),  2)`) based on the method proposed by @sheather1991reliable:


```{r}
#| fig-cap: Kernel density estimate with a good bandwidth choice.
#| label: fig-KDEhgood
data  <- inc76$income
h     <- bw.SJ(inc76$income) # good bandwidth
KDE   <- density(data, bw = h, from = min(data), to = max(data))

plot(x = KDE$x, y = KDE$y, 
     ylab="Density", xlab="Income", 
     main = "Kernels Density Estimtion With a Good Bandwidth Choice", type="l")
```



## Accuracy of the Kernel Density Estimator

The accuracy of kernel desnity estimator depends on 

1. the choice of the kernel function $K,$ 
2. the choice of the bandwidth $h,$ and 
3. the complexity of the unknown density $f$ to be estimated. 

The choice of the kernel function $K$ is (by far) less important/critical for the accuracy of the kernel density estimator than than the choice of the bandwidth $h.$

**Most common goal:** Automatic data-dependent bandwidth selection that is **"globally optimal"** ; i.e. optimal for all relevant $x$ in $f(x).$

We call a bandwidth choice method optimal, if it minimizes a loss-function, which quantifies the estimation errors. 

Commonly used loss-functions: 

* **Integrated Squared Error (ISE)**
  $$
  \mathrm{ISE}(\hat{f}_{nh}(x))=\int (\hat{f}_{nh}(x)-f(x))^2\,dx
  $$ 
  Caution: The $\mathrm{ISE}(\hat{f}_{nh}(x))$ is a random quantity.
* **Mean Integrated Squared Error (MISE):**
  $$
  \mathrm{MISE}(\hat{f}_{nh})=\int\mathrm{MSE}(\hat{f}_{nh}(x))\,dx=\int \mathbb{E}(\hat{f}_{nh}(x)-f(x))^2\,dx
  $$
* **Asymptotic Mean Integrated Squared Error (AMISE):** 
  $$
  \mathrm{AMISE}(\hat{f}_{nh}) = \mathrm{MISE}(\hat{f}_{nh}) + o_P(1)
  $$
  I.e. the asymptotic approximation of the $\mathrm{MISE}.$



The minimum requirement on $\hat{f}_{nh}$ is **consistency**. That is, the estimated density function $\hat{f}_{nh}$ should approach the true (unknown!) density $f$, under the hypothetical assumption that the sample size grows indefinitely ($n \to \infty$),


Remember: We'll call an estimator $\hat\theta_n\equiv\hat\theta(X_1,\dots,X_n)$ **consistent,** if 
$$
\hat\theta_n\to_p\theta.
$$ 
Unless a different notion of convergence (such as MSE or "almost surely") has been specified. 


When estimating functions, such as density functions $f(x),$ a distinction is made between the following concepts of consistency:

* **Pointwise consistency:** 
  $$
  \hat{f}_{nh}(x)\to_P f(x),\quad \text{as}\quad n\to\infty
  $$
  for an arbitrary, given $x\in\mathbb{R}.$
* **Uniform consistency:**
  $$
  \sup_{x}\left|\hat{f}_{nh}(x) - f(x)\right|\to_P 0,\quad \text{as}\quad n\to\infty.
  $$



For **pointwise consistency** the following Assumptions are needed:

::: {.callout-note  appearance="minimal"}

## Assumptions for Showing Pointwise Consistency and Asymptotic Normality

Under the following assumptions: 

* Sequence of bandwidth parameters $h = h_n:$
   *  $h_n \to 0$ as $n\to\infty$, 
   *  $n h_n \to \infty$ as $n \to \infty$
* True density function $f:$
   * $f$ continuouse and sufficiently often differentiable for all $x$
* Kernel function $K$: 
    * continuouse and non-negative $K(z)\geq 0$ for all $z$,
    * $K$ symmetric around $0$ 
    * $\lim_{|y| \to \infty} | y K(y) | = 0$ 
    * $K$ bounded ($\int |K(z)|dz < \infty$), 
    * $\int K(z)dz = 1$
 
one can show **pointwise consistency** for a given $x,$

$$
\hat{f}_{nh}(x)\to_P f(x),\quad \text{as}\quad n\to\infty
$$

<!-- %(ErfÃ¼llt, falls $K$ eine um $0$ symmetrische Dichte auf einer kompakten Menge, z.B., $K:[-1,1]\to[0,\infty[$, ist.  
-->

::: {.callout-tip appearance="minimal"}
**Note:** Under the additional, more restrictive bandwidth assumption that
$$
h_n \to 0\quad\text{and}\quad n h_{n}^{\textcolor{red}{2}} \to \infty,\quad\text{as}\quad n\to\infty,
$$
one can also show **uniform consistency** (see @parzen1962estimation), 
$$
\sup_{x}\left|\hat{f}_{nh}(x) - f(x)\right|\to_P 0,\quad \text{as}\quad n\to\infty.
$$
:::
:::

#### **Proof-Strategy for Showing Pointwise Consistency** {-} 

Under the above assumptions, show that the pointwise Mean Squared Error (MSE) converges to zero as $n\to\infty$ for a given $x,$
$$
\mathrm{MSE}(\hat{f}_{nh}(x))\to 0,\quad \text{as}\quad n\to\infty.
$$
by showing that both, the variance 
$$
Var\left(\hat{f}_{h_n}(x)  \right)
$$ 
and the squared bias 
$$
\left(\mathrm{Bias} \left( \hat{f}_{h_n}(x) \right)\right)^2
$$ 
converge to zero as $n\to\infty,$ 
$$
\begin{align*}
\mathrm{MSE}\left(\hat{f}_{h_n}(x) \right)
&=
\mathbb{E}\left(\hat{f}_{h_n}(x) - f(x) \right)^2\\[2ex]
&=
\underbrace{Var\left(\hat{f}_{h_n}(x)  \right)}_{\underset{n\to\infty}\longrightarrow 0}
+
\underbrace{\left(\mathrm{Bias} \left( \hat{f}_{h_n}(x) \right)\right)^2.}_{\underset{n\to\infty}\longrightarrow 0}
\end{align*}
$$



**Remember:** Convergence in quadratic mean implies convergence in probability.


::: {.callout-note appearance="minimal"}

## Pointwise Asymptotic Normality

Under the above pointwise consistency assumptions one can also show pointwise asymptotic normality:
$$
\begin{equation*}
\frac{ \sqrt{n\,h_n}\left(\hat{f}_{nh}(x) - \mathbb{E} \left( \hat{f}_{nh}(x) \right)\right)}{
     \sqrt{ Var\left( \hat{f}_{nh}(x) \right) }}
   \overset{a}{\sim}\mathcal{N}(0,1)
\end{equation*}
$$
for $n\to\infty,$ which implies that we can construct pointwise confidence intervals (etc.)

**Caution:** Generally, we have for finite $n$ that:
$$
\mathbb{E}\left( \hat{f}_{nh}(x) \right)\neq f(x);
$$ 
i.e. for finite $n$ there is generally a non-negligible estimation bias. 
:::


## Globally Optimal Bandwidth Choice 


One distinguishes **locally optimal** bandwidth choices for estimating $f(x)$ at a given $x,$ and **globally optimal** bandwidth choices that are optimal with respect to a **global loss function**.  


Typically, one determines a **globally optimal** bandwidth by minimizing the Mean Integrated Squared Error loss function 
$$
\begin{align*}
\mathrm{MISE}(\hat{f}_{nh})
&=\int\mathrm{MSE}(\hat{f}_{nh}(x))\,dx\\[2ex]
&=\int \mathbb{E}\left[(\hat{f}_{nh}(x)-f(x))^2\right]\,dx\\[2ex]
&=\int Var\left(\hat{f}_{nh}(x)  \right) \,dx + 
  \int \left(\mathrm{Bias} \left( \hat{f}_{nh}(x) \right)\right)^2\,dx.
\end{align*}
$$


**Problem:** The exact computation of $\mathrm{MISE}(\hat{f}_{nh})$ is only possible under very restrictive, simplifying assumptions such as assuming that $f$ is a specific parametric density function---an assumption we do not want to make! 

**Solution:** Therefore, we compute an asymptotic approximation to $\mathrm{MISE}(\hat{f}_{nh}),$
$$
\begin{align*}
\mathrm{MISE}(\hat{f}_{nh})&\approx \mathrm{AMISE}(\hat{f}_{nh})\\[2ex]
&=\int AVar\left(\hat{f}_{nh}(x)  \right) \,dx + 
  \int \left(\mathrm{ABias} \left( \hat{f}_{nh}(x) \right)\right)^2\,dx\\[2ex]
\end{align*}
$$
which becomes good as $n\to\infty,$ and which can be done without making restrictive parametric assumptions on $f.$ 

The asymptotic approximation $\mathrm{AMISE}(\hat{f}_{nh})$ for $\mathrm{MISE}(\hat{f}_{nh})$ is the most often used loss function for determining a global choice of the bandwidth $h.$

#### **Asymptotic Approximation of $\mathrm{MISE}(\hat{f}_h)$** {-}

Computing the pointwise mean:
$$
\begin{align*}
\mathbb{E}\left(\hat{f}_{nh}(x)\right)
&=\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^{n}\frac{1}{h}K\left(\frac{x-X_i}{h}\right)\right)\\[2ex]
&=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{h}\mathbb{E}\left(K\left(\frac{x-X_i}{h}\right)\right)\\[2ex]
  &=\mathbb{E}\left(\frac{1}{h} K\left(\frac{x - X_i}{h}\right)\right)\\[2ex]
  &=\int_{-\infty}^\infty \frac{1}{h} K\left(\frac{x - u}{h}\right)f(u)du\\[2ex]
  &\left[\text{Substitution: $u=x+yh\;\Rightarrow \frac{du}{dy}=h$}\right]\\[2ex]
  &=\int_{-\infty}^\infty \frac{h}{h} K(y) f(x + y h ) dy\\[2ex]
  &\left[\text{Taylor expansion of $f(x+yh)$ around $f(x)\colon$}\right]\\[2ex]
  &=\int_{-\infty}^\infty K(y) \left\{ f(x) + f'(x) y h + \frac{1}{2!} f''(x) y^2 h^2 + o(h^2) \right\} dy\\[2ex]
  &=f(x) \underbrace{\int_{-\infty}^\infty K(y)\,dy}_{=1} + 
  f(x) h \underbrace{\int_{-\infty}^\infty y K(y)\,dy}_{=0}\\[2ex]
  &\;\; + h^2 \frac{1}{2} f''(x) \underbrace{\int_{-\infty}^\infty K(y)y^2 dy}_{\nu_2(K)}+ o(h^2)  \underbrace{\int_{-\infty}^\infty K(y)\,dy}_{=1}\\[3ex]
  &=f(x) + h^2 \frac{1}{2} f''(x) \nu_2(K) + o(h^2) 
\end{align*}
$$


Thus, the pointwise squared bias is given by
$$
\begin{align*}
  &\left(\mathrm{Bias}(\hat{f}_h(x))\right)^2 \\[2ex] 
  &=\left(\mathbb{E}(\hat{f}_h(x))-f(x)\right)^2 \\[2ex] 
  &=\left(h^2 \frac{1}{2} f''(x) \nu_2(K) + o(h^2)\right)^2\\[2ex]
  &=\left(h^2 \frac{1}{2} f''(x) \nu_2(K)\right)^2 + \underbrace{2\,\left({\color{red}h^2} \frac{1}{2} f''(x) \nu_2(K)\right)  o({\color{red}h^2})}_{o({\color{red}h^4})} + o(h^4)\\[2ex] 
  &=\underbrace{h^4 \frac{1}{4} \nu_2(K)^2 f''(x)^2}_{=:\left(\mathrm{ABias} \left( \hat{f}_{nh}(x) \right)\right)^2}  + o(h^4)\\[2ex]
&\Rightarrow\quad \left(\mathrm{ABias} \left( \hat{f}_{nh}(x) \right)\right)^2=h^4 \frac{1}{4} \nu_2(K)^2 f''(x)^2
\end{align*}  
$$





## References {-}