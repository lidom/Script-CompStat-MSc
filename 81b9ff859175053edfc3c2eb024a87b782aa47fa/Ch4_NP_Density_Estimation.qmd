<!-- LTeX: language=en-US -->
# Nonparametric Density Estimation

## Introduction

```{r}
#| echo: false
data_path <- "81b9ff859175053edfc3c2eb024a87b782aa47fa/data/"
data_path <- "data/"
# Libraries
suppressPackageStartupMessages(library("ggplot2"))    # Plotting 
suppressPackageStartupMessages(library("np"))         # Nonparametric Statistics
suppressPackageStartupMessages(library("KernSmooth")) #
suppressPackageStartupMessages(library("gridExtra"))  # To arrange multiple plots 
suppressPackageStartupMessages(library("gtable")) 
suppressPackageStartupMessages(library("scales"))     # Transparent colors 
```


### Example: Income Data Analysis


```{r}
#| echo: false
inc76 <- read.table(paste0(data_path,"inc76.txt"), col.names="income")
```

```{r}
c(inc76$income)[1:6]
```

Typical aim: Characterizing the income distribution (density function) $f$ given a random sample 
$$
\{X_1,\dots,X_n\}
$$
with 
$$
X_i\overset{\text{i.i.d.}}{\sim} f
$$


Traditional statistical key figures:

* Empirical mean 
* Empirical median
* Empirical variance 
* Empirical interquartile distance, etc. 

Only summarize single aspects of a distribution. 

Estimating the total density function $f$ can provide more detailed, more wholistic information. 


@fig-HistOrdinary shows a histogram (i.e., a very simple density estimator) of the income data.

```{r}
#| echo: true
#| fig-cap: Histrogram of the income data.
#| label: fig-HistOrdinary
hist(inc76$income, 
     freq = FALSE,
     xlab = "Income", 
     ylab = "Density", main = "")
```


Disadvantages of the histogram: 

* Choice of the bin width (and of the start point)
* Discontinuous, locally constant $\Rightarrow$ A histogram cannot be a very efficient estimator for a (continuous) density function $f(x).$


The choice of the bin width is crucial. @fig-HistBinWidths shows examples for a too small and a too large bin width choice.

```{r}
#| echo: false
#| fig-cap: Histrograms of income data for different choices of the bin width. 
#| label: fig-HistBinWidths
par(mfrow = c(1,2))
hist(inc76$income, 
     freq = FALSE,
     breaks = 25,
     xlab = "Income", 
     ylab = "Density", main = "")
hist(inc76$income, 
     freq = FALSE,
     breaks = 4,
     xlab = "Income", 
     ylab = "Density", main = "")
par(mfrow = c(1,1))
```


@fig-HistKDE shows a comparison of the histogram versus the density estimate from a kernel density estimator. 

```{r}
#| echo: false
#| fig-cap: Histogram versus a nonparametric kernel density estimation (green solid line).
#| label: fig-HistKDE
hist(inc76$income, 
     freq = FALSE,
     xlab = "Income", 
     ylab = "Density", main = "")
lines(density(inc76$income), col = "darkgreen")
```


Advantages of the kernel density estimator: 

* Choice of the bandwidth (similar to bin width) can be done using statistical theory
* Continuous estimate for a (continuous) density function $f.$


### From the Histogram to the Kernel Density Estimator


Consider a histogram with $J$ (e.g., $J=8$ as in @fig-HistKDE) bins all having the same bin-width $2h,$ defined by equidistant intervals
$$
(x_{j-1},x_j]
$$ 
with 
$$
x_j-x_{j-1}=2h\quad \text{for all} \quad j=1,\dots,J.
$$
The bin-height is determined **locally** at the $j$th interval mid-point 
$$
\bar{x}_j=(x_{j-1}+x_j)/2
$$
by the relative frequency of data points $X_1,\dots,X_n$ that fall within the $j$th interval $(x_{j-1},x_j],$


$$ 
\begin{align*}
\hat f_{hist}(\bar{x}_j)
& =\frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{2hn}
\end{align*}
$${#eq-fHist}
**Note:** The scaling by $2hn$ is necessary to guarantee that the area of each bin equals the relative frquency of data points $X_1,\dots,X_n$ that fall into the interval $(x_{j-1},x_j],$
$$
\begin{align*}
\text{Area of $j$th Bin} 
& = (\text{bin-width}) \cdot (\text{bin-height of $j$th bin})\\[2ex]
& = \qquad 2h\quad\, \cdot \frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{2hn}\\[2ex]
& = \frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{n}\\[2ex]
& = \frac{1}{n}\sum_{i=1}^n \; 1_{(X_i\in(x_{j-1},x_j])}, 
\end{align*}
$$
where $1_{(\cdot)}$ denotes the indicator function, i.e., 
$1_{(\text{TRUE})}=1$ and $1_{(\text{FALSE})}=0.$ Thus, the scaling guarantees that the bin areas of the histogram sum up to one
$$
\begin{align*}
\sum_{j=1}^J\text{Area of $j$th Bin}
&=\sum_{j=1}^J \frac{1}{n}\sum_{i=1}^n \; 1_{(X_i\in(x_{j-1},x_j])}\\[2ex]
&=\frac{1}{n} \sum_{j=1}^J \sum_{i=1}^n \; 1_{(X_i\in(x_{j-1},x_j])}\\[2ex]
&=\frac{n}{n}=1.
\end{align*}
$$


Doing some rearrangements of @eq-fHist, allows writing the histogram as a type of kernel density estimator:  
$$ 
\begin{align*}
\hat f_{hist}(\bar{x}_j)
& =\frac{\hbox{Number of } X_{i}\hbox{ in } (x_{j-1},x_j]}{2hn}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(x-h < X_i\leq x+h\right)}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(-1<\left(\frac{X_i-x}{h}\right)\leq 1\right)}\\[2ex]
& =\frac{1}{nh}\sum_{i=1}^n K\left(\frac{X_{i}-\bar{x}_j}{h}\right),
\end{align*}
$$
where $K(z)$ denotes the symmetric $(K(z)=K(-z))$ kernel function
$$
K(z)=\left\{
\begin{array}{ll}
1/2 & \hbox{ if } z\in (-1,1] \\
0   & \hbox{ else}.
\end{array}\right.
$$


A kernel density estimator generalizes the histogram $\hat f_{hist}(\bar{x}_j)$ by estimating the unknown density $f$ not only at the mid-points $\bar{x}_j,$ but at *every* $x,$ which yields the **moving histogram**
$$
\begin{align*}
{\hat{f}}_{nh}(x)=\frac{1}{nh}\sum_{i=1}^nK\left(
\frac{x-X_{i}}{h}\right)
\end{align*}
$${#eq-MovHist}
where 

* $K$ denotes the kernel function and  
* $h>0$ the bandwidth  


## A More Formal Motivation of the Kernel Density Estimator


Let 
$$
\{X_1, \ldots, X_n\}
$$
denote a random sample with 
$$
X_i\overset{\text{i.i.d.}}{\sim} f\quad\text{for}\quad i=1,\dots,n.
$$


**Aim:** Find a density estimator 
$$
\hat{f}
$$ 
for the true, unknown density $f$ **without** making a parametric assumption on $f$ such as assuming that $f$ is the density of a normal distribution with unknown mean and unknown variance. 


The only assumption on $f$ is the following **qualitative smoothness assumption:** The density function $f(x)$ is assumed to be sufficiently smooth---i.e., to be sufficiently often differentiable for all $x.$  


**Starting point:** We use the connection between density functions $f$ and distribution functions $F(x) = P(X \leq x);$ namely,  
$$
\begin{equation*}
  f(x) = \frac{d}{dx} F(x) = F'(x), \qquad x \in \mathbb{R}.
\end{equation*}
$$


**Idea:** Approximate the derivative of the distribution function using the difference quotient. For small $h > 0,$ we have that 
$$
\begin{align*}
f(x) =   F'(x) &\approx \frac{F(x+h) - F(x)}{h}%\\[2ex]
%  F'(x) &= \frac{F(x+h) - F(x)}{h} + O(h)
\end{align*}
$${#eq-DiffQ1}
or
$$
\begin{align*}
f(x) = F'(x) &\approx \frac{F(x) - F(x-h)}{h}.%\\[2ex]
%F'(x)      &= \frac{F(x) - F(x-h)}{h} + O(h),
\end{align*}
$${#eq-DiffQ2}
<!-- where the "Big-Oh" term $O(h)$ refers to a $h\to 0$ with $h>0$ asymptotic.  -->


In both the right-hand (@eq-DiffQ1) and the left-hand difference quotient (@eq-DiffQ2) have an approximation error of the order $O(h)$ for $h\to 0$ with $h>0.$ 

<!-- 
$$
\left|F'(x)-\frac{F(x) - F(x-h)}{h}\right|\to 0\quad\text{as}\quad h\to 0 
$$
such that
$$
\frac{\left|F'(x)-\frac{F(x) - F(x-h)}{h}\right|}{h}\to c,
$$
where $0\leq c < \infty.$
 -->


In his seminal work on the kernel density estimator, @Rosenblatt_1956 uses an even better derivative approximation based on the symmetric difference quotient
$$
\begin{equation*}
    f(x) = F'(x) = \frac{F(x+h) - F(x-h)}{2h} + O(h^2),
\end{equation*}
$${#eq-DiffQ3}
where the approximation error is of the order $O(h^2)$ for $h\to 0$ with $h>0.$ This means that the approximation error goes, in absolute values, to zero as fast as $h^2\to 0$ *or faster*; i.e.
$$
\begin{align*}
& O\big(h^2\big)=\\[2ex]
=&\left\{\text{Any function}\;g(h)\text{ such that }\frac{|g(h)|}{h^2}\to c\;\text{ as }\;h\to 0\text{, where }0\leq c<\infty\right\}.
\end{align*}
$$

::: {.callout-note collapse="true"}
## Deriving the approximation error in @eq-DiffQ3 
Let $F$ be three times continuously differentiable; i.e. $F^{(3)}(x)$ is a continuous function for all $x.$ 
$$
\begin{align*}
&\frac{F(x+h) - F(x-h)}{2h}=\\[2ex]
&[\text{Taylor polynomial approximations for $F(x+h)$ and $F(x-h)$ around $x\colon$}]\\[2ex]
=&\frac{\left(F(x)+F^{(1)}(x)h + \frac{1}{2}\,F^{(2)}(x) h^2 +\frac{1}{6}\,F^{(3)}(x)h^3 + o(h^3)\right)}{2h}-\\[2ex]
&\frac{\left(F(x)-F^{(1)}(x)h+ \frac{1}{2}\,F^{(2)}(x) h^2-\frac{1}{6}\,F^{(3)}(x)h^3+o(h^3)\right)}{2h}\\[2ex]
&[\text{Applying straight forward simplifications:}]\\[2ex]
=&\frac{2\,F^{(1)}(x)h+\frac{2}{6}\,F^{(3)}(x)h^3+o(h^3)}{2h}\\[2ex]
=&\frac{2\,F^{(1)}(x)h}{2h}+\frac{\frac{1}{3}\,F^{(3)}(x)h^3}{2h}+\frac{o(h^3)}{2h}\\[2ex]
&[\text{Using that $h^{-1}o(h^3)=o(h^2)\colon$}]\\[2ex]
=&F'(x)
+\frac{1}{6}\,F^{(3)}(x)h^2
+\frac{1}{2}\,o(h^2)\\[2ex]
&[\text{Using that $\texttt{constant}\times h^2=O(h^2)$ and that $\texttt{constant}\times o(h^2)=o(h^2)\colon$}]\\[2ex]
=&F'(x)+O(h^2) + o(h^2)\\[2ex]
&[\text{Using that $o(h^2)=O(h^2)$ and that $2O(h^2)=O(h^2)\colon$}]\\[2ex]
=&F'(x)+O(h^2)
\end{align*}
$$
:::

Using the definition of the distribution function $F(x)=P(X\leq x)$ we get
$$
\begin{align*}
    f(x) & = \frac{F(x+h) - F(x-h)}{2h} + O(h^2) \\[2ex]
    & = \frac{P(X \leq x+h) - P(X \leq x-h)}{2h} + O(h^2)\\[2ex]
    & =\frac{1}{2h} P(x-h < X \leq x+h) + O(h^2)\\[2ex]
\Rightarrow\quad        f(x) & \approx \frac{1}{2h} P(x-h < X \leq x+h) ,
\end{align*}
$$
where the approximation is very good for smallish $h>0.$

Estimating the unknown probability $P(x-h < X \leq x+h)$ by its empirical counterpart (relative frequency) yields the **moving histogram** estimator as in @eq-MovHist:
$$
\begin{align*}
\hat{f}_{nh}(x) 
&=\frac{1}{2h}\hat{P}\left(x-h < X \leq x+h\right)\\[2ex]
&=\frac{1}{2h}\frac{\text{Number of $X_i$ in $(x-h, x+h]$}}{n}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(x-h < X_i\leq x+h\right)}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\left(-1<\left(\frac{X_i-x}{h}\right)\leq 1\right)}\\[2ex]
&=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_{i}}{h}\right)
\end{align*}
$$
where $K(z)$ denotes the symmetric $(K(z)=K(-z))$ kernel function
$$
K(z)=\left\{
\begin{array}{ll}
1/2 & \hbox{ if } z\in (-1,1] \\
0   & \hbox{ else}.
\end{array}\right.
$$


```{r}
my_movhist <- function(x, data, h){
      n      <- length(data)
      dat_x  <- data[data > x-h & data <= x+h]
      result <- length(dat_x)/(n*2*h)
      return(result)
}
my_movhist <- Vectorize(my_movhist, "x")
```

```{r}
#| fig-cap: Income density estimate based on the moving histogram density estimator.
#| label: fig-MovHistInc
data  <- inc76$income
h     <- 12  # bandwidth
xx    <- seq(from=(min(data)), to=(max(data)), len=750)
yy    <- my_movhist(x = xx, data = data, h = h)                

plot(x = xx, y = yy, 
     ylab="Density", xlab="Income", 
     main = "Density Estimation using the Moving Histogram", type="l")
```

Density estimates of the moving histrogram estimator are discontinuous---even though, we aim to estimate a smooth (e.g. three times continuously differentiable) denstiy function $f.$

**Kernel Density Estimator:**

Replacing the symmetric, discontinuous rectengular kernel function of the naive moving histogram by a symmetric, continuous and differentiable function, such as

* $K(z) = \frac{3}{4}(1-z^2)$ for $z \in [-1,1]$ and $0$ else
* or the density function of the standard normal distribution:
$$
K(z) = \phi(z) = \frac{1}{\sqrt{2\pi}} \exp( -z^2 / 2 )
$$
yields to the **kernel density estimator** with kernel $K$ and bandwidth $h\colon$
$$
\begin{equation*}
  \hat{f}_{nh}(x)=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{h} K\left(\frac{x - X_i}{h} \right),
  \quad\text{for}\quad x \in \mathbb{R}.
\end{equation*}
$$


The kernel density estimator yields a smooth density estimate. It is more efficient than the (moving) histrogram and provides a flexible adjustment to the data. 

* Bandwidth $h$ is also called **smoothing parameter**. The bandwidth (and the kernel $k$) need to be selected by the user (or autmatically by the computer). 
* New, compact notation for the scaled kernel function: $K_h(u) := K(u / h) / h$.
$$
\begin{align*}
\hat{f}_{nh}(x) 
&=  n^{-1} \sum_{i=1}^{n} \frac{1}{h}  K\left(\frac{x - X_i}{h}\right)\\[2ex]
&=  n^{-1} \sum_{i=1}^{n} K_h(x - X_i)
\end{align*}
$$


::: {.callout-note}
## Notation 

* $\hat{f}_{nh}:$ the index $nh$ means here that the estimator depends on the sample size $n$ and the bandwidth $h$. 
* $K_h:$ the index $h$ is here an abbreviation for $K( \cdot / h) / h$.
::::


## Properties of the Kernel Density Estimator

::: {.callout-tip}
## Reminder: Definition of a Density Function
We call a function $f$ a density function if it fulfilles the following properties: 

* Non-negative: $f(x)\geq 0$ for all $x\in\mathbb{R}$
* Normed: $\int f(x)dx = 1$
:::


It can be shown that $f_{nh}$ is a density function if the kernel function $K$ is 

* Non-negative: $K(x) \geq 0$ for all $x\in\mathbb{R}\quad \Rightarrow\quad \hat{f}_{nh}(x) \geq 0$  for all $x\in\mathbb{R}$
* Normed: $\int K(x)dx = 1\quad \Rightarrow\quad \int \hat{f}_{nh}(x)dx = 1$

That is, if $K$ is a density function, then also $f_{nh}$ is a density function. 

This inheritance of the properties of $K$ to the properties of $f_{nh}$ also holds for the smoothness properties: 

* Smoothness of $K$: If $K$ is continuously differentiable $\quad\Rightarrow\quad$ $\hat{f}_h$ is continuously differentiable


### Theoretical Requirements on the Kernel Function

* **Normed:** 
$$
\int K(x)dx=1
$$
* **Symmetric around zero:** 
$$
\int xK(x)dx=0
$$ 


Typical choices for the kernel function $K$ are: 

* Smooth density functions that are symmetric around zero. 