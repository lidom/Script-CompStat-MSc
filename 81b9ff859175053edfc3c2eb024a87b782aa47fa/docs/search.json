[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Organization of the Course\n\nTimetable\n\n\n\n\n\nDay\nTime\nLecture Hall\n\n\n\n\nMonday\n16:15-18:00\nLecture Hall F\n\n\nWednesday\n08:45-10:00\nLecture Hall N\n\n\n\n\n\n\n\n\n\nSyllabus\n\nMaximum Likelihood Estimation\nEM Algorithm & Cluster Analysis\nBootstrap\nNonparametric Density Estimation\nNonparametric Regression \n\n\n\nLecture Material\n\nThis online script available at: https://www.dliebl.com/Script-CompStat-MSc/ (pwd: compstat)\neWhiteboard for technical derivations and extra explanations.\nBasic material from the basis module in econometrics:\n\nIntroduction to R\nProbability\n\nIntro-Slides: “Prolog: Statistics, Computer & Women Who Code” (not relevant for the exam)\nBooks I can recommend:\n\nPattern Recognition and Machine Learning (by Christopher M. Bishop). Freely available pdf version\nElements of Statistical Learning: Data Mining, Inference and Prediction (by by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie). Freely available pdf version\nAll of Statistics: A Concise Course in Statistical Inference (by Larry A. Wasserman)\nAll of Nonparametric Statistics (by Larry A. Wasserman)\n\n\n\n\nMiscellaneous\n\nGrace Wahba, “the mother of smoothing splines,” won the International Prize (aka the Nobel Prize) in Statistics. Some of her well known contributions are:\n\nThe representer theorem; see also the YouTube-Video by Very Normal\nGeneralized Cross-Validation (see Chapter 5.3.3 Generalized Cross-Validation (GCV))",
    "crumbs": [
      "Organization of the Course"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html",
    "href": "Ch1_MaximumLikelihood.html",
    "title": "1  Maximum Likelihood",
    "section": "",
    "text": "1.1 Introduction: The Likelihood Principle\nThe basic idea behind maximum likelihood estimation is very simple: Assume that the data is generated by some distribution with a certain (finite) set of unknown distribution parameters (e.g., the normal distribution with unknown mean and variance). Then find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed.\nIn (classic) maximum likelihood estimation we must be rather specific about the process that generated the data. This is a trade-off: by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question remains, however, whether we have made the right decision about the general distribution/density function family.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "href": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "title": "1  Maximum Likelihood",
    "section": "1.2 Numeric Optimization",
    "text": "1.2 Numeric Optimization\nUsually, we are not so fortunate as to have an analytical solution for the MLE, and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\n\nGeneral idea: Try to find the root of \\(\\ell'\\)\n\nStart at some value, \\(\\theta_{(0)},\\) in the parameter space \\(\\Theta.\\)\nSearch across the parameter space \\(\\Theta\\) using a step-wise procedure \\[\n\\theta_{(0)},\\theta_{(1)},\\dots,\\theta_{(m)}\n\\] until an updated parameter value \\(\\theta_{(m)}\\) is found that yields a derivative of the log likelihood that is effectively zero (i.e. smaller than some convergence/stopping criterion), \\[\n\\ell'(\\theta_{(m)})\\approx 0.\n\\]\n\n\n1.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nIn the following, we consider the univariate case \\(\\theta\\in\\mathbb{R}.\\) However, the multivariate case \\(\\theta\\in\\mathbb{R}^K\\) is treated likewise, but requires substituting first derivatives by gradients, second derivatives by the Hessian, etc.\n\n\n\n\n\n\nNote\n\n\n\nMinimization and maximization are essentially the same problems, since minimizing a function \\(f(x)\\) with respect to \\(x\\) is equivalent to maximizing \\(-f(x)\\) with respect to \\(x.\\)\n\n\nLet \\(f\\) be a two times differentiable function to be optimized (here maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial of order 1}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial of order 2}},\n\\end{align*}\n\\] Locally, i.e. for \\(|h|\\approx 0,\\) (e.g. \\(h=\\pm 0.04\\)) the Taylor polynomials are very good approximations of \\(f(\\theta + h);\\) see Figure 1.2.\n\n\n\n\n\n\n\n\nFigure 1.2: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta=1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Taylor’s Theorem)  Today, there are many different versions of Taylor’s theorem. We consider the following two:\n1. Peano form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k\\) times differentiable at \\(x\\in\\mathbb{R}\\) and let \\(h\\in\\mathbb{R}.\\) Then there exists a function \\(P_{k,x}:\\mathbb{R}\\to\\mathbb{R}\\) such that \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!} h^\\ell\n+ P_{k,x}(h) \\cdot h^k\n\\end{align*}\n\\] with \\[\nP_{k,x}(h)\\to 0\\quad\\text{as}\\quad |h|\\to 0,\n\\] where \\(f^{(\\ell)}(x)\\) denotes the \\(\\ell\\)th derivative of \\(f\\) at \\(x.\\)\n2. Lagrange or Mean-value form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k+1\\) times differentiable on the open interval between \\(x\\) and \\(x+h,\\) with \\(h\\in\\mathbb{R},\\) and let \\(f^{(k)}\\) be continuous on the closed interval between \\(x\\) and \\(x+h,\\). Then \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!} h^\\ell\n+ M_{k,x}(h)\n\\end{align*}\n\\] with \\[\nM_{k,x}(h)=\\frac{f^{(k+1)}(\\xi)}{(k+1)!} h^{k+1}\n\\] for some real number \\(\\xi\\) between \\(x\\) and \\(x+h.\\) This form of Taylor’s theorem is based on the mean-value Theorem 1.2. Note that \\[\nM_{k,x}(h)\\to 0\\quad\\text{as}\\quad |h|\\to 0.\n\\]\n\n\n\n\n\n\nQualitative version using the small-\\(o\\) notation:\n\n\n\n\\[\n\\begin{align*}\nf(x + h) & =\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!} h^\\ell\n+ o\\big(|h|^k\\big),\n\\end{align*}\n\\] where \\(o\\big(|h|^k\\big)\\) denotes the family of real-valued functions, \\(g(h)\\) say, that are in absolute values of a strictly smaller \\(o\\)rder of magnitude than the function \\(|h|^k\\) as \\(|h|\\to 0;\\) i.e.\n\\[\n\\begin{align*}\n& o\\big(|h|^k\\big)=\\\\[2ex]\n=& \\left\\{\\text{Any function}\\;g(h)\\text{ such that }\\frac{|g(h)|}{|h|^k}\\to 0\\;\\text{ as }\\; |h|\\to 0\\right\\}.\n\\end{align*}\n\\]\n1. Note: Peano form of the remainder term: \\[\nP_{k,x}(h)\\cdot h^k=o\\big(|h|^k\\big)\n\\] since \\[\n\\frac{|P_{k,x}(h)\\;h^k|}{|h|^k}\n%=\\frac{|P_k(x+h)|\\cdot |h|^k|}{|h|^k}\n=|P_{k,x}(h)|\\to 0\\quad\\text{as}\\quad |h|\\to 0.\n\\]\n2. Note: Mean-value form of the remainder term: \\[\nM_{k,x}(h)=o\\big(|h|^k\\big)\n\\] since \\[\n\\frac{|M_{k,x}(h)|}{|h|^k}=\n\\left|\\frac{f^{(k+1)}(\\xi)}{(k+1)!}\\right|\\cdot|h|\\to 0\\quad\\text{as}\\quad |h|\\to 0.\n\\]\n\n\n\n\n\n\n\nOptimization Idea\nLet \\(\\ell_{n,obs}\\) be an observed realization of the log-likelihood function with continuous first, \\(\\ell_n',\\) and second, \\(\\ell_n'',\\) derivative.\nTo optimize the log-likelihood function \\(\\ell_{n,obs},\\) we try to find the root of \\(\\ell_{n,obs}',\\) i.e. the value of \\(\\theta\\in\\Theta\\) such that \\[\n\\ell_{n,obs}'(\\theta)=0.\n\\] That is, we try to find the value of \\(\\theta\\) that fulfills the first order condition of the optimization problem. We do so using a step-wise optimization approach, where each step has a smallish size \\(h.\\)\nInitialization: Let \\(\\theta_{(0)}\\in\\Theta\\) be our first guess of the root of \\(\\ell'_{n,obs}.\\)\n\\(h\\)-Steps: Typically, our guess is not perfect and thus \\(\\ell_{n,obs}'(\\theta_{(0)})\\neq 0.\\) Therefore, we want to move from \\(\\theta_{(0)}\\) to a new root-candidate \\(\\theta_{(1)}\\) by doing an \\(h\\)-step update \\[\n\\theta_{(1)} = \\theta_{(0)} + h.\n\\]\n\n\nThe first-order Taylor-series approximation of \\(\\ell_{n,obs}'\\) around our first guess \\(\\theta_{(0)}\\) gives \\[\n\\begin{align*}\n\\ell_{n,obs}'(\\theta_{(0)} + h) & = \\ell_{n,obs}'(\\theta_{(0)}) + \\ell_{n,obs}''(\\theta_{(0)})h + o(|h|)\\\\[2ex]\n{\\color{gray}\\big(f(\\theta_{(0)} + h) }\\,&\\,{\\color{gray} = f(\\theta_{(0)}) + f'(\\theta_{(0)})h \\,+\\, o(|h|)\\big)}\n\\end{align*}\n\\] Thus, in a small \\(h\\)-neigborhood (\\(|h|\\to 0\\)) around \\(\\theta_{(0)},\\) \\(\\ell_{n,obs}'(\\theta_{(0)} + h)\\) approximately equals the linear function (in \\(h\\)) \\[\n\\ell_{n,obs}'(\\theta_{(0)}) + \\ell_{n,obs}''(\\theta_{(0)})h\n\\] plus negligible terms, \\((o(|h|)),\\) that are of a smaller order of magnitude.\nTherefore, to find the \\(h\\)-step that brings us closer to the root of \\(\\ell_{n,obs}',\\) we can (approximatively) use the \\(h\\)-step that brings us to the root of its first-order approximation, i.e. \\[\n\\begin{align*}\n\\ell_{n,obs}'(\\theta_{(0)}) + \\ell_{n,obs}''(\\theta_{(0)}) h_{(0)} = 0\\\\[2ex]\n\\Rightarrow h_{(0)} = -\\frac{\\ell_{n,obs}'(\\theta_{(0)})}{\\ell_{n,obs}''(\\theta_{(0)})}.\n\\end{align*}\n\\] Based on this \\(h\\)-step, the new root-candidate is \\[\n\\begin{align*}\n\\theta_{(1)}\n& = \\theta_{(0)} + h_{(0)}\\\\[2ex]\n& = \\theta_{(0)} - \\frac{\\ell_{n,obs}'(\\theta_{(0)})}{\\ell_{n,obs}''(\\theta_{(0)})}.\n\\end{align*}\n\\] Likewise, the \\(m\\)th root-candidate is \\[\n\\begin{align*}\n\\theta_{(m)}\n& = \\theta_{(m-1)} + h_{(m-1)}\\\\[2ex]\n& = \\theta_{(m-1)} - \\frac{\\ell_{n,obs}'(\\theta_{(m-1)})}{\\ell_{n,obs}''(\\theta_{(m-1)})};\n\\end{align*}\n\\] see also Figure 1.3.\n\n\n\n\n\n\n\n\nFigure 1.3: The \\(m\\)th update step in the Newton-Raphson root-finding algorithm.\n\n\n\n\n\n\n\n\n1.2.2 Convergence of the Newton-Raphson Algorithm\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_{n,obs}';\\) i.e.  \\[\n\\ell_{n,obs}'(\\theta_{root})=0.\n\\] We aim to find \\(\\theta_{root}\\) using the Newton-Raphson algorithm and call our best approximation of \\(\\theta_{root}\\) the maximum likelihood estimate; i.e. \\(\\hat{\\theta}_{ML}\\approx\\theta_{root}.\\)\nLet \\[\ne_{(0)}=\\theta_{root}-\\theta_{(0)}\n\\] denote the start value error and let \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start value error neighborhood around \\(\\theta_{root}.\\)\nOne can shown that if \\(\\ell_{n,obs}'\\) is “well behaved” over \\(I;\\) i.e. \n\nif \\(\\ell_{n,obs}''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\nif \\(\\ell_{n,obs}'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I,\\)\n\nand if our first guess \\(\\theta_{(0)}\\) is “close enough;” i.e. \n\nif \\(M|e_{(0)}|&lt;1,\\) where \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_{n,obs}'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_{n,obs}''(\\theta)|}\\right)\\geq 0,\n\\]\n\nthen \\(\\theta_{(m)}\\) will converge to \\(\\theta_{root}\\) as \\(m\\to\\infty.\\)\n\n\n\n\n\n\nWarning\n\n\n\nUnfortunately, we may not know if \\(\\ell_{n,obs}'\\) is “well behaved,” and we may also don’t know whether our first guess is “close enough”. So, we may not be able to guarantee convergence of the Newton-Raphson algorithm. 😭 \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor problems that are globally concave, the starting value \\(\\theta_0\\) doesn’t matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nIn practice, the implementation of the Newton-Raphson algorithm can be tricky. We may have \\(\\ell_{n,obs}''(\\theta_{(m)})=0,\\) in which case the function looks locally like a straight line, with no solution to the Taylor series approximation \\[\n\\begin{align*}\n\\ell_{n,obs}'(\\theta_{(m)} + h) & \\approx \\ell_{n,obs}'(\\theta_{(m)}) + \\ell_{n,obs}''(\\theta_{(m)})h = \\ell_{n,obs}'(\\theta_{(m)}).\n\\end{align*}\n\\] In this case a simple strategy is to move a small step in the direction which decreases the function value, based only on \\(\\ell_{n,obs}'(\\theta_m).\\)\nIn other cases where \\(\\theta_{(m)}\\) is too far from the true root \\(\\theta_{root}\\), the Taylor approximation may be so inaccurate that \\(\\ell_{n,obs}(\\theta_{(m+1)})\\) is actually more distant from zero than \\(\\ell_{n,obs}(\\theta_{(m)}).\\) When this happens one may replace \\(\\theta_{(m+1)}\\) with \\((\\theta_{(m+1)}+\\theta_{(m)})/2\\) (or some other value between \\(\\theta_{(m)}\\) and \\(\\theta_{(m+1)}\\)) in the hope that a smaller step will produce a better result.\n\n\n\nStopping Criterion: Since we are expecting that \\(\\ell_{n,obs}'(\\theta_{(m)})\\to 0,\\) as \\(m\\to\\infty,\\) a good stopping condition for the Newton-Raphson algorithm is \\[\n|\\ell_{n,obs}'(\\theta_{(m)})|\\leq \\varepsilon\n\\] for some (small) tolerance \\(\\varepsilon&gt;0.\\)\n\n\n\n\n\n\nPseudo-Code: Newton-Raphson Algorithm\n\n\n\n\\[\n\\begin{array}{ll}\n\\texttt{\\textbf{select }} \\theta_{(0)}\\in\\Theta\\;\\;\\text{ and}&\\varepsilon&gt;0 \\\\[2ex]\n\\texttt{\\textbf{let }} m=0         &  \\\\\n\\texttt{\\textbf{while }}  | \\ell_{n,obs}'(\\theta_{(m)}) | &gt;\\varepsilon & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} m = m+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_{(m)} = \\theta_{(m-1)} - \\frac{\\ell_{n,obs}'(\\theta_{(m-1)})}{\\ell_{n,obs}''(\\theta_{(m-1)})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta_{ML}=\\theta_{(m)} & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta_{ML} &  \\\\\n\\end{array}\n\\]\n\n\n\n\n1.2.3 Newton-Raphson Algorithm: Coin-Flipping Example\nLet’s return to our earlier coin flipping example.\nIf we observe, for instance, only one head \\(N_{H,obs}=1\\) for a sample size of \\(n=5,\\) we already know from Equation 1.1 that \\[\n\\hat\\theta_{ML,obs}=\\frac{N_{H,obs}}{n}=\\frac{1}{5}=0.2,\n\\] but let us, nevertheless, apply the Newton-Raphson algorithm.\nThe first and second derivatives of \\[\n\\ell_{n,obs}(\\theta)=\\sum_{i=1}^n\\big(X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\big)\n\\] are \\[\n\\begin{align*}\n\\ell_{n,obs}'(\\theta)&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta}\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n\\ell_{n,obs}''(\\theta) &= -\\dfrac{N_{H,obs}}{\\theta^2} + \\dfrac{n}{(1-\\theta)^2}(-1)-\\dfrac{N_{H,obs}}{(1-\\theta)^2}(-1)\\\\[2ex]\n&= -\\dfrac{N_{H,obs}}{\\theta^2} - \\dfrac{n-N_{H,obs}}{(1-\\theta)^2}.\n\\end{align*}\n\\]\nWe consider a sample size of \\(n=5\\) with the following observed outcome:\n\nOne Head: \\(\\quad N_{H,obs}=1\\)\nFour Tails: \\(\\quad n-N_{H,obs}=4\\)\n\nSetting \\(\\varepsilon=10^{-10}\\) as our stopping criterion and \\(\\theta_{(0)}=0.4\\) as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table 1.1. The numeric optimization solution,\n\\[\n\\hat\\theta_{ML,obs} = \\hat\\theta_{(5)} = 0.2,\n\\] equals the analytic solution.\n\n\n\nTable 1.1: Result of applying the Newton Raphson optimization algorithm to our coin flipping example for given data with \\(N_{H,obs}=1,\\) sample size \\(n=5,\\) starting value \\(\\theta_{(0)}=0.4,\\) and convergence criterion \\(\\varepsilon=10^{-10}.\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(m\\)\n\\(\\hat\\theta_{(m)}\\)\n\\(h_{{(m)}}=\\frac{-\\ell_{n,obs}'(\\hat\\theta_{(m)})}{\\ell_{n,obs}''(\\hat\\theta_{(m)})}\\)\n\\(|\\ell_{n,obs}'(\\hat\\theta_{(m)})|\\gtrless \\varepsilon\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-2.4\\cdot 10^{-1}\\)\n\\({\\color{red}|4.2| &gt; \\varepsilon}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}3.3\\cdot 10^{-2}\\)\n\\({\\color{red}|1.5| &gt; \\varepsilon}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}6.6\\cdot 10^{-3}\\)\n\\({\\color{red}|2.2\\cdot 10^{-1}| &gt; \\varepsilon}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}1.7\\cdot 10^{-4}\\)\n\\({\\color{red}|5.4\\cdot 10^{-3}| &gt; \\varepsilon}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}1.1\\cdot 10^{-7}\\)\n\\({\\color{red}|3.5\\cdot 10^{-6}| &gt; \\varepsilon}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}4.8\\cdot 10^{-14}\\)\n\\({\\color{darkgreen}|1.5\\cdot 10^{-12}| &lt; \\varepsilon}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "href": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "title": "1  Maximum Likelihood",
    "section": "1.3 Linear Regression under Normality",
    "text": "1.3 Linear Regression under Normality\nNow, let’s return to the linear regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{1.2}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable, \\[\n\\beta_0\\in\\mathbb{R}^K\n\\] denotes the vector of unknown slope parameters, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{iK})^T\\in\\mathbb{R}^K\n\\] denotes the vector of predictor variables, where the (i.i.d.) random sample\n\\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design with homoskedastic errors (see Definition 1.3).\n\n\n\n\n\n\n\nDefinition 1.3 (Random Design (Regression Analysis)) \nA random desgin in regression analysis is given by the following setup:\nLet \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n),\n\\] or equivalently \\[\n(X_1,\\varepsilon_1), (X_2,\\varepsilon_2), \\dots, (X_n,\\varepsilon_n),\n\\] denote a (i.i.d.) random sample with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\), intertable \\((K\\times K)\\) matrix \\(\\mathbb{E}(X_iX_i^T)=\\Sigma_{X^TX}\\), \\(i=1,\\dots,n,\\) and with either\n\nhomoskedastic errors: \\(0&lt;\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0&lt;\\infty\\)\n\nor\n\nheteroskedastic errors: \\(0&lt;\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)&lt;\\infty\\), for a strictly positive and finite variance function \\(\\sigma^2_0(\\cdot).\\)\n\n\n\n\n\nFor the following, it is convenient to write Equation 1.2 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta_0} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nUnder normally distributed and homoskedastic error terms, \\(\\varepsilon_i,\\) we have that \\[\n\\begin{align}\n\\underset{(n\\times 1)}{\\varepsilon} &\\sim \\mathcal{N}_n\\left(0, \\sigma_0^2I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y-X\\beta_0)|X &\\sim \\mathcal{N}_n\\left(0, \\sigma^2_0I_n\\right).\n\\end{align}\n\\] That is, for each \\(i=1,\\dots,n,\\) we have that \\[\n\\begin{align}\n(Y_i-X_i^T\\beta_0)|X_i &\\sim \\mathcal{N}\\left(0, \\sigma^2_0\\right)\\\\[2ex]\n\\Rightarrow\\quad\nY_i|X_i &\\sim \\mathcal{N}\\left(X_i^T\\beta_0, \\sigma^2_0\\right)\n\\end{align}\n\\tag{1.3}\\]\n\n\n\n\n\n\n\nUnder Equation 1.3, we have \\[\nf(Y_i|X_i;\\beta_0^T,\\sigma_0^2)=\n\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i^T\\beta_0)^2}{2\\sigma_0^2}\\right),\n\\] where \\[\n\\theta_0=(\\beta_0^T,\\sigma_0^2)^T\\in\\mathbb{R}^K\\times\\mathbb{R}_{&gt;0}\n\\] denotes the \\(((K+1)\\times 1)\\) dimensional unknown parameter vector.\nThis allows us to setup the likelihood function, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\beta^T,\\sigma^2)\n& =\\prod_{i=1}^n f(Y_i|X_i;\\beta^T,\\sigma^2)\\\\[2ex]\n& =\\prod_{i=1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i^T\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n& =\\left(\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\right)^{n}\\exp\\left(-\\frac{\\sum_{i=1}^n (Y_i-X_i^T\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n%& =\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)\\\\[2ex]\n& =(2\\pi)^{-n/2} \\cdot (\\sigma^2)^{-n/2}\\cdot  \\exp\\left(-\\frac{(Y-X\\beta)^T(Y-X\\beta)}{2\\sigma^2}\\right),\\\\[2ex]\n\\end{align*}\n\\]   and the log-likelihood function, \\[\n\\begin{align*}\n\\ell_n(\\beta^T,\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)^T(Y-X\\beta).\n\\end{align*}\n\\] \nTaking first derivatives gives \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta^T,\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X^TY + X^TX\\beta)\\\\[2ex]\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta^T,\\sigma^2)}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)^T(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)^T(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}}\\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)^T(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] Putting the above derivative functions into one column vector yields the \\(((K+1)\\times 1)\\)-dimensional gradient called score function in ML-theory: \\[\n\\nabla\\ell_n(\\theta^T)=\n\\left(\\begin{matrix}\n\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta^T,\\sigma^2)\\\\\n\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta^T,\\sigma^2)\n\\end{matrix}\\right)\n\\tag{1.4}\\]\n\n\n\n\n\n\n\nDefinition 1.4 (Score Function) More generally, let \\(\\ell_n(\\theta)\\) denote the log-likelihood function evaluated at a \\(p\\)-dimensional parameter vector \\(\\theta=(\\theta_1,\\dots,\\theta_p)^T.\\)\nThen the \\((p\\times 1)\\) dimensional gradient \\[\n\\nabla\\ell_n(\\theta^T)=\\left(\\begin{matrix}\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_1}(\\theta^T)\\\\ \\vdots\\\\\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_p}(\\theta^T)\n  \\end{matrix}\n  \\right)\n\\] is called the score-function.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe score function is random, since it depends on the random sample. For a given set of observed data, we compute one realization of the score function.\nAt the true parameter vector \\(\\theta_0\\in\\mathbb{R}^p,\\) the score function satisfies \\[\n\\mathbb{E}\\left(\\dfrac{\\partial \\ell_n}{\\partial \\theta_j}(\\theta_0^T)\\right)=0\n\\] for all \\(j=1,\\dots,p;\\) i.e.  \\[\n\\mathbb{E}\\left(\\nabla\\ell_n(\\theta^T_0)\\right)=\\left(\\begin{matrix}\n  \\mathbb{E}\\left(\\dfrac{\\partial \\ell_n}{\\partial \\theta_1}(\\theta^T_0)\\right)\\\\ \\vdots\\\\\n  \\mathbb{E}\\left(\\dfrac{\\partial \\ell_n}{\\partial \\theta_p}(\\theta^T_0)\\right)\n  \\end{matrix}\n  \\right) = \\underset{(p\\times 1)}{0}\n\\] We prove this below in Section 1.4.\n\n\nSetting the score function in Equation 1.4 equal to zero yields a system of \\(K+1\\) equations with \\(K+1\\) unknowns, which identifies the ML-estimators \\((\\hat\\beta_{ML}\\) and \\(s_{ML}^2),\\) \\[\n\\begin{align*}\n&\\nabla\\ell_n(\\hat\\theta^T_{ML})\n=\n\\left(\\begin{matrix}\n\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\hat\\beta^T_{ML},s_{ML}^2)\\\\\n\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\hat\\beta^T_{ML},s_{ML}^2),\n\\end{matrix}\\right)\\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow &\n\\left(\\begin{matrix}\n- \\dfrac{1}{s_{ML}^2}(-X^TY + X^TX\\hat\\beta_{ML})\\\\\n-\\frac{n}{2 s_{ML}^2}+\\left[\\frac{1}{2}(Y-X\\hat\\beta_{ML})^T(Y-X\\hat\\beta_{ML})\\right]\\frac{1}{\\left(s_{ML}^2\\right)^{2}}\n\\end{matrix}\\right)\n\\overset{!}{=} \\underset{((K+1)\\times 1)}{0}\n\\end{align*}\n\\] and which we can solve for the maximum likelihood estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}.\\)\nSolving for \\(\\hat\\beta_{ML}:\\) \\[\n\\begin{align*}\n%& \\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\hat\\beta_{ML}',s^2_{ML}) \\overset{!}{=}0\\\\[2ex]\\Leftrightarrow\\quad\n& - \\dfrac{1}{s_{ML}^2}(-X^TY + X^TX\\hat\\beta_{ML})  \\overset{!}{=}0\\\\[2ex]\n\\Rightarrow\\quad & \\hat\\beta_{ML}=(X^TX)^{-1}X^TY\\\\[2ex]\n\\end{align*}\n\\] Solving for \\(s^2_{ML}:\\) \\[\n\\begin{align*}\n%& \\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\hat\\beta_{ML}',s^2_{ML}) \\overset{!}{=}0\\\\[2ex]\\Leftrightarrow\\quad\n&-\\frac{n}{2 s_{ML}^2}+\\left[\\frac{1}{2}(Y-X\\hat\\beta_{ML})^T(Y-X\\hat\\beta_{ML})\\right]\\frac{1}{\\left(s_{ML}^2\\right)^{2}}  \\overset{!}{=}0\\ \\\\[2ex]\n\\Rightarrow\\quad  &\ns_{ML}^2 =\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})^T(Y-X\\hat\\beta_{ML})\\\\[2ex]\n&\\phantom{s_{ML}^2}=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2,\n\\end{align*}\n\\] where \\(\\hat\\varepsilon_i = Y_i - X_i^T\\hat{\\beta}_{ML}.\\)\nObservations:\n\n\\(\\hat\\beta_{ML}\\) equals the OLS estimator \\(\\hat\\beta=(X^TX)^{-1}X^TY.\\)  Since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\n\\(s_{ML}^2\\) differs from the unbiased variance estimator \\(s_{UB}^2=\\frac{1}{n-K}\\hat{\\varepsilon}_i^2.\\)\n\n\n1.3.1 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\n\n\n\n\n\n\nComputing the Asymptotic Variance\n\n\n\nTo compute the asymptotic variance of the ML-estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML},\\) we need to\n\ncompute the Hessian matrix (i.e. all second partial derivatives) of \\(\\ell_n\\) and evaluate it at the true parameter values \\(\\beta_0\\) and \\(\\sigma_0,\\)\n\ntake the expectation of this Hessian matrix and multiply it by \\(-1/n\\), which gives us the Fisher Information matrix.\nInverting the Fisher information matrix gives the asymptotic variance expression.\n\n\n\n\nPartial second derivatives with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta^T_0,\\sigma^2_0)}    \n&= - \\dfrac{1}{\\sigma^2_0}(-X^TY + X^TX\\beta_0)\\\\[2ex]\n\\Rightarrow\\quad\n\\underset{(K\\times K)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)}\n&= - \\dfrac{1}{\\sigma^2_0}(X^TX)\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{1}{\\sigma^2_0} \\mathbb{E}(X^TX)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{n}{\\sigma^2_0} \\Sigma_{X^TX}\\right)\\\\[2ex]\n&=  \\dfrac{1}{\\sigma^2_0} \\Sigma_{X^TX},\n\\end{align*}\n\\] where\n\\[\n\\mathbb{E}\\left(X^TX\\right)\n=\\mathbb{E}\\left(\\sum_{i=1}^nX_iX_i^T\\right)\n=n\\underbrace{\\mathbb{E}\\left(X_iX_i^T\\right)}_{=:\\Sigma_{X^TX}} = n\\Sigma_{X^TX}.\n\\]\nSecond derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0)}\n&=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2}\\frac{(Y-X\\beta_0)^T(Y-X\\beta_0)}{\\left(\\sigma^2_0\\right)^{2}}\\\\[2ex]\n\\Rightarrow\\quad\\underset{(1\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0)}\n&=\\frac{n}{2 \\left(\\sigma^2_0\\right)^2}-\\dfrac{(Y-X\\beta_0)^T(Y-X\\beta_0)}{\\left(\\sigma^2_0\\right)^{3}} \\\\[2ex]\n&=\\frac{n}{2\\sigma_0^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^6_0} \\\\[2ex]\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma_0^4}-\\frac{\\mathbb{E}\\left(\\sum_{i=1}^n\\varepsilon_i^2\\right)}{\\sigma_0^6} \\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma_0^4}-\\frac{n\\sigma_0^2}{\\sigma^6_0}\\right)\\\\[2ex]\n&=\\left(-\\frac{1}{2\\sigma^4_0}+\\frac{1}{\\sigma^4_0}\\right)\\\\[2ex]\n&=\\frac{1}{2\\sigma^4_0}\\\\[2ex]\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta^T_0,\\sigma^2_0)}    \n&= - \\dfrac{1}{\\sigma^2_0}(-X^TY + X^TX\\beta_0)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2_0}X^T(Y - X\\beta_0)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2_0}X^T\\varepsilon\\\\[2ex]\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\Rightarrow\\quad\n\\underset{(K\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta \\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0)}\n=   \\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)\\right)^T\n& = -\\frac{X^T\\varepsilon}{\\sigma_0^4}\\\\\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&\\;\\;\\;\\;-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\left(\\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta^T_0,\\sigma^2_0)\\right)\\right)^T\\\\[2ex]\n&=\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X^T\\varepsilon)}{\\sigma^4_0}\\\\[2ex]\n&=\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(\\mathbb{E}(X^T\\varepsilon|X))}{\\sigma^4_0}\\\\[2ex]\n&=\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X^T\\mathbb{E}(\\varepsilon|X))}{\\sigma^4_0}\\\\[2ex]\n&=\\frac{1}{n}\\cdot 0=0,\n\\end{align*}\n\\] since \\(\\mathbb{E}(\\varepsilon|X)=0\\) is an \\((n\\times 1)\\) zero vector.\nCollecting the above results, allows us to write down the expression for \\((-1/n)\\) times the expectation of the Hessian matrix of \\(\\ell_n,\\) evaluated at the true parameter values \\(\\theta_0=(\\beta_0^T,\\sigma_0)^T,\\) which yields the Fisher Information (Matrix):\n\\[\n\\begin{align*}\n\\mathcal{I}(\\theta_0) &:=\\; -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta^T_0,\\sigma^2_0)\\right)\\\\[2ex]\n&=\n-\\frac{1}{n}\\cdot \\mathbb{E}\n\\left[\\begin{array}{cc}\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\sigma^2 }(\\beta^T_0,\\sigma^2_0)\\right)\\\\\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0) \\right)\n\\end{array}\\right]\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2_0}\\Sigma_{X^TX}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4_0}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nDefinition 1.5 (Fisher Information Matrix) The matrix \\[\n\\mathcal{I}(\\theta_0) := -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\theta_0)\\right)\n\\] is called Fisher Information Matrix.\n\n\n\n\n\nAsymptotic Variance and Fisher Information Matrix\nThe asymptotic variance of the MLE \\[\n\\hat{\\theta}_{ML}=\\left(\\begin{array}{c}\\hat\\beta_{ML,n} \\\\ s_{ML,n}^2\\end{array}\\right)\n\\] is given by the inverse of the Fisher information matrix evaluated at the true parameter values \\(\\beta_0\\) and \\(\\sigma^2_0.\\) \\[\n\\begin{align*}\n&AVar\\left(\\begin{array}{c}\\hat\\beta_{ML,n} \\\\ s_{ML,n}^2\\end{array}\\right)\n=\\lim_{n\\to\\infty} n\\cdot Var\\left(\\begin{array}{c}\\hat\\beta_{ML,n} \\\\ s_{ML,n}^2\\end{array}\\right)\\\\[2ex]\n&=\\left(\\mathcal{I}(\\beta^T_0,\\sigma^2_0)\\right)^{-1}\\\\[2ex]\n&=\\left(-\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta^T_0,\\sigma^2_0)\\right)\\right)^{-1}\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0)\\right)\\\\\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta^T}(\\beta^T_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta^T_0,\\sigma^2_0) \\right)\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2_0}\\Sigma_{X^TX}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4_0}}\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\sigma^2_0\\Sigma_{X^TX}^{-1}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{2\\sigma^4_0}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\nThat is, \\[\n\\begin{align*}\nAVar\\left(\\begin{array}{c}\\hat\\beta_{ML,n} \\\\ s_{ML,n}^2\\end{array}\\right)\n&=\\lim_{n\\to\\infty} n\\cdot Var\\left(\\begin{array}{c}\\hat\\beta_{ML,n} \\\\ s_{ML,n}^2\\end{array}\\right)\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\sigma^2_0\\Sigma_{X^TX}^{-1} & 0 \\\\[2ex]\n0 & \\ 2\\sigma^4_0\n\\end{array}\\right].\n\\end{align*}\n\\tag{1.5}\\]\nEquation 1.5 means that \\[\n\\begin{align*}\n\\underset{(K\\times K)}{Var\\left(\\begin{array}{c}\\hat\\beta_{ML,n} \\\\ s_{ML,n}^2\\end{array}\\right)}\n&\\approx\n\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{n}\\,\\sigma^2_0\\Sigma_{X^TX}^{-1}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{n}\\,2\\sigma^4_0}\n\\end{array}\\right],\n\\end{align*}\n\\tag{1.6}\\] where the approximation is good for largish sample sizes \\(n.\\)\n\n\nOf course, the variance expressions in Equation 1.5 and Equation 1.6, respectively, contain unknown quantities and are, therefore, not directly usable in practice. However, we can plug in estimates for the unknown quantities; namely \\[\ns_{ML}^2                         \\quad\\text{for}\\quad \\sigma^2_0\n\\] and \\[\nS_{X^TX}^{-1}=\\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i^T\\right)^{-1} \\quad \\text{for}\\quad \\Sigma_{X^TX}^{-1}.\n\\]\nThis leads to estimators for the variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\)  \\[\n\\begin{align}\n\\widehat{Var}(\\hat{\\beta}_{ML})\n=\\frac{1}{n}\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=\\frac{1}{n} \\,s_{ML}^2 S_{X^TX}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\sum_{i=1}^nX_i X_i^T\\right)^{-1}\\\\[2ex]\n\\widehat{Var}(s^2_{ML})\n=\\frac{1}{n}\\widehat{AVar}(s^2_{ML})\n&=\\frac{1}{n}2\\left(s_{ML}^2\\right)^2.\n\\end{align}\n\\]\n\n\n\n\n\n1.3.2 Asymptotic Distribution and Single Parameter Testing\nIt follows from asymptotic maximum likelihood theory (see Section 1.4) that the probability distribution of the vector of parameter estimates \\[\n\\begin{pmatrix}\n\\hat\\beta_{ML,n}\\\\\n\\hat s_{ML,n}^2\n\\end{pmatrix}\n\\] can be approximated (for largish \\(n\\)) by the following multivariate normal distribution \\[\n\\begin{align*}\n\\sqrt{n}\\left(\n\\hat\\theta_{ML,n}-\\theta_0\n\\right)\n&\\to_d\n\\mathcal{N}_{p}\\left(\n0,\n\\left(\\mathcal{I}(\\beta^T_0,\\sigma^2_0)\\right)^{-1}\n\\right)\\\\[2ex]\n\\sqrt{n}\\left(\n\\begin{pmatrix}\n\\hat\\beta_{ML,n}\\\\\n\\hat s_{ML,n}^2\n\\end{pmatrix}-\n\\begin{pmatrix}\n\\beta_0\\\\\n\\sigma_0^2\n\\end{pmatrix}\\right)\n&\\to_d\n\\mathcal{N}_{K+1}\\left(\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix},\n\\begin{pmatrix}\n\\sigma_0^2\\Sigma_{X^TX}^{-1} & 0\\\\\n0 & 2\\sigma_0^4\n\\end{pmatrix}\n\\right)\\\\[2ex]\n\\Leftrightarrow\n\\begin{pmatrix}\n\\hat\\beta_{ML,n}\\\\\n\\hat s_{ML,n}^2\n\\end{pmatrix}\n&\\overset{a}{\\sim}\n\\mathcal{N}_{K+1}\\left(\n\\begin{pmatrix}\n\\beta_0\\\\\n\\sigma_0^2\n\\end{pmatrix},\n\\begin{pmatrix}\n\\frac{1}{n}\\sigma_0^2\\Sigma_{X^TX}^{-1} & 0\\\\\n0 & \\frac{1}{n}2\\sigma_0^4\n\\end{pmatrix}\n\\right)\n\\end{align*}\n\\]\nPlugging-in estimators for the unknown variance components, i.e.\n\n\\(\\frac{1}{n}\\,s_{ML}^2 S_{X^TX}^{-1}\\quad\\) for \\(\\quad\\frac{1}{n}\\,\\sigma_0^2 \\Sigma_{X^TX}^{-1}\\)\n\nand\n\n\\(\\frac{1}{n}2\\left(s_{ML}^2\\right)^2\\quad\\) for \\(\\quad\\frac{1}{n}2\\sigma_0^4\\)\n\nallows using this asymptotic normality result in testing.\n\nSingle Parameter Testing\nFor instance, we can do a single-parameter test for\n\\[\n\\begin{align*}\nH_0\\colon \\beta_{0,k} & =   \\beta_{0,k}^{(0)}\\\\\nH_1\\colon \\beta_{0,k} &\\neq \\beta_{0,k}^{(0)},\\\\\n\\end{align*}\n\\] where \\(\\beta_{0,k}^{(0)}\\) denotes the null-hypothetical value (typically, \\(\\beta_{0,k}^{(0)}=0\\)), using the Wald statistic \\[\nT_{\\beta_{0,k},n}=\\frac{\\hat\\beta_{ML,k} - \\beta_{0,k}^{(0)}}{\\sqrt{s_{ML}^2 \\frac{1}{n}\\left[S_{X^TX}^{-1}\\right]_{(k,k)}}}\\overset{H_0}{\\to_d}\n\\mathcal{N}(0,1)\\quad \\text{as}\\quad n\\to\\infty.\n\\] Testing: We reject \\(H_0,\\) if the observed value \\(T_{\\beta_{0,k},n,obs}\\) is in absolute values larger than the \\((1-\\alpha/2)\\)-qantile of the standard normal distribution, \\[\n|T_{\\beta_{0,k},n,obs}| &gt; z_{1-\\alpha/2},\n\\] where \\(\\alpha\\in(0,1)\\) denotes the chosen significance level, such as \\(\\alpha=0.01.\\)\nLikewise, for \\[\n\\begin{align*}\nH_0\\colon \\sigma_0^2 & = (\\sigma_0^{(0)})^2\\\\\nH_1\\colon \\sigma_0^2 &\\neq (\\sigma_0^{(0)})^2,\\\\\n\\end{align*}\n\\] where \\((\\sigma_0^{(0)})^2\\) denotes the null-hypothetical value, using the Wald statistic \\[\nT_{\\sigma_0,n}=\\frac{s_{ML}^2 - (\\sigma_0^{(0)})^2}{\\sqrt{\\frac{1}{n}2\\left(s_{ML}^2\\right)^2}}\\overset{H_0}{\\to_d}\n\\mathcal{N}(0,1)\\quad \\text{as}\\quad n\\to\\infty.\n\\]\nTesting: We reject \\(H_0,\\) if the observed value \\(T_{\\sigma_{0},n,obs}\\) is in absolute values larger than the \\((1-\\alpha/2)\\)-qantile of the standard normal distribution, \\[\n|T_{\\sigma_{0},n,obs}| &gt; z_{1-\\alpha/2},\n\\] where \\(\\alpha\\in(0,1)\\) denotes the chosen significance level, such as \\(\\alpha=0.01.\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "href": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "title": "1  Maximum Likelihood",
    "section": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators\nIn the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume a random sample\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where \\(X\\in\\mathbb{R}\\) is a univariate random variable with density function \\[f(x;\\theta_0),\n\\] where the true (unknown, univariate) parameter \\(\\theta_0\\in\\Theta\\) is an interior point of a compact parameter interval \\[\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\n\\] Note: \\(\\theta_0\\) is an “interior point” of \\(\\Theta\\) if \\(\\theta_l&lt;\\theta_0&lt;\\theta_u.\\)\nMoreover, we consider the following setup.\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i;\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i;\\theta)\n\\]\nMaximum-likelihood estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_n=\\arg\\max_{\\theta\\in\\Theta}\\ell_n(\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\ell_n'(\\hat\\theta_n)=0\\quad\\text{and}\\quad\\ell_n''(\\hat\\theta_n)&lt;0\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x;\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x;\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x;\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x;\\theta)dx\n\\end{align*}\n\\] for all \\(\\theta\\in\\Theta.\\)\n\n\n\n\n\n\n\nAn example that fits into the above setup is the density of the exponential distribution \\[\nf(x;\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x &lt; 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown rate parameter \\(\\theta&gt;0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x;\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\n\nThe derivation of the asymptotic distribution of the ML estimator, \\(\\hat\\theta_n,\\) relies on an expansion of the derivative of the log-likelihood function, \\[\n\\ell_n'(\\cdot),\n\\] around \\(\\theta_0\\) (see Equation 1.7 below) using the Mean Value Theorem (Theorem 1.2).\n\n\n\n\n\n\n\nTheorem 1.2 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\n\n\n\nBy the Mean Value Theorem (Theorem 1.2), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\tag{1.7}\\] for some \\(\\psi_n\\) between \\(\\hat{\\theta}_n\\) and \\(\\theta_0;\\) i.e.\n\n\\(\\psi_n\\in(\\theta_0,\\hat{\\theta}_n)\\quad\\) if \\(\\quad\\theta_0&lt;\\hat{\\theta}_n\\)\n\\(\\psi_n\\in(\\hat{\\theta}_n,\\theta_0)\\quad\\) if \\(\\quad\\hat{\\theta}_n&lt;\\theta_0\\)\n\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation 1.7, this implies that \\[\n\\overbrace{\\ell_n'(\\hat{\\theta}_n)}^{=0}=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] \\[\n\\Rightarrow\\quad \\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0).\n\\tag{1.8}\\]\n\nEquation 1.8 provides a useful expression that allows us to derive the asymptotic distribution of \\(\\hat{\\theta}_n.\\)\n\nLet us derive two (Equation 1.9 and Equation 1.10) auxiliary results. Note that necessarily, \\[\n\\int f(x;\\theta)dx=1\n\\] for all possible values of \\(\\theta\\in\\Theta,\\) since \\(f\\) is a density function.\nTherefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\underbrace{\\int f(x;\\theta)dx}_{=1}&=\\frac{\\partial}{\\partial \\theta}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int \\frac{\\partial}{\\partial \\theta}f(x;\\theta)dx\n=\\frac{\\partial}{\\partial \\theta}\\int f(x;\\theta)dx\n=0\n\\tag{1.9}\\] for all \\(\\theta\\in\\Theta.\\)\nLikewise, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\underbrace{\\int f(x;\\theta)dx}_{=1}&=\\frac{\\partial^2}{\\partial \\theta^2}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int \\frac{\\partial^2}{\\partial \\theta^2}f(x;\\theta)dx\n=\\frac{\\partial^2}{\\partial \\theta^2}\\int f(x;\\theta)dx\n=0\n\\tag{1.10}\\] for all \\(\\theta\\in\\Theta.\\)\nUsing Equation 1.9, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta_0)=\\frac{1}{n}\\underbrace{\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)}_{\\ell_n'(\\theta_0)}\n\\] is asymptotically normal. This is done in the following by checking the three conditions for applying the Lindeberg-Lévy central limit theorem.\nFirstly, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\n\\] is taken over i.i.d. random variables: \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_1;\\theta_0),\\dots,\\frac{\\partial}{\\partial \\theta} \\ln f(X_n;\\theta_0)\\overset{\\text{i.i.d.}}{\\sim}\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\n\\]\nSecondly, for the mean one gets: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X;\\theta_0)}{f(X;\\theta_0)}\\right)\\quad[\\text{chain rule}]\\\\[2ex]\n&=\\int \\frac{\\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)}\n{f(x;\\theta_0)}f(x;\\theta_0)dx\\quad[\\text{Def. of $\\mathbb{E}$}]\\\\[2ex]\n&=\\int \\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)dx\\\\[2ex]\n&=0,\n\\end{align*}\n\\tag{1.11}\\] where the last step follows from Equation 1.9.\nThirdly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X;\\theta_0)}{f(X;\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\\\\\n&=\\frac{1}{n}\\mathcal{I}(\\theta_0),\n\\end{align*}\n\\tag{1.12}\\] where the simplification of the variance expression to a second moment expression follows from Equation 1.11. \n\nWe can write the last expression in Equation 1.12 using the Fisher Information \\(\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0))\\) since below in Equation 1.14 we’ll see that \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n& =-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)) = \\mathcal{I}(\\theta_0).\n\\end{align*}\n\\]\n\n\nThus, we can apply the Lindeberg-Lévy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\theta_0)-\\overbrace{\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)}^{=0}}{\\sqrt{\\frac{1}{n}\\mathcal{I}(\\theta_0)} } = \\frac{\\ell_n'(\\theta_0)}{\\sqrt{n\\mathcal{I}(\\theta_0)} } \\to_d \\mathcal{N}(0,1)\n\\] as \\(n\\to\\infty.\\)\nBy our mean value expression in Equation 1.8 \\[\n\\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] we thus have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\mathcal{I}(\\theta_0)}}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\;\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{1.13}\\] The \\(\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\)-part in Equation 1.13 is our object of interest.\nThe further analysis requires us to study the asymptotic behavior of\n\\[\n-\\frac{1}{n}\\ell_n''(\\psi_n)\n\\] which will help us to understand the behavior of \\(\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\) in Equation 1.13.\n\n\n\n\n\n\nImportant\n\n\n\nBefore we consider \\(-\\frac{1}{n}\\ell_n''(\\psi_n),\\) we begin with studying the mean and the variance of the simpler statistic \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n\\] with \\(\\psi_n\\) replaced by \\(\\theta_0.\\)\n\n\nFirst, the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i;\\theta_0)\\right) f(X_i;\\theta_0)-\\frac{\\partial}{\\partial\\theta}f(X_i;\\theta_0)\\frac{\\partial}{\\partial\\theta} f(X_i;\\theta_0)}{\\left(f(X_i;\\theta_0)\\right)^2}\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}\\right)^2  \n\\right).\n\\end{align*}\n\\] Taking the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) yields that \\[\n\\begin{align*}\n\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}\n{f(X;\\theta_0)}+\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}\n{f(X;\\theta_0)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)+\\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}\n{f(X;\\theta_0)}\\right)^2\\right)\n\\end{align*}\n\\] Equation 1.10 implies that \\(\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)=0\\) thus \\[\n\\begin{align*}\n\\underbrace{\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)}_{=\\mathcal{I}(\\theta_0)}\n&=0 + \\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n\\end{align*}\n\\] \\[\n\\Rightarrow \\qquad\n\\mathcal{I}(\\theta_0) =\n\\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n\\tag{1.14}\\]\nThis means that \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n\\] is an unbiased estimator of the Fisher information \\(\\mathcal{I}(\\theta_0).\\)\nMoreover, Equation 1.14 provides an alternative expression for the Fisher information \\(\\mathcal{I}(\\theta_0).\\)\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nFor multivariate (\\(p\\)-dimensional) parameters \\(\\theta_0,\\) the Fisher information \\(\\mathcal{I}(\\theta_0)=(-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta_0)\\right)\\) becomes the (\\(p\\times p\\)) Fisher information matrix (see Section 1.3.1).\n\n\nSecond, the variance of variance of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=Var\\left(-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X;\\theta_0)\\right)}_{=\\texttt{constant}}\\\\[2ex]\n&=\\frac{1}{n}\\texttt{constant},\n\\end{align*}\n\\] which implies that \\[\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results for \\(-\\frac{1}{n}\\ell_n''(\\theta_0),\\) we can write down the Mean Squared Error (MSE) of the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) of \\(\\mathcal{I}(\\theta_0):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[2ex]\n&=\n\\mathbb{E}\\left(\\left(-\\frac{1}{n}\\ell_n''(\\theta_0) -\\mathcal{I}(\\theta_0)\\right)^2\\right)\\\\[2ex]\n&=\\underbrace{\\left(\\operatorname{Bias}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\right)^2}_{=0}+Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[3ex]\n&=Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) is a mean square consistent estimator, i.e. \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_{m.s.} \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta_0)\\) is also a (weakly) consistent estimator, i.e.  \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_p \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\n🤔 Remember, we wanted to study \\(-\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation 1.13 not \\(-\\frac{1}{n}\\ell_n''(\\theta_0).\\) Studying \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) was only the simpler thing to do.\nLuckily, we are actually close now.\n\n\nNext, we use that ML estimators \\(\\hat\\theta_n\\) are (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\nExample: Our results in Section 1.3 imply, for instance, that the ML estimator \\(\\hat{\\beta}_n\\) is consistent for \\(\\beta.\\)\n\nSince \\(\\psi_n\\) is a mean value between \\(\\theta_0\\) and \\(\\hat{\\theta}_n\\) (Equation 1.7), consistency of \\(\\hat{\\theta}_n\\) implies that \\[\n\\psi_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have by the continuous mapping theorem that \\[\n\\begin{align}\n-\\frac{1}{n}\\ell_n''(\\psi_n) & \\to_p \\phantom{-}\\mathcal{I}(\\theta_0)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow\\qquad\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)&\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} \\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\]\nNow, using Slutsky’s theorem, we can connect the above consistency result with the asymptotic normality result in Equation 1.13 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)}_{\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d \\mathcal{N}\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right),\n\\end{align*}\n\\tag{1.15}\\] where \\(1/\\mathcal{I}(\\theta_0)\\) is the asymptotic variance of the ML estimator \\(\\hat{\\theta}_n\\) and equals the inverse of the (here scalar valued) Fisher information \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)).\n\\]\nEquation 1.15 is the asymptotic normality result we aimed for.\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nThe above arguments can easily be generalized to multivariate (\\(p\\)-dimensional) parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{I}(\\theta_0)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d \\mathcal{N}_p\\left(0, \\mathcal{I}(\\theta_0)^{-1}\\right),\n\\] where \\(\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}\\left(H_{\\ell_n}(\\theta_0)\\right)\\) is the \\((p\\times p)\\) Fisher information matrix with \\(H_{\\ell_n}(\\theta_0)\\) denoting the Hesse matrix of \\(\\ell_n(\\cdot)\\) evaluated at \\(\\theta_0.\\)\n\n\n\n\n\n\n\n\nML-Theory and Machine learning\n\n\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (Kirkpatrick et al. (2017)).\nFisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (Martens (2020)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#invariance-property-of-the-ml-estimator",
    "href": "Ch1_MaximumLikelihood.html#invariance-property-of-the-ml-estimator",
    "title": "1  Maximum Likelihood",
    "section": "1.6 Invariance Property of the ML-Estimator",
    "text": "1.6 Invariance Property of the ML-Estimator\nSuppose that a distribution has the parameter \\(\\theta_0,\\) but we are interested in finding an estimator of a function of \\(\\theta_0,\\) say \\[\n\\eta_0=\\tau(\\theta_0).\n\\] The invariance property of ML-estimators says that if \\(\\hat{\\theta}_n\\) is the ML-estimator of \\(\\theta_0,\\) then \\(\\tau(\\hat{\\theta}_n)\\) is the ML-estimator of \\(\\eta_0=\\tau(\\theta_0).\\)\n\nOne-to-One Functions\nLet the function \\[\n\\eta = \\tau(\\theta)\n\\] be a one-to-one function. That is, for each value of \\(\\theta\\) there is a unique value of \\(\\eta\\) and vice versa.\nImportant property of one-to-one functions: A one-to-one function \\(\\eta = \\tau(\\theta)\\) possesses a well-defined inverse \\[\n\\theta=\\tau^{-1}(\\eta).\n\\]\n\n\n\n\n\n\nExample:\n\n\n\nFor instance, the functions \\[\n\\begin{align*}\n\\eta = \\tau(\\theta) & = \\theta + 3\n\\quad\\Rightarrow\\quad \\theta = \\tau^{-1}(\\eta) = \\eta -3 \\\\[2ex]\n\\eta = \\tau(\\theta) & = \\theta/5  \n\\quad\\Rightarrow\\quad \\theta = \\tau^{-1}(\\eta) = 5 \\eta\n\\end{align*}\n\\] are one-to-one functions. However, for instance, the functions \\[\n\\begin{align*}\n\\tau(\\theta) & = \\sin(\\theta)\\\\[2ex]\n\\tau(\\theta) & = \\theta^2  \n\\end{align*}\n\\] are not one-to-one functions.\n\n\nIn this one-to-one case, it is easily seen that it makes no difference whether we maximize the likelihood function as a function of \\(\\theta\\) or as a function of \\(\\eta = \\tau(\\theta)\\)—in each case we get the same answer.\nThe likelihood function of \\(\\tau(\\theta),\\) written as a function of \\(\\eta,\\) is given by \\[\n\\begin{align*}\n\\mathcal{L}^*(\\eta)\n&= \\prod_{i=1}^n f\\big(X_i;\\tau^{-1}(\\eta)\\big)\n= \\mathcal{L}\\big(\\;\\overbrace{\\tau^{-1}(\\eta)}^{=\\theta}\\;\\big)\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n  \\sup_{\\eta}  \\mathcal{L}^*(\\eta)\n= \\sup_{\\eta}  \\mathcal{L}\\big(\\;\\overbrace{\\tau^{-1}(\\eta)}^{=\\theta}\\;\\big)\n= \\sup_{\\theta}\\mathcal{L}\\big(\\theta\\big).\n\\end{align*}\n\\] Thus, the maximum of \\(\\mathcal{L}^*(\\eta)\\) is attained at \\[\n\\eta=\\tau(\\theta)=\\tau(\\hat\\theta_n),\n\\] showing that the ML-estimator of \\(\\tau(\\theta_0)\\) is \\(\\tau(\\hat\\theta_n).\\)\n\n\n\n\n\n\n\nMore general (not one-to-one) functions\n\n\n\nIn many cases, however, this simply version of the invariance of ML-estimators is not useful since many functions of interest are not one-to-one.\nLuckily, the invariance property of the ML-estimator also holds for functions that are not one-to-one; see Chapter 7 in Casella and Berger (2001).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#exercises",
    "href": "Ch1_MaximumLikelihood.html#exercises",
    "title": "1  Maximum Likelihood",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nProgram the Newton-Raphson algorithm for a numerical computation of the ML estimate \\(\\hat\\theta\\) of the parameter \\(\\theta=P(\\text{Coin}=\\texttt{HEAD})\\) in our coin toss example of this chapter. Replicate the results shown in Table 1.1.\n\n\nExercise 2.\nAssume an i.i.d. random sample \\(X_1,\\dots,X_n\\) from an exponential distribution, i.e. the density of \\(X_i\\) is given by \\[\nf(x;\\theta_0)=\n\\left\\{\\begin{array}{ll}\\theta_0\\exp(-\\theta_0 x),&x\\geq 0\\\\0,&x&lt;0\\end{array}\\right.\n\\] with \\(\\theta_0&gt;0,\\) where \\[\n\\mu:=\\mathbb{E}(X_i)=\\frac{1}{\\theta_0}\n\\] and \\[\nVar(X_i)=\\frac{1}{\\theta_0^2}.\n\\]\n\nWhat is the log-likelihood function for the i.i.d. random sample \\(X_1,\\dots,X_n\\)?\nDerive the maximum likelihood (ML) estimator \\(\\hat\\theta_n\\) of \\(\\theta_0.\\)\nFrom maximum likelihood theory we know that \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\to_d \\mathcal{N}\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right).\n\\] Derive the expression for the Fisher information \\(\\mathcal{I}(\\theta_0).\\) Use the Fisher information to give the explicit formula for the asymptotic distribution of \\(\\hat\\theta_n\\).\n\n\n\nExercise 3.\n\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Unif}(0,\\theta_0).\\)\n\nWhat is the likelihood function?\nWhat is the maximum likelihood estimator of \\(\\theta_0\\)?\n\n\n\nExercise 4.\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Poisson}(\\lambda_0).\\) That is \\(X\\sim f\\) with density function \\[\nf(x;\\lambda_0) = \\frac{\\lambda_0^x \\exp(-\\lambda_0)}{x!}.\n\\]\n\nFind the maximum likelihood estimator, \\(\\hat{\\lambda},\\) of \\(\\lambda_0.\\)\nLet \\(0&lt;\\lambda_0\\leq 4.\\) Find the maximum likelihood estimator, \\(\\hat{P}(X=4),\\) of \\(P(X=4).\\)\n\n\n\nExercise 5.\nShow that the Newton-Raphson algorithm converges; i.e. that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\] under the setup outlined in Section 1.2.2.\nTip: Use the first-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with explicit reminder term \\(R\\) given by \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + R,\n\\end{align*}\n\\] where \\[\nR=\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\] for a mean-value \\(\\xi_{(m)}\\) between \\(\\theta_{(m)}\\) and \\(\\theta_{root}\\). This is called the Lagrange form of the Taylor-Series reminder term and follows from the Mean-Value Theorem 1.2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#references",
    "href": "Ch1_MaximumLikelihood.html#references",
    "title": "1  Maximum Likelihood",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCasella, George, and Roger Berger. 2001. Statistical Inference. 2nd ed. Duxbury.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, et al. 2017. “Overcoming Catastrophic Forgetting in Neural Networks.” Proceedings of the National Academy of Sciences 114 (13): 3521–26.\n\n\nMartens, James. 2020. “New Insights and Perspectives on the Natural Gradient Method.” The Journal of Machine Learning Research 21 (1): 5776–5851.\n\n\nWhite, Halbert. 1982. “Maximum Likelihood Estimation of Misspecified Models.” Econometrica, 1–25.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html",
    "href": "Ch2_EMAlgorithmus.html",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "",
    "text": "Possible Applications of the EM-Algorithm and Gaussian mixture distributions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "href": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models",
    "text": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models\nAs a data example we use the palmerpenguins data (Horst, Hill, and Gorman (2020)).\nThese data are from surveys of penguin populations on the Palmer Archipelago (Antarctic Peninsula). Penguins are often difficult to distinguish from one another (Figure 2.1). We will try to find groupings in the penguin data (fin length) using a Gaussian mixture distribution. To be able to estimate such mixing distributions, we introduce the EM algorithm.\n\n\n\n\n\n\nFigure 2.1: Cheeky penguin in action.\n\n\n\nThe following code chunk prepares the dataset and visualizes it using a histogram.\n\n## Select a color palette\ncol_v &lt;- RColorBrewer::brewer.pal(n = 3, name = \"Set2\")\n\n## Preparing the data:\npenguins &lt;- palmerpenguins::penguins %&gt;%  # penguin data\n  tidyr::as_tibble() %&gt;%                  # 'tibble'-dataframe\n  dplyr::filter(species!=\"Adelie\") %&gt;%    # remove penguin species 'Adelie' \n  droplevels() %&gt;%                        # remove the non-used factor level\n  tidyr::drop_na() %&gt;%                    # remove NAs\n  dplyr::mutate(species = species,        # rename variables \n                flipper = flipper_length_mm) %&gt;% \n  dplyr::select(species, flipper)         # select variables \n\n##  \nn      &lt;- nrow(penguins)                  # sample size (n=187)\n\n## Pulling out the variable 'penguin_species':\npenguin_species &lt;- dplyr::pull(penguins, species)\n\n## Pulling out the variable 'penguin_flipper':\npenguin_flipper &lt;- dplyr::pull(penguins, flipper)\n\n## Plot\n## Histogramm:\nhist(x = penguin_flipper, freq = FALSE, \n     xlab=\"Flipper-Length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039))\n## Stipchart hinzufügen:\nstripchart(x = penguin_flipper, method = \"jitter\", \n           jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[3],.5), \n           bg=alpha(col_v[3],.5), cex=1.3, add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution: We do not use the grouping label (penguin species)\n\n\n\nWe have the information about the different penguin species (penguin_species) but in the following we pretend not to know this information.\nWe want to determine the group memberships (species) by cluster analysis on the basis of the fin lengths (penguin_flipper) alone.\nAfterward we can use the grouping label in penguin_species to check how good our cluster analysis is.\n\n\n\nClustering using Gaussian Mixture Distributions\nAt the end of this chapter (Section 2.4), we’ll be able to\n\nEstimate the Gaussian mixture distribution using the EM algorithm (see Figure 2.2).\nAssign each penguin \\(i\\) to one of the two species groups according to its predictor value \\(X_i\\) (flipper length).\n\n\n\n\n\n\n\n\n\nFigure 2.2: Cluster analysis based on a mixture distribution with two weighted normal distributions.\n\n\n\n\n\nFigure Figure 2.2 shows the result of a cluster analysis based on a mixture distribution of two weighted normal distributions. Cluster result (training data): 95% of the penguins could be correctly assigned - based only on their flipper lengths.\nThe following R codes can be used to reproduce the above cluster analysis (using the R function Mclust of the package mclust) and Figure 2.2:\n\n## mclust R package:\n## Cluster analysis using Gaussian mixture distributions\nsuppressMessages(library(\"mclust\"))\n\n## Number of Groups\nG &lt;- 2 \n\n## Schätzung des Gaußschen Mischmodells (per EM Algorithmus)\n## und Clusteranalyse\nmclust_obj &lt;- mclust::Mclust(data       = penguin_flipper, \n                             G          = G, \n                             modelNames = \"V\", \n                             verbose    = FALSE)\n\n# summary(mclust_obj)\n# str(mclust_obj)\n\n## estimated group assignment \nclass &lt;- mclust_obj$classification\n\n## Fraction of correct group assignments:\n# cbind(class, penguin_species)\nround(sum(class == as.numeric(penguin_species))/n, 2)\n\n## estimated means of the two Gaussian distributions\nmean_m &lt;- t(mclust_obj$parameters$mean)\n\n## estimated variances (and possibly covariances) \ncov_l  &lt;- list(\"Cov1\" = mclust_obj$parameters$variance$sigmasq[1], \n               \"Cov2\" = mclust_obj$parameters$variance$sigmasq[2])\n\n## estimated mixture weights (prior-probabilities) \nprop_v &lt;- mclust_obj$parameters$pro\n\n## evaluating the Gaussian mixture density function \nnp      &lt;- 100 # number of evaluation points\nxxd     &lt;- seq(min(penguin_flipper)-3, \n               max(penguin_flipper)+5, \n               length.out = np)\n## mixture density\nyyd     &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +\n           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n## single densities\nyyd1    &lt;- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]\nyyd2    &lt;- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n\n## Plot\nhist(x = penguin_flipper, xlab=\"Flipper length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))\nlines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))\nlines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)\nlines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)\nabline(v=203.1, lty=3)\nstripchart(penguin_flipper[class==1], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)\nstripchart(penguin_flipper[class==2], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)\n\nBut coding without understanding is dangerous. We’ll learn the statistical method implemented in the R function Mclust it in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "href": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions",
    "text": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions\n\n2.2.1 Gaussian Mixture Models (GMM)\nWe denote a random variable \\(X\\) that follows a Gaussian mixture distribution as \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)\n\\]\nThe corresponding density function of a Gaussian mixture distribution is defined as follows: \\[\nf_{GMM}(x;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)=\\sum_{g=1}^G\\pi_{g,0}\\varphi(x;\\mu_{g,0},\\sigma_{g,0})\n\\tag{2.1}\\]\n\nWeights: \\(\\boldsymbol{\\pi}_0=(\\pi_{1,0},\\dots,\\pi_{G,0})\\) with \\(\\pi_{g,0}&gt;0\\) and \\(\\sum_{g=1}^G\\pi_{g,0}=1\\)\nMeans: \\(\\boldsymbol{\\mu}_0=(\\mu_{1,0},\\dots,\\mu_{G,0})\\) with \\(\\mu_{g,0}\\in\\mathbb{R}\\)\nStandard deviations: \\(\\boldsymbol{\\sigma}_0=(\\sigma_{1,0},\\dots,\\sigma_{G,0})\\) with \\(\\sigma_{g,0}&gt;0\\)\nNormal density of group \\(g=1,\\dots,G\\): \\[\n\\varphi(x;\\mu_{g,0},\\sigma_{g,0})=\\frac{1}{\\sqrt{2\\pi}\\sigma_{g,0}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_{g,0}}{\\sigma_{g,0}}\\right)^2\\right)\n\\]\nUnknown parameters: \\(\\boldsymbol{\\pi}_0\\), \\(\\boldsymbol{\\mu}_0\\) und \\(\\boldsymbol{\\sigma}_0\\)\n\n\n\n2.2.2 Maximum Likelihood (ML) Estimation\nWe could try estimating the unknown parameters\n\n\\(\\boldsymbol{\\pi}_0=(\\pi_{1,0},\\dots,\\pi_{G,0})\\),\n\\(\\boldsymbol{\\mu}_0=(\\mu_1,\\dots,\\mu_G)\\) and\n\\(\\boldsymbol{\\sigma}_0=(\\sigma_1,\\dots,\\sigma_G)\\)\n\nusing the maximum likelihood method.\n\nI’ll say it right away: This attempt generally fails.\n\n\nBasic Idea of ML Estimation\n\nAssumption: The data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) is a realization of a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\n\\] with \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0).\n\\]\n\n\n\n\nEstimate the unknown parameters \\(\\boldsymbol{\\pi}_0\\), \\(\\boldsymbol{\\mu}_0\\) and \\(\\boldsymbol{\\sigma}_0\\) by maximizing the log-likelihood function \\[\n\\begin{align*}\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n=&\\sum_{i=1}^n\\ln\\left(f_{GMM}(X_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\\n=&\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\n\\end{align*}\n\\tag{2.2}\\] taking into account the following maximization constraints:\n\n\n\n\n\n\n\nMaximization constraints\n\n\n\nThe maximization must take into account the parameter constraints in Equation 2.1; namely,\n\n\\(\\sigma_g&gt;0\\) and\n\\(\\pi_g&gt;0\\) for all \\(g=1,\\dots,G\\) such that\n\\(\\sum_{g=1}^G\\pi_g=1\\).\n\n\n\nThe maximizing parameter values \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) and \\(\\hat{\\boldsymbol{\\sigma}}\\) are the ML-Estimators:\n\\[\n(\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\]\n😒 Problems with singularities in numerical solutions: If one tries to solve the above maximization problem numerically with the help of the computer, one will quickly notice that the results are highly unstable, implausible and not very trustworthy. The reason for these unstable estimates are problems with singularities.\nFor real GMMs (i.e. GMMs with more than one group \\(G&gt;1\\)), problems with singularities occur very easily during a numerical maximization. This happens, for instance, when one of the normal distribution components tries to describe only one single data point \\(X_i.\\) This leads to a Gaussian density function centered around the single data point \\(X_i\\) such that\n\\[\n\\varphi(X_i;{\\color{red}\\mu_g=X_i},\\sigma_g),\n\\] where \\[\n\\sigma_g\\to 0.\n\\] This degenerating situation leads to very large density function values, \\[\n\\varphi(X_i;\\mu_g=X_i,\\sigma_g)\\to\\infty\\quad\\text{for}\\quad \\sigma_g\\to 0,\n\\] and thus “maximizes” the log-likelihood in an undesirable way (see Figure 2.3).\n\n\n\n\n\n\n\n\nFigure 2.3: Gaussian density with \\(\\mu_g=X_i\\) for \\(\\sigma_g\\to 0\\).\n\n\n\n\n\nSuch undesirable, trivial maximization solutions typically lead to implausible, non-useful estimation results.\n🤓 Analytic solution: It is a bit tedious, but one can maximize the log-likelihood function of the GMM (see Equation 2.2) analytically. If you do this, you will get the following expressions: \\[\n\\begin{align*}\n\\hat\\pi_g&=\\frac{1}{n}\\sum_{i=1}^np_{ig,0},\\quad\n\\hat\\mu_g=\\sum_{i=1}^n\\frac{p_{ig,0}}{\\left(\\sum_{j=1}^np_{jg}\\right)}X_i\\\\[2ex]\n\\hat\\sigma_g&=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig,0}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(X_i-\\hat\\mu_g\\right)^2},\n\\end{align*}\n\\tag{2.3}\\] where \\[\np_{ig,0}=\\frac{\\pi_{g,0}\\varphi(X_i;\\mu_{g,0},\\sigma_{g,0})}{f_{GMM}(X_i;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)}\n\\tag{2.4}\\] for \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G\\).\n\n\n\n\n\n\nNote\n\n\n\nDeriving the expressions for \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) and \\(\\hat{\\pi}_g\\) in Equation 2.3 requires multiple applications of the chain rule, product rule, etc., as well as an application of the Lagrange multiplier method for optimization under side-constraints to take into account the maximization constraints.\n\n\n\n🙈 However: The above expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) and \\(\\hat\\sigma_g\\) depend themselves on the unknown parameters\n\n\\(\\boldsymbol{\\pi}_0=(\\pi_{1,0},\\dots,\\pi_{G,0})\\),\n\\(\\boldsymbol{\\mu}_0=(\\mu_{1,0},\\dots,\\mu_{G,0})\\) and\n\\(\\boldsymbol{\\sigma}_0=(\\sigma_{1,0},\\dots,\\sigma_{G,0})\\),\n\nbecause the probabilities \\(0\\leq p_{ig,0}\\leq 1\\) (defined in Equation 2.4) depend on these unknown parameters.\nThus, the expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation 2.3 do not allow a direct estimation of the unknown parameters \\(\\pi_{g,0}\\), \\(\\mu_{g,0}\\), and \\(\\sigma_{g,0}\\).\n\n\n\n\n\n\nPrior and Posterior Probabilities\n\n\n\n\nPrior Probability: The probability \\[\n\\pi_{g,0} = P\\left(\\text{Penguine $i$ belongs to group}\\;g\\right)\n\\]\nin Equation 2.4 is called the prior probability. The prior probability \\(\\pi_g\\) is the probability that a penguine \\(i\\), from which we know nothing about its flipper length, belongs to group \\(g\\).\nPosterior Probability: The conditional probability. \\[\np_{ig,0} = P\\left(\\text{Penguine $i$ belongs to group}\\;g|X_i\\right)\n\\] in Equation 2.4 is called the posterior probability. The posterior probability \\(p_{ig,0}\\) is the probability that penguin \\(i\\) with flipper length \\(X_{i}\\) belongs to group \\(g.\\)\n\nWe’ll discuss the prior and the posterior probability in more detail in Section 2.3.2.\n\n\n🥳 Solution: The EM Algorithm\n\n\n\n2.2.3 The EM Algorithm for GMMs\nThe expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation 2.3 suggest a simple iterative maximum likelihood estimation procedure; namely, an alternating estimation of\n\nthe posterior probabilities \\[\n\\begin{align*}\np_{ig,0}\n&= P\\left(\\text{Penguine $i$ belongs to group}\\;g|X_i\\right)\\\\[2ex]\n& = \\frac{\\pi_{g,0}\\varphi(X_i;\\mu_{g,0},\\sigma_{g,0})}{f_{GMM}(X_i;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)}\n\\end{align*}\n\\] for \\(i=1,\\dots,n,\\) and \\(g=1,\\dots,G,\\) using estimates \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\), and \\(\\hat{\\boldsymbol{\\sigma}}\\) of the unknown \\(\\boldsymbol{\\pi}_0\\), \\(\\boldsymbol{\\mu}_0\\), and \\(\\boldsymbol{\\sigma}_0.\\)\n\nand of\n\nthe model parameters \\[\n(\\pi_{g,0},\\mu_{g,0},\\sigma_{g,0}) \\quad\\text{for}\\quad g=1,\\dots,G\n\\] using \\(\\hat{\\pi}_g\\), \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) defined in Equation 2.3 using estimates \\(\\hat{p}_{ig}\\) for the unknown \\(p_{ig,0}.\\)\n\n\n\n\n\n\n\nGiven an estimate \\(\\hat{p}_{ig},\\) you can compute \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\) using Equation 2.3.\nGiven the estimates \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g),\\) you can compute \\(\\hat{p}_{ig}\\) using Equation 2.4.\n\n\n\nThe EM Algorithm:\n\nInitialization:  Set starting values \\[\n\\begin{align*}\n&\\boldsymbol{\\hat{\\pi}}^{(0)}=(\\hat{\\pi}_1^{(0)},\\dots,\\hat{\\pi}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\mu}}^{(0)}=(\\hat{\\mu}_1^{(0)},\\dots,\\hat{\\mu}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\sigma}}^{(0)}=(\\hat{\\sigma}_1^{(0)},\\dots,\\hat{\\sigma}_G^{(0)})\n\\end{align*}\n\\]\nLoop:  For \\(r=1,2,\\dots\\)\n\n(Expectation) Compute: \\[\\hat{p}_{ig}^{(r-1)}=\\frac{\\hat{\\pi}_g^{(r-1)}\\varphi(X_i;\\hat{\\mu}^{(r-1)}_g,\\hat{\\sigma}_g^{(r-1)})}{f_{GMM}(X_i;\\boldsymbol{\\hat{\\pi}}^{(r-1)},\\boldsymbol{\\hat{\\mu}}^{(r-1)},\\boldsymbol{\\hat{\\sigma}}^{(r-1)})}\\]\n(Maximization) Compute:\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^n\\hat{p}_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{\\hat{p}_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^n\\hat{p}_{jg}^{(r-1)}\\right)}X_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{\\hat{p}_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^n\\hat{p}_{jg}^{(r-1)}\\right)}\\left(X_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence:  Stop if the value of the maximized log-likelihood function, \\(\\ell(\\boldsymbol{\\hat{\\pi}}^{(r)},\\boldsymbol{\\hat{\\mu}}^{(r)},\\boldsymbol{\\hat{\\sigma}}^{(r)};\\mathbf{x})\\), does not change any more substantially. Or, equivalently, if the parameter estimates do not change any more substantially.\n\nThe above pseudocode is implemented in the following code chunk:\n\nlibrary(\"MASS\")\nlibrary(\"mclust\")\n\n## data:\nx &lt;- cbind(penguin_flipper) # data [n x d]-dimensional. \nd &lt;- ncol(x)                # dimension (d=1: univariat)\nn &lt;- nrow(x)                # sample size\nG &lt;- 2                      # number of groups\n\n## further stuff \nllk       &lt;- matrix(NA, n, G)\np         &lt;- matrix(NA, n, G)  \nloglikOld &lt;- 1e07\ntol       &lt;- 1e-05\nit        &lt;- 0\ncheck     &lt;- TRUE \n\n## EM Algorithm\n\n## 1. Starting values for pi, mu and sigma:\npi    &lt;- rep(1/G, G)              # naive pi \nsigma &lt;- array(diag(d), c(d,d,G)) # varianz = 1\nmu    &lt;- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )\n\nwhile(check){\n  \n  ## 2.a Expectation step\n  for(g in 1:G){\n    p[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  p &lt;- sweep(p, 1, STATS = rowSums(p), FUN = \"/\")\n  \n  ## 2.b Maximization step \n  par   &lt;- mclust::covw(x, p, normalize = FALSE)\n  mu    &lt;- par$mean\n  sigma &lt;- par$S\n  pi    &lt;- colMeans(p)\n  \n  ## 3. Check convergence \n  for(g in 1:G) {\n    llk[,g] &lt;- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  loglik &lt;- sum(log(rowSums(llk))) # current max. log-likelihood value\n  ##\n  diff      &lt;- abs(loglik - loglikOld)/abs(loglik) # rate of change\n  loglikOld &lt;- loglik\n  it        &lt;- it + 1\n  ## Check whether rate of change is still large enough (&gt; tol)?\n  check     &lt;- diff &gt; tol\n}\n\n## Estimation results:\nresults &lt;- matrix(c(pi, mu, sqrt(sigma)), \n                  nrow = 3, \n                  ncol = 2, \n                  byrow = TRUE,\n                  dimnames = list(c(\"weights\", \n                                    \"means\", \n                                    \"standard-deviations\"),\n                                  c(\"group 1\", \n                                    \"group 2\"))) \n##\nresults %&gt;% round(., 2)\n\n                    group 1 group 2\nweights                0.69    0.31\nmeans                216.20  194.26\nstandard-deviations    7.32    6.27",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "href": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n  =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n2.3.1 Data Completion\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((z_{i1},z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G>2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) for each \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G.\\) However, it must hold true that each \\(i\\) belongs to only one group, i.e. \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g. member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_1,\\dots,\\pi_G\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g\\varphi(x;\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot;\\mu_g,\\sigma_g)\\) in the total distribution \\(f_{GMM}\\). Therefore, we know that, on average, \\[\n\\pi_g\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(z_{ig}\\) as a unobserved realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] and with the restriction that \\[\n\\sum_{g=1}^GZ_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.$\n\\]\nNote that the condition \\(\\sum_{g=1}^GZ_{ig}=1\\) implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\[\n\\pi_g = P(Z_{ig}=1)\n\\] If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n“With probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\[\np_{ig}=P(Z_{ig}=1|X_i=x_i)\n\\] If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem (see Equation 2.5) which leads to the posterior probability: \n\n“With probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).”\n\n\n\n\n\n\n\n\nBayes’ Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere’s the Expectation  in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig}\\) is the Expectation-step of the EM algorithm (Section 2.2.3).\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e. the predictors like the flipper lengths), \\[\n\\mathbf{x}=(x_1,\\dots,x_n),\n\\] we had also observed the group assignments, \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{g=1}^G\\left(\\sum_{i=1}^nz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\right)\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation 2.2), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize: We can effectively maximize separately for each group \\(g,\\) which then involves only a single normal density function and not a too flexible mixture of density functions. This simplifies the maximization problem considerably, since the normal density belongs to the exponential family (see Section 1.4) which is not the case for the normal mixture distribution.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version (in \\(\\mathbf{Z}\\)) of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, we can calculate the conditional expected value (using Equation 2.6), which motivates the “Expectation”-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathbb{E}_{\\boldsymbol{\\theta}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\end{align*}\n\\] where \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) is used to denote the parameter vector.\nThe following EM algorithm differs only in notation from the version already discussed in Section 2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step.\nThe chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. \n\nInitialization: Set starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\np_{ig}^{(r-1)} = \\frac{\\pi_g^{(r-1)} \\varphi(x_i;\\mu_g^{(r-1)},\\sigma_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\]\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\n\\] does not change anymore substantially."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "href": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.4 (Unsupervised) Classification",
    "text": "2.4 (Unsupervised) Classification\nThe problem of predicting a discrete random variable \\(Y\\) (i.e. the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] where \\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and \\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n(Note: Above we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.)\nExample: Predict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g. passing the exam (\\(Y=1\\)) vs. failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g. previous gradings, number of hours studied, etc.)\nA classification rule is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\nIf there are learning/training data with group-labels \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] that can be used to estimate \\(h,\\) it’s called a supervised classification (computer science: supervised learning) problem.\nIf there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it’s called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e. the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] From Bayes’s theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where \\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes’ classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)>\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition 2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown quantities and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation 2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G>1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard — particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure 2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\nFigure 2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure 2.2.\nBut well, the average penguin probably doesn’t care about the EM Algorithm.\n\n\n\nFigure 2.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#exercises",
    "href": "Ch2_EMAlgorithmus.html#exercises",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\n\nConsider \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\sim\\text{Bernoulli}(p_0).\\) Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nNow let \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\) is a Bernoulli mixture random variable with parameters \\(p_{g,0}\\) and prior probabilities \\(\\pi_{g,0}\\), \\(g=1,\\dots,G\\). Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nLet \\[\n(Z_{i1},\\dots,Z_{iG})\\in\\{0,1\\}^G\n\\] denote the vector of latent group indicator random variables with \\[\nZ_{i1}+\\dots + Z_{iG}=1\n\\] and \\[\nP(Z_{ig}=1)=\\pi_{g,0},\\quad g=1,\\dots,G.\n\\] Thus, the realization \\(Z_i=(0,1,0,\\dots,0)\\) means that the \\(i\\)th observation comes from the \\(2\\)nd Bernoulli distribution \\(\\text{Bernoulli}(p_{2,0})\\). Write the expressions of the (log) likelihood functions \\(\\tilde{\\mathcal{L}}(\\mathbf{p},\\boldsymbol{\\pi})\\) and \\(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi})\\) that take into account the latend group indicator random variables.\nWrite down the expression for the posterior probability \\[\n\\mathfrak{p}_{ig,0} = \\mathbb{E}_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}(Z_{ig}| X_i)=P_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}(Z_{ig}=1 | X_i).\n\\]\nDerive the conditional expectation of \\(\\tilde\\ell,\\) given \\(X_1,\\dots,X_n,\\) \\[\\mathbb{E}_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi})\\big|X_1,\\dots,X_n\\right).\n\\]\nMaximize \\(\\mathbb{E}_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi})\\big|X_1,\\dots,X_n\\right)\\) with respect to \\(p_g\\) for \\(g=1,\\dots,G.\\)\nMaximize \\(\\mathbb{E}_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi})\\big|X_1,\\dots,X_n\\right)\\) with respect to \\(\\pi_g\\) for \\(g=1,\\dots,G\\) such that \\(\\sum_{g=1}^G\\pi_g=1.\\)\nSketch the EM-Algorithm",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#references",
    "href": "Ch2_EMAlgorithmus.html#references",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B 39 (1): 1–22.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nTraiberman, Sharon. 2019. “Occupations and Import Competition: Evidence from Denmark.” American Economic Review 109 (12): 4260–4301.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#cluster-analysis",
    "href": "Ch2_EMAlgorithmus.html#cluster-analysis",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.4 Cluster Analysis",
    "text": "2.4 Cluster Analysis\nThe problem of predicting a discrete random variable \\(Y\\) (i.e. the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\\overset{\\text{i.i.d.}}{\\sim}(Y,X)\n\\] where\n\n\\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and\n\\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n\n\nNote: Above in Section 2.3, we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.\n\n\n\n\n\n\n\nExample\n\n\n\nPredict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g. passing the exam (\\(Y=1\\)) vs. failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g. previous gradings, number of hours studied, etc.)\n\n\nA classification rule \\(h\\) is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\n\n\n\n\n\n\n(Un-)Supervised Classification\n\n\n\n\nSupervised Classification: If there are learning/training data with group-labels \\[\n({\\color{red}Y_1},X_1),\\dots,({\\color{red}Y_n},X_n)\n\\] that can be used to estimate \\(h,\\) it’s called a supervised classification (computer science: supervised learning) problem.\nUnsupervised Classification/Cluster Analysis: If there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it’s called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n\n\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e. the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] That is, the conditonal mean \\(m(x)\\) is the posterior probability, i.e. the probability of \\(Y=1\\) given \\(X=x.\\)\nFrom Bayes’ theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where\n\n\\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\n\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density or a Gaussian mixture.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\nNote\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes’ classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)&gt;P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)&gt;\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition 2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown \\(m(x)=P(Y=1|X=x)\\) and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests, neural nets, etc.\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation 2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G&gt;1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), and \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard — particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian (mixture) density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x_i|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure 2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\n\n\n\nFigure 2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\n\nThe final estimation result replicates Figure 2.2.\nThe average penguin probably doesn’t care about our EM Algorithm.\n\n\n\n\n\n\nFigure 2.5: Penguin research on the limit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#sec-TrueViewEM",
    "href": "Ch2_EMAlgorithmus.html#sec-TrueViewEM",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (“latent”) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were not able to properly maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n  =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n2.3.1 Data Completion\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((Z_{i1},Z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(Z_{i1},Z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G&gt;2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(Z_{i1},\\dots,Z_{ig},\\dots,Z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(Z_{ig}\\) can take values \\(Z_{ig}\\in\\{0,1\\},\\) for each \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G.\\) However, it must hold true that each \\(i\\) belongs to only one group, i.e. \\[\n\\sum_{g=1}^G Z_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^G Z_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g. member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(Z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_{1,0},\\dots,\\pi_{G,0}\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)=\\sum_{g=1}^G\\pi_{g,0}\\varphi(x;\\mu_{g,0},\\sigma_{g,0}),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot;\\mu_{g,0},\\sigma_{g,0})\\) in the total distribution \\(f_{GMM}.\\) Therefore, we know that, on average, \\[\n\\pi_{g,0}\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(Z_{ig}\\) as a unobserved realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_{g,0}\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_{g,0})\\\\[2ex]\n\\end{align*}\n\\] and with the restriction that \\[\n\\sum_{g=1}^GZ_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\nNote that the condition \\(\\sum_{g=1}^GZ_{ig}=1\\) implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\[\n\\pi_{g,0} = P(Z_{ig}=1)\n\\] If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability:\n\n\n“With probability \\(\\pi_{g,0}=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).”\n\n\nPosterior Probability \\[\np_{ig,0}=P(Z_{ig}=1|X_i)\n\\] If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes’ Theorem (see Equation 2.5) which leads to the posterior probability:\n\n\n“With probability \\(p_{ig,0}=P(Z_{ig}=1|X_i),\\) penguin \\(i\\) with flipper length \\(X_i\\) belongs to group \\(g.\\)”\n\n\n\n\n\n\n\n\nBayes’ Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig,0}\n&=\\overbrace{P(Z_{ig}=1|X_i)}^{\\text{Posterior Prob.}}\\\\[2ex]\n&\\equiv P_{\\theta_0}(Z_{ig}=1|X_i)\\\\[2ex]\n&=\\frac{\\pi_{g,0}\\varphi(X_i;\\mu_{g,0},\\sigma_{g,0})}{f_{GMM}(X_i;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{Prior Prob.}}\\varphi(X_i;\\mu_{g,0},\\sigma_{g,0})}{f_{GMM}(X_i;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere’s the Expectation  in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig,0}\\) are conditional means: \\[\n\\begin{align*}\np_{ig,0}\n&=P_{\\theta_0}(Z_{ig}=1|X_i)\\\\[2ex]\n&= 1 \\cdot P_{\\theta_0}(Z_{ig}=1|X_i)+0\\cdot P_{\\theta_0}(Z_{ig}=0|X_i)\\\\[2ex]\n&= \\mathbb{E}_{\\theta_0}(Z_{ig}|X_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig,0}\\) is the Expectation-step of the EM algorithm (Section 2.2.3).\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e., the predictors such as the flipper lengths), \\[\n(X_1,\\dots,X_n),\n\\] we could also observe the group assignments, \\[\n(Z_{11},\\dots,Z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(X_i;\\mu_g,\\sigma_g)\\right)^{Z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n&=\\sum_{i=1}^n\\sum_{g=1}^G Z_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{g=1}^G\\left(\\sum_{i=1}^n Z_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\right)\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation 2.2), the new log-likelihood function \\(\\tilde\\ell\\) is easy to maximize: We can effectively maximize separately for each group \\(g,\\) which then involves only a single normal density function and not a too flexible mixture of normal density functions. By contrast to the normal mixture distribution, the normal density belongs to the exponential family (see Section 1.4) for which maximum likelihood is known to work very well. This stabilizes the maximization problem.\nUnfortunately, we only observe realizations of \\(X_{i}=X_{i,obs},\\) but we do not observe realizations of the group assignment variables \\[\n(Z_{11},\\dots,Z_{nG}).\n\\]\nIdea: Substitute the unobservable \\(Z_{ig},\\) by their best (in the mean square sense) prediction \\(\\hat{Z}_{ig,0}\\); namely, the conditional mean (using Equation 2.5 and Equation 2.6) \\[\n\\begin{align*}\n\\hat{Z}_{ig,0}=p_{ig,0}\n&=\\mathbb{E}_{\\boldsymbol{\\theta}_0}\\left(Z_{ig}|X_i\\right)\\\\[2ex]\n&=\\overbrace{P_{\\boldsymbol{\\theta}_0}(Z_{ig}=1|X_i)}^{\\text{Posterior Prob.}}\\\\[2ex]\n&=\\frac{\\pi_{g,0}\\varphi(X_i;\\mu_{g,0},\\sigma_{g,0})}{f_{GMM}(X_i;\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)}.\n\\end{align*}\n\\] To actually compute these predictions, we need to substitute the unknown parameters \\(\\boldsymbol{\\theta}_0=(\\boldsymbol{\\pi}_0,\\boldsymbol{\\mu}_0,\\boldsymbol{\\sigma}_0)\\) by their estimates \\(\\boldsymbol{\\hat{\\theta}}=(\\boldsymbol{\\hat{\\pi}},\\boldsymbol{\\hat{\\mu}},\\boldsymbol{\\hat{\\sigma}}),\\) \\[\n\\begin{align*}\n\\hat{Z}_{ig}=\\hat{p}_{ig}\n&=\\mathbb{E}_{\\boldsymbol{\\hat{\\theta}}}\\left(Z_{ig}|X_i\\right)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_{g}\\varphi(X_i;\\hat{\\mu}_{g},\\hat{\\sigma}_{g})}{f_{GMM}(X_i;\\boldsymbol{\\hat{\\pi}},\\boldsymbol{\\hat{\\mu}},\\boldsymbol{\\hat{\\sigma}})}\n\\end{align*}\n\\]\nEffectively, this means to compute the conditional mean of \\(\\tilde{\\ell}\\) given \\(X_1,\\dots,X_n,\\) which motivates the “Expectation”-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\hat{\\theta}})=\\mathbb{E}_{\\boldsymbol{\\hat{\\theta}}}\\Big(\\tilde{\\ell}(\\;\\underbrace{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}_{\\boldsymbol{\\theta}}\\;)|X_1,\\dots,X_n\\Big)\\\\[2ex]\n&\\qquad=\\mathbb{E}_{\\boldsymbol{\\hat{\\theta}}}\\left(\\sum_{i=1}^n\\sum_{g=1}^G Z_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\Big|X_1,\\dots,X_n\\right)\\\\[2ex]\n&\\qquad=\\sum_{i=1}^n\\sum_{g=1}^G\\mathbb{E}_{\\boldsymbol{\\hat{\\theta}}}\\left(Z_{ig}|X_i\\right)\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&\\qquad=\\sum_{i=1}^n\\sum_{g=1}^G\\;\\;\\;\\;\\;\\;\\hat{Z}_{ig}\\;\\;\\;\\;\\;\\;\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\end{align*}\n\\] where \\(\\boldsymbol{\\hat{\\theta}}=(\\boldsymbol{\\hat{\\pi}},\\boldsymbol{\\hat{\\mu}},\\boldsymbol{\\hat{\\sigma}})\\) denotes the estimated parameter vector used for computing the prediction \\(\\hat{Z}_{ig}\\) of \\(Z_{ig}\\).\nThus, the new log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\hat{\\theta}})\n=\\sum_{i=1}^n\\sum_{g=1}^G \\hat{Z}_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] is the best approximation of the ideal, but infeasible, log-likelihood function \\[\n\\tilde{\\ell}(\\;\\underbrace{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}_{\\boldsymbol{\\theta}}\\;)\n=\\sum_{i=1}^n\\sum_{g=1}^G Z_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\] using an estimate \\(\\boldsymbol{\\hat{\\theta}}=(\\boldsymbol{\\hat{\\pi}},\\boldsymbol{\\hat{\\mu}},\\boldsymbol{\\hat{\\sigma}})\\) for computing the prediction \\(\\hat{Z}_{ig}\\) of the unobserved \\(Z_{ig}.\\)\n\n\n\nThe following EM algorithm differs only in notation from the version already discussed in Section 2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step.\nThe chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. \n\nInitialization: Set starting values \\(\\boldsymbol{\\hat{\\theta}}^{(0)}=(\\boldsymbol{\\hat{\\pi}}^{(0)}, \\boldsymbol{\\hat{\\mu}}^{(0)}, \\boldsymbol{\\hat{\\sigma}}^{(0)}),\\) where \\[\n\\begin{align*}\n&\\boldsymbol{\\hat{\\pi}}^{(0)}=(\\hat{\\pi}_1^{(0)},\\dots,\\hat{\\pi}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\mu}}^{(0)}=(\\hat{\\mu}_1^{(0)},\\dots,\\hat{\\mu}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\sigma}}^{(0)}=(\\hat{\\sigma}_1^{(0)},\\dots,\\hat{\\sigma}_G^{(0)})\n\\end{align*}\n\\]\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n&\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n=\\mathbb{E}_{\\boldsymbol{\\hat{\\theta}}^{(r-1)}}\\Big(\\tilde{\\ell}(\\underbrace{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}_{\\boldsymbol{\\theta}})\\big|X_1,\\dots,X_n\\Big)\\\\[2ex]\n&=\\sum_{i=1}^n\\sum_{g=1}^G\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}\\left(Z_{ig}\\big|X_i\\right)\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{i=1}^n\\sum_{g=1}^G\\;\\;\\;\\;\\;\\;\\hat{Z}_{ig}^{(r-1)}\\;\\;\\;\\;\\;\\;\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(X_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\n\\hat{Z}_{ig}^{(r-1)} =\\hat{p}_{ig}^{(r-1)} = \\frac{\\hat{\\pi}_g^{(r-1)} \\varphi(X_i;\\hat{\\mu}_g^{(r-1)},\\hat{\\sigma}_g^{(r-1)})}{f_{GMM}(X_i;\\boldsymbol{\\hat{\\pi}}^{(r-1)},\\boldsymbol{\\hat{\\mu}}^{(r-1)},\\boldsymbol{\\hat{\\sigma}}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\hat{\\theta}}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n\\end{align*}\n\\] where \\(\\boldsymbol{\\hat{\\theta}}^{(r)}=(\\boldsymbol{\\hat{\\pi}}^{(r)},\\boldsymbol{\\hat{\\mu}}^{(r)},\\boldsymbol{\\hat{\\sigma}}^{(r)}),\\) \\[\n\\begin{align*}\n&\\boldsymbol{\\hat{\\pi}}^{(r)}=(\\hat{\\pi}_1^{(r)},\\dots,\\hat{\\pi}_G^{(r)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\mu}}^{(r)}=(\\hat{\\mu}_1^{(r)},\\dots,\\hat{\\mu}_G^{(r)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\sigma}}^{(r)}=(\\hat{\\sigma}_1^{(r)},\\dots,\\hat{\\sigma}_G^{(r)}),\n\\end{align*}\n\\] with\n\n\\(\\hat{\\pi}_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^n\\hat{p}_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{\\hat{p}_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^n\\hat{p}_{jg}^{(r-1)}\\right)}X_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{\\hat{p}_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^n\\hat{p}_{jg}^{(r-1)}\\right)}\\left(X_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\hat{\\theta}}^{(r)},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n\\] does not change any more substantially; i.e., if \\[\n\\mathcal{Q}(\\boldsymbol{\\hat{\\theta}}^{(r)},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n\\approx\n\\mathcal{Q}(\\boldsymbol{\\hat{\\theta}}^{(r+1)},\\boldsymbol{\\hat{\\theta}}^{(r)}),\n\\] or, equivalently, if \\[\n\\left|\\boldsymbol{\\hat{\\theta}}^{(r+1)} - \\boldsymbol{\\hat{\\theta}}^{(r)}\\right|\\approx 0.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#solutions",
    "href": "Ch1_MaximumLikelihood.html#solutions",
    "title": "1  Maximum Likelihood",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\nBelow I use the same data (one H, four T) that was used to produce the results in Table 1.1 of our script. However, you can produce new data by setting another seed-value different to set.seed(1).\n\ntheta_true &lt;- 0.2    # unknown true theta value\nn          &lt;-  5     # sample size\n\nset.seed(1)\n\n# simulate data: n many (unfair) coin tosses\nx &lt;- sample(x       = c(0,1), \n            size    = n, \n            replace = TRUE, \n            prob    = c(1-theta_true, theta_true)) \n\n## number of heads (i.e., the number of \"1\"s in x)\nN_H &lt;- sum(x)\n\n## First derivative of the log-likelihood function\nLp_fct   &lt;- function(theta, N_H = N_H, n = n){\n    (N_H/theta) - (n - N_H)/(1 - theta)    \n}\n## Second derivative of the log-likelihood function\nLpp_fct   &lt;- function(theta, N_H = N_H, n = n){\n    - (N_H/theta^2) - (n - N_H)/(1 - theta)^2    \n}\n\nt     &lt;- 1e-10   # convergence criterion\ncheck &lt;- TRUE    # check object to stop the while-loop\ni     &lt;- 0       # count iterations\n\n## Initializations \ntheta  &lt;- 0.4     # starting value theta_{(0)}\nh_step &lt;- NULL    # empty value \nLp     &lt;- Lp_fct( theta, N_H=N_H, n=n)\nLpp    &lt;- Lpp_fct(theta, N_H=N_H, n=n)\n\nwhile(check){\n    i         &lt;- i + 1\n    ##\n    h_step_new &lt;- -1 * (Lp_fct(theta[i], N_H=N_H, n=n) / Lpp_fct(theta[i], N_H=N_H, n=n))    \n    h_step     &lt;- c(h_step, h_step_new)\n    theta_new  &lt;- theta[i] + h_step_new\n    Lp_new     &lt;- Lp_fct( theta_new, N_H=N_H, n=n)\n    Lpp_new    &lt;- Lpp_fct(theta_new, N_H=N_H, n=n)\n    ##\n    theta      &lt;- c(theta, theta_new) \n    Lp         &lt;- c(Lp,    Lp_new) \n    Lpp        &lt;- c(Lpp,   Lpp_new) \n    ##\n    if( abs(Lp_fct(theta_new, N_H=N_H, n=n)) &lt; t ){\n      check &lt;- FALSE\n    }\n}\n\nresults           &lt;- cbind(1:length(theta)-1, theta, -Lp/Lpp, Lp)\ncolnames(results) &lt;- c(\"m\", \"theta_m\", \"h_m\", \"Lp(theta_m)\")\nresults\n\n     m   theta_m           h_m   Lp(theta_m)\n[1,] 0 0.4000000 -2.400000e-01 -4.166667e+00\n[2,] 1 0.1600000  3.326733e-02  1.488095e+00\n[3,] 2 0.1932673  6.558924e-03  2.159084e-01\n[4,] 3 0.1998263  1.736356e-04  5.433195e-03\n[5,] 4 0.1999999  1.132731e-07  3.539786e-06\n[6,] 5 0.2000000  4.814638e-14  1.504574e-12\n\n\n\n\nSolutions of Exercise 2.\n\n(a) Log-Likelihood Function\nThe (random) log-likelihood function is given by \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\sum_{i=1}^n \\ln (\\theta\\exp(-\\theta X_i))\\\\\n&=\\sum_{i=1}^n (\\ln \\theta -\\theta X_i)\\\\\n&=n \\ln \\theta -\\sum_{i=1}^n \\theta X_i\n\\end{align*}\n\\]\n\n\n(b) ML-Estimator\nThe (random) ML estimator is defined as \\(\\hat{\\theta}_{n}=\\arg\\max_{\\theta}\\ell_{n}(\\theta)\\). Deriving the ML estimator \\(\\hat\\theta_n\\): \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=n\\frac{1}{\\theta} - \\sum_{i=1}^n X_i\\\\\n\\ell_n'(\\hat\\theta_n)\\overset{!}{=}0\\quad \\Leftrightarrow &\\quad 0=n\\frac{1}{\\hat\\theta_n} - \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad n\\frac{1}{\\hat\\theta_n} = \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad \\hat\\theta_n = \\frac{1}{\\frac{1}{n}\\sum_{i=1}^n X_i}= \\frac{1}{\\bar{X}_n}\n\\end{align*}\n\\]\n\n\n(b) Fisher Information\nThe Fisher information is given by \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell''(\\theta_0)).\n\\] The second derivative of \\(\\ell_n(\\theta)\\) evaluated at \\(\\theta=\\theta_0\\) is given by \\[\n\\ell''_n(\\theta_0)=-n\\frac{1}{\\theta^2_0}.\n\\]\n\nNote: In this example, the second derivative of \\(\\ell_n(\\theta)\\) is so simple that it no longer depends on the random sample \\(X_1, \\dots, X_n.\\)\n\nThus, \\[\n\\begin{align*}\n\\mathcal{I}(\\theta_0)\n&=-\\frac{1}{n}\\mathbb{E}(\\ell''_n(\\theta_0))\\\\[2ex]\n&=-\\frac{1}{n}\\left(-n\\frac{1}{\\theta^2_0}\\right)\\\\[2ex]\n&=\\frac{1}{\\theta^2_0}.\n\\end{align*}\n\\] Therefore, the asymptotic distribution of \\(\\hat\\theta_n\\) is \\[\n\\begin{align*}\n\\sqrt{n}(\\hat\\theta_n-\\theta_0)&\\to_d \\mathcal{N}\\left(0,\\theta^2_0\\right),\\;n\\to\\infty.\n\\end{align*}\n\\]\nSince we do not know the asymptotic variance \\[\n\\lim_{n\\to\\infty}Var\\big(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\big)=\\theta_0^2,\n\\] we need to use a consistent estimator; namely, \\[\n\\hat{\\theta}_n^2 = \\left(\\frac{1}{\\bar{X}_n}\\right)^2 \\approx \\theta^2_0.\n\\] This yields to the following Normal approximation which can then be used for constructing statistical hypothesis tests and confidence intervals, etc: \\[\n\\begin{align*}\n\\sqrt{n}\\big(\\hat\\theta_n - \\theta_0\\big)&\\overset{a}{\\sim}\\mathcal{N}\\left(0,\\hat{\\theta}_n^2\\right)\\\\[2ex]\n\\Leftrightarrow\\qquad\\hat\\theta_n&\\overset{a}{\\sim}\\mathcal{N}\\left(\\theta_0,\\frac{\\hat{\\theta}_n^2}{n}\\right)\n\\end{align*}\n\\] This approximation can be expected to be good/useful for largish sample sizes (i.e. roughly \\(n\\geq 30\\)).\n\nRemark: The above asymptotic normality result coincides with the result obtained by the delta-method.\n\n\n\n\nSolutions of Exercise 3.\n\n(a) Likelihood Function\nRecall that the density function of \\(\\mathcal{Unif}(0,\\theta_0)\\) is \\[\nf(x;\\theta_0)\n=\\left\\{\n\\begin{array}{ll}\n\\frac{1}{\\theta_0} & 0\\leq x\\leq \\theta_0\\\\\n0                  & \\text{otherwise}\\\\\n\\end{array}\n\\right.\n\\] Thus, the likelihood function is \\[\n\\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i;\\theta).\n\\] for \\(\\theta&gt;0.\\)\nNote: If \\(\\theta&lt;X_i\\) for any \\(i=1,\\dots,n,\\) we have that \\[\n\\mathcal{L}_n(\\theta)=0.\n\\] Putting it differently, let \\[\nX_{(n)}=\\max\\{X_1,\\dots,X_n\\}\n\\] denote the \\(n\\)th order-statistic, then \\[\n\\mathcal{L}_n(\\theta)=0\\quad\\text{for all}\\quad \\theta&lt;X_{(n)}.\n\\]\nHowever, for all values of \\(\\theta\\) with \\(X_{(n)} \\leq \\theta,\\) we have that \\[\nf(X_i;\\theta)=\\frac{1}{\\theta}\\quad\\textbf{for all}\\quad i=1,\\dots,n.\n\\] Thus, for all values of \\(\\theta\\) with \\(X_{(n)}\\leq \\theta,\\) \\[\n\\mathcal{L}_n(\\theta)=\\left(\\frac{1}{\\theta}\\right)^n.\n\\]\nSumming up, \\[\n\\mathcal{L}_n(\\theta)\n=\\left\\{\n\\begin{array}{ll}\n\\left(\\frac{1}{\\theta}\\right)^n & \\theta \\geq  X_{(n)}\\\\\n0                               & \\theta &lt; X_{(n)}\\\\\n\\end{array}\n\\right.\n\\tag{1.16}\\]\n\n\n\n(b) Maximum Likelihood Estimator of \\(\\theta_0\\)\n\\(\\mathcal{L}_n(\\theta)\\) is strictly decreasing over the interval \\([X_{(n)},\\infty);\\) see Figure 1.4.\n\nn          &lt;- 20   # sample size\nX_max      &lt;- 0.25\n\ntheta_vec  &lt;- seq(from = 0, \n                  to   = X_max * 1.5, \n                  len  = 100) \nlikelihood_fun &lt;- function(theta, X_max, n){ \n    likelihood                &lt;- 1/(theta^n)\n    likelihood[theta &lt; X_max] &lt;- 0 \n    return(likelihood) \n}\n\nlikelihood_vec &lt;- likelihood_fun(theta = theta_vec,\n                                 X_max = X_max, \n                                 n     = n)\n\nplot(y = likelihood_vec, \n     x = theta_vec, \n     type = \"l\", \n     xlab = expression(theta),\n     ylab = \"Likelihood\", \n     main = \"\")            \naxis(1, at = X_max, labels = expression(X[(n)]))                  \n\n\n\n\n\n\n\nFigure 1.4: Graph of the likelihood function \\(\\mathcal{L}_n(\\theta)\\) given in Equation 1.16.\n\n\n\n\n\nThus, the maximum likelihood estimator of \\(\\theta_0\\) is \\[\n\\begin{align}\n\\hat{\\theta}_{n}\n& =\\arg\\max_{\\theta&gt;0}\\mathcal{L}_n(\\theta)\\\\\n& = X_{(n)}.\n\\end{align}\n\\]\n\n\nSolutions of Exercise 4.\n\n(a) Finding the Maximum Likelihood Estimator of \\(\\lambda_0\\)\n\\[\n\\begin{align}\n\\mathcal{L}_n(\\lambda)\n& = \\prod_{i=1}^n f(X_i;\\lambda)\\\\[2ex]\n& = \\prod_{i=1}^n \\frac{\\lambda^{X_i} \\exp(-\\lambda)}{X_i!} \\\\[2ex]\n& = \\frac{\\lambda^{\\sum_{i=1}^n X_i}  \\exp(-n \\lambda)}{\\prod_{i=1}^n (X_i!)} \\\\[4ex]\n\\ell_n(\\lambda)\n&= \\left(\\sum_{i=1}^n X_i\\right) \\ln(\\lambda) -n\\lambda\\cdot 1 - \\sum_{i=1}^n \\ln(X_i!)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell'_n(\\lambda)\n&= \\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\lambda}  - n\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell''_n(\\lambda)\n&= -\\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\lambda^2} \\leq 0,  \n\\end{align}\n\\] since by the properties of the Poisson distribution \\(X_1,\\dots,X_n\\geq 0\\) and \\(\\lambda&gt;0.\\)\nThus the maximum likelihood estimator of \\(\\lambda_0\\) is given by \\[\n\\begin{align}\n&\\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\hat\\lambda_n}  - n \\overset{!}{=} 0\\\\[2ex]\n\\Rightarrow \\qquad & \\hat \\lambda_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\end{align}\n\\]\n\n\n(b) Finding the Maximum Likelihood Estimator of \\(P(X=4)\\)\nObserve that \\[\n\\begin{align}\nP(X=4) = \\frac{\\lambda_0^4 \\exp(-\\lambda_0)}{4!}.\n\\end{align}\n\\] Thus \\(P(X=4)\\) is a function of \\(\\lambda\\) (see Figure 1.5) \\[\n\\begin{align}\nP(X=4)\\equiv P_\\lambda(X=4) = \\frac{\\lambda^4 \\exp(-\\lambda)}{4!} = \\tau(\\lambda)\n\\end{align}\n\\]\n\nlambda_vec &lt;- seq(from = .0001, to = 15, len = 100)\ng_vec      &lt;- (lambda_vec^4 * exp(-1*lambda_vec))/( factorial(4) )\n\nplot(x = lambda_vec, y = g_vec, \n     type = \"l\", xlab=expression(lambda), ylab=expression(tau(lambda)))\nabline(v = 4)\naxis(1, at = 4)\n\n\n\n\n\n\n\nFigure 1.5: For \\(0\\leq \\lambda \\leq 4\\) the function \\(\\tau(\\lambda)\\) is a one-to-one function.\n\n\n\n\n\n(For \\(0&lt;\\lambda\\leq 4,\\) \\(g(\\lambda)\\) is even a one-to-one mapping.)\nThus, by the invariance property of the maximum likelihood estimator (which also applies to functions \\(\\tau\\) that are not one-to-one) we have \\[\n\\begin{align}\n\\hat{P}(X=4)\\equiv \\hat{P}_{\\hat{\\lambda}_n}(X=4) = \\frac{\\hat{\\lambda}^4_n \\exp(-\\hat{\\lambda}_n)}{4!}\n\\end{align}\n\\] with \\(\\hat{\\lambda}_n=\\frac{1}{n}\\sum_{i=1}^n X_i.\\)\n\n\n\nSolutions of Exercise 5.\nSetup of Section 1.2.2:\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_n';\\) i.e.  \\[\n\\ell_n'(\\theta_{root})=0.\n\\] Let \\[\n\\begin{align*}\ne_{(0)}&=\\theta_{root}-\\theta_{(0)}\\\\[2ex]\ne_{(m)}&=\\theta_{root}-\\theta_{(m)}\n\\end{align*}\n\\] denote the start-value error and the \\(m\\)th step error, respectively.\nLet \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start-value neighborhood around \\(\\theta_{root}.\\)\nLet \\(\\ell_n'\\) be “well behaved” over \\(I;\\) such that\n\n\\(\\ell_n''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\n\\(\\ell_n'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I.\\)\n\nLet \\(\\theta_{(0)}\\) be “close enough;” i.e. let\n\n\\(M|e_{(0)}|&lt;1,\\)\n\nwhere \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_n'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_n''(\\theta)|}\\right)\\geq 0,\n\\]\nIn the following, we show that the Newton-Raphson algorithm converges under this setup.\nBy the first-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with the mean-value form of the remainder term, we have that \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) +\n    \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) +\n    \\frac{1}{2}\\ell'''(\\xi)(\\theta_{root}-\\theta_{(m)})^2\n\\end{align*}\n\\] for some real-valued number \\(\\xi_{(m)}\\) between \\(\\theta_{root}\\) and \\(\\theta_{(m)}.\\)\nSince \\(\\ell'(\\theta_{root})=0,\\) we have that \\[\n\\begin{align*}\n0\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + \\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\end{align*}\n\\] Some rearrangments lead \\[\n\\begin{align*}\n\\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)})\n& = -\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n{\\color{blue}\\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}} + (\\theta_{root}-\\theta_{(m)})\n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\qquad[\\text{dividing by}\\;\\ell''(\\theta_{(m)})]\\\\[2ex]\n\\end{align*}\n\\] Using the update steps of the alrorithm \\[\n\\theta_{(m+1)} = \\theta_{(m)} - \\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}\n\\quad\\Leftrightarrow\\quad\n{\\color{magenta}\\theta_{(m)} - \\theta_{(m+1)}}  = {\\color{blue}\\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}}\n\\] yields \\[\n\\begin{align*}\n{\\color{magenta}\\theta_{(m)} - \\theta_{(m+1)}} + (\\theta_{root}-\\theta_{(m)})\n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad\n\\theta_{root} - \\theta_{(m+1)}  \n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad\ne_{(m+1)}  \n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(e_{(m)})^2\n\\end{align*}\n\\] Taking absolute values (we are not interested in the sign of the approximation errors) yields \\[\n\\begin{align*}\n|e_{(m+1)}|  \n&= \\frac{1}{2} \\; |\\ell'''(\\xi_{(m)})|\\;\\frac{1}{|\\ell''(\\theta_{(m)})|}(e_{(m)})^2\n\\end{align*}\n\\] Considering the worst case within \\(I,\\) leads to the following inequality \\[\n\\begin{align*}\n|e_{(m+1)}|  \n&\\leq  \\overbrace{\\frac{1}{2} \\; \\left(\\sup_{\\theta\\in I}|\\ell'''(\\theta)|\\right)\\;\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell''(\\theta)|}\\right)}^{=M}\\;(e_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad |e_{(m+1)}|  \n&\\leq  M\\;(e_{(m)})^2\\\\[-2ex]\n\\end{align*}\n\\tag{1.17}\\] To show that the Newton-Raphon algorithm converges, we need to show that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\]\nFor \\(0\\leq M\\,|e_{(0)}|&lt;1\\) the inequality in Equation 1.17 becomes a strict inequality \\[\n\\begin{align*}\n&|e_{(1)}|\\leq \\overbrace{M\\;|e_{(0)}|}^{&lt;1}\\,|e_{(0)}|\n\\quad\\Rightarrow\\quad |e_{(1)}|&lt; \\,|e_{(0)}|\\\\[3ex]\n&{\\color{darkgreen}[\\text{Using that $M\\;|e_{(0)}|&lt;1$ and that $|e_{(1)}|&lt;|e_{(0)}|$}]}\\\\[2ex]\n\\Rightarrow\\quad\n& |e_{(2)}|\\leq {\\color{darkgreen}\\overbrace{M\\;|e_{(1)}|}^{&lt;1}}\\,|e_{(1)}|\n\\quad\\Rightarrow\\quad |e_{(2)}|&lt; \\,|e_{(1)}|\\\\[2ex]\n&\\phantom{|e_{(2)}|\\leq \\overbrace{M\\;|e_{(1)}|}^{&lt;1}\\,|e_{(1)}|}\\vdots\\\\[2ex]\n\\Rightarrow\\quad\n& |e_{(m+1)}|&lt; \\,|e_{(m)}| \\quad\\text{for all }m=0,1,2\\dots\n\\end{align*}\n\\] which shows the convergence of the Newton Raphson algorithm. (The inequality in Equation 1.17 implies that the convergence is even quadratic; i.e. very fast.)\nSpecial Case \\(M=0\\):  For \\[\n\\sup_{\\theta\\in I}|\\ell'''(\\theta)|=0\n\\] we have that \\(M=0\\) which implies that we find the root, \\(\\theta_{root},\\) already in the first \\((m=1)\\) update step, since\n\\[\n\\begin{align*}\n|e_{(1)}|  \n&\\leq \\overbrace{\\left(M |e_{(0)}|\\right)}^{=0}\\;|e_{(0)}|\\\\[2ex]\n\\Rightarrow\\quad |e_{(1)}|&=0\\\\[2ex]\n\\Rightarrow\\quad \\theta_{(1)}&=\\theta_{root},\n\\end{align*}\n\\] even when \\(|e_{(0)}|\\gg 0.\\)\nThis makes sense, since \\(\\sup_{\\theta\\in I}|\\ell'''(\\theta)|=0\\) implies that \\(\\ell'(\\theta)\\) has no curvature; i.e. \\(\\ell'(\\theta)\\) is a straight line for all \\(x\\in I\\) which implies that the Taylor approximation used in the update steps of the Newton-Raphson algorithm is just perfect (no approximation error).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html",
    "href": "Ch3_Bootstrap.html",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "3.1 Illustration: When are you happy about the Bootstrap?\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y.\\) These returns \\(X\\) and \\(Y\\) are random with\nWe want to invest a fraction \\(\\alpha\\in(0,1)\\) in \\(X\\) and invest the remaining \\(1-\\alpha\\) in \\(Y.\\)\nOur aim is to minimize the variance (risk) of our investment, i.e., we want to minimize \\[\nVar\\left(\\alpha X + (1-\\alpha)Y\\right).\n\\] One can show that the value \\(\\alpha\\) that minimizes this variance is \\[\n\\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n\\tag{3.1}\\] Using a data set that contains past measurements \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] for \\(X\\) and \\(Y,\\) we can estimate the unknown \\(\\alpha\\) by plugging in estimates of the variances and covariances \\[\n\\hat\\alpha_n = \\frac{\\hat\\sigma^2_{Y,n} - \\hat\\sigma_{XY,n}}{\\hat\\sigma^2_{X,n} + \\hat\\sigma^2_{Y,n} - 2\\hat\\sigma_{XY,n}}\n\\tag{3.2}\\] with \\[\n\\begin{align*}\n\\hat{\\sigma}^2_{X,n}&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2\\\\\n\\hat{\\sigma}^2_{Y,n}&=\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2\\\\\n\\hat{\\sigma}_{XY,n}&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)\\left(Y_i-\\bar{Y}\\right),\n\\end{align*}\n\\] where \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\) and \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^nY_i.\\)\nIt is natural to wish to quantify the accuracy of our estimator \\[\n\\hat\\alpha_n\\approx \\alpha.\n\\]\nFor instance, to construct a confidence interval we need to know the standard error of the estimator \\(\\hat\\alpha\\), \\[\n\\sqrt{Var(\\hat\\alpha_n)} = \\operatorname{SE}(\\hat\\alpha_n)=?\n\\] However, deriving an explicit expression for \\(\\operatorname{SE}(\\hat\\alpha)\\) is difficult here due to the definition of \\(\\hat\\alpha_n\\) in Equation 3.2 which contains variance estimators also in the denominator.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-Illustration",
    "href": "Ch3_Bootstrap.html#sec-Illustration",
    "title": "3  The Bootstrap",
    "section": "",
    "text": "\\(Var(X)=\\sigma^2_X\\)\n\\(Var(Y)=\\sigma^2_Y\\)\n\\(Cov(X,Y)=\\sigma_{XY}\\)\n\n\n\n\n\n\n\n\n\n\n\nConclusion: Why Bootstrap?\n\n\n\nIn cases as described above, we are happy to use the Basic Bootstrap Method (Section 3.3) which allows estimating \\(\\operatorname{SE}(\\hat\\alpha)\\) by resampling from the data observed; i.e. without the need of an explicit formula of a consistent estimator of \\(\\operatorname{SE}(\\hat\\alpha).\\) The Basic Bootstrap Method is found to be as accurate as the standard asymptotic normality results which, however, require an explicit formula of an estimator of \\(\\operatorname{SE}(\\hat\\alpha)\\) to become useful.\nIf we have a consistent estimator for the \\(\\operatorname{SE}(\\hat\\alpha),\\) then we can make use of this estimator by applying the Bootstrap-\\(\\mathbf{t}\\) Method (Section 3.4). The Bootstrap-\\(t\\) Method is found to be more accurate than the standard asymptotic normality results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#recap-the-empirical-distribution-function",
    "href": "Ch3_Bootstrap.html#recap-the-empirical-distribution-function",
    "title": "3  The Bootstrap",
    "section": "3.2 Recap: The Empirical Distribution Function",
    "text": "3.2 Recap: The Empirical Distribution Function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its (cumulative) distribution function\n\n\n\n\n\n\n\nDefinition 3.1 ((Cumulative) Distribution Function (CDF)) \\[\nF(x)=P(X \\leq x)\\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\]\n\n\n\n\nThe sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim X\n\\] denote a real-valued random sample with \\(X\\sim F,\\) and let \\(1_{(\\cdot)}\\) denote the indicator function, i.e., \\[\n\\begin{align*}\n1_{(\\text{TRUE})} &=1\\quad\\text{and}\\quad 1_{(\\text{FALSE})}=0.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nDefinition 3.2 (Empirical (Cumulative) Distribution Function (ECDF)) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\] I.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\n\n\n\nProperties of the ECDF:\n\\(F_n\\) is a monotonically increasing right-continuous step function that is bounded between zero and one, \\[\n0\\le F_n(x)\\le 1,\n\\] where \\[\nF_n(x)=\\left\\{\n  \\begin{array}{ll}\n  0&\\text{ if }x  &lt; X_{(1)}\\\\\n  1&\\text{ if }x\\ge X_{(n)}\\\\\n  \\end{array}\n\\right.\n\\] where \\[\nX_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\n\\] denotes the order-statistic.\n\\(F_n\\) is itself a distribution function according to Definition 3.1; namely, the distribution function of the discrete random variable \\(X^*,\\) where\n\\[\nX^*\\in\\{X_1,\\dots,X_n\\}\n\\] and \\[\nP(X^*=X_i)=\\frac{1}{n}\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\] Thus \\[\n\\begin{align*}\nF_n(x)\n&=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\\\[2ex]\n&= P\\left(X^*\\leq x\\right).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nExample 3.1 (Computing the empirical distribution function \\(F_n\\) in R) \nSome data, i.e. an observed realization of a random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}F:\\)\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.30\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample &lt;- c(5.20, 4.80, 5.30, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     &lt;- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\n\n\n\n\nThe R function ecdf() returns a function that gives the values of \\(F_n(x):\\)\n\n## Note: ecdf() returns a function that can be evaluated! \nmyecdf_fun(5.0)\n\n[1] 0.25\n\n\nSampling (iid) from the empirical distribution function \\(F_n\\) is equivalent to resampling (with replacement and with equal probabilities) data points from the observed data observedSample. The following code generates three “bootstrap” samples from \\(F_n\\colon\\)\n\nn         &lt;- length(observedSample)\n\nresample1 &lt;- sample(observedSample, \n                   size    = n, \n                   replace = TRUE)\nresample1\n\n[1] 5.3 4.8 5.3 5.4 5.8 6.1 5.4 5.4\n\nresample2 &lt;- sample(observedSample, \n                   size    = n, \n                   replace = TRUE)\nresample2\n\n[1] 5.4 4.6 5.3 5.3 4.6 6.1 5.2 5.2\n\nresample3 &lt;- sample(observedSample, \n                   size    = n, \n                   replace = TRUE)\nresample3\n\n[1] 4.8 5.2 6.1 5.2 5.2 4.6 6.1 5.8\n\n\n\n\n\n\n\nStatistical Properties of \\(F_n\\)\n\\(F_n(x)\\) depends on the i.i.d. random sample \\(X_1,\\dots,X_n\\) and thus is itself a random function.\nWe obtain \\[\nnF_n(x)\\sim B(n, p=F(x))\\quad\\text{for each}\\quad x\\in\\mathbb{R}\n\\tag{3.3}\\]\nI.e., \\(nF_n(x)\\) has a binomial distribution with parameters:\n\n\\(n\\) (“number of trials”)\n\\(p=F(x)\\) (“probability of success on a single trial”).\n\n\nNote: The result in Equation 3.3 holds for any \\(F.\\) Therefore, \\(nF_n,\\) and thus also \\(F_n,\\) is called distribution free\n\nEquation 3.3 implies that \\[\n\\begin{align*}\n\\mathbb{E}(nF_n(x))& = np = nF(x)\\\\[2ex]\n\\Rightarrow \\quad \\mathbb{E}(F_n(x))& = p = F(x)\\\\[2ex]\n\\Rightarrow \\quad \\operatorname{Bias}(F_n(x))& = \\mathbb{E}(F_n(x)) - F(x) =0\\\\\n\\end{align*}\n\\] and \\[\n\\begin{align*}\nVar(nF_n(x))& = np(1-p) = nF(x)(1-F(x))\\\\[2ex]\n\\Rightarrow \\quad Var(F_n(x))& = \\frac{nF(x)(1-F(x))}{n^2}=\\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\operatorname{MSE}(F_n(x))\n& = (\\operatorname{Bias}(F_n(x)))^2 + Var(F_n(x))\\\\[2ex]\n& =\\frac{F(x)(1-F(x))}{n}\\\\[2ex]\n\\Rightarrow\\quad\\operatorname{MSE}(F_n(x))&\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\] pointwise for each \\(x\\in\\mathbb{R}.\\)\nThis allows us to conclude that \\[\n\\begin{align*}\nF_n(x) & \\to_{m.s.} F(x)\\quad\\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow \\quad F_n(x) & \\to_{p} F(x)\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\] pointwise for each \\(x\\in\\mathbb{R}.\\)\nThat is, \\(F_n(x)\\) is a pointwise consistent estimator of \\(F(x)\\) for each \\(x\\in\\mathbb{R}.\\)\nThe Clivenko-Cantelli Theorem 3.1 states that \\(F_n\\) is even an uniformly consistent estimator of \\(F.\\)\n\n\n\n\n\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim X\\) denote a real-valued random sample with \\(X\\sim F.\\) Then \\[\n\\begin{align*}\n&\\quad P\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\\\\[2ex]\n\\Leftrightarrow &\\quad\n\\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\n\n\n\n\n\n\n3.2.1 Idea of the Bootstrap\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible “Monte Carlo simulation”).\nSampling from the population distribution \\(F\\) (infeasible Monte Carlo simulation) The random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\] Let \\(\\theta_0\\) denote a distribution parameter of \\(F\\) which we want to estimate, and let \\(\\hat\\theta_n\\) denote an estimator of \\(\\theta_0.\\) If we would know \\(F,\\) we could generate arbitrarily many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_{n,1}, \\hat{\\theta}_{n,2}, \\dots, \\hat{\\theta}_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these realizations.\n\nUnfortunately, we don’t know \\(F,\\) thus Monte Carlo inference is infeasible.\nThe idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:  Instead of random sampling from \\(F,\\) which is infeasible (as we don’t know \\(F\\)), the bootstrap uses random sampling from the known empirical distribution function \\(F_n\\) to generate arbitrarily many bootstrap realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}^*_{n,1}, \\hat{\\theta}^*_{n,2}, \\dots, \\hat{\\theta}^*_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these bootstrap realizations. This is justified asymptotically since for large \\(n,\\) the empirical distribution \\(F_n\\) is “close” to the unknown distribution \\(F\\) (Glivenko-Cantelli Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\)\n\\[\n  \\begin{align*}\n  \\underbrace{\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}}_{=F_n(b)-F_n(a)}&\\to_p \\underbrace{P(X\\in [a,b])}_{=F(b)-F(a)}\n  \\end{align*}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "href": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 Basic Idea of the Bootstrap",
    "text": "3.3 Basic Idea of the Bootstrap\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\nSampling from the population distribution \\(F\\) (infeasible Monte Carlo simulation) The random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\] Let \\(\\theta_0\\) denote a distribution parameter of \\(F\\) which we want to estimate, and let \\(\\hat\\theta_n\\) denote an estimator of \\(\\theta_0.\\) If we would know \\(F,\\) we could generate arbitrarily many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_{n,1}, \\hat{\\theta}_{n,2}, \\dots, \\hat{\\theta}_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these realizations. Unfortunately, we don’t know \\(F,\\) thus Monte Carlo inference is infeasible.\nThe idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:  Instead of random sampling from \\(F,\\) which is infeasible, the bootstrap uses random sampling from the known empirical distribution function \\(F_n\\) to generate arbitrarily many bootstrap realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}^*_{n,1}, \\hat{\\theta}^*_{n,2}, \\dots, \\hat{\\theta}^*_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these bootstrap realizations. This is justified asymptotically since for large \\(n,\\) the empirical distribution \\(F_n\\) is “close” to the unknown distribution \\(F\\) (Glivenko-Cantelli Theorem 3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\)\n\\[\n  \\begin{align*}\n  \\underbrace{\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}}_{=F_n(b)-F_n(a)}&\\to_p \\underbrace{P(X\\in [a,b])}_{=F(b)-F(a)}\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-basic-bootstrap-method",
    "href": "Ch3_Bootstrap.html#the-basic-bootstrap-method",
    "title": "3  The Bootstrap",
    "section": "3.4 The Basic Bootstrap Method",
    "text": "3.4 The Basic Bootstrap Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption. The basic bootstrap method is often also called:\n\n(Standard) Nonparametric Bootstrap Method\n\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with real valued \\(X\\sim F.\\)\nThe distribution \\(F\\) is depends on an unknown parameter \\(\\theta_0.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate \\(\\theta_0\\in\\mathbb{R}.\\)\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta_n\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\nMoreover, for simplicity let us focus on unbiased and \\(\\boldsymbol{\\sqrt{n}}\\)-consistent estimators, i.e.\n\n\\(\\mathbb{E}\\left(\\hat\\theta_n\\right)=\\theta_0\\)\n\\(\\operatorname{SE}\\left(\\hat\\theta_n\\right)=\\sqrt{Var\\left(\\hat\\theta_n\\right)}=\\frac{1}{\\sqrt{n}}\\cdot\\text{constant}\\)\n\n\nInference: In order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis, we need to know the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e. in learning the limit of the distribution function \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWe could do asymptotic statistics. For instance, using the Lindeberg-Lévy CLT, we may be able to show that the limit of \\(H_{n}(x)\\) is the distribution function of the Normal distribution with mean zero and asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big).\\)\nHowever, deriving a useful, explicit expression of the asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big)\\) can be very hard (see Section 3.1). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific version of the Bootstrap can be even more accurate then a standard asymptotic Normality result.\n\n\n\n\n\n\nThe Core Part of the Bootstrap Algorithm\n\n\n\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*_n\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (for a large value of \\(m,\\) such as \\(m=5000\\) or \\(m=10000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\]\n\n\n\nBy the Clivenko-Cantelli (Theorem 3.1) the bootstrap estimators \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] allow us to approximate the bootstrap distribution\n\\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\right|\\mathcal{S}_n\\right)\n\\] arbitrarily well, i.e., \\[\n\\sup_{x\\in\\mathbb{R}}\\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\\right|\\to_{a.s} 0\\quad\\text{as}\\quad m\\to\\infty,\n\\] where \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\left(\\hat\\theta^*_{n,j}-\\hat\\theta_n\\right)\\leq x\\right)}\n\\] denotes the empirical distribution function based on the \\(\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\\) centered by \\(\\hat{\\theta}_n\\) and scaled by \\(\\sqrt{n}.\\)\nSince we can choose \\(m\\) arbitrarily large, we can effectively ignore the approximation error between \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Boot}_{n}(x).\\) That is, we can (and will do so) treat the bootstrap distribution \\(H^{Boot}_{n}(x)\\) as known.\nThe crucial question is, however, whether the bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] is able to approximate the unknown distribution \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\n\\] as \\(n\\to\\infty.\\) This is an important requirement called bootstrap consistency.\n\nBootstrap Consistency\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)|\\mathcal{S}_n\\) is a good approximation of the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\), i.e. \\[\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)\\ |{\\cal S}_n\\right)}_{H_n^{Boot}}\\approx\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\right)}_{H_n}.\n\\] The following definition states this more precisely.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Bootstrap consistency)  Let the limit (as \\(n\\to\\infty\\)) of \\(H_n\\) be a non-degenerate distribution. Then the bootstrap is consistent if and only if \\[\n\\sup_{x\\in\\mathbb{R}} \\Big|\\;\n\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta^*_n-\\hat\\theta_n\\big)\\le x \\ |{\\cal S}_n\\Big)}_{H_n^{Boot}(x)}\n  -\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta_n -\\theta_0\\big)\\le x\\Big)}_{H_n(x)}\n  \\Big|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\n\n\nLuckily, the standard bootstrap is consistent in a large number of statistical problems. However, there are some requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nTypically, the distribution of the estimator \\(\\hat\\theta_n-\\theta_0\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) does not properly reflect the way how \\(X_1,\\dots,X_n\\) are generated in a first place. (For instance, when \\(X_1,\\dots,X_n\\) is generated by a time-series process with auto-correlated data.)\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (For instance, in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.4.1 Example: Inference About the Population Mean\nSetup:\n\n\\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\)\nContinuous random variable \\(X\\sim F\\)\nNon-zero, finite variance \\(0<Var(X)=\\sigma_0^2<\\infty\\)\nUnknown mean \\(\\mathbb{E}(X)=\\mu_0,\\) where\n\\[\n\\mu_0 = \\int x f(x) dx = \\int x d F(x),\n\\] where \\(f=F'\\) denotes the density function.\nEstimator: Empirical mean \\[\n\\begin{align*}\n\\bar{X}_n\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nInference Problem: What is the (asymptotic) distribution of \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\n\\] as \\(n\\to\\infty\\)?\n\n\n\n\n\n\nRecall: Inference using Classic Asymptotic Statistics\n\n\n\nThis example is so simple that we know (by the Lindeberg-Lévy CLT) that \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\to_d\\mathcal{N}\\left(0,\\sigma_0^2\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., that \\[\n%\\bar{X}_n\\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{1}{n}\\sigma_0\\right).\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x,\\) where \\[\n\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n\\] with \\(\\Phi\\) denoting the distribution function of the standard normal distribution, i.e. \\[\n\\Phi_{\\sigma_0}(x)\n=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x/\\sigma_0}\\exp\\left(-\\frac{1}{2}z^2\\right)\\,dz.\n\\]\n\n\nYes, the asymptotic result is simple here (boring), but can we alternatively use the Bootstrap to approximate this limit result? I.e., is \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] able to approximate \\(\\Phi_{\\sigma_0}(x)\\) for all \\(x\\in\\mathbb{R}\\)?\n\n3.4.1.1 Practice: Empirical Consideration of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\nLet us consider the following observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) shown in Table 3.1. The data was generated by drawing from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\) That is, \\(Var(X)=\\sigma_0^2=2\\cdot \\operatorname{df}=4.\\)\n\n\n\n\n\nTable 3.1: Observed realization of the random sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) drawn from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\)\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n0.36\n\n\n2\n3.39\n\n\n3\n3.24\n\n\n4\n4.90\n\n\n5\n1.76\n\n\n6\n5.33\n\n\n7\n7.77\n\n\n8\n1.93\n\n\n\n\n\nobservedSample <- c(0.36, 3.39, 3.24, 4.90, \n                    1.76, 5.33, 7.77, 1.93)\n\nSo the observed sample mean is\n\n\\(\\bar X_{n,obs} =\\) mean(observedSample) \\(=\\) 3.585\n\n\nBootstrap:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical “population” in order to generate the i.i.d. bootstrap sample \\[\nX_1^*,\\dots,X_n^*\n\\]\nThese i.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}.\\)\nEach realization of the bootstrap sample leads to a new realization of the bootstrap estimator \\(\\bar{X}^*_n\\) as demonstrated in the following R code:\n\n## generating one realization of the bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n## computing the corresponding realization of the bootstrap estimator\nmean(bootSample)\n\n[1] 4.21375\n\n\nWe can now approximate the bootstrap distribution \\[\nH^{Boot}_n(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] using the empirical distribution function \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)}\n\\] based on the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\n\\] generated using the bootstrap algorithm\n\nGenerate bootstrap sample\nCompute bootstrap estimator\nRepeat Steps 1 and 2 \\(m\\) times\n\nwith a (very) large \\(m.\\) The following R code demonstrates this:\n\nn                <- length(observedSample)\nXbar             <- mean(observedSample)\n\nm                <- 10000 # number of bootstrap samples \nXbar_boot        <- vector(mode = \"double\", length = m)\n\n## Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          <- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        <- mean(bootSample)\n}\n\nplot(ecdf( sqrt(n) * (Xbar_boot - Xbar) ), \n     xlab = \"\", ylab = \"\", \n     main = \"Bootstrap Distribution vs Normal Limit Distribution\")\ncurve(pnorm(x, mean = 0, sd = sqrt(4)), col = \"red\", add = TRUE)     \nlegend(\"topleft\", \n       legend = c(\"Bootstrap Distribution\", \n                  \"Normal Limit Distribution with\\nMean = 0 and Variance = 4\"), \n      col = c(\"black\", \"red\"), lty = c(1,1))\n\n\n\n\nNote: To plot the Normal limit distribution we need to make use of our knowledge that \\(X_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^2_{(\\operatorname{df}=2)}\\) which implies that we know (the usually unknpown) asymptotic variance of the estimator \\(\\bar{X}_n,\\) \\[\nnVar(\\bar{X}_n)=Var(\\sqrt{n}(\\bar{X}_n-\\mu_0))=\\sigma_0^2=2\\cdot\\operatorname{df}=4,\n\\] for each \\(n=1,2,\\dots,\\) thus also \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=4.\\)\nUsually, however, we do not know the value of the asymptotic variance, but need an estimator for this quantity. (Which can be hard to derive.)\nBy contrast, we get the complete bootstrap distribution directly from the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\n\\] That is, to estimate the usually unknown value of the asymptotic variance \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=\\sigma_0^2=4,\\) we can simply use the empirical variance of the bootstrap estimators \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\\) multiplied by \\(n,\\) as done in the following R-code:\n\nround(n * var(Xbar_boot), 2)\n\n[1] 4.82\n\n\n\n\n\n3.4.1.2 Theory (Part 1): Mean and Variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\cdot),\\;Var^*(\\cdot),\\;\\text{and}\\;P^*(\\cdot)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\cdot)=\\mathbb{E}(\\cdot|\\mathcal{S}_n),\n\\] the conditional variance \\[\nVar^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the bootstrap distribution, i.e. on the conditional distribution of \\[\n\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n.\n\\]\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze the bootstrap distribution of \\(\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n,\\) since we know 🤟 the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\\;i=1,\\dots,n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F,\\) \\(i=1,\\dots,n.\\)\n\n\n\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable: \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the discrete conditional random variable \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X_i^*)\n&=\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X_n.\n\\end{align*}\n\\] I.e., the empirical mean \\(\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\nThe conditional variance of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}\\left((X_i^* - \\mathbb{E}(X_i^*|{\\cal S}_n))^2|{\\cal S}_n\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\\\[2ex]\n&=\\hat\\sigma^2_0.\n\\end{align*}\n\\] I.e., the empirical variance \\(\\hat\\sigma^2_{0}=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X_i^*))=\\mathbb{E}(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq \\delta)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important.\nThe unconditional distribution of \\(X_i^*\\) is equal to the unknown distribution \\(F.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X_i^*\\leq x)\n&= P(1_{(X_i^*\\leq x)}=1) \\\\[2ex]\n&= P(1_{(X_i^*\\leq x)}=1) \\cdot 1 + P(1_{(X_i^*\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= \\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}|\\mathcal{S}_n\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq x\\right)}}\\right)\\quad[\\text{{\\color{blue}from our derivations above}}]\\\\[2ex]\n&= \\frac{n}{n}\\mathbb{E}\\left(1_{\\left(X_i\\leq x\\right)}\\right)\\\\[2ex]\n&= P(1_{(X_i\\leq x)}=1) \\cdot 1 + P(1_{(X_i\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= P\\left(X_i\\leq x\\right)=F(x)\n\\end{align*}\n\\]\n\n\nNow we can consider the mean and the variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\nThe conditional mean of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=\\mathbb{E}\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\,\\mathbb{E}\\left(\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right)- \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right)\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\color{red}\\mathbb{E}\\left(X^*_i|{\\cal S}_n\\right)}- \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}\\mathbb{E}\\left(X_i|{\\cal S}_n\\right)}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{n}{n}{\\color{red}\\bar{X}_n} - \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}X_i}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\bar{X}_n - \\bar{X}_n\\right)\\\\[2ex]\n&= 0.\n\\end{align*}\n\\]\nThe conditional variance of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\nVar^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=Var\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\left(\\big(\\bar X^*_n-\\bar{X}_n\\big)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\big(\\bar X^*_n|{\\cal S}_n\\big)\\quad[\\text{cond.~on $\\mathcal{S}_n,$ $\\bar{X}_n$ is a constant}]\\\\[2ex]\n&=n\\,Var\\Big(\\frac{1}{n}\\sum_{i=1}^n X_i^*\\Big|{\\cal S}_n\\Big)\\\\\n&=n\\,\\frac{1}{n^2}\\sum_{i=1}^n Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=n\\,\\frac{n}{n^2} Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=Var\\big(X_i^*|{\\cal S}_n\\big)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\quad[\\text{derived above}]\\\\[2ex]\n&=\\hat\\sigma^2_0,\n\\end{align*}\n\\] where \\[\n\\hat\\sigma^2_0\\to_p \\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThus, we know now that for large \\(n\\) (\\(n\\to\\infty\\)) the mean and the variance of the bootstrap distribution of \\[\n\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n\n\\] matches the mean (zero) and the variance (\\(\\sigma_0^2\\)) of the limit distribution \\(\\Phi_{\\sigma_0}.\\)\nBootstrap consistency, however, addresses the total distribution—not only the first two moments.\n\n\n\n\n3.4.1.3 Theory (Part 2): Bootstrap Consistency\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.4 (Characteristic Function) Let \\(X\\in\\mathbb{R}\\) be a random variable and let \\(\\mathcal{i}=\\sqrt{-1}\\) be the imaginary unit. Then the function \\(\\psi_X:\\mathbb{R}\\to\\mathbb{C}\\) defined by \\[\n\\psi_X(t) = \\mathbb{E}(\\exp(\\mathcal{i}tX))\n\\] is called the characteristic function of \\(X.\\)\n\n\n\n\n\n\n\n\n\nCharacteristic Function: Some useful facts\n\n\n\nThe characteristic function …\n\n… uniquely determines its associated probability distribution.\n… can be used to easily derive (all) the moments of a random variable.\n… is often used to prove that two distributions are equal.\nThe characteristic function of \\(\\Phi_{\\sigma_0}\\) is \\[\n\\psi_{\\Phi_{\\sigma_0}}(t)=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)=\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\n\\tag{3.3}\\]\nThe characteristic function of \\(\\sum_{i=1}^nW_i,\\) where \\(W_1,\\dots,W_n\\) are i.i.d., is \\[\n\\psi_{\\sum_{i=1}^nW_i}(t)=\\left(\\psi_{W_1}(t)\\right)^n.\n\\tag{3.4}\\]\nLet \\(W\\) be a random variable with \\(\\mathbb{E}(W)=0\\) and \\(Var(W)=\\sigma_W^2.\\) Then, we have that (see Equation (26.11) in Billingsley (1995)) \\[\n\\psi_W(t)=1-\\frac{1}{2}\\sigma_W^2 \\, t^2 + \\lambda(t),\n\\tag{3.5}\\] where \\(|\\lambda(t)|\\leq |t^2|\\,\\mathbb{E}\\left(\\min(|t|\\,|W|^3, W^2)\\right).\\)\n\n\n\n\nThe following can be found in Example 3.1 in Shao and Tu (1996)\n\nIt follows from the Lindeberg-Lévy CLT that \\[\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all \\(x\\in\\mathbb{R}.\\) This result can be proven by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) To see this, rewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n& = \\sum_{i=1}^n\\frac{X_i-\\mu_0}{\\sqrt{n}}\\\\[2ex]\n& = \\sum_{i=1}^n W_{i,n}\n\\end{align*}\n\\] where\n\n\\(W_{1,n},\\dots,W_{n,n}\\) are i.i.d. with\n\\(\\mathbb{E}(W_{i,n})=0\\) and\n\\(Var(W_{i,n})=\\frac{1}{n}\\sigma_0^2.\\)\n\nTherefore, by Equation 3.4 together with Equation 3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n(t)|\n&\\leq |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,\\left|W_{1,n}\\right|^3, \\left|W_{1,n}\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_1-\\mu_0\\right|^3, n^{-1}\\left|X_1-\\mu_0\\right|^2\\big)\\right).\n\\end{align*}\n\\] That is, \\[\nn|\\lambda_n(t)|\\to 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] which means that \\(|\\lambda_n(t)|\\to 0\\) faster than \\(n^{-1}.\\)\nThus, by Equation 3.3 \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&= \\lim_{n\\to\\infty}\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t)\n\\end{align*}\n\\]\nOK, we have shown that \\(H_n\\) tends to \\(\\Phi_{\\sigma_0}\\) by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) (I.e. we have shown the Lindeberg-Lévy CLT.)\nTo show bootstrap consistency we need to show that \\(H_n^{Boot}\\) tends to \\(\\Phi_{\\sigma_0}.\\) To do so, we can mimic the above prove, by showing that the characteristic function of \\(H_n^{Boot}\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nRewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}^*_n- \\bar{X}_n\\right)|\\mathcal{S}_n\n& = \\sum_{i=1}^n\\frac{X^*_i- \\bar{X}_n}{\\sqrt{n}}|\\mathcal{S}_n\\\\[2ex]\n& = \\sum_{i=1}^n W^*_{i,n}|\\mathcal{S}_n\n\\end{align*}\n\\] where\n\n\\(W^*_{1,n}|\\mathcal{S}_n,\\dots,W^*_{n,n}|\\mathcal{S}_n\\) is i.i.d. with\n\\(\\mathbb{E}^*(W^*_{n})=\\mathbb{E}(W^*_{n}|\\mathcal{S}_n)=0\\) and\n\\(Var^*(W^*_{n})=Var(W^*_{n}|\\mathcal{S}_n)=\\frac{1}{n}\\hat{\\sigma}_0^2=\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right)\\)\n\nTherefore, by Equation 3.4 together with Equation 3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}|\\mathcal{S}_n}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}|\\mathcal{S}_n}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}{\\color{darkgreen}\\hat{\\sigma}_0^2} \\, t^2 + {\\color{red}\\lambda_n^*(t)}\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n^*(t)|\n&\\leq |t^2|\\,{\\color{blue}\\mathbb{E}^*}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_1-\\bar{X}_n\\right|^3, n^{-1}\\left|X_1^* - \\bar{X}_n\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,{\\color{blue}\\frac{1}{n}\\sum_{i=1}^n}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_i-\\bar{X}_n\\right|^3, n^{-1}\\left|X_i^* - \\bar{X}_n\\right|^2\\big)\\right).\n\\end{align*}\n\\] By the Marcinkiewicz strong law of large numbers, we obtain that \\[\nn{\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., \\({\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\) faster than \\(n^{-1}.\\) Moreover, since \\[\n{\\color{darkgreen}\\hat\\sigma_0^2} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\to_{a.s.}\\sigma_0^2\n\\] we have that (using Equation 3.3) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n\\to_{a.s.}&\n\\lim_{n\\to\\infty}\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t).\n\\end{align*}\n\\] This implies that the limit (\\(n\\to\\infty\\)) of \\(H_n^{Boot}\\) is \\(\\Phi_{\\sigma_0}\\) almost surely.\nHence we have shown that the basic bootstrap is consistent for doing inference about \\(\\mu_0\\) using \\(\\bar{X}_n.\\)\n\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nH_n^{Boot}(x)=P\\left(\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x|\\mathcal{S}_n)\\right) \\approx\n\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar X^*_{n,j}-\\bar X_n\\right)\\leq x\\right)}=H_{n,m}^{Boot}(x),  \n\\] as done in Section 3.4.1.1.\n\n\n\n\n3.4.2 The Basic Bootstrap Confidence Interval\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\theta_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\n\\(\\theta_0\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2)\\) as \\(n\\to\\infty,\\)\n\\(\\hat{v}_n\\to_{p} v_0\\) as \\(n\\to\\infty\\)\n\nAn approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}_n - z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}},\n\\hat{\\theta}_n + z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}}\n\\right],\n\\] where \\(z_{1-\\frac{\\alpha}{2}}\\) denotes the \\((1-\\alpha)/2\\) quantile of the standard Normal distribution. This confidence interval is approximate, since it is only asymptotically justified; i.e. it is not exact in finite samples.\n\n\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v_n\\) of \\(v_0\\) (see ?sec-Illustraction). Statistical inference is then usually based on the bootstrap confidence intervals.\nIn many situations it can be shown that bootstrap confidence intervals (or tests) are even more precise than asymptotic normality based confidence intervals. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\nAlgorithm of the Basic Bootstrap Confidence Interval for \\(\\theta_0\\):\nSetup:\n\nData: i.i.d. random sample \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}\n\\] with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nProblem: Construct a confidence interval for \\(\\theta_0\\in\\mathbb{R}.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is Consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^*_n -\\hat{\\theta}_n)|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}_n-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}_n -\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\n\n\nAlgorithm (3 Steps):\n\nGenerate \\(m\\) bootstrap estimates\n\\[\n\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] by repeatedly (\\(m\\) times) drawing bootstrap samples \\(X_{1}^*,\\dots,X_{n}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUse the \\(m\\) bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantiles \\[\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\quad\\text{and}\\quad \\hat q^*_{n,1-\\frac{\\alpha}{2}}\n\\] of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat q^*_{n,p}=\\left\\{\n  \\begin{array}{ll}\n  \\hat\\theta^*_{n,(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n  (\\hat\\theta^*_{n,(mp)}+\\hat\\theta^*_{n,(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.6}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(j)}^*\\) denotes the \\(j\\)th order statistic \\[\n\\hat\\theta_{n,(1)}^* \\leq \\hat\\theta_{n,(2)}^*\\leq \\dots\\leq \\hat\\theta_{n,(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g. \\(\\lfloor 4.9\\rfloor = 4\\)).\nThe approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval is then given by \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\tag{3.7}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe quantiles \\(\\hat q^*_{n,p}\\) are those of the distribution \\[\nG_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\hat{\\theta}^*_{n,j}\\leq x\\right)}.\n\\] However, we’ll treat the quantiles \\(\\hat q^*_{n,p}\\) as quantiles of the distribution \\[\nG_{n}^{Boot}(x)=P\\left(\\hat{\\theta}^*_{n}\\leq x\\,\\big|\\,\\mathcal{S}_n\\right),\n\\] since for large \\(m\\) (\\(m\\to\\infty\\)) the difference between \\(G_{n,m}^{Boot}\\) and \\(G_{n}^{Boot}\\) is negligible (Glivenko-Cantelli Theorem 3.1) and we can choose \\(m\\) to be large.\n\n\nJustifying the Basic Bootstrap CI (Equation 3.7) for \\(\\theta_0\\): \\[\n\\begin{align*}\n&P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\leq \\hat{\\theta}^*_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n \\leq\\hat{\\theta}^*_n -\\hat{\\theta}_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\]\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\leq\\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat{\\theta}_n-(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\le \\theta_0\\le \\hat{\\theta}_n-\n(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\le \\theta_0\\le 2\\hat{\\theta}_n-\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\] This demonstrates that the basic bootstrap confidence interval in Equation 3.7 \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] is indeed an asymptotically valid (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval.\n\n\nExample: Basic Bootstrap Confidence Interval for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0.\\)\nEstimator: \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu_0.\\)\nInference Problem: Construct a confidence interval for \\(\\mu_0.\\)\n\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\mu_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\nBy the CLT: \\(\\sqrt{n}(\\bar X_n - \\mu_0)\\to_d\\mathcal{N}(0,\\sigma^2_0)\\) as \\(n\\to\\infty\\)\nEstimation of \\(\\sigma^2_0\\): \\(s^2_n=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X_n -\\mu_0)/s_n)\\to_d\\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\)\n\nLet \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1).\\) Since \\(z_{\\alpha/2} = -z_{1-\\alpha/2},\\) we have that \\[\n\\begin{align*}\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\le \\frac{\\sqrt{n}(\\bar X_n -\\mu_0)}{s_n}\\le z_{1-\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\bar X_n -\\mu_0\\le z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\mu_0\\le\n        \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\n  \\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\nApproximate \\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}},\n    \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right]\n\\]\n\n\n\nAlgorithm of the basic bootstrap confidence interval for \\(\\mu_0\\):\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation (see Section 3.4.1.3).\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10,000\\)) and calculate the corresponding estimates \\[\n\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\n\\]\nCompute the empirical quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval according to Equation 3.7: \\[\n\\left[2\\bar X_n -\\hat q^*_{n,1-\\frac{\\alpha}{2}},\n   2\\bar X_n -\\hat q^*_{n,\\frac{\\alpha}{2}}\\right]\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-bootstrap-t-method",
    "href": "Ch3_Bootstrap.html#the-bootstrap-t-method",
    "title": "3  The Bootstrap",
    "section": "3.5 The Bootstrap-\\(t\\) Method",
    "text": "3.5 The Bootstrap-\\(t\\) Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption.\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the (nonparametric) bootstrap-\\(t\\) method (one also speaks of the “studentized bootstrap”). The construction relies on so-called (asymptotically) pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.5 ((Asymptotically) Pivotal Statistics) \nA statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called exact pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter.\nA statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications.\nIt is, however, often possible to construct an asymptotically pivotal statistic. Consider, for instance, an asymptotically normal \\(\\sqrt{n}\\)-consistent estimator \\(\\hat{\\theta}_n\\) of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator of \\(v_0^2\\) \\[\n\\hat v_n^2 \\rightarrow_p v_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\] which implies that also \\[\n\\hat v_n \\rightarrow_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Then, \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] is asymptotically pivotal, since \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\n\nExample: \\(\\bar{X}_n\\) is a Pivotal Statistic\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu_0\\), variance \\(0<Var(X)=\\sigma_0^2<\\infty\\), and \\(\\mathbb{E}(|X|^4)=\\beta<\\infty\\).\n\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\sim t_{n-1}\\quad\\text{for any}\\quad n=2,3,\\dots\n\\] with \\(s_n^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is exact pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\rightarrow_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistic.\n\n\n\nBootstrap-\\(t\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*\\big|\\mathcal{S}_n =\\sqrt{n}\\frac{(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}{\\hat v_n^*}\\Big|\\mathcal{S}_n,\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\(\\hat{v}_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*=\\hat{v}(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(t\\) consistency follows if the basic bootstrap is consistent\n\n\n\nIf the basic bootstrap is consistent, i.e. if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_{x\\in\\mathbb{R}} \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{\\hat v_n^*}\\le x \\;\\right|\\;{\\cal S}_n\\right)-\\Phi(x)\\right|\\rightarrow_p 0,\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.5.1 The Bootstrap-\\(t\\) Confidence Interval\nSetup:\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2\\) of \\(\\hat{\\theta}_n,\\) i.e.  \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\hat v^2_n\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty,\n\\]\nand that \\[\n\\hat v_n\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\nAlgorithm of the Bootstrap-\\(t\\) Confidence Interval for \\(\\theta_0\\):\nAlgorithm (3 Steps):\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*_n\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\n\\hat v^*_n\\equiv \\hat v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT_n^*&=\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g. \\(m=100,000\\)) many bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\n\\]\nUse the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) empirical quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) (see Equation 3.6).\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval\n\\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n   \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\tag{3.8}\\] where \\(\\hat\\theta_n\\) and \\(\\hat v_n\\) are the estimates of \\(\\theta_0\\) and \\(v_0\\) based on the original sample \\(X_1,\\dots,X_n.\\)\n\nJustifying the Bootstrap-\\(t\\) CI (Equation 3.8) for \\(\\theta_0\\):\nThe bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\n\\] yield the empirical bootstrap distribution \\[\nH_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right)}\n\\] which approximates the bootstrap distribution \\[\nH_{n}^{Boot}(x)=P\\left(\\left.\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] arbitrarily precise as \\(m\\to\\infty\\) (Glivenko-Cantelli Theorem 3.1).\nThus, the empirical bootstrap quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) of \\(H_{n,m}^{Boot}\\) are indeed consistent (\\(m\\to\\infty\\)) for the quantiles \\(\\hat \\tau_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution \\(H_{n}^{Boot}.\\) This implies, for large \\(m,\\) \\[\nP^*\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha.\n\\]\nMoreover, due to the assumed consistencies of the bootstrap and of the estimator \\(\\hat v_n,\\) we have that for large \\(n\\) that \\[\n\\left.{\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v_n^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq  \\hat{\\theta}_n-\\theta_0 \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(- \\hat{\\theta}_n + \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\leq -\\theta_0 \\leq - \\hat{\\theta}_n + \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq \\theta_0 \\leq \\hat{\\theta}_n - \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval (Equation 3.8) \\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\] is indeed an asymptotic (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) CI.\n\n\nExample: Bootstrap-\\(t\\) Confidence Interval for the Mean\nHere \\(\\hat\\theta_n = \\bar{X}_n\\) and the estimator of the asymptotic variance of \\(\\bar{X}_n\\) is \\(s^2\\approx \\lim_{n\\to\\infty}n Var(\\bar{X}_n)=\\sigma_0^2\\), where \\(s^2\\) denotes the sample variance \\[\ns_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2.\n\\]\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(s_n^*=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*_n)^2}\\) to generate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) from \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\] using Equation 3.6.\nThis yields the \\((1-\\alpha)\\times 100 \\%\\) confidence interval (using Equation 3.8): \\[\n\\left[\\bar X_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right),\n    \\bar X_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right)\\right],\n\\] where \\(s_n\\) is computed from the original sample, i.e., \\[\ns_n=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2}.\n\\]\n\n\n\n\n3.5.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the basic bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{v^*_n}\\;\\right|\\;\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(v^*_n\\) depends on the bootstrap sample — not the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)\\;\\right|\\;\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. basic bootstrap vs. bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nBasic bootstrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3  The Bootstrap",
    "section": "3.6 Regression Analysis: Bootstrapping Pairs",
    "text": "3.6 Regression Analysis: Bootstrapping Pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Random and fixed design) \nRandom Design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0,\\) \\(M=\\mathbb{E}(X_iX_i^T)\\) non-singular, and with either\n\nhomoskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0\\), \\(i=1,\\dots,n\\), for a constant \\(\\sigma^2<\\infty\\) or\nheteroskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed Design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean, \\(\\mathbb{E}(\\varepsilon_i)=0,\\) and homoskedastic errors, \\(\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0,\\) for all \\(i=1,\\dots,n.\\)\n\n\n\nThe least squares estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i.\n\\end{align*}\n\\]\nUsing that \\(Y_i=X_i^\\top\\beta_0+\\varepsilon_i,\\) one can derive that \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\beta_0+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\n\n3.6.1 Bootstrapping Pairs: Bootstrap under Random Design\nUnder a random design (Definition 3.6), we assume that there exists a non-singular (thus invertible) matrix \\(M\\) \\[\nM=\\mathbb{E}(X_iX_i^T).\n\\] This implies that the following matrix \\(Q\\) is also non-singular: \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\varepsilon_i^2X_iX_i^T)\\\\[2ex]\n&=\\mathbb{E}(\\mathbb{E}(\\varepsilon_i^2X_iX_i^T|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T\\mathbb{E}(1|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIn case of homoskedastic errors, we have that \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\\\\\n&=\\sigma^2_0\\;\\mathbb{E}(X_iX_i^T)\\\\[2ex]\n&=\\sigma^2_0\\;M.\n\\end{align*}\n\\]\n\n\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta_0)\\rightarrow_d\\mathcal{N}_p(0,M^{-1}QM^{-1}),\\quad n\\to\\infty,\n\\] where \\(\\mathcal{N}_p(0,M^{-1}QM^{-1})\\) denotes the \\(p\\)-dimensional normal distribution with \\((p\\times 1)\\)-dimensional mean \\(0\\) and \\((p\\times p)\\)-dimensional variance-covariance matrix \\(M^{-1}QM^{-1}.\\)\nThe idea of bootstraping pairs is very simple: The procedure builds upon the assumption that \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. which suggests a bootstrap based on resampling the pairs \\((Y_i,X_i),\\) \\(i=1,\\dots,n.\\)\nBootstraping Pairs Algorithm:\n\nGenerate bootstrap samples \\[\n  (Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n  \\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*):\\) \\[\n\\hat\\beta^*_n=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nRepeating Steps 1-2 \\(m\\)-many times yields \\(m\\) (e.g. \\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\\approx\\mathcal{N}_p(0,M^{-1}QM^{-1})\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3  The Bootstrap",
    "section": "3.7 Regression Analysis: Residual Bootstrap",
    "text": "3.7 Regression Analysis: Residual Bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure (Section 3.6.1) proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoskedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] under fixed design (Definition 3.6), where \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d. with zero mean \\[\n\\mathbb{E}(\\varepsilon_i)=0\n\\] and homoskedastic errors \\[\n\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0.\n\\]\n\n\n\n\n\n\nApplicability of the Residual Bootstrap under Random Designs\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs—even when the \\(X\\)-variables are correlated (e.g. time-series).\nIn such cases, the following arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\).\nThe above assumptions on the error terms then, of course, also have to be satisfied conditionally on \\(X_1,\\dots,X_n.\\)\n\n\nThe idea of the residual bootstrap is very simple: The procedure builds upon the assumption that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i=Y_i-X_i^T\\hat\\beta_n, \\quad i=1,\\dots,n,\n\\] where \\[\n\\hat\\beta_n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator based on the original sample \\(\\mathcal{S}_n\\).\nIt is well known that \\[\n\\hat\\sigma^2_n= \\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides a consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\n\\hat\\sigma^2_n\\rightarrow_p \\sigma_0^2\n\\] as \\(n\\to\\infty.\\)\nResidual Bootstrap Algorithm:\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta_n\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta_n + \\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*_n = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) (e.g. \\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\hat\\beta^*_{n,2},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\n\nMotivating the Residual Bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoskedastic (!) errors. We have \\[\n\\hat\\beta_n-\\beta_0=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) we have that \\[\n\\sqrt{n}(\\hat\\beta_n - \\beta_0)\\to_d\\mathcal{N}_p(0,\\sigma^2_0 M^{-1}),\n\\] where \\(\\mathcal{N}_p(0,\\sigma^2 M^{-1})\\) denotes the \\(p\\) dimensional normal distribution with \\((p\\times 1)\\) mean \\(0\\) and \\((p\\times p)\\) variance-covariance matrix \\(\\sigma^2_0 M^{-1}.\\)\nOn the other hand (the bootstrap world), we have the construction \\[\n\\hat\\beta^*_n - \\hat\\beta_n\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*.\n\\] Conditionally on \\({\\cal S}_n,\\) the bootstrap error terms \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) are i.i.d with \\[\n\\mathbb{E}(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 = \\hat\\sigma^2_n,\n\\] where \\(\\hat\\sigma^2_n\\to\\sigma^2_0\\) as \\(n\\to\\infty.\\)\nAn appropriate central limit theorem argument implies that \\[\n\\left.\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n)\\right|\\mathcal{S}_n\\to_d\\mathcal{N}\\left(0,\\sigma^2_0\\, M\\right),\n\\] as \\(n\\to\\infty.\\)\nThat is, for large \\(n\\), we have that \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta_n-\\beta_0))}_{\\mathcal{N}\\left(0,\\sigma^2_0\\, M\\right)}\n\\]\n\n\n3.7.1 Bootstrap Confidence Intervals for the Regression Coefficients\n\nBasic Bootstrap Confidence Intervals for \\(\\beta_{0,j}\\)\nThis allows to construct basic bootstrap confidence intervals for the \\(j\\)th regression coefficient \\(\\beta_{0,j}\\), \\(j=1,\\dots,p\\), of \\(\\beta_0\\in\\mathbb{R}^p.\\)\n\nGenerate \\(m\\) (e.g. \\(m=100,000\\)) bootstrap realizations \\[\n\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast\n\\]\nDetermine the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) from the bootstrap realizations \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast\\) using Equation 3.7.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval as in Equation 3.8: \\[\n\\left[2\\hat\\beta_{nj}-\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_{nj}-\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\right],\n\\] where \\(\\hat\\beta_{nj}\\) denotes the \\(j\\)th component of \\(\\hat\\beta_{n}\\) computed from the original sample \\(\\mathcal{S}_n,\\) and where the empirical quantiles \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) are computed from the bootstrap estimates \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast.\\)\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroskedastic.\nThis is not true for the standard confidence intervals usually provided by standard software packages. For instance, the standard confint(object) function in R for an object returned by the lm() function uses the standard error formula for homoskedastic errors.\n\n\n\n\n\nBootstrap-\\(t\\) Confidence Intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), i.e., \\[\n\\gamma_{jj}:=\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_{n,j}-\\beta_{0,j})}{\\hat\\sigma_n\\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\n\\] is an asymptotically pivotal statistic, \\[\n\\frac{\\sqrt{n}(\\hat\\beta_{n,j}-\\beta_{0,j})}{\\hat\\sigma_n\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_{0,j}\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\n\nGenerate bootstrap estimates \\[\nT^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*,\n\\] where \\[\nT^*_{n}=\\frac{\\hat\\beta_{n,j}^*-\\hat\\beta_{0,j}}{\\hat\\sigma_n^* \\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat\\sigma^{*2}_n:=\\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2}.\n\\]\nCompute the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q_{n,1-\\frac{\\alpha}{2},j}\\) (see Equation 3.7) from the bootstrap estimates \\(T^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*.\\)\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.9: \\[\n\\left[\n  \\hat\\beta_{n,j}-\\hat q_{1-\\frac{\\alpha}{2},n,j}\\hat\\sigma_n \\sqrt{\\gamma_{jj}},\\;\n  \\hat\\beta_{n,j}-\\hat q_{\\frac{\\alpha}{2},n,j}\\hat\\sigma_n \\sqrt{\\gamma_{jj}}\n\\right],\n\\] where \\(\\hat\\beta_{n,j}\\) and \\(\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\\) are computed from the original sample \\(\\mathcal{S}_n.\\)\n\n\n\n\n\n\n\nTip\n\n\n\nThere are furhter bootstrap procedures. In case of heteroskedastic errors, for instance, there’s also the Wild Bootstrap or the Multiplier Bootstrap (see Section 6 in Horowitz (2001)). These two methods are similar to the residual bootstrap since it generates new outcome variables \\(Y^\\ast_i\\) by generating new residuals, conditionally on the predictors \\(X_i\\). However these Boostrap methods generate new residuals by multiplying a real random variable \\(W_i\\in\\mathbb{R}\\) to the observed (not resampled) residuals \\[\nY_i^\\ast = X_i^\\top \\hat\\beta_n + \\hat\\varepsilon_n \\cdot W_i,\n\\] \\(W_i\\) has a specific distribution (Rademacher distribution, Mammen’s distribution, Normal distribution, etc). This approach is very successful and also works in high-dimensional (\\(p\\) as large as \\(n\\) or larger) problems."
  },
  {
    "objectID": "Ch3_Bootstrap.html#exercises",
    "href": "Ch3_Bootstrap.html#exercises",
    "title": "3  The Bootstrap",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nConsider the empirical distribution function \\[\nF_n(x) = \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] for a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim} F.\n\\]\n\nDerive the exact distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nDerive the asymptotic distribution of \\(F_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nShow that \\(F_n(x)\\) is a point-wise (weakly) consistent estimator of \\(F(x)\\) for each given \\(x\\in\\mathbb{R}\\).\n\n\n\nExercise 2.\n\n\n\n\n\n\nTip\n\n\n\nExercise 1 shows that the empirical distribution function is a point-wise consistent estimator for each given \\(x\\in\\mathbb{R}.\\) However, point-wise consistency generally does not imply uniformly consistency for all \\(x\\in\\mathbb{R},\\) and therefore the Clivenko-Cantelli (Theorem 3.1), which shows uniform consistency of the empirical distribution function, is so important.\nThis exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.\n\n\nPoint-wise convergence of a function \\(g_n(x),\\) i.e., \\[\n|g_n(x) - g(x)|\\to 0\n\\] for each \\(x\\in\\mathcal{X}\\subset\\mathbb{R}\\) as \\(n\\to\\infty\\) generally does not imply uniform convergence, i.e., \\[\n\\sup_{x\\in\\mathcal{X}}|g_n(x) - g(x)|\\to 0\n\\] as \\(n\\to\\infty.\\)\nShow this by providing an example for \\(g_n\\) which converges point-wise, but not uniformly for \\(x\\in\\mathcal{X}\\).\n\n\n\nExercise 3.\nConsider the following setup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(Var(X_i)=\\sigma^2&lt;\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\n\nDerive the classic confidence interval for \\(\\mu\\) using the asymptotic normality of the estimator \\(\\bar{X}.\\) Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of \\(n=20\\) and,\n\n\nPart 1: For \\(F\\) being the normal distribution with \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\), and\nPart 2: For \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom.\n\n\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-\\(t\\) confidence interval.\n\n\n\nExercise 4.\n\nLet \\[\n\\mathcal{S}_n = \\{Y_1 , \\dots, Y_n\\}\n\\] be an i.i.d. random sample \\[\nY_1 , \\dots, Y_n\\overset{\\text{i.i.d.}}{\\sim} Y\n\\] with mean \\(\\mathbb{E}(Y)=\\mu.\\) Let \\(\\bar{Y}\\) be the sample mean computed from \\(\\mathcal{S}_n.\\)\nLet \\[\n\\mathcal{S}^*_n = \\{Y_1^∗,\\dots, Y_n^∗\\}\n\\] be an i.i.d. random sample taken independently and with replacement from \\(\\mathcal{S}_n= \\{Y_1 , \\dots, Y_n\\}.\\) Let \\(\\bar{Y}^*\\) be the sample mean computed from \\(\\mathcal{S}^*_n = \\{Y_1^∗,\\dots, Y_n^∗\\}.\\)\n\nShow that \\[\n\\mathbb{E}^*(\\bar{Y}^*) = \\bar{Y}\n\\]\nShow that \\[\n\\mathbb{E}(\\bar{Y}^*) = \\mu\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#references",
    "href": "Ch3_Bootstrap.html#references",
    "title": "3  The Bootstrap",
    "section": "References",
    "text": "References\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. 3rd ed. Wiley.\n\n\nDavidson, Russell, and Emmanuel Flachaire. 2008. “The Wild Bootstrap, Tamed at Last.” Journal of Econometrics 146 (1): 162–69.\n\n\nDavison, Anthony Christopher, and David Victor Hinkley. 2013. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\n\n\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer Science.\n\n\nHorowitz, Joel L. 2001. “The Bootstrap.” In Handbook of Econometrics, 5:3159–3228.\n\n\nKoike, Yuta. 2024. “High-Dimensional Bootstrap and Asymptotic Expansion.” arXiv Preprint arXiv:2404.05006.\n\n\nMammen, Enno. 1992. “When Does Bootstrap Work: Asymptotic Results and Simulations.” Lecture Notes in Statistics 77.\n\n\n———. 1993. “Bootstrap and Wild Bootstrap for High Dimensional Linear Models.” The Annals of Statistics 21 (1): 255–85.\n\n\nShao, Jun, and Dongsheng Tu. 1996. The Jackknife and Bootstrap. Springer Science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-BasicBootstrap",
    "href": "Ch3_Bootstrap.html#sec-BasicBootstrap",
    "title": "3  The Bootstrap",
    "section": "3.3 The Basic Bootstrap Method",
    "text": "3.3 The Basic Bootstrap Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption on \\(F.\\) The basic bootstrap method is often also called:\n\n(Standard) Nonparametric Bootstrap Method or\nNonparametric Percentile Bootstrap Method\n\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with real valued \\(X\\sim F.\\)\nThe distribution \\(F\\) is depends on an unknown parameter \\(\\theta_0.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate \\(\\theta_0\\in\\mathbb{R}.\\)\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta_n\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\nMoreover, for simplicity let us focus on unbiased and \\(\\boldsymbol{\\sqrt{n}}\\)-consistent estimators, i.e.\n\n\\(\\mathbb{E}\\left(\\hat\\theta_n\\right)=\\theta_0\\)\n\\(\\operatorname{SE}\\left(\\hat\\theta_n\\right)=\\sqrt{Var\\left(\\hat\\theta_n\\right)}=\\frac{1}{\\sqrt{n}}\\cdot\\text{constant}\\)\n\n\nInference (approximate for \\(\\boldsymbol{n\\to\\infty}\\)): In order to provide standard errors, construct confidence intervals, and perform tests of hypothesis, we need to know the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\] I.e. we need to know the limit of the distribution function \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWe could use asymptotic statistics to derive this limit. For instance, using the Lindeberg-Lévy CLT, we may be able to show that the limit of \\(H_{n}(x)\\) is the distribution function of the normal distribution with mean zero and asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big).\\)\nHowever, deriving a useful, explicit expression of the asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big)\\) can be very hard (see Section 3.1). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific versions of the Bootstrap (Section 3.4) can be even more accurate then an asymptotic normality result.\n\n\n\n\n\n\nThe Core Part of the Bootstrap Algorithm\n\n\n\n\nDraw a bootstrap sample: Generate a new random sample \\[\nX_1^*,\\dots,X_n^*\n\\] by drawing observations independently and with replacement from the available sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*_n\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (for a large value of \\(m,\\) such as \\(m=10000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\]\n\n\n\nBy the Clivenko-Cantelli (Theorem 3.1) the bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] allow us to approximate the bootstrap distribution\n\\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\right|\\mathcal{S}_n\\right)\n\\] arbitrarily well, i.e., \\[\n\\sup_{x\\in\\mathbb{R}}\\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\\right|\\to_{a.s} 0\\quad\\text{as}\\quad m\\to\\infty,\n\\] where \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\left(\\hat\\theta^*_{n,j}-\\hat\\theta_n\\right)\\leq x\\right)}\n\\] denotes the empirical distribution function based on the \\(\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\\) centered by \\(\\hat{\\theta}_n\\) and scaled by \\(\\sqrt{n}.\\)\nSince we can choose \\(m\\) arbitrarily large, we can effectively ignore the approximation error between \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Boot}_{n}(x)\\) and proceed as if \\[\nH^{Boot}_{n,m}(x)\\overset{(m\\to\\infty)}{=}H^{Boot}_{n}(x)\\quad\\text{for all}\\quad x.\n\\]\nThe crucial question is, however, whether the (effectively known) bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] can be used to estimate the limit distribution \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty?\n\\] This is a basic requirement called bootstrap consistency. If a bootstrap method is inconsistent, you shall not use it in practice.\n\nBootstrap Consistency\nThe bootstrap does not always work.\nA necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\[\n\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)|\\mathcal{S}_n\n\\] is a good approximation of the distribution of \\[\n\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big);\n\\] i.e., if \\[\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)\\ |{\\cal S}_n\\right)}_{H_n^{Boot}}\\approx\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\right)}_{H_n}.\n\\] for large \\(n.\\)\nThe following definition states this more precisely.\n\n\n\n\n\n\n\nDefinition 3.3 (Bootstrap Consistency)  Let the limit (as \\(n\\to\\infty\\)) of \\(H_n\\) be a non-degenerate distribution. Then the bootstrap is consistent if and only if \\[\n\\sup_{x\\in\\mathbb{R}} \\Big|\\;\n\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta^*_n-\\hat\\theta_n\\big)\\le x \\ |{\\cal S}_n\\Big)}_{H_n^{Boot}(x)}\n  -\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta_n -\\theta_0\\big)\\le x\\Big)}_{H_n(x)}\n  \\Big|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\n\n\n\nLuckily, the standard bootstrap is consistent in a large number of statistical problems. Typically, the bootstrap is consistent if the following two requirements hold:\n\nThe bootstrap sampling process must reflect the original sampling process. For instance:\n\nif the original sample was generated by i.i.d. sampling, then also the bootstrap samples need to be generated by i.i.d. sampling.\nif the original sample was generated by cluster sampling, then also the bootstrap samples need to be generated by cluster sampling.\n\nTypically, the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\n\\] needs to be asymptotically (\\(n\\to\\infty\\)) normal.\n\n\nTheorem 1 in Mammen (1992) shows that the basic bootstrap is consistent if \\(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\to_d\\mathcal{N}(0,v_0^2),\\) under the assumption that the bootstrap sampling process (e.g. i.i.d.) equals the original sampling process.\n\nThe standard bootstrap will usually fail if one of the above conditions is violated.\n\nNote: In order to deal with more complex sampling schemes alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference About the Population Mean\nSetup:\n\n\\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\)\nContinuous random variable \\(X\\) (not necessary, but allows working with the density function \\(f\\))\nNon-zero, finite variance \\(0&lt;Var(X)=\\sigma_0^2&lt;\\infty\\)\nUnknown mean \\(\\mathbb{E}(X)=\\mu_0,\\) where\n\\[\n\\mu_0 = \\int x f(x) dx = \\int x d F(x),\n\\] where \\(f=F'\\) denotes the density function.\nEstimator: Empirical mean \\[\n\\begin{align*}\n\\bar{X}_n\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nFor doing inference (approximate for \\(\\boldsymbol{n\\to\\infty}\\)) about \\(\\boldsymbol{\\mu_0}\\) we need to know the (asymptotic) distribution of \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nExample 3.2 (Recap: Inference about \\(\\mu_0\\) using Classic Asymptotic Statistics) By the Lindeberg-Lévy CLT we know that \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\to_d\\mathcal{N}\\left(0,v^2\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] which is equivalent to \\[\n%\\bar{X}_n\\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{1}{n}\\sigma_0\\right).\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{v}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x,\\) where \\(\\Phi_{v}\\) denotes the distribution function of the normal distribution with mean zero and standard deviation \\(v,\\) (variance \\(v^2\\)), \\[\n\\Phi_{v}(x)=\\Phi\\left(\\frac{x}{v}\\right)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x/v}\\exp\\left(-\\frac{1}{2}z^2\\right)\\,dz\n\\] with \\(\\Phi\\) denoting the distribution function of the standard normal distribution.\nHowever, without knowledge of \\(v^2,\\) the above result is of limited use. Luckily, in this example, it is rather simple to derive an useful expression for the asymptotic variance \\(v^2\\colon\\) \\[\n\\begin{align*}\nv^2\n&=\\lim_{n\\to\\infty}Var\\left(\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\right)%\\\\[2ex]\n=\\lim_{n\\to\\infty}nVar(\\bar{X}_n)\\\\[2ex]\n&=\\lim_{n\\to\\infty}nVar\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right)\\\\[2ex]\n&\\left[\\text{Under our iid setup: }X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\right]\\\\[2ex]\n&=\\lim_{n\\to\\infty}\\frac{n}{n^2} nVar\\left(X\\right)%\\\\[2ex]\n=Var\\left(X\\right) = \\sigma_0^2.\n\\end{align*}\n\\tag{3.4}\\]\nThat is, \\[\n\\Phi_{v}(x)=\\Phi_{\\sigma_0}(x)\\quad\\text{for all}\\quad x.\n\\]\nThe parameter \\(\\sigma_0^2=Var\\left(X\\right)\\) is usually unknown, but can be estimated consistently by \\[\n\\hat{\\sigma}^2_n=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i - \\bar{X}_n\\right)^2.\n\\] Thus, for doing inference (constructing confidence intervals, statistical tests, etc.) about \\(\\mu_0,\\) we actually use the estimated distribution \\[\nH^{Asymp}_n(x)=\\Phi_{\\hat\\sigma_n}(x)=\\Phi\\left(\\frac{x}{\\hat\\sigma_n}\\right)\\quad\\text{for all}\\quad x,\n\\] where \\(H^{Asymp}_n(x)\\to\\Phi_{\\sigma_0}(x)\\) as \\(n\\to\\infty\\) for all continuity points \\(x.\\) That is, we use the approximate distribution \\[\n\\begin{align*}\n\\bar{X}_n & \\overset{a}{\\sim}H^{Asymp}_n\n\\quad\\Leftrightarrow\\quad\n\\bar{X}_n \\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{\\hat{\\sigma}_n^2}{n}\\right)\n\\end{align*}\n\\] for computing, for instance, an estimator for the standard error \\(\\operatorname{SE}\\big(\\bar{X}_n\\big),\\) \\[\n\\widehat{\\operatorname{SE}}^{Asymp}_{n}\\big(\\bar{X}_n\\big)=\\sqrt{\\frac{\\hat{v}^2}{n}}=\\sqrt{\\frac{\\hat\\sigma_n^2}{n}}.\n\\tag{3.5}\\] Using \\(\\widehat{\\operatorname{SE}}^{Asymp}_{n}\\big(\\bar{X}_n\\big),\\) we can compute confidence intervals and statistical tests in the usual way.\nNote: The estimator of the standard error in Equation 3.5 requires a formula for the asymptotic variance \\(v^2=\\sigma_0^2\\) as derived in Equation 3.4. Such derivations can be very challenging (see Section 3.1).\n\n\n\n\nLet us now check, whether the Bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] is a legit alternative to \\(H^{Asymp}_n(x).\\) I.e. we check whether we can use \\(H^{Boot}_{n}(x)\\) as an alternative for estimating the limit distribution \\(\\Phi_{v}(x)=\\Phi_{\\sigma_0}(x).\\)\nBefore we answer this question theoretically (see Section 3.3.1.2 and Section 3.3.1.3), we check it empirically using an artifical data example. This is then, of course, not a generally valid mathematical proof.\n\n3.3.1.1 Practice: Empirical Consideration of the Bootstrap distribution \\(\\boldsymbol{H^{Boot}_{n}}\\)\nLet us consider the following (usually unknown) specific setup: \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where\n\n\\(X\\sim \\chi^2_{\\operatorname{df}}\\) with \\(\\operatorname{df}=2\\)\nsample size \\(n=100\\)\n\nThis setup implies (see Example 3.2) that\n\nTrue (usually unknown) mean: \\(\\mu_0=\\mathbb{E}(X)=\\operatorname{df}=2\\)\nTrue (usually unknown) variance: \\(\\sigma_0^2=Var(X)=2\\cdot\\operatorname{df}=4\\)\nTrue (usually unknown) standard deviation: \\(\\sigma_0=\\sqrt{Var(X)}=2\\)\nTrue (usually unknown) asymptotic limit distribution: \\[\n\\begin{align*}\n\\sqrt{n}(\\bar{X}_n-\\mu_0)&\\to_d\\mathcal{N}(0,4)\\quad\\text{as}\\quad n\\to\\infty,\\\\[2ex]\n\\Leftrightarrow\\quad H_n(x)&\\to \\Phi_{2}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\end{align*}\n\\] for all continuity points \\(x.\\)\n\nWhen doing classic asymptotic inference (see Example 3.2), we would use the feasible \\[\nH^{Asymp}_n(x)=\\Phi_{\\hat\\sigma_n}(x)\n\\] to estimate the (usually unknown) asymptotic limit distribution \\(\\Phi_{2}(x).\\)\nLet us now check (by simulation of artifical data) whether the bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] can be a legit alternative for estimating the (usually unknown) \\(\\Phi_{2}(x).\\)\nThe following R-code generates artifical data \\[\n\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\n\\] by (i.i.d.) sampling \\(n=100\\) data points from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\) The first six data points are shown in Table 3.1.\n\nset.seed(123)\nobservedSample &lt;- rchisq(n = 100, df = 2)\n\n\n\n\nTable 3.1: First six data points of the observed realization of the random sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=100\\) drawn from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\)\n\n\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n0.36\n\n\n2\n3.39\n\n\n3\n3.24\n\n\n4\n4.90\n\n\n5\n1.76\n\n\n6\n5.33\n\n\n…\n…\n\n\n\n\n\n\nSo the observed sample mean is\n\n\\(\\bar X_{n} =\\) mean(observedSample) \\(=\\) 1.8\n\n\nBasic Bootstrap Method:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical “population” from which we generate i.i.d. bootstrap samples \\[\nX_1^*,\\dots,X_n^*.\n\\] by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}.\\)\nEach realization of the bootstrap sample leads to a new realization of the bootstrap estimator \\[\n\\bar{X}^*_n=\\bar{X}^*(X_1^*,\\dots,X_n^*).\n\\] The following R code generates realizations of \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\n\\] for \\(m=10000\\) Bootstrap replications.\n\n# Number of bootstrap replications \nm                &lt;- 10000 \n# Container to save the bootstrap estimates\nXbar_boot        &lt;- vector(mode = \"double\", length = m)\n\n# Basic Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          &lt;- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        &lt;- mean(bootSample)\n}\n\n# Pinting the first three elements of Xbar_boot:\nXbar_boot[1:3]\n\n[1] 0.6649553 2.2564627 1.8510396\n\n\nBased on the realizations of \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m},\n\\] we can compute the empirical Bootstrap distribution function \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)},\n\\] which approximates (arbirary well as \\(m\\to\\infty\\)) the bootstrap distribution \\[\nH^{Boot}_n(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\n\\]\n\nThus, by choosing a large \\(m\\) (e.g. \\(m=10000\\)) we proceed as if \\[\nH^{Boot}_{n,m}(x)=H^{Boot}_{n}(x)\\quad\\text{for all}\\quad x.\n\\]\n\nIf \\[\nH^{Boot}_{n,m}(x)\\approx \\Phi_{2}(x)\\quad\\text{for all}\\quad x,\n\\] then we can use \\(H^{Boot}_{n,m}(x)\\) (as an alternative to \\(H^{Asymp}_{n}(x)\\)) to do inference about \\(\\mu_0.\\)\nThe following R-code computes and compares graphically the following three distribution functions:\n\n\\(H^{Boot}_{n,m}(x) =\\) ecdf( sqrt(n) * (Xbar_boot - Xbar) ) \n\\(H^{Asymp}_{n}(x) = \\Phi_{\\hat{\\sigma}_n}(x)=\\Phi\\left(\\frac{x}{\\hat{\\sigma}_n}\\right)\\)\n\\(\\Phi_{2}(x)=\\Phi\\left(\\frac{x}{2}\\right)\\)\n\n\nn                &lt;- length(observedSample)\nXbar             &lt;- mean(observedSample)\n\nm                &lt;- 10000 # number of bootstrap samples \nXbar_boot_vec    &lt;- vector(mode = \"double\", length = m)\n\n## Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          &lt;- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot_vec[k]        &lt;- mean(bootSample)\n}\n\nplot(ecdf( sqrt(n) * (Xbar_boot_vec - Xbar) ), \n     xlab = \"\", ylab = \"\", \n     main = \"Bootstrap Distribution vs Normal Limit Distribution\")\ncurve(pnorm(x, mean = 0, sd = sqrt(4)), col = \"red\", add = TRUE)\ncurve(pnorm(x, mean = 0, sd = sqrt(var(observedSample))), col = \"blue\", lty = 2, add = TRUE)     \nlegend(\"topleft\", \n       legend = c(expression(H['n,m']^{'Boot'}),\n                  expression(H['n']^{'Asymp'}), \n                  \"Normal Distr. (Mean = 0 and SD = 2)\"), \n      col = c(\"black\", \"blue\", \"red\"), lty = c(1,2,1))\n\n\n\n\n\n\n\nFigure 3.1: Both \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Asymp}_{n}(x)\\) provide here good approximations to the (usually unknown) asymptotic limit distribution \\(\\Phi_{2}(x).\\)\n\n\n\n\n\nFigure 3.1 shows that both \\[\nH^{Boot}_{n,m}(x)\\quad\\text{and}\\quad H^{Asymp}_{n}(x)\n\\] provide good approximations to the (usually unknown) asymptotic limit distribution \\(\\Phi_{2}(x)\\).\nThe bootstrap distribution \\[\n\\begin{align*}\nH^{Boot}_{n,m}(x)&=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)}\\\\[2ex]\n&\\overset{m\\to\\infty}{=}P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)=H^{Boot}_n(x)\n\\end{align*}\n\\] can be computed directly from \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\\) There’s no need for deriving a formula \\(v^2=\\sigma_0^2\\) for the asymptotic variance as it is necessary for \\(H^{Asymp}_{n}(x)\\) (see Example 3.2).\nIn fact, using \\[\nH^{Boot}_n(x) = P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right),\n\\] we can derive an estimator for the asymptotic variance \\[\nv^2=\\lim_{n\\to\\infty}Var\\big(\\sqrt{n}\\big(\\bar{X}_n-\\mu_0\\big)\\big)\n\\] directly from \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\\colon\\)\n\\[\n\\begin{align*}\n\\hat{v}^2_{Boot,n}\n&=Var\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\\\\[2ex]\n&=n \\; Var\\left(\\left.\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\\\\[2ex]\n&\\left[\\text{Conditionally on $\\mathcal{S}_n,$ $\\bar{X}_n$ is constant}\\right]\\\\[2ex]\n&=n \\; Var\\left(\\left.\\bar{X}^*_n\\;\\right|\\;\\mathcal{S}_n\\right),\n\\end{align*}\n\\] where the unknown \\(Var\\big(\\left.\\bar{X}^*_n\\;\\right|\\;\\mathcal{S}_n\\big)\\) can be consistently (for \\(m\\to\\infty\\)) estimated by \\[\n\\begin{align*}\n\\widehat{Var}_m\\left(\\left.\\bar{X}^*_n\\;\\right|\\;\\mathcal{S}_n\\right) = \\frac{1}{m}\\sum_{j=1}^m \\left(\\bar{X}^*_{n,j} - \\left(\\frac{1}{m}\\sum_{j=1}^m\\bar{X}^*_{n,j}\\right)\\right)^2\n\\end{align*}\n\\]\n\nBy choosing a large \\(m\\) (e.g. \\(m=10000\\)) we proceed as if \\[\n\\widehat{Var}_m\\big(\\left.\\bar{X}^*_n\\;\\right|\\;\\mathcal{S}_n\\big)=Var\\big(\\left.\\bar{X}^*_n\\;\\right|\\;\\mathcal{S}_n\\big)\n\\]\n\nThe estimator \\[\n\\hat{v}^2_{Boot,n}=n \\; \\widehat{Var}_m\\left(\\left.\\bar{X}^*_n\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] can be computed directly from \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\\) There’s no need for deriving a formula \\(v^2=\\sigma_0^2\\) for the asymptotic variance as in Example 3.2. For our artifical data example, we have that\n\n\\(\\hat{v}^2_{Boot,n}=\\) n * var(Xbar_boot_vec) \\(=\\) 3.41\n\n\nThis yields an estimate of the standard error \\(\\operatorname{SE}\\left(\\bar{X}_n\\right)\\)\n\n\\(\\widehat{SE}_{Boot,n}\\left(\\bar{X}_n\\right)=\\sqrt{\\frac{\\hat{v}^2_{Boot,n}}{n}}\\;=\\) sd(Xbar_boot_vec) \\(=\\) 0.18\n\n\n\n\n\n\n\n\nNote\n\n\n\nComputing the Basic Bootstrap estimate of the standard error of an estimator is typically that simple: Compute the standard deviation of \\(\\hat\\theta^*_{n,1},\\dots,\\hat\\theta^*_{n,m}\\) for a large \\(m.\\)\n\n\n\n\n\n3.3.1.2 Theory (Part 1): Mean and Variance of the Bootstrap distribution\nLet \\[\n\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\n\\] where \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\quad\\text{with}\\quad X\\sim F,\n\\] and let \\[\nX_1^*,\\dots,X_n^*\\overset{\\text{i.i.d.}}{\\sim}X^*\\quad\\text{with}\\quad X^*\\sim F_n,\n\\] where \\(F_n\\) denotes the ecdf computed from \\(X_1,\\dots,X_n.\\)\nIn this chapter we begin with the theoretical consideration of the Bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right).\n\\]\nWe begin with focusing on the mean and the variance and check whether for large \\(n\\) (\\(b\\to\\infty\\)) the mean and the variance of \\(H^{Boot}_{n}\\) equals the mean and the variance of \\(H_n.\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\,\\cdot\\,),\\) \\(Var^*(\\,\\cdot\\,),\\) and \\(P^*(\\,\\cdot\\,)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\,\\cdot\\,),\\;Var^*(\\,\\cdot\\,),\\;\\text{and}\\;P^*(\\,\\cdot\\,)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\,\\cdot\\,)=\\mathbb{E}(\\,\\cdot\\,|\\mathcal{S}_n),\n\\] the conditional variance \\[\nVar^*(\\,\\cdot\\,)=Var(\\,\\cdot\\,|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\,\\cdot\\,)=P(\\,\\cdot\\,|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the bootstrap distribution, i.e. on the conditional distribution of \\[\n\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n.\n\\]\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze and simulate the bootstrap distribution of \\(\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n,\\) since we know 🤟 the discrete distribution of the conditional random variables \\[\nX^*|\\mathcal{S}_n\n\\] even though, we do not know the distribution of \\(X\\sim F.\\)\n\n\n\nThe possible values of the discrete random variable \\(X^*|\\mathcal{S}_n\\) are \\[\nX^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values are equally probable: \\[\n\\begin{align*}\nP^*(X^*=X_1)&= P(X^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X^*=X_2)&= P(X^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X^*=X_n)&= P(X^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance:\n\nThe conditional mean of \\(X^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X^*)\n&=\\mathbb{E}(X^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X_n.\n\\end{align*}\n\\] I.e., the empirical mean \\(\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” mean of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\nThe conditional variance of \\(X^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\nVar^*(X^*)\n&=Var(X^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}\\left((X^* - \\mathbb{E}(X^*|{\\cal S}_n))^2|{\\cal S}_n\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\\\[2ex]\n&=\\hat\\sigma^2_n.\n\\end{align*}\n\\] I.e., the empirical variance \\(\\hat\\sigma^2_n=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\) of the original sample \\(X_1,\\dots,X_n\\) is the “population” variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X^*))=\\mathbb{E}(g(X^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq x)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is crucial.\nThe unconditional distribution of \\(X^*\\) is equal to the unknown distribution \\(F.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X^*\\leq x)\n&= P(1_{(X^*\\leq x)}=1) \\\\[2ex]\n&= P(1_{(X^*\\leq x)}=1) \\cdot 1 + P(1_{(X^*\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= \\mathbb{E}\\left(1_{\\left(X^*\\leq x\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\mathbb{E}\\left(1_{\\left(X^*\\leq x\\right)}|\\mathcal{S}_n\\right)}\\right)\\quad[\\text{{\\color{blue}law of iterated expectations}}]\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq x\\right)}}\\right)\\quad[\\text{{\\color{blue}from our derivations above}}]\\\\[2ex]\n&\\quad[\\text{Using that $X_1\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X$ with $X\\sim F\\colon$}]\\\\[2ex]\n&= \\frac{n}{n}\\mathbb{E}\\left(1_{\\left(X\\leq x\\right)}\\right)\\\\[2ex]\n&= P(1_{(X\\leq x)}=1) \\cdot 1 + P(1_{(X\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= P\\left(X\\leq x\\right)=F(x)\n\\end{align*}\n\\]\n\n\nUsing our above results, \\[\n\\begin{align*}\n\\mathbb{E}(X^*|\\mathcal{S}_n)&=\\bar{X}_n\\\\[2ex]\nVar(X^*|\\mathcal{S}_n) &=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2,\n\\end{align*}\n\\] we can consider the conditional mean and the conditional variance of \\[\n\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n,\n\\] for a given realization of \\(\\mathcal{S}_n.\\)\n\nThe conditional mean of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=\\mathbb{E}\\left(\\left.\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\,\\mathbb{E}\\left(\\left.\\bar X^*_n-\\bar{X}_n\\right|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right)- \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right)\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\color{red}\\mathbb{E}\\left(X^*_i|{\\cal S}_n\\right)}- \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}\\mathbb{E}\\left(X_i|{\\cal S}_n\\right)}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{n}{n}{\\color{red}\\bar{X}_n} - \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}X_i}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\bar{X}_n - \\bar{X}_n\\right)\\\\[2ex]\n&= 0%\\\\[2ex]\n%\\Leftrightarrow\\quad \\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right) & =  \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right).\n\\end{align*}\n\\]\nThe conditional variance of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\nVar^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=Var\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\left(\\big(\\bar X^*_n-\\bar{X}_n\\big)|{\\cal S}_n\\right)\\\\[2ex]\n&[\\text{Conditionally on a given}\\\\\n&\\text{ realization of $\\mathcal{S}_n,$ $\\bar{X}_n$ is a constant:}]\\\\[2ex]\n&=n\\,Var\\big(\\bar X^*_n|{\\cal S}_n\\big)\\\\[2ex]\n&=n\\,Var\\Big(\\frac{1}{n}\\sum_{i=1}^n X_i^*\\Big|{\\cal S}_n\\Big)\\\\\n&=n\\,\\frac{1}{n^2}\\sum_{i=1}^n Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=n\\,\\frac{n}{n^2} Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=Var\\big(X_i^*|{\\cal S}_n\\big)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\quad[\\text{derived above}]\\\\[2ex]\n&=\\hat\\sigma^2_n,\n\\end{align*}\n\\] where \\[\n\\hat\\sigma^2_n\\to_p \\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nSummary\n\n\n\nFor large \\(n\\) (\\(n\\to\\infty\\)) the bootstrap distribution, i.e. the distribution of \\[\n\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n\n\\] and the distrubtion of \\[\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n\\] have both mean zero and variance \\(\\sigma^2_0.\\)\nThat is, as \\(n\\to\\infty,\\) both distributions become equal with respect to their means and variances.\n\n\n\n\n3.3.1.3 Theory (Part 2): Bootstrap Consistency\nIn this chapter we continue our theoretical consideration of the Bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right),\n\\] but consider now the total distribution—not only mean and variance.\n\n\n\n\n\n\n\nDefinition 3.4 (Characteristic Function) Let \\(X\\in\\mathbb{R}\\) be a random variable and let \\(\\mathcal{i}=\\sqrt{-1}\\) be the imaginary unit. Then the function \\(\\psi_X:\\mathbb{R}\\to\\mathbb{C}\\) defined by \\[\n\\psi_X(t) = \\mathbb{E}(\\exp(\\mathcal{i}tX))\n\\] is called the characteristic function of \\(X.\\)\n\n\n\n\n\n\n\n\n\n\nCharacteristic Function: Some useful facts\n\n\n\nThe characteristic function …\n\n… uniquely determines its associated probability distribution \\(F\\) of \\(X.\\)\n… can be used to easily derive (all) the moments of a random variable by \\[\n\\mathbb{E}(X^n) = \\mathcal{i}^n \\left.\\frac{d^n}{d t^n}\\psi_X(t)\\right|_{t=0}\n\\]\n… is often used to prove that two distributions are equal.\nThe characteristic function of \\(\\Phi_{\\sigma_0}\\) is \\[\n\\begin{align*}\n\\psi_{\\Phi_{\\sigma_0}}(t)\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\n\\end{align*}\n\\tag{3.6}\\]\nThe characteristic function of \\(\\sum_{i=1}^nW_i,\\) where \\(W_1,\\dots,W_n\\overset{\\text{i.i.d.}}{\\sim}W,\\) is \\[\n\\psi_{\\sum_{i=1}^nW_i}(t)=\\big(\\psi_{W}(t)\\big)^n\n\\tag{3.7}\\]\nLet \\(W\\) be a random variable with \\(\\mathbb{E}(W)=0\\) and \\(Var(W)=\\sigma_W^2.\\) Then, we have that (see Equation (26.11) in Billingsley (1995)) \\[\n\\psi_W(t)=1-\\frac{1}{2}\\sigma_W^2 \\, t^2 + \\lambda(t),\n\\tag{3.8}\\] where \\(|\\lambda(t)|\\leq t^2\\,\\mathbb{E}\\left(\\min(|t|\\,|W|^3, W^2)\\right).\\)\n\n\n\n\nThe following is taken from Example 3.1 in Shao and Tu (1996). First, we sketch the proof for the Lindeberg-Lévy CLT. Second, we use this principle to sketch the proof for bootstrap consistency.\n\nLet \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\n\\] where \\(X\\sim F\\) has\n\nmean \\(\\mathbb{E}(X)=\\mu_0\\) and\nvariance \\(Var(X)=\\sigma^2_0.\\)\n\nIt follows then from the Lindeberg-Lévy CLT that \\[\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x\\in\\mathbb{R}.\\) This CLT-result can be proven by showing that the characteristic function of \\(H_n\\) tends the characteristic function of \\(\\Phi_{\\sigma_0}.\\)\nTo see this, rewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n& = \\sum_{i=1}^n\\overbrace{\\;\\left(\\frac{X_i-\\mu_0}{\\sqrt{n}}\\right)\\;}^{=W_{i,n}}\\\\[2ex]\n& = \\sum_{i=1}^n W_{i,n}\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n&W_{1,n},\\dots,W_{n,n}\\overset{\\text{i.i.d.}}{\\sim}W_n,\\\\[2ex]\n&\\mathbb{E}(W_{n})=\\mathbb{E}\\left(\\frac{X_i-\\mu_0}{\\sqrt{n}}\\right)=\\frac{\\mathbb{E}(X_i)-\\mu_0}{\\sqrt{n}}=0\\quad\\text{for all}\\quad n,\\quad\\text{and}\\\\[2ex]\n&Var(W_{n})=Var\\left(\\frac{X_i-\\mu_0}{\\sqrt{n}}\\right)=\\frac{1}{n}Var(X_i-\\mu_0)=\\frac{1}{n}Var(X_i)=\\frac{1}{n}\\sigma_0^2.\n\\end{align*}\n\\]\nTherefore, by Equation 3.7 together with Equation 3.8 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{n}}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n(t)|\n&\\leq t^2\\,\\mathbb{E}\\left(\\min\\left(|t|\\,\\left|W_{n}\\right|^3, \\left|W_{n}\\right|^2\\right)\\right)\\\\[2ex]\n&= t^2\\,\\mathbb{E}\\left(\\min\\left(|t|\\,\\left|\\frac{X-\\mu_0}{\\sqrt{n}}\\right|^3, \\left|\\frac{X-\\mu_0}{\\sqrt{n}}\\right|^2\\right)\\right)\\\\[2ex]\n&= t^2\\,\\mathbb{E}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X-\\mu_0\\right|^3, n^{-1}\\left|X-\\mu_0\\right|^2\\big)\\right)\\\\[2ex]\n&[\\text{for sufficiently large $n$ and any fixed, finite $t$}\\\\\n&\\text{(Note: relevant $t$-values to get all moments are}\\\\\n&\\text{values around zero.):}]\\\\[2ex]\n&= t^2\\,\\mathbb{E}\\left(|t|\\,n^{-3/2}\\left|X-\\mu_0\\right|^3\\right)\\\\[2ex]\n&= n^{-3/2}\\underbrace{\\;t^2\\,|t|\\,\\mathbb{E}\\left(\\left|X-\\mu_0\\right|^3\\right)}_{\\texttt{constant}}\\\\[2ex]\n%\\Rightarrow\\quad n |\\lambda_n(t)|& \\leq n \\cdot n^{-3/2}\\cdot \\texttt{constant}\\\\[2ex]\n%\\Rightarrow\\quad n |\\lambda_n(t)|& \\leq n^{-1/2} \\cdot \\texttt{constant}.\n\\Rightarrow\\quad |\\lambda_n(t)|& \\leq n^{-3/2}\\cdot \\texttt{constant}\\\\[2ex]\n\\Rightarrow\\quad |\\lambda_n(t)|& =O(n^{-3/2}) \\\\[2ex]\n&=o(n^{-1})\\\\[2ex]\n\\end{align*}\n\\] The latter step follows since sequences of the order of magnitude \\(n^{-3/2}\\) are of a smaller order of magnitude than sequences of the order of magnitude \\(n^{-1}.\\)\nThat is, for large \\(n\\) (\\(n\\to\\infty\\)) and fixed finite \\(t,\\) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\Big(1-\\;\\underbrace{\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2}_{=O(n^{-1})}\\; +\\; \\underbrace{\\lambda_n(t)}_{=o(n^{-1})}\\Big)^n\\\\[2ex]\n\\end{align*}\n\\] the \\(\\lambda_n(t)=o(n^{-1})\\) term becomes negligible in comparison to the \\(\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2=O(n^{-1})\\) term and the constant term (i.e. the \\(1\\)) such that\n \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&= \\lim_{n\\to\\infty}\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t),\n\\end{align*}\n\\] where the latter step follows from Equation 3.6.\nOK, we have shown that \\(H_n\\) tends to \\(\\Phi_{\\sigma_0}\\) by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) That is, we just sketched the proof for the Lindeberg-Lévy CLT.**\nTo show bootstrap consistency we need to show that \\(H_n^{Boot}\\) also tends to \\(\\Phi_{\\sigma_0}.\\) To do so, we can mimic the above proof sketch and show that the characteristic function of \\(H_n^{Boot}\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nLet \\[\nX_1^*,\\dots,X_n^*\\overset{\\text{i.i.d.}}{\\sim}X^*\n\\] where \\(X^*\\sim F_n\\) has\n\nconditional mean \\(\\mathbb{E}^*(X^*)=\\mathbb{E}(X_i^*|\\mathcal{S}_n)=\\bar{X}_n\\) and\nconditional variance \\(Var^*(X)=Var^*(X|\\mathcal{S}_n)=\\hat\\sigma^2_n.\\)\n\nRewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left.\\left(\\bar{X}^*_n- \\bar{X}_n\\right)\\right|\\mathcal{S}_n\n& = \\sum_{i=1}^n\\left.\\;\\overbrace{\\left(\\frac{X^*_i- \\bar{X}_n}{\\sqrt{n}}\\right)}^{W^*_n}\\right.|\\mathcal{S}_n\\\\[2ex]\n& = \\sum_{i=1}^n W^*_{i,n}|\\mathcal{S}_n\n\\end{align*}\n\\] where\n\\[\n\\begin{align*}\n&W^*_{1,n}|\\mathcal{S}_n,\\dots,W^*_{n,n}|\\mathcal{S}_n\\overset{\\text{i.i.d.}}{\\sim}W_n^*|\\mathcal{S}_n,\\\\[2ex]\n&\\mathbb{E}(W^*_{n}|\\mathcal{S}_n)=\\mathbb{E}\\left(\\left.\\frac{X_i^*-\\bar{X}_n}{\\sqrt{n}}\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n&\\phantom{\\mathbb{E}(W^*_{n}|\\mathcal{S}_n)}=\\frac{\\mathbb{E}(X_i^*|\\mathcal{S}_n)-\\bar{X}_n}{\\sqrt{n}}=0\\quad\\text{for all}\\quad n,\\quad\\text{and}\\\\[2ex]\n&Var(W^*_{n}|\\mathcal{S}_n)=Var\\left(\\left.\\frac{X_i^*-\\bar{X}_n}{\\sqrt{n}}\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n&\\phantom{Var(W^*_{n}|\\mathcal{S}_n)}=\\frac{1}{n}Var(X_i^*-\\bar{X}_n|\\mathcal{S}_n)=\\frac{1}{n}Var(X_i^*|\\mathcal{S}_n)=\\frac{1}{n}\\hat\\sigma_n^2\n\\end{align*}\n\\]\nTherefore, by Equation 3.7 together with Equation 3.8 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}^*|\\mathcal{S}_n}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{n}^*|\\mathcal{S}_n}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}{\\color{darkgreen}\\hat{\\sigma}_n^2} \\, t^2 + {\\color{red}\\lambda_n^*(t)}\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n^*(t)|\n&\\leq |t^2|\\,{\\color{blue}\\mathbb{E}^*}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_1-\\bar{X}_n\\right|^3, n^{-1}\\left|X_1^* - \\bar{X}_n\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,{\\color{blue}\\frac{1}{n}\\sum_{i=1}^n}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_i-\\bar{X}_n\\right|^3, n^{-1}\\left|X_i - \\bar{X}_n\\right|^2\\big)\\right).\n\\end{align*}\n\\] By the Marcinkiewicz strong law of large numbers, we obtain that \\[\n\\begin{align*}\nn{\\color{red}|\\lambda^*_n(t)|}&\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\Leftrightarrow\\quad {\\color{red}|\\lambda^*_n(t)|}&=o_{a.s.}(1).\n\\end{align*}\n\\]\nMoreover, \\[\n{\\color{darkgreen}\\hat\\sigma_n^2} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\to_{a.s.}\\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nThus, we have that (using Equation 3.6) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n\\to_{a.s.}&\n\\lim_{n\\to\\infty}\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t).\n\\end{align*}\n\\] This implies that the limit (\\(n\\to\\infty\\)) of \\(H_n^{Boot}\\) is \\(\\Phi_{\\sigma_0}\\) almost surely.\nHence, we have shown that the basic bootstrap is consistent for doing inference about \\(\\mu_0\\) using \\(\\bar{X}_n.\\)\n\n\nIn the following section, we show how to build a confidence interval using the bootstrap distribution of a general estimator \\(\\hat\\theta_n.\\)\n\n\n\n\n3.3.2 The Basic Bootstrap Confidence Interval\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\theta_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup (Classic Asymptotic Statistics):\n\n\\(\\theta_0\\in\\mathbb{R}\\quad\\) and\n\\(\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2)\\quad\\) as \\(\\quad n\\to\\infty,\\)\n\\(\\hat{v}_n\\to_{p} v_0\\quad\\) as \\(\\quad n\\to\\infty\\)\n\nAn approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}_n - z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}},\n\\hat{\\theta}_n + z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}}\n\\right],\n\\] where \\(z_{1-\\frac{\\alpha}{2}}\\) denotes the \\((1-\\alpha)/2\\) quantile of the standard Normal distribution \\((z_{0.975}=1.96).\\) This confidence interval is approximate, since it is only asymptotically justified, and, thus, is generally not exact in finite samples.\nNote: Often, however, very difficult to obtain a consistent estimator \\(\\hat v_n\\) of \\(v_0\\) (see Section 3.1). Statistical inference is then usually based on the bootstrap confidence intervals.\n\n\n\nAlgorithm of the Basic Bootstrap Confidence Interval for \\(\\theta_0:\\)\nSetup:\n\nData: i.i.d. random sample \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}\n\\] with \\(X_i\\overset{\\text{i.i.d.}}{\\sim} F\\) for all \\(i=1,\\dots,n.\\)\nThe parameter of interest \\(\\theta_0\\in\\mathbb{R}\\) is an parameter of \\(F\\).\n\\(\\hat{\\theta}_n\\) denotes the estimator of \\(\\theta_0\\in\\mathbb{R}.\\)\nProblem: Construct a confidence interval for \\(\\theta_0\\in\\mathbb{R}.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is Consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e. that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^*_n -\\hat{\\theta}_n)|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}_n -\\theta_0)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\n\n\n\n\n\n\nGood to know: Theorem 1 in Mammen (1992) shows that the basic bootstrap is consistent if \\(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\to_d\\mathcal{N}(0,v_0^2),\\) under the assumption that the bootstrap sampling process (e.g. i.i.d.) equals the original sampling process.\n\n\n\n\n\nAlgorithm (3 Steps):\n\nGenerate \\(m\\) bootstrap estimates\n\\[\n\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] by repeatedly (\\(m\\) times) drawing bootstrap samples \\(X_{1}^*,\\dots,X_{n}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) and computing \\[\n\\hat{\\theta}^\\ast_{n,j}=\\hat{\\theta}^\\ast_{j}(X_{1}^*,\\dots,X_{n}^*),\\quad j=1,\\dots,m.\n\\]\nUse the \\(m\\) bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantiles of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat q^*_{n,p}=\\left\\{\n  \\begin{array}{ll}\n  \\hat\\theta^*_{n,(\\lfloor mp\\rfloor+1)},           &\\text{if $mp$ is not an integer}\\\\\n  (\\hat\\theta^*_{n,(mp)}+\\hat\\theta^*_{n,(mp+1)})/2,&\\text{if $mp$ is an integer}\n\\end{array}\\right.\n\\tag{3.9}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{n,(j)}^*\\) denotes the \\(j\\)th order statistic \\[\n\\hat\\theta_{n,(1)}^* \\leq \\hat\\theta_{n,(2)}^*\\leq \\dots\\leq \\hat\\theta_{n,(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g. \\(\\lfloor 4.9\\rfloor = 4\\)).\nThe \\((1-\\alpha)\\times 100\\%\\) basic bootstrap confidence interval is then given by \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\tag{3.10}\\] where\n\n\\(\\hat{\\theta}_n\\) is computed from the original sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) and\n\\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*.\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe quantiles \\(\\hat q^*_{n,p}\\) are those of the distribution \\[\nG_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\hat{\\theta}^*_{n,j}\\leq x\\right)}.\n\\] However, we’ll treat the quantiles \\(\\hat q^*_{n,p}\\) as quantiles of the distribution \\[\nG_{n}^{Boot}(x)=P\\left(\\hat{\\theta}^*_{n}\\leq x\\,\\big|\\,\\mathcal{S}_n\\right),\n\\] since for large \\(m\\) (\\(m\\to\\infty\\)) the difference between \\(G_{n,m}^{Boot}\\) and \\(G_{n}^{Boot}\\) is negligible (Glivenko-Cantelli Theorem 3.1) and we can choose \\(m\\) to be large.\n\n\nJustifying the Basic Bootstrap CI (Equation 3.10) for \\(\\theta_0\\):\nThe following three approximate statements \\((\\approx (1-\\alpha))\\) are exact for \\(m\\to\\infty:\\) \\[\n\\begin{align*}\n&P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\leq \\hat{\\theta}^*_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow\\; & P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n \\leq\\hat{\\theta}^*_n -\\hat{\\theta}_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow\\; & P^*\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\]\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}.\n\\] Therefore, for large \\(n\\) and large \\(m,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow\\; &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\leq\\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow\\; &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-2\\hat{\\theta}_n\\leq-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-2\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n%\\Rightarrow &P\\left(\\hat{\\theta}_n-(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\le \\theta_0\\le \\hat{\\theta}_n-(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow\\; &P\\left(2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\le \\theta_0\\le 2\\hat{\\theta}_n-\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow\\; &P\\left(\\theta_0\\in\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, \\; 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right]\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\] This demonstrates that the basic bootstrap confidence interval in Equation 3.10 \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, \\; 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] is indeed an asymptotically (\\(n\\to\\infty\\) and \\(m\\to\\infty\\)) valid \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\nExample: Basic Bootstrap Confidence Interval for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0.\\)\nEstimator: \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu_0.\\)\nInference Problem: Construct a confidence interval for \\(\\mu_0.\\)\n\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\mu_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\nBy the CLT: \\(\\sqrt{n}(\\bar X_n - \\mu_0)\\to_d\\mathcal{N}(0,\\sigma^2_0)\\) as \\(n\\to\\infty\\)\nEstimation of \\(\\sigma^2_0\\): \\(\\hat{\\sigma}^2_n=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2,\\) where \\(\\hat{\\sigma}^2_n\\to_p\\sigma^2_0\\) as \\(n\\to\\infty.\\)\nThis implies: \\(\\sqrt{n}((\\bar X_n -\\mu_0)/\\hat{\\sigma}_n)\\to_d\\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\)\n\nLet \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1).\\) Since \\(z_{\\alpha/2} = -z_{1-\\alpha/2},\\) we have that \\[\n\\begin{align*}\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\le \\frac{\\sqrt{n}(\\bar X_n -\\mu_0)}{\\hat{\\sigma}_n}\\le z_{1-\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\le \\bar X_n -\\mu_0\\le z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\le \\mu_0\\le\n        \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\n  \\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\nApproximate \\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\left(\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\right),\n    \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\left(\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\right)\\right]\n\\]\n\n\n\n\n\nAlgorithm of the basic bootstrap confidence interval for \\(\\boldsymbol{\\mu_0}\\):\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation (see Section 3.3.1.3).\n\nDraw \\(m\\) bootstrap samples (e.g. \\(m=10,000\\)) and calculate the corresponding estimates \\[\n\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\n\\]\nCompute the empirical quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}\\) using Equation 3.9.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) basic bootstrap confidence interval according to Equation 3.10: \\[\n\\left[2\\bar X_n -\\hat q^*_{n,1-\\frac{\\alpha}{2}},\n   2\\bar X_n -\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] where\n\n\\(\\bar{X}_n\\) is computed from the original sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) and\n\\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimators \\(\\bar{X}_{n,1}^*,\\dots,\\bar{X}_{n,m}^*.\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-BootT",
    "href": "Ch3_Bootstrap.html#sec-BootT",
    "title": "3  The Bootstrap",
    "section": "3.4 The Bootstrap-\\(\\boldsymbol{t}\\) Method",
    "text": "3.4 The Bootstrap-\\(\\boldsymbol{t}\\) Method\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of the “studentized bootstrap”). As the basic bootstrap method, the bootstrap-\\(t\\) method is a nonparametric bootstrap method. The construction relies on so-called (asymptotically) pivotal statistics.\nSetup (as in classic asymptotic statistics):\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter of interest \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2=\\lim_{n\\to\\infty}Var\\left(\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\right)\\) i.e.  \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\begin{align*}\n\\hat v^2_n &\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\hat v_n   &\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\nDefinition 3.5 ((Asymptotically) Pivotal Statistics) \nA statistic (i.e. a function of the random sample) \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called exact pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter.\nA statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications.\nIt is, however, often possible to construct an asymptotically pivotal statistic. Consider, for instance, an asymptotically normal \\(\\sqrt{n}\\)-consistent estimator \\(\\hat{\\theta}_n\\) of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2),\n\\] where \\(v^2=\\lim_{n\\to\\infty}Var(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0))\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator of \\(v_0^2\\) \\[\n\\hat v_n^2 \\rightarrow_p v_0^2\\quad\\text{as}\\quad n\\to\\infty,\n\\] which implies (Continuous Mapping Theorem) that also \\[\n\\hat v_n \\rightarrow_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Then, \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] is asymptotically pivotal, since \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\n\nExample: \\(\\boldsymbol{\\bar{X}_n}\\) is Asymptotically Pivotal\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu_0\\), variance \\(0&lt;Var(X)=\\sigma_0^2&lt;\\infty\\), and finite fourth moment \\(\\mathbb{E}(|X|^4)=\\beta&lt;\\infty\\) for estimating \\(\\sigma_0^2.\\)\n\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\sim t_{n-1}\\quad\\text{for any}\\quad n=2,3,\\dots\n\\] with \\(s_n^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is exact pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\rightarrow_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistic. Note: One can replace \\(s_n = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2}\\) by \\(\\hat\\sigma_n = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (X_i-\\bar X_n)^2},\\) since both are asymptotically equivalent, i.e. \\[\n\\frac{s_n}{\\hat\\sigma_n}=\\sqrt{\\frac{n}{n-1}}\\to_p 1\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\nBootstrap-\\(\\boldsymbol{t}\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*\\big|\\mathcal{S}_n =\\sqrt{n}\\frac{(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}{\\hat v_n^*}\\Big|\\mathcal{S}_n,\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the standard deviation estimate \\(\\hat{v}_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*\\equiv \\hat{v}(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(\\boldsymbol{t}\\) consistency follows if the basic bootstrap is consistent\n\n\n\nIf the basic bootstrap is consistent and if the variance estimator \\(\\hat{v}_n^2\\) is consistent, then also the bootstrap-\\(t\\) method is consistent.\n\n\n\n\n\n3.4.1 The Bootstrap-\\(\\boldsymbol{t}\\) Confidence Interval\nSetup (as in classic asymptotic statistics):\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter of interest \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2=\\lim_{n\\to\\infty}Var\\left(\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\right)\\) i.e.  \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\begin{align*}\n\\hat v^2_n &\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\hat v_n   &\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\n\n\nAlgorithm of the Bootstrap-\\(\\boldsymbol{t}\\) Confidence Interval for \\(\\boldsymbol{\\theta_0}\\):\nAlgorithm (3 Steps):\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*_n\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\n\\hat v^*_n\\equiv \\hat v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT_n^*&=\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g. \\(m=10,000\\)) many bootstrap estimators \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\n\\] conditionally on \\(\\mathcal{S}_n.\\)\nCompute the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\\) (see Equation 3.9).\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval\n\\[\n\\left[\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n   \\hat{\\theta}_n - \\hat q^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\tag{3.11}\\] where\n\n\\(\\hat\\theta_n\\) and \\(\\hat v_n\\) are the estimates of \\(\\theta_0\\) and \\(v_0\\) computed from the original sample \\(\\mathcal{S}_n=\\left\\{X_1,\\dots,X_n\\right\\},\\) and\n\\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimators \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\\)\n\n\nJustifying the Bootstrap-\\(\\boldsymbol{t}\\) CI (Equation 3.11) for \\(\\boldsymbol{\\theta_0}\\):\nThe bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\n\\] yield the empirical bootstrap distribution \\[\nH_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(T_{n,j}^*\\;\\leq\\; x\\right)}\n\\] which approximates the bootstrap distribution \\[\nH_{n}^{Boot}(x)=P\\left(\\left.T_{n}^*\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] arbitrarily precise as \\(m\\to\\infty\\) (Glivenko-Cantelli Theorem 3.1).\nThus, the empirical bootstrap quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) of \\(H_{n,m}^{Boot}\\) are indeed consistent (\\(m\\to\\infty\\)) for the quantiles \\(\\hat q_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution \\(H_{n}^{Boot}.\\) This implies, for large \\(m,\\) \\[\nP^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}} \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha.\n\\]\nBy the (assumed) bootstrap consistency, we have for large \\(n\\) that \\[\n\\left.{\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v_n^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}}.\n\\] Therefore, for large \\(n\\) and large \\(m,\\) \\[\n\\begin{align*}\n& P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}} \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow & P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq  \\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow & P\\left(- \\hat{\\theta}_n + \\hat q^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\leq -\\theta_0 \\leq - \\hat{\\theta}_n + \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow & P\\left(\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq \\theta_0 \\leq \\hat{\\theta}_n - \\hat q^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Leftrightarrow & P\\left(\\theta_0\\in\\left[\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat q^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right]\\right)\n\\approx 1-\\alpha.\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval (Equation 3.11) \\[\n\\left[\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat q^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\] is indeed an asymptotic (i.e. approximate) \\((1-\\alpha)\\times 100\\%\\) CI.\n\n\nExample: Bootstrap-\\(\\boldsymbol{t}\\) Confidence Interval for \\(\\boldsymbol{\\mu_0}\\)\nHere \\(\\hat\\theta_n = \\bar{X}_n\\) and the estimator of the asymptotic variance \\(\\sigma_0^2=\\lim_{n\\to\\infty}n Var(\\bar{X}_n)\\) is \\[\n\\hat\\sigma_n^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2.\n\\]\nAlgorithm:\n\nRepeatedly draw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\[\n\\bar X^*_n=\\bar X^*(X_1^*,\\dots,X_n^*)\\quad\\text{and}\\quad\n\\hat\\sigma^*_n=\\hat\\sigma^*(X_1^*,\\dots,X_n^*)\n\\] to generate \\(m\\) (e.g. \\(m=10,000\\)) bootstrap realizations \\[\nT^*_{n,1}=\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{\\hat\\sigma^*_{n,1}},\\dots,T^*_{n,m}=\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{\\hat\\sigma^*_{n,m}}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\[\nT^*_{n,1},\\dots,T^*_{n,m}\n\\] using Equation 3.9.\nThis yields the \\((1-\\alpha)\\times 100 \\%\\) confidence interval (using Equation 3.11): \\[\n\\left[\\bar X_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right),\n    \\bar X_n - \\hat q^*_{n,  \\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right)\\right],\n\\] where\n\n\\(\\bar X_n=\\frac{1}{n}\\sum_{i=1}^n X_i\\) and \\(\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2}\\) are computed from the original sample \\(X_1,\\dots,X_n\\) and\n\n\\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimates \\(T^*_{n,1},\\dots,T^*_{n,m}.\\)\n\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(\\boldsymbol{t}\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the basic bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{\\hat{v}^*_n}\\;\\right|\\;\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(\\hat{v}^*_n\\) depends also on the bootstrap sample—not on the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)\\;\\right|\\;\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\[\n[L_n,U_n]\n\\] of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g. basic bootstrap vs. bootstrap-\\(t\\)).\n\nTwo-sided \\((1-\\alpha)\\cdot 100\\%\\) confidence intervals \\([L_n,U_n]\\) are said to be first-order accurate if there exist some constant \\(c&lt;\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta_0\\in [L_n,U_n])-(1-\\alpha)\\right|\\le \\frac{c}{\\sqrt{n}}\n\\end{align*}\n\\]\nTwo-sided \\((1-\\alpha)\\cdot 100\\%\\) confidence intervals \\([L_n,U_n]\\) are said to be second-order accurate if there exist some constant \\(c&lt;\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta_0\\in[L_n,U_n])-(1-\\alpha)\\right|\\le \\frac{c}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta_n\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on classic asymptotic normality approximations are first-order accurate.\nBasic bootstrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field (see, for instance, Koike (2024)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#solutions",
    "href": "Ch2_EMAlgorithmus.html#solutions",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\n\n(a) Likelihood function\nThe probability mass function of \\(X\\sim\\text{Bernoulli}(p_0)\\) is \\[\nf(x)=p_0^{x}(1-p_0)^{1-x},\\quad\\text{with}\\quad x\\in\\{0,1\\}.\n\\] Thus the likelihood and the log-likelihood functions are \\[\n\\begin{align*}\n\\mathcal{L}(p)    & = \\prod_{i=1}^n p^{X_i}(1-p)^{1-X_i}\\\\\n\\mathcal{\\ell}(p) & = \\sum_{i=1}^n \\ln\\left(p^{X_i}(1-p)^{1-X_i}\\right),\n\\end{align*}\n\\] where \\(p\\in[0,1]\\) denotes a parameter candidate value.\n\n\n(b) Likelihood function for a Bernoulli mixture distribution\nThe probability mass function of a Bernoulli mixture distribution is \\[\n\\begin{align*}\nf_G(x)\n& =\\sum_{g=1}^G \\pi_{g,0}\\; f_g(x)\\\\[2ex]\n& =\\sum_{g=1}^G \\pi_{g,0}\\; p_{g,0}^{x}(1-p_{g,0})^{1-x}\\quad\\text{with}\\quad x\\in\\{0,1\\}.\n\\end{align*}\n\\] Thus the likelihood function is \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{p},\\boldsymbol{\\pi})    & = \\prod_{i=1}^n\\left(\\sum_{g=1}^G \\pi_g\\; p_g^{X_i}(1-p_g)^{1-X_i}\\right)\\\\[2ex]\n\\ell(\\mathbf{p},\\boldsymbol{\\pi}) & = \\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\; p_g^{X_i}(1-p_g)^{1-X_i}\\right)\n\\end{align*}\n\\] where \\(\\mathbf{p}=(p_1,\\dots,p_G)\\) and \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) are parameter candidate values with \\(p_g\\in[0,1]\\) and \\(\\pi_g\\in[0,1]\\) for all \\(g=1,\\dots,G\\) and with \\(\\sum_{g=1}^G\\pi_g=1.\\)\n\n\n(c) Likelihood function with group indicator random variables\n\\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\mathbf{p},\\boldsymbol{\\pi})\n& = \\prod_{i=1}^n \\prod_{g=1}^G\\left(\\pi_g p_g^{X_i}(1-p_g)^{1-X_i}\\right)^{Z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi})    \n& = \\sum_{i=1}^n \\sum_{g=1}^G Z_{ig}\\left(\\ln(\\pi_g) + \\ln\\left(p_g^{X_i}(1-p_g)^{1-X_i}\\right)\\right)\n\\end{align*}\n\\]\n\n\n(d) Posterior probability\nTrue (unknow) posterior probability \\[\n\\begin{align*}\n\\mathfrak{p}_{ig,0}\n&= \\mathbb{E}_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}(Z_{ig} | X_i)\\\\[2ex]\n&= P_{\\mathbf{p}_0,\\boldsymbol{\\pi}_0}(Z_{ig}=1 | X_i = x_i) \\\\[2ex]\n%&=\\frac{ P(Z_{ig}=1) f_{X|Z}(x_i|Z_{ig}=1)}{f_G(x_i)} \\\\[2ex]\n%&=\\frac{ P(Z_{ig}=1) f_g(x_i)}{f_G(x_i)} \\\\[2ex]\n&=\\frac{\\pi_{g,0}\\;\\; p_{g,0}^{X_i} (1-p_{g,0})^{1-X_i}}{\\sum_{g=1}^G \\pi_{g,0}\\; p_g^{X_i} (1-p_g)^{1-X_i}}\n\\end{align*}\n\\]\n\n\n(e) Conditional Expectation of \\(\\tilde\\ell\\)\n\\[\n\\begin{align*}\n&\\mathbb{E}_{\\mathbf{p}_{0},\\boldsymbol{\\pi}_{0}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi})\\mid X_1,\\dots,X_n\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\sum_{g=1}^G \\mathbb{E}_{\\mathbf{p}_{0},\\boldsymbol{\\pi}_{0}}\\left(Z_{ig}\\big|X_i\\right)\\left(\\ln(\\pi_g) + \\ln\\left(p_g^{X_i}(1-p_g)^{1-X_i}\\right)\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\left(\\ln(\\pi_g) + \\ln\\left(p_g^{X_i}(1-p_g)^{1-X_i}\\right)\\right),\n\\end{align*}\n\\]\n\n\n(f) Maximize \\(\\mathbb{E}_{\\mathbf{p}_{0},\\boldsymbol{\\pi}_{0}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi})\\mid X_1,\\dots,X_n\\right)\\) with respect to \\(p_g:\\)\n\\[\n\\begin{align*}\n&\\frac{\\partial}{\\partial p_g}\\mathbb{E}_{\\mathbf{p}_{0},\\boldsymbol{\\pi}_{0}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi})\\mid X_1,\\dots,X_n\\right)\\\\[2ex]\n& = \\frac{\\partial}{\\partial p_g}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\left(\\ln(\\pi_g) + \\ln\\left(p_g^{X_i}(1-p_g)^{1-X_i}\\right)\\right)\\\\[2ex]\n& = \\frac{\\partial}{\\partial p_g}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\left(\\ln(\\pi_g) + \\left(X_i\\ln(p_g) + (1-X_i)\\ln(1-p_g)\\right)\\right)\\\\[2ex]\n%& = \\sum_{i=1}^n \\mathfrak{p}_{ig} \\frac{\\partial}{\\partial p_g} \\left(x_i\\ln(p_g) + (1-x_i)\\ln(1-p_g)\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\mathfrak{p}_{ig,0} \\left(\\frac{X_i}{p_g} - \\frac{(1-X_i)}{(1-p_g)}\\right)\\\\[2ex]\n\\end{align*}\n\\] First order condition \\[\n\\begin{align*}\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0} \\left(\\frac{X_i}{\\hat{p}_g} - \\frac{(1-X_i)}{(1-\\hat{p}_g)}\\right)&\\overset{!}{=}0\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0} \\left(\\frac{X_i(1-\\hat{p}_g)}{\\hat{p}_g(1-\\hat{p}_g)} - \\frac{(1-X_i)\\hat{p}_g}{\\hat{p}_g(1-\\hat{p}_g)}\\right)&=0\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0} \\left(X_i(1-\\hat{p}_g) - (1-X_i)\\hat{p}_g\\right)&=0\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0} \\left(X_i - \\hat{p}_g \\right)&=0\\\\[2ex]\n\\hat{p}_g\\sum_{i=1}^n \\mathfrak{p}_{ig,0} &= \\sum_{i=1}^n \\mathfrak{p}_{ig,0}X_i \\\\[2ex]\n\\hat{p}_g & = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}X_i}{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}}\n\\end{align*}\n\\]\n\n\n(g) Maximize \\(\\mathbb{E}_{\\mathbf{p}_{0},\\boldsymbol{\\pi}_{0}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi})\\mid X_1,\\dots,X_n\\right)\\) with respect to \\(\\pi_g:\\)\nNote:\n\nWe only need to focus on the \\(\\pi_g\\) terms since the other terms in\n\\[\n\\begin{align*}\n&\\mathbb{E}_{\\mathbf{p}_{0},\\boldsymbol{\\pi}_{0}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi})\\mid X_1,\\dots,X_n\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\left(\\ln(\\pi_g) + \\ln\\left(p_g^{X_i}(1-p_g)^{1-X_i}\\right)\\right)\n\\end{align*}\n\\] do not depend on \\(\\pi_g.\\)\nHowever, we need to do maximization under the side constraint that \\(\\sum_{g=1}^G\\pi_g=1.\\) So, we use the method of Lagrange multipliers.\n\nThe Lagrange function is given by \\[\n\\begin{align*}\nL(\\boldsymbol{\\pi},\\lambda)=\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\ln(\\pi_g) - \\lambda \\left(\\sum_{g=1}^G\\pi_g-1\\right).\n\\end{align*}\n\\] First order condition with respect to \\(\\pi_g\\): \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\pi_g}L(\\boldsymbol{\\pi},\\lambda)=\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0}\\frac{1}{\\pi_g}  - \\lambda &\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0}\\frac{1}{\\hat{\\pi}_g}  - \\lambda  &\\overset{!}{=}0 \\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig,0}   &=\\lambda \\hat{\\pi}_g\\\\[2ex]\n\\hat{\\pi}_g & = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}}{\\lambda}  \\\\[2ex]\n\\end{align*}\n\\] Plugging \\(\\hat{\\pi}_g = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\lambda}\\) into \\(L(\\boldsymbol{\\pi},\\lambda)\\): \\[\n\\begin{align*}\nL(\\lambda)&=\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\ln\\left(\\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}}{\\lambda}\\right) - \\lambda \\left(\\sum_{g=1}^G\\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}}{\\lambda}-1\\right) \\\\[2ex]\n&=\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\left(\\ln\\left(\\sum_{i=1}^n \\mathfrak{p}_{ig,0}\\right) - \\ln\\left(\\lambda\\right)\\right) -  \\left(\\sum_{g=1}^G\\sum_{i=1}^n \\mathfrak{p}_{ig,0}-\\lambda \\right) \\\\[2ex]\n\\end{align*}\n\\] First order condition with respect to \\(\\lambda\\): \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} L(\\lambda)=\n- \\frac{1}{\\lambda}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} +1 &\\\\[2ex]\n- \\frac{1}{\\hat\\lambda}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} +1 & \\overset{!}{=}0\\\\[2ex]\n\\hat\\lambda & = \\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0} \\\\[2ex]\n\\end{align*}\n\\] So, we have that \\[\n\\begin{align*}\n\\hat{\\pi}_g\n& = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}}{\\hat{\\lambda}}\\\\[2ex]\n& = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig,0}}{\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig,0}}\n\\end{align*}\n\\]\n\n\n(h) Sketch of the EM-Algorithm\nAs we do not know the true parameter vectors \\[\n\\mathbf{p}_{0}=(p_{1,0},\\dots,p_{G,0})\\quad\\text{and}\\quad\\boldsymbol{\\pi}_{0}=(\\pi_{1,0},\\dots,\\pi_{G,0}),\n\\] we do not know the true posterior probability \\(\\mathfrak{p}_{ig,0}.\\)\nBut given estimates \\[\n(\\hat{p}_{1},\\dots,\\hat{p}_{G})\\quad\\text{and}\\quad (\\hat{\\pi}_{1},\\dots,\\hat{\\pi}_{G}),\n\\] we can estimate the unknown posterio propability by \\[\n\\begin{align*}\n\\hat{\\mathfrak{p}}_{ig}\n&=\\frac{\\hat{\\pi}_g\\; \\left(\\hat{p}_g\\right)^{X_i} \\left(1-\\hat{p}_g\\right)^{1-X_i}}{\\sum_{g=1}^G \\hat{\\pi}_g\\; \\left(\\hat{p}_g\\right)^{X_i} \\left(1-\\hat{p}_g\\right)^{1-X_i}}\n\\end{align*}\n\\]\nThis motivates the following EM-Algorithm:\n\nInitialization \\[\n\\hat{p}_1^{(0)},\\dots,\\hat{p}_G^{(0)}\n\\] \\[\n\\hat{\\pi}_1^{(0)},\\dots,\\hat{\\pi}_G^{(0)}\n\\] For instance: \\(\\hat{p}_g^{(0)}=0.5\\) and \\(\\hat{\\pi}_g^{(0)}=\\frac{1}{G}\\) for all \\(g=1,\\dots,G.\\)\nFor \\(r=1,2,\\dots\\)\n\nExpectation-Step: \\[\n\\begin{align*}\n\\hat{\\mathfrak{p}}_{ig}^{(r-1)}\n&=\\frac{\\hat{\\pi}_g^{(r-1)}\\; \\left(\\hat{p}_g^{(r-1)}\\right)^{X_i} \\left(1-\\hat{p}_g^{(r-1)}\\right)^{1-X_i}}{\\sum_{g=1}^G \\hat{\\pi}_g^{(r-1)}\\; \\left(\\hat{p}_g^{(r-1)}\\right)^{X_i} \\left(1-\\hat{p}_g^{(r-1)}\\right)^{1-X_i}}\n\\end{align*}\n\\]\nMaximization-Step: \\[\n\\begin{align*}\n\\hat{p}_g^{(r)} & = \\frac{\\sum_{i=1}^n \\hat{\\mathfrak{p}}_{ig}^{(r-1)}X_i}{\\sum_{i=1}^n \\hat{\\mathfrak{p}}_{ig}^{(r-1)}}\\\\[2ex]\n\\hat{\\pi}_g^{(r)} & = \\frac{\\sum_{i=1}^n \\hat{\\mathfrak{p}}_{ig}^{(r-1)}}{\\sum_{i=1}^n \\sum_{g=1}^G \\hat{\\mathfrak{p}}_{ig}^{(r-1)}}  \\\\[2ex]\n\\end{align*}\n\\]\n\nCheck convergence (e.g., no relevant change of the maximized log-likelihood function).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html",
    "href": "Ch4_NPRegression.html",
    "title": "4  Nonparametric Regression",
    "section": "",
    "text": "4.1 Introduction\nLet us consider the case of univariate nonparametric regression, i.e., with one single explanatory variable \\(X\\in\\mathbb{R}\\).\nData: \\[\n(Y_{1},X_{1}),\\dots,(Y_{n},X_{n})\\overset{\\text{i.i.d}}{\\sim}(Y,X)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html#basis-function-expansions",
    "href": "Ch4_NPRegression.html#basis-function-expansions",
    "title": "4  Nonparametric Regression",
    "section": "4.2 Basis Function Expansions",
    "text": "4.2 Basis Function Expansions\nSome frequently used approaches to nonparametric regression rely on expansions of the form \\[\nm(x)\\approx \\sum_{j=1}^p \\beta_j b_j(x),\n\\] where \\(b_1(x),b_2(x),\\dots\\) are suitable basis functions \\[\nb_j:\\mathbb{R}\\to\\mathbb{R},\\quad j=1,\\dots,p.\n\\]\nThe basis functions \\(b_1,b_2,\\dots\\) have to be chosen in such a way that for any possible smooth function \\(m\\) the squared approximation error tends to zero as \\(p\\rightarrow\\infty,\\)\n\\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\left(m(x)-\\sum_{j=1}^p \\vartheta_j b_j(x)\\right)^2\\to 0,\\quad p\\to\\infty.\n\\] However, for every fixed value \\(p,\\) we typically have that \\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\left(m(x)-\\sum_{j=1}^p \\vartheta_j b_j(x)\\right)^2 \\neq 0\n\\] which leads to a biased estimation procedure.\nFor a fixed value \\(p,\\) an estimator \\(\\hat m_p\\) of \\(m\\) is determined by \\[\n\\hat m_p(x)=\\sum_{j=1}^p \\hat\\beta_j b_j(x),\n\\] where the coefficients \\(\\hat\\beta_j\\) are obtained by ordinary least squares \\[\n\\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\hat\\beta_j \\underbrace{b_j(X_i)}_{X_{ij}}\\right)^2\n=\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\vartheta_j \\underbrace{b_j(X_i)}_{X_{ij}}\\right)^2\n\\] with \\[\n\\hat\\beta = \\left(\\begin{matrix}\\hat\\beta_1\\\\ \\vdots\\\\\\hat\\beta_p\\end{matrix}\\right)=\\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top Y,\n\\] where \\(\\mathbf{X}\\) denotes the \\((n\\times p)\\)-dimensional matrix with elements \\[\nX_{ij}=b_j(X_i),\n\\] \\(i=1,\\dots,n\\) and \\(j=1,\\dots,p,\\) and \\(Y=(Y_1,\\dots,Y_n)^\\top.\\)\nExamples of basis functions \\(b_1(x),\\,b_2(x)\\,\\dots\\):\n\npolynomials (monomial basis)\nspline functions\nwavelets\nFourier expansions (for periodic functions)\n\n\n4.2.1 Polynomial Regression\nTheoretical Justification: Every smooth function can be well approximated by a polynomial of sufficiently high degree.\nApproach:\n\nMonomial basis functions \\(b_j(x) = x^{j-1}\\), \\(\\;\\;j=1,\\dots,p.\\)\nChoose \\(p\\) and fit a polynomial of degree \\(p-1\\) (order \\(p\\)): \\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p}\\sum_{i=1}^n \\left(Y_i-\\sum_{j=1}^p \\vartheta_{j} X_i^{j-1}\\right)^2\n\\] \\[\n\\Rightarrow\\quad {\\hat m}_p(X)={\\hat \\beta}_{1}+\\sum_{j=2}^{p}\n{\\hat \\beta}_{j} X_i^{j-1}\n\\]\nThis corresponds to an approximation with basis functions \\[\n\\begin{align*}\nb_1(x)&=1\\\\[2ex]\nb_2(x)&=x\\\\[2ex]\nb_3(x)&=x^2\\\\[2ex]\n\\vdots\\\\[2ex]\nb_{p}(x)&=x^{p-1}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIt is only assumed that \\(m(x)\\) can be approximated by a polynomial \\(\\hat{m}_p(x)\\) as \\(p\\to\\infty.\\) However, for a given choice of \\(p,\\) there will usually still exist an approximation error.\nTherefore, \\(\\hat{m}_p(x)\\) is typically a biased estimator for given values of \\(p,\\) and a given value of \\(x\\in[a,b],\\) \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat{m}_p(x))\\quad & \\neq 0\\\\\n\\mathbb{E}(\\hat{m}_p(x)) - m(x)        & \\neq 0.\n\\end{align*}\n\\]\n\n\nR-Code to Compute Polynomial Regressions:\nGenerate some artificial data, where the usually unknown \\[\nm(x)=\\sin(5 x)\n\\] with \\(x\\in[0,1]\\):\n\nset.seed(1)\n# Generate some data: \nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n\n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\n# Save all in a dataframe\ndb     &lt;-  data.frame(x=x_vec,y=y_vec)\n\nCompute the ordinary least squares regressions of different polynomial regression models:\n\n# Fitting of polynomials to the data (parametric models):\n# Constant line fit: (Basis function x^0)\nreg_p1 &lt;- lm(y ~ 1, data=db)\n\n# Basis functions: x^0 + ... + x^3\nreg_p4 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)\n\n# Basis functions: x^0 + ... + x^6\nreg_p7 &lt;- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)\n\nTake a look at the fits:\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(db, main=\"Truth\")\n# True (usually unknown) regression function\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n\n## Fit by degree 0 polynomial\nplot(db, main=\"Degree 0\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p1, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n## Fit by degree 3 polynomial\nplot(db, main=\"Degree 3\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p4, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n## Fit by degree 6 polynomial\nplot(db, main=\"Degree 6\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p7, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n\n\n\n\n\n\n\nThe quality of the approximation obviously depends on the choice of the model selection parameter \\(p\\) which serves as a smoothing parameter.\nLet’s look at the fits across 200 Monte Carlo replications:\n\nm_true    &lt;- sin(x_vec * 5)\n\nn_MCrepl  &lt;- 200 # MC-replications\n\nm_hat_p1  &lt;- matrix(NA, n, n_MCrepl)\nm_hat_p4  &lt;- matrix(NA, n, n_MCrepl)\nm_hat_p7  &lt;- matrix(NA, n, n_MCrepl)\n\nfor(r in 1:n_MCrepl){\n    # Generate some data: \n    e_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n    y_vec  &lt;-  sin(x_vec * 5) + e_vec\n    db     &lt;-  data.frame(x = x_vec,y = y_vec)\n    # Estimations\n    reg_p1 &lt;- lm(y ~ 1, data=db)\n    reg_p4 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)\n    reg_p7 &lt;- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)\n    # Save predictions (y hat)\n    m_hat_p1[,r] &lt;- predict(reg_p1, newdata = db)\n    m_hat_p4[,r] &lt;- predict(reg_p4, newdata = db)\n    m_hat_p7[,r] &lt;- predict(reg_p7, newdata = db)\n}\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(db, main=\"Truth\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nsubSelect &lt;- 25\nmatplot(y = m_hat_p1[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",  \n        ylim = range(m_hat_p1[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=0.5, main = \"Degree p=0\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nmatplot(y = m_hat_p4[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",\n        ylim = range(m_hat_p4[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=.5, main = \"Degree p=3\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nmatplot(y = m_hat_p7[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",\n        ylim = range(m_hat_p7[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=.5, main = \"Degree p=6\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\n\nUsing the 200 fits from above, we can approximate the\n\nsquared bias \\(\\left(\\operatorname{Bias}(\\hat{m}_p(x))\\right)^2\\) and the\nvariance \\(Var(\\hat{m}_p(x))\\)\n\npoint-wise at each \\(x\\)-value in x_vec.\n\n## Pointwise (for each x) biases of \\hat{m}(x): \nPt_Bias_p1 &lt;- rowMeans(m_hat_p1) - m_true\nPt_Bias_p4 &lt;- rowMeans(m_hat_p4) - m_true\nPt_Bias_p7 &lt;- rowMeans(m_hat_p7) - m_true\n\n## Pointwise squared biases\nPt_BiasSq_p1 &lt;- Pt_Bias_p1^2\nPt_BiasSq_p4 &lt;- Pt_Bias_p4^2\nPt_BiasSq_p7 &lt;- Pt_Bias_p7^2\n\n## Pointwise (for each x) variances \\hat{m}(x):\nPt_Var_p1  &lt;- apply(m_hat_p1, 1, var)\nPt_Var_p4  &lt;- apply(m_hat_p4, 1, var)\nPt_Var_p7  &lt;- apply(m_hat_p7, 1, var)\n\npar(mfrow=c(1,2))\nmatplot(y = cbind(Pt_BiasSq_p1, Pt_BiasSq_p4, Pt_BiasSq_p7), \n        x = x_vec, type = \"l\", col = c(1,2,\"darkgreen\"), \n        main = \"Pointwise Squared Bias\", ylab=\"\", xlab=\"x\")\nlegend(\"topleft\", col = c(1,2,\"darkgreen\"), lty = c(1,2,3), \nlegend = c(\"Degree p=0\", \"Degree p=3\", \"Degree p=6\"))\nmatplot(y = cbind(Pt_Var_p1, Pt_Var_p4, Pt_Var_p7), \n        x = x_vec, type = \"l\", col = c(1,2,\"darkgreen\"), \n        main = \"Pointwise Variance\", ylab=\"\", xlab=\"x\")\nlegend(\"top\", col = c(1,2,\"darkgreen\"), lty = c(1,2,3), \nlegend = c(\"Degree p=0\", \"Degree p=3\", \"Degree p=6\"))\n\n\n\n\n\n\n\n\nThe Bias-Variance Trade-Off:\n\n\\(p\\) small: Variance of the estimator is small, but (squared) bias is large.\n\\(p\\) large: Variance of the estimator is large, but (squared) bias is small.\n\n\n\n\n\n\n\nRemark\n\n\n\nPolynomial regression is not very popular in practice. Reasons are numerical problems in fitting high dimensional polynomials. Furthermore, high order polynomials often posses an erratic, difficult to interpret behavior at the boundaries.\n\n\n\n\n4.2.2 Regression Splines\nThe practical disadvantages of global basis functions (like polynomials), explain the success of local basis functions. A frequently used system of basis functions are local polynomials, i.e., so-called spline functions.\nA spline function is a piece wise polynomial function. They are defined with respect to a pre-specified sequence of \\(q\\) knots \\[\na=\\tau_1&lt;\\tau_2\\leq\\dots\\leq \\tau_{q-1}&lt;\\tau_q=b.\n\\] Different specifications of the knot sequence lead to different splines.\nMore precisely, for a given knot sequence, a spline function \\(s(x)\\) of degree \\(k\\) is defined by the following properties:\n\n\\(s(x)\\) is a polynomial of degree \\(k\\) (i.e. of order1 \\(k+1\\)) in every interval \\([\\tau_j,\\tau_{j+1}]\\), i.e. \\[\ns(x)=s_0+s_1x+s_2x^2+\\dots+s_kx^{k},\\quad x\\in[\\tau_j,\\tau_{j+1}]\n\\] with \\(s_0,\\dots,s_k\\in\\mathbb{R}.\\)\n\n\\(s(x)\\) is called a linear spline if \\(k=1\\)\n\\(s(x)\\) is a quadratic spline if \\(k=2\\)\n\\(s(x)\\) is a cubic spline if \\(k=3\\)\n\n\\(s(x)\\) is \\(k-1\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\nIn practice, the most frequently used splines are cubic spline functions based on an equidistant sequence of \\(q\\) knots, i.e., \\[\n\\tau_{j+1}-\\tau_j=\\tau_j-\\tau_{j-1}\n\\] for all \\(j=2,\\dots,q-1.\\)\nThe space of all spline functions of degree \\(k\\) defined with respect to a given knot sequence \\[\na=\\tau_1&lt;\\tau_2\\leq\\dots\\leq \\tau_{q-1}&lt;\\tau_q=b\n\\] is a \\[\n\\begin{array}{lccccc}\np & = & \\text{number of knots} &+& \\text{polynomial degree} & -\\;\\;1\\\\\n& = & q & + & k & -\\;\\;1\n\\end{array}\n\\] dimensional linear function space \\[\n{\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}=\\operatorname{span}(b_{1,k},\\dots,b_{p,k}),\n\\] where \\(b_{1,k},\\dots,b_{p,k}\\) denote the basis-functions.\n\n\nConstruction of B-Spline Basis Functions\nThe so-called B-spline basis functions are almost always used in practice, since they possess a number of advantages from a numerical point of view.\nThe B-spline basis functions for splines of degree \\(k\\) are defined with respect to a given knot sequence \\[\n\\underbrace{a=\\tau_1}_{\\text{lower boundary knot}}\\quad {\\color{red}&lt;}\\quad\\overbrace{\\tau_2\\leq\\dots\\leq \\tau_{q-1}}^{\\text{interior knots}}\\quad{\\color{red}&lt;}\\quad\\underbrace{\\tau_q=b}_{\\text{upper boundary knot}}.\n\\]\nTo construct the B-spline basis functions, one augments the knot sequence by repeating each of the boundary knots \\(k+1\\) times: \\[\n\\underbrace{\\tau_{-(k-1)}=\\dots=\\tau_0=\\tau_1}_{\\text{$k+1$ lower boundary knots}}\\quad {\\color{red}&lt;}\\quad\\overbrace{\\tau_2\\leq\\dots\\leq \\tau_{q-1}}^{\\text{interior knots}}\\quad{\\color{red}&lt;}\\quad\\underbrace{\\tau_q=\\tau_{q+1}=\\dots=\\tau_{q+k}}_{\\text{$k+1$ upper boundary knots}}.\n\\] Let \\[\n\\tau^\\ast_{1}, \\dots,\\tau^\\ast_{q+2k}\n\\] denote the augmented knot sequence after resetting the index to start at \\(1.\\)\nThe spline basis functions are calculated by a recursive (over \\(l=0,1,\\dots,k\\)) procedure.\nThe initial level (\\(l=0\\)) are piece-wise constant functions \\[\nb_{j,0}(x)=\\left\\{\n\\begin{matrix}  \n1 & \\text{ if } \\tau_{j}^*\\leq x &lt;\\tau_{j+1}^*\\\\\n0 & \\text{ else}\n\\end{matrix}\n\\right.,\n\\tag{4.1}\\] for \\(j=1,\\dots,q+2k,\\) and \\(x\\in [a,b].\\)\nFor \\(l=1,\\dots,k\\) the recursion is defined by \\[\n\\begin{align*}\nb_{j,l}(x)\n& =\\frac{x-\\tau_j^*}{\\tau_{l+j}^*-\\tau_j^*}b_{j,l-1}(x)\\\\\n& +\\frac{\\tau_{l+j+1}^*-x}{\\tau_{l+j+1}^*-\\tau_{j+1}^*}b_{j+1,l-1}(x),\n\\end{align*}\n\\tag{4.2}\\] \\(j=1,\\dots,q+2k\\), and \\(x\\in [a,b].\\)\nNote: The definitions in Equation 4.1 and Equation 4.2 are understood that if the denominator is 0, then the function is defined to be 0. The remaining non-degenerated basis functions are then the \\[\nb_{j,k},\\quad j=1,\\dots,p=q+k-1,\n\\] B-spline basis functions.\n\n\nR-Code to Compute B-Spline Basis Functions:\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=1} - 1=7\n\\] linear (\\(k=1\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         &lt;- 1\nknots          &lt;- seq(from = 0, to = 1, len = 7)\ninternal_knots &lt;- knots[-c(1, length(knots))]\nboundary_knots &lt;- knots[ c(1, length(knots))]\n\n## evaluation points (for plotting)\nx_vec &lt;- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree1 &lt;- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree1, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 1\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree 1\n\n\n\nHere, the basis functions are piecewise linear (\\(k=1\\)) functions with local support over at most \\(k+1=1+1=2\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns &\\in\\mathcal{S}_{k=1,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=7)\\\\[2ex]\ns(x)&= \\sum_{j=1}^{7}\\vartheta_j b_{j,1}(0)\n\\end{align*}\n\\] is \\(k-1=1-1=0\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=2} - 1=8\n\\] quadratic (\\(k=2\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         &lt;- 2\nknots          &lt;- seq(from = 0, to = 1, len = 7)\ninternal_knots &lt;- knots[-c(1, length(knots))]\nboundary_knots &lt;- knots[ c(1, length(knots))]\n\n## evaluation points (for plotting)\nx_vec &lt;- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree2 &lt;- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree2, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 2\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree 2\n\n\n\nHere, the basis functions are piecewise quadratic (\\(k=2\\)) functions with local support over at most \\(k+1=2+1=3\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns&\\in\\mathcal{S}_{k=2,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=7)\\\\[2ex]\ns(x)&= \\sum_{j=1}^{8}\\vartheta_j b_{j,1}(x)\n\\end{align*}\n\\] is \\(k-1=2-1=1\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=3} - 1=9\n\\] cubic (\\(k=3\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         &lt;- 3\nknots          &lt;- seq(from = 0, to = 1, len = 7)\ninternal_knots &lt;- knots[-c(1, length(knots))]\nboundary_knots &lt;- knots[ c(1, length(knots))]\n\n\n## evaluation points (for plotting)\nx_vec &lt;- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree3 &lt;- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree3, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 3\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree 3 (usual case)\n\n\n\nHere, the basis functions are piecewise cubic (\\(k=3\\)) functions with local support over at most \\(k+1=3+1=4\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns &\\in\\mathcal{S}_{k=3,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=7)\\\\[2ex]\ns(x) &= \\sum_{j=1}^{9} \\vartheta_j b_{j,1}(x)\n\\end{align*}\n\\] is \\(k-1=3-1=2\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nNormalized: The B-spline basis system has a property that is often useful: the sum of the B-spline basis function values at any point \\(x\\) is equal to one. Note, for example, that the first and last basis functions are exactly one at the boundaries. This is because all the other basis functions go to zero at these end points.\nCompact Support: Basis functions are positive only over at most \\(k+1\\) intervals and zero over the remaining intervals. This compact support property is important for computational efficiency. \nMultiple knots: A multiple interior knot (\\(\\tau_j=\\tau_{j+1}\\)) reduces the degree of continuity at that knot value. At a normal interior knot, a spline function is \\(k-1\\) times continuously differentiable. Each extra knot with the same value reduces continuity at that knot by one. This is the only way to reduce the continuity of the curve at the knot values. If there are \\(k\\) (or more) equal knots, then you get a discontinuity in the curve at this knot-location.\n\n\n\n\n4.2.2.1 Regression Splines with Equidistant Knots\nRemember the nonparametric regression model setup:\n\\[\nY_i=m(X_i)+\\varepsilon_i\n\\]\n\n\\(m(X)=\\mathbb{E}(Y_i|X=X_i)\\) regression function\n\\(\\mathbb{E}(\\varepsilon_i)=0\\)\n\\(Var(\\varepsilon_i|X_i) = Var(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) and \\(X_i\\) are independent or at least mean-independent, i.e., \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\)\n\nThe so-called regression spline (or B-spline) approach to estimating a regression function \\(m(x)\\) is based on fitting a set of spline basis functions to the data.\nTypically, cubic splines (\\(k=3\\)) with equidistant knots are applied:\n\n\\(k=3\\) (cubic splines)\n\\(\\tau_1=a\\)\n\\(\\tau_{j+1}=\\tau_j + (b-a)/(q-1),\\quad j=1,\\dots,q-1\\)\n\nsuch that \\(\\tau_q=b\\)\n\nIn this case the number of knots \\(q,\\) or more precisely the total number of basis functions \\(p\\) \\[\n\\begin{align*}\np\n&=q+k-1\\\\[2ex]\n&=q+2\\qquad (\\text{using that}\\quad k=3)\n\\end{align*}\n\\] serves as the smoothing parameter which has to be selected by the statistician.\nFor a given choice of \\(p,\\) let \\[\n\\underset{(n\\times p)}{\\mathbf{X}}\n\\] denote the \\(n\\times p\\) matrix with elements \\[\nX_{ij}=b_{j,k}(X_i),\\quad i=1,\\dots,n,\\quad j=1,\\dots,p,\n\\] and let \\[\nY=(Y_1,\\dots,Y_n)^\\top\n\\] denote the vector of response variables.\nAn estimator \\(\\hat{m}_p(x)\\) of \\(m(x)\\) is then given by \\[\n\\hat m_p(x)=\\sum_{j=1}^p \\hat\\beta_j b_{j,k}(x),\n\\] where the coefficients \\(\\hat\\beta_j\\) are determined by ordinary least squares \\[\n\\hat\\beta_1,\\dots,\\hat\\beta_p=\\arg\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\vartheta_j \\underbrace{b_{j,k}(X_i)}_{X_{ij}}\\right)^2.\n\\] That is, the vector of coefficients \\[\n\\hat \\beta=(\\hat\\beta_1,\\dots,\\hat\\beta_p)^\\top\n\\] can be written as \\[\n\\hat\\beta=(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top Y.\n\\] The fitted values are given by \\[\n\\left(\\begin{array}{c}\n{\\hat m}_p(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ {\\hat m}_p(X_n)\n\\end{array}\\right)=\\mathbf{X}\\hat\\beta=\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top}_{=:S_p}Y = S_p Y\n\\]\nThe matrix \\[\nS_p = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n\\] is referred to as the smoothing matrix.\n\n\n\n\n\n\nRemark\n\n\n\nQuite generally, the most important nonparametric regression procedures are linear smoothing methods. This means that in dependence of some smoothing parameter (here \\(p\\)), estimates of the vector \\[\n(m(X_1),\\dots,m(X_n))^\\top\n\\] are obtained by multiplying a smoother matrix \\(S_p\\) with \\(Y\\).\nThat is, \\[\n\\left(\\begin{array}{c}\nm(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ m(X_n)\n\\end{array}\\right)\\approx\n\\left(\\begin{array}{c}\n{\\hat m}_p(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ {\\hat m}_p(X_n)\n\\end{array}\\right)=S_p Y\n\\]\n\n\nR Code to Compute Regression Splines:\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\nThen, we generate cubic B-spline basis functions with equidistant knot sequence (different to x_vec) and evaluate them at x_vec:\n\ndegree      &lt;- 3 # piecewise cubic splines\n\nknot_seq_5  &lt;- seq(from = 0, to = 1, len = 5)# knots\n\nX_mat_p7    &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq_5[-c(1, length(knot_seq_5))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq_5[ c(1, length(knot_seq_5))]\n    )\n\nknot_seq_15  &lt;- seq(from = 0, to = 1, len = 15)# knots\n\nX_mat_p17    &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq_15[-c(1, length(knot_seq_15))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq_15[ c(1, length(knot_seq_15))]\n    )    \n\nComputing the smoothing matrices \\(S_p\\) for \\(p=7\\) and \\(p=17\\):\n\nS_p7  &lt;- X_mat_p7  %*% solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) \nS_p17 &lt;- X_mat_p17 %*% solve(t(X_mat_p17) %*% X_mat_p17) %*% t(X_mat_p17) \n\nComputing the estimates \\(\\hat{m}_p(X_1),\\dots,\\hat{m}_p(X_n)\\) for \\(p=7\\) and \\(p=17\\):\n\nm_hat_p7  &lt;- S_p7  %*% y_vec\nm_hat_p17 &lt;- S_p17 %*% y_vec\n\nPlotting the estimation results:\n\nplot(y=y_vec, x=x_vec, xlab=\"X\", ylab=\"Y\", \n     main=\"Regression Splines\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"red\", lty=2, lwd=1.5)\nlines(y=m_hat_p7, x=x_vec, col=\"blue\", lwd=1.5)\nlines(y=m_hat_p17, x=x_vec, col=\"darkorange\", lwd=1.5)\nlegend(\"bottomleft\", \n       c(\"(Unknown) Regression Function m\", \n         \"Regr.-Spline Fit with p=7\", \n         \"Regr.-Spline Fit with p=17\"), \n       col=c(\"red\",\"blue\", \"darkorange\"), \n       lty=c(2,1,1), lwd=c(2,2,2))\n\n\n\n\n\n\n\n\nThe following plot shows the regression spline fit (with \\(p=7\\)) \\[\n\\hat{m}_{7}(x)=\\sum_{j=1}^{7}\\hat\\beta_j b_{j,3}(x)\n\\] along with the \\(p=7\\) basis functions each multiplied by the fitted linear coefficient \\[\n\\hat\\beta_j b_{j,3}(x),\\quad j=1,\\dots,7.\n\\]\n\nbeta_hat_p7 &lt;- solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) %*% y_vec\nplot(y=m_hat_p7, x=x_vec, ylim = c(-3.5,1.5),\n     col=\"blue\", lwd=1.5, type = \"l\", xlab=\"x\", ylab=\"\")\nabline(v = knot_seq_5, col = gray(0.5))     \nmatlines(X_mat_p7 * matrix(rep(beta_hat_p7, each = n), ncol=7), \n         x = x_vec, type=\"l\", lty = 2, col = c(1,2,3,4,5,6,7))\nlegend(\"topright\", \n       c(\"Regr.-Spline Fit with p=7\"), \n       col = \"blue\", lty= 1, lwd= 2)\nlegend(\"bottomleft\", \n       c(expression(paste(\"1. basis function times \",hat(beta)[1])),\n         expression(paste(\"2. basis function times \",hat(beta)[2])),\n         expression(paste(\"3. basis function times \",hat(beta)[3])),\n         expression(paste(\"4. basis function times \",hat(beta)[4])),\n         expression(paste(\"5. basis function times \",hat(beta)[5])),\n         expression(paste(\"6. basis function times \",hat(beta)[6])),\n         expression(paste(\"7. basis function times \",hat(beta)[7]))\n       ), \n       col = c(1,2,3,4,5,6,7), lty = 2, lwd = 1)       \n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Mean Average Squared Error of Regression Splines\nIn a nonparametric regression context we do not assume that the unknown true regression function \\(m(x)\\) exactly corresponds to a spline function. Thus, \\[\n\\hat m_p=(\\hat{m}_p(X_1),\\dots,\\hat{m}_p(X_n))^\\top\n\\] typically possesses a systematic estimation error (bias). That is, \\[\n\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\neq m(X_i).\n\\]\nTo simplify notation, we will in the following write \\[\n\\mathbb{E}_\\varepsilon(\\cdot)\\quad\\text{and}\\quad Var_\\varepsilon(\\cdot)\n\\] to denote expectation and variance “with respect to the random variable \\(\\varepsilon\\), only”.\nIn the case of random design, \\[\n\\mathbb{E}_\\varepsilon(\\cdot)\\quad\\text{and}\\quad Var_\\varepsilon(\\cdot)\n\\] thus denote the conditional expectation \\[\n\\mathbb{E}(\\cdot|X_1,\\dots,X_n)\n\\] and the conditional variance \\[\nVar(\\cdot|X_1,\\dots,X_n)\n\\] given the observed \\(X\\)-values.\nFor random design, these conditional expectations depend on the observed sample, and thus are random. For fixed design, such expectations are of course fixed values.\nIt will always be assumed that the matrix \\[\n\\mathbf{X}^\\top \\mathbf{X},\n\\] with \\(\\mathbf{X}=(b_{j,k}(X_i))_{i,j}\\), is invertible (under our conditions on the design density this holds with probability 1 for the random design).\nThe behavior of nonparametric function estimates is usually evaluated with respect to quadratic risk (i.e. mean squared error).\nA commonly used global measure of accuracy of a spline estimator \\(\\hat m_p\\) is the Mean Average Squared Error (MASE), which averages the local (at each \\(X_i\\)) mean squared errors over all \\(X_i,\\) \\(i=1,\\dots,n:\\) \\[\n\\begin{align*}\n&\\operatorname{MASE}(\\hat m_p):=\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left(m(X_i)-\\hat{m}_p(X_i)\\right)^2\\\\\n= &\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\left(\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))-m(X_i)\\right)^2}_{(\\operatorname{Bias}_\\varepsilon(\\hat{m}_p(X_i)))^2} + \\\\[2ex]\n&\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\mathbb{E}_\\varepsilon\\left((\\hat{m}_p(X_i)-\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\right)^2}_{Var_\\varepsilon(\\hat{m}_p(X_i))}\n\\end{align*}\n\\]\nAnother frequently used measure is the Mean Integrated Squared Error (MISE): \\[\n\\begin{align*}\n\\operatorname{MISE}(\\hat m_p):=\\int_a^b \\mathbb{E}_\\varepsilon\\left(m(x)-\\hat m_p(x)\\right)^2dx\n\\end{align*}\n\\]\n\nMASE versus MISE:\n\nEquidistant design: \\[\n(b-a)\\operatorname{MASE}(\\hat m_p)=\\operatorname{MISE}(\\hat m_p) + O(n^{-1})\n\\]\nMISE and MASE are generally not asymptotically equivalent in the case of random design \\[\n(b-a)\\operatorname{MASE}(\\hat m_p)=\\int_a^b \\mathbb{E}_\\varepsilon\\left(m(x)-\\hat m_p(x)\\right)^2 f(x)dx + O_P(n^{-1}).\n\\]\n\n\n\n\n\n\n\nLandau symbol “Big Oh” \\(O(r_n)\\)\n\n\n\n\\[\nO(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all sequences \\(x_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\dfrac{|x_n|}{r_n}\\to c\\) as \\(n\\to\\infty,\\) where \\(c\\) is a constant with \\(0\\leq c &lt; \\infty.\\)\n\nNote: This includes the case \\(\\dfrac{|x_n|}{r_n}\\to 0.\\)\nExamples:\n\n\\(-\\dfrac{1}{n}=O(n^{-1})\\)\n\\(\\dfrac{1}{n^2}=O(n^{-2})\\)\n\\(\\dfrac{1}{n^2}=O(n^{-1})\\)\n\\(\\displaystyle\\sum_{j=1}^{m} g\\left(x_j\\right)(x_{j}-x_{j-1}) = \\int_a^b g(x)dx + O(m^{-1})\\),  for every sufficiently smooth function \\(g\\) (\\(g\\) being continuously differentiable over \\((a,b)\\)), where \\(x_0=a\\) and \\(x_j=x_{0}+j(b-a)/m,\\) for \\(j=1,\\dots,m\\) such that \\(x_{j}-x_{j-1}=(b-a)/m.\\)\n\n\n\n\n\n\n\n\n\nLandau symbol “Small oh” \\(o(r_n)\\)\n\n\n\n\\[\no(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all sequences \\(x_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\dfrac{|x_n|}{r_n}\\to 0\\) as \\(n\\to\\infty.\\)\n\nExamples:\n\n\\(\\dfrac{1}{n^2}=o(n^{-1})\\)\n\\(n^{-a}=o(n^{-b})\\) for all \\(a&gt;b&gt;0.\\)\n\nNote: For every sequence \\(x_n=o(r_n)\\) it holds that \\(x_n=O(r_n),\\) but not the other way round.\n\n\n\n\n\n\n\n\nSpecial Cases \\(O(1)\\) and \\(o(1)\\)\n\n\n\n\\[\nO(1)\\quad \\text{and}\\quad o(1)\n\\]\nExamples:\n\n\\(1 + \\dfrac{1}{n} = O(1)\\)\n\\(\\dfrac{1}{n^2}=o(1)\\)\n\n\n\n\n\n\n\n\n\nStochastic Landau symbol “Big Oh P” \\(O_P(r_n)\\)\n\n\n\n\\[\nO_P(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all stochastic sequences \\(X_n\\), \\(n=1,2,\\dots,\\) for which\n\nthere exists for each (small) \\(\\epsilon&gt;0\\) a sufficiently large threshold \\(\\delta&gt;0\\) such that \\(\\displaystyle P\\left(\\frac{|X_n|}{r_n}&gt;\\delta\\right)&lt;\\epsilon\\) for all sufficiently large \\(n\\).  Plain English: “such that \\(\\frac{|X_n|}{r_n}\\) is bounded in probability for all large enough \\(n\\)”\n\nExample:\n\nIf \\(\\displaystyle \\sqrt{n}(\\bar{X}_n-\\mu)\\to_d\\mathcal{N}(0,\\sigma^2),\\) then \\[\n\\begin{align*}\n\\sqrt{n}(\\bar{X}_n-\\mu) &= O_P(1)\\\\[2ex]\n(\\bar{X}_n-\\mu) &= O_P(n^{-1/2})\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nStochastic Landau symbol “Small oh P” \\(o_P(r_n)\\)\n\n\n\n\\[\no_P(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all stochastic sequences \\(X_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\displaystyle \\frac{|X_n|}{r_n}\\to_P 0\\quad\\) as \\(\\quad n\\to\\infty\\)\n\nExample:\n\nIf \\(\\displaystyle \\sqrt{n}(\\bar{X}_n-\\mu)\\to_d\\mathcal{N}(0,\\sigma^2),\\) then \\[\n\\begin{align*}\n(\\bar{X}_n-\\mu)         &= o_P(1)\n\\end{align*}\n\\]\n\n\n\nIn the following we focus on the MASE which has the advantage that we can use matrix algebra.\nFirst, we look at the local bias of \\(\\hat{m}_p(X_i)\\) at a single evaluation point \\(X_i:\\) \\[\n\\operatorname{Bias}_\\varepsilon(\\hat{m}_p(X_i))=\\mathbb{E}_\\varepsilon(\\hat m_p(X_i))-m(X_i),\n\\] where \\[\n\\begin{align*}\n  \\mathbb{E}_\\varepsilon(\\hat m_p(X_i))&=\\mathbb{E}_\\varepsilon\\Big(\\sum_{j=1}^p \\hat{\\beta}_j b_{j,k}(X_i)\\Big)\\\\\n&=\\sum_{j=1}^p\\mathbb{E}_\\varepsilon(\\hat{\\beta}_j) b_{j,k}(X_i),\n\\end{align*}\n\\] with \\[\n\\hat{\\beta}=\\left(\\begin{matrix}\\hat{\\beta}_1\\\\\\vdots\\\\\\hat{\\beta}_p\\end{matrix}\\right)=(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top Y\n\\] and with \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\hat\\beta)\n&=\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  (\\overbrace{m+\\varepsilon}^{=Y})\\Big)\\\\[2ex]\n&=\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m\\Big)\\;+\\;\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\varepsilon\\Big)\\\\[2ex]\n&=\\overbrace{(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m}^{=(\\beta_1,\\dots,\\beta_p)^\\top=\\beta}\\;+\\;\\underbrace{(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\overbrace{\\mathbb{E}_\\varepsilon(\\varepsilon)}^{=0}}_{=0}\\\\[2ex]\n&=(\\beta_1,\\dots,\\beta_p)^\\top=\\beta\n\\end{align*}\n\\] where \\[\nm=\\left(\\begin{matrix}m(X_1)\\\\\\vdots\\\\m(X_n)\\end{matrix}\\right)\n\\] denotes the vector of true function values, and \\[\n\\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\\\vdots\\\\\\varepsilon_n\\end{matrix}\\right)\n\\] denotes the vector of error terms.\nThat is, the mean of the spline regression estimator evaluated at \\(X_i\\) is given by \\[\n\\mathbb{E}_\\varepsilon(\\hat m_p(X_i)) = \\sum_{j=1}^p\\beta_j b_{j,k}(X_i),\n\\] where \\[\n\\beta=\\left(\\begin{matrix}\\beta_1\\\\\\vdots\\\\\\beta_p\\end{matrix}\\right)=(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m\n\\] is a least squares solution; namely of the following least squares problem that tries to approximate the unknown vector \\(m=(m(X_1),\\dots,m(X_n))^\\top\\) using a spline function \\(s\\in {\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}:\\) \\[\n\\begin{align*}\n&\\sum_{i=1}^n \\left(m(X_i)-\\sum_{j=1}^p \\beta_j b_{j,k}(X_i)\\right)^2\\\\[2ex]\n&=\\min_{\\vartheta_1,\\dots,\\vartheta_p}\\sum_{i=1}^n \\left(m(X_i)-\\sum_{j=1}^p \\vartheta_j  b_{j,k}(X_i)\\right)^2\\\\[2ex]\n&=\\min_{s\\in {\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}} \\sum_{i=1}^n \\left(m(X_i)-s(X_i)\\right)^2.\n\\end{align*}\n\\]\nThat is, the mean of the spline regression estimator evaluated at \\(X_i\\) \\[\n\\mathbb{E}_\\varepsilon(\\hat m_p(X_i))=\\sum_{j=1}^p \\beta_j b_j(X_i)=:\\tilde m_p(X_i)\n\\] is the best least squares (\\(L_2\\)) approximation of the true, but unknown, regression function vector \\(m=(m(X_1),\\dots,m(X_n))^\\top\\) by means of a spline function vector \\(s=(s(X_1),\\dots,s(X_n))^\\top\\) with \\(s\\in{\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}.\\)\n\n\n\n\n\n\nImportant\n\n\n\nIf the true, but unknown, regression function \\(m\\) happens to be an element of the space of spline functions \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q},\\) then \\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) = 0,\\quad i=1,\\dots,n.\n\\] However, generally we do not expect that \\(m\\) is actually an element of \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q},\\) such that\n\\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) \\neq  0,\\quad i=1,\\dots,n.\n\\] For consistency, however, we need that \\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) \\to 0,\\quad i=1,\\dots,n.\n\\] as \\(n\\to\\infty\\) and \\(p\\equiv p_n\\to\\infty;\\) i.e. that \\(m\\) becomes eventually an element of the then very large space \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_{q_n}}\\) as \\(q_n\\to\\infty\\) with \\(n\\to\\infty.\\)\n\n\nFrom the general approximation properties of cubic splines (\\(k=3\\)) with \\(q=p-2\\) equidistant knots (De Boor and De Boor (1978) or Eubank (1999)), we know that:\n\nIf \\(m\\) is twice continuously differentiable over \\((a,b)\\), then \\[\n\\begin{align*}\n&(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n \\left(\\tilde m_p(X_i) - m(X_i)\\right)^2\n=\\left\\{\\begin{array}{ll}\nO(p^{-4})&\\quad\\text{fixed design}\\\\\nO_p(p^{-4})&\\quad\\text{random design}\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\]\nIf \\(m\\) is four times continuously differentiable, then \\[\n\\begin{align*}\n&(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\tilde m_p(X_i) - m(X_i)\\right)^2\n=\\left\\{\\begin{array}{ll}\nO(p^{-8})&\\quad\\text{fixed design}\\\\\nO_p(p^{-8})&\\quad\\text{random design}\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\]\n\nThe next step is to compute the (average) variance of the estimator, which can be obtained by the usual type of arguments applied in parametric regression:  \\[\n\\begin{align*}\nVar_\\varepsilon(\\hat m_p)\n&=\\frac{1}{n}\\sum_{i=1}^nVar_\\varepsilon(\\hat{m}_p(X_i))\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}_\\varepsilon\\big((\\hat{m}_p(X_i)-\\overbrace{\\tilde{m}(X_i)}^{=\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))})^2\\big)\\\\[2ex]\n%\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y-\n% \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\tilde m_p\\Vert_2^2\\right)\\\\\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\left\\Vert \\left(\\begin{matrix}\\hat{m}_p(X_1)\\\\\\vdots\\\\\\hat{m}_p(X_n)\\end{matrix}\\right)-\\left(\\begin{matrix}\\tilde{m}_p(X_1)\\\\\\vdots\\\\\\tilde{m}_p(X_n)\\end{matrix}\\right)\\right\\Vert_{2}^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y-\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top m\\Vert_2^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top (Y-m)\\Vert_2^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\Vert_2^2\\right)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  (\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top )^\\top}_{(1\\times n)}\\;\\;\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(n\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  (\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top )}_{(1\\times n)}\\;\\;\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(n\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(1\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\operatorname{trace}\\left(\\varepsilon^\\top  \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\right)\\right)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\operatorname{trace}\\left(  (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\varepsilon^\\top\\mathbf{X}\\right)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\operatorname{trace}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\mathbb{E}_\\varepsilon(\\varepsilon\\varepsilon^\\top ) \\mathbf{X}\\right)\\quad(\\text{with }\\mathbb{E}_\\varepsilon(\\varepsilon\\varepsilon^\\top )=I_n\\sigma_\\varepsilon)\\\\[2ex]\n&=\\frac{1}{n} \\sigma^2 \\text{trace}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\mathbf{X}\\right)\\\\[2ex]\n&=\\frac{1}{n} \\sigma^2 \\text{trace}\\left(I_p\\right)\\\\[2ex]\n&=\\sigma^2  \\frac{p}{n}\n\\end{align*}\n\\]\n\n\n\n\n\n\nTrace-Trick\n\n\n\nFor any \\((m\\times n)\\) matrix \\(A\\) and any \\((n\\times m)\\) matrix \\(B\\) we have the identity \\[\\text{trace}(AB)=\\text{trace}(BA)\\]\n\n\n\n\nSummary\nFor cubic (\\(k=3\\)) splines with \\(q\\) equidistant knots and a twice differentiable function \\(m\\) we have that:\n\\[\n\\begin{align*}\n\\displaystyle(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2&=\\left\\{\\begin{array}{ll}\nO(p^{-4})&\\quad\\text{fixed design}\\\\\nO_p(p^{-4})&\\quad\\text{random design}\\\\\n\\end{array}\n\\right.\\\\[2ex]\n\\displaystyle Var_\\varepsilon(\\hat m_p)&= \\sigma^2\\frac{p}{n},\n\\end{align*}\n\\tag{4.3}\\] where \\(p=q+2\\) is the number of basis functions (i.e. the smoothing parameter).\nThis leads to the classical trade-off between (average) squared bias and (average) variance that is typical for nonparametric statistics:\n\n\\(p\\) small: \\(\\displaystyle Var_\\varepsilon(\\hat m_p)\\) is small, but squared bias \\(\\displaystyle (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\) is large.\n\\(p\\) large: \\(\\displaystyle Var_\\varepsilon(\\hat m_p)\\) is large, but squared bias \\(\\displaystyle (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\) is small.\n\n\nLet us focus now on the case of a fixed design. From Equation 4.3 we have that \\[\n\\begin{align*}\n\\operatorname{MASE}(\\hat m_{p})\n&= (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2 + Var_\\varepsilon(\\hat m_p)\\\\[2ex]\n&=O(p^{-4}) + \\sigma^2\\frac{p}{n}\n\\end{align*}\n\\] I.e., using the definition of \\(O(\\cdot)\\), we have that \\[\n\\begin{align*}\n\\underbrace{\\operatorname{MASE}(\\hat m_{p})}_{\\geq 0}\n&\\leq \\underbrace{\\texttt{const}}_{&gt;0}\\;\\; p^{-4} + \\sigma^2\\frac{p}{n}\n\\end{align*}\n\\]\nThus, if \\[\np\\equiv p_n\\to\\infty\\quad\\text{as}\\quad n\\to\\infty,\n\\] but sufficiently slow, such that \\[\n\\dfrac{p_n}{n}\\to 0,\n\\] then \\[\n\\begin{align*}\n\\operatorname{MASE}(\\hat m_{p}) \\to 0\n\\end{align*}\n\\] as \\(n\\to 0.\\)\nAn optimal smoothing parameter \\(p_{opt}\\) that minimizes \\(\\operatorname{MASE}(\\hat m_{p})\\) will balance the squared bias and variance: \\[\n\\begin{align*}\n\\frac{d}{d\\,p}\\operatorname{MASE}(\\hat m_{p})\n&=\\frac{d}{d\\,p}\\left(\\texttt{const}\\cdot p^{-4}+\\sigma^2\\frac{p}{n}\\right)\\\\[2ex]\n&=-4\\cdot\\texttt{const}\\cdot p^{-5}+\\sigma^2\\frac{1}{n}.\n\\end{align*}\n\\] Setting zero and solving for \\(p_{opt}\\) yields \\[\np_{opt}=\\texttt{const}\\cdot n^{1/5},\n\\] where \\(\\texttt{const}\\) denotes here a generic factor collecting all factors that do not depend on \\(p_n\\) or \\(n.\\)\nThus \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O(n^{-4/5}).\n\\]\nIn the case of a random design \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O_p(n^{-4/5}).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nFor an estimator \\(\\hat m\\) based on a valid (!) parametric model we have \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O_p(n^{-1}),\n\\] since parametric models have no bias—provided, the model assumption is correct.\n\n\nSimilar results can be obtained for the mean integrated squared error (MISE): If \\(m\\) is twice continuously differentiable, and \\(p_{opt} \\sim n^{1/5}\\), then \\[\n\\operatorname{MISE}(\\hat m_{p_{opt}})=\\mathbb{E}_\\varepsilon\\left(\\int_a^b(m(x)-\\hat m_{p_{opt}}(x))^2dx\\right)=O_p(n^{-4/5}).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html#selecting-the-smoothing-parameter-p",
    "href": "Ch4_NPRegression.html#selecting-the-smoothing-parameter-p",
    "title": "4  Nonparametric Regression",
    "section": "4.3 Selecting the Smoothing Parameter \\(p\\)",
    "text": "4.3 Selecting the Smoothing Parameter \\(p\\)\nAim: We need to choose the smoothing parameter \\(p\\) in an (somehow) optimal and objective manner.\nProblem: Since \\(m\\) is unknown, we cannot directly compute the \\(\\operatorname{Bias}(\\hat{m}_p)\\) and thus not \\(\\operatorname{MASE}(\\hat{m}_p).\\) This renders a direct computation of the optimal smoothing parameter \\(p_{opt}\\) impossible.\nApproach: Determine an estimate \\(\\hat p_{opt}\\) of the unknown optimal smoothing parameter \\(p_{opt}\\) by minimizing a suitable error criterion with the following properties:\n\nFor every possible \\(p\\) the error criterion function can be calculated from the data.\nFor every possible \\(p\\) the error criterion provides “information” about the respective \\(\\operatorname{MASE}(\\hat{m}_p)\\).\n\n \nRecall: We have \\[\n\\hat m_p=\n\\left(\\begin{matrix}\n\\hat m_p(X_1)\\\\ \\vdots\\\\\\hat m_p(X_n)\n\\end{matrix}\\right)=\\mathbf{X}\\hat\\beta=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y=S_pY,\n\\] where\n\\[\n\\begin{align*}\n\\operatorname{trace}(S_p)\n&=\\operatorname{trace}\\big(\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\big)\\\\[2ex]\n&=\\operatorname{trace}\\big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{X}\\big)\\\\[2ex]\n&=\\operatorname{trace}\\big(I_p\\big)=p\n\\end{align*}\n\\]\nThat is, for given \\(p\\), the number of parameters to estimate by the spline method (one also speaks of the degrees of freedom of the smoothing procedure) is equal to \\(p\\) which corresponds to the trace of the smoother matrix \\(S_p=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top.\\)\nMost frequently used error criteria are Cross-Validation (CV) and Generalized Cross-Validation (GCV).\n\n4.3.1 Leave One Out Cross-Validation (LOOCV)\nFor a given value \\(p,\\) cross-validation tries to approximate the out-of-sample prediction error \\[\n\\operatorname{LOOCV}(p)=\\frac{1}{n} \\sum_{i=1}^n\\biggl( Y_i-\n{\\hat m}_{p,-i}(X_i)\\biggr)^2\n\\] Here, for any \\(i=1,\\dots,n\\), \\({\\hat m}_{p,-i}(\\cdot)\\) is the “leave-one-out” estimator of \\(m(\\cdot)\\) to be obtained when a spline function is fitted using the \\(n-1\\) observations: \\[\n(Y_1,X_1),\\dots,(Y_{i-1},X_{i-1}),(Y_{i+1},X_{i+1}),\\dots,(Y_{n},X_{n}).\n\\] Motivation: We have \\[\n\\begin{align*}\n&\\mathbb{E}_\\varepsilon(\\operatorname{LOOCV}(p))\\\\[2ex]\n= & \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\biggl( \\overbrace{m(X_i)+\\varepsilon_i}^{=Y_i}-\n{\\hat m}_{p,-i}(X_i)\\biggr)^2\\right]\\\\[2ex]\n= & \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\left(\\left( m(X_i)-\n{\\hat m}_{p,-i}(X_i) \\right)^2 +2\\left( m(X_i)-\n{\\hat m}_{p,-i}(X_i) \\right)\\varepsilon_i +\\varepsilon_i^2\\right)\\right]\\\\[2ex]\n= &\\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\left(m(X_i)-\n{\\hat m}_{p,-i}(X_i)\\right)^2\\right]}_{\\approx\\operatorname{MASE}(\\hat m_p)} \\\\[2ex]\n&+ 2\\frac{1}{n} \\sum_{i=1}^n\n\\underbrace{\\mathbb{E}_\\varepsilon\\left[( m(X_i)-\n{\\hat m}_{p,-i}(X_i))\\varepsilon_i\\right]}_{=0}+\\sigma^2\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{LOOCV}(p)) = \\operatorname{MASE}(\\hat m_p) + \\sigma^2,\n\\end{align*}\n\\] such that \\[\n\\begin{align*}\np_{opt}\n&= \\arg\\min_p\\mathbb{E}_\\varepsilon(\\operatorname{LOOCV}(p))\\\\[2ex]\n&= \\arg\\min_p\\operatorname{MASE}(\\hat m_p).\n\\end{align*}\n\\] That is, at least on average, minimizing \\(CV(p)\\) is equivalent to minimizing \\(\\operatorname{MASE}(\\hat m_p)\\).\nR-Code to Compute an Estimate of the Optimal Smoothing Parameter using LOOCV\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\nThen, compute the CV scores for different numbers of basis functions \\(p\\) and plot them to select an estimate for the optimal value of the smoothing parameter \\(p\\).\n\np_vec &lt;- 6:12 \nLOOCV_p &lt;- numeric(length(p_vec))\n\nfor(j in 1:length(p_vec)){\n\np         &lt;- p_vec[j] # number of basis functions\nq         &lt;- p - 2    # number of equidistant knots \nknot_seq  &lt;- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nLOOCV_scoreSq &lt;- numeric(n)\n\nfor(i in 1:n){\nm_hat_p_i  &lt;- X_mat[i,] %*% \n              solve(t(X_mat[-i,])  %*% X_mat[-i,])  %*% t(X_mat[-i,]) %*% y_vec[-i] \n\nLOOCV_scoreSq[i] &lt;- (y_vec[i] - m_hat_p_i)^2\n}\nLOOCV_p[j] &lt;- mean(LOOCV_scoreSq)\n}\nplot(y = LOOCV_p, x = p_vec, type=\"o\")\n\n\n\n\n\n\n\n\n\n\n4.3.2 \\(k\\)-fold Cross-Validation\nIn practice, one usually works with \\(k\\)-fold CV (\\(k=5\\) or \\(k=10\\)). For this the index set \\(I=\\{1,\\dots,n\\}\\) is partitioned into \\(k\\) disjoint index sets \\(I_1,\\dots,I_k\\) of (roughly) equal sizes, i.e. \\(|I_1|\\approx|I_2|\\approx\\dots\\approx|I_k|\\), such that \\(I_1\\cup I_1\\cup \\dots \\cup I_k=I\\) and \\(I_j\\cap I_k=\\emptyset\\) for all \\(j\\neq k\\). \\[\n\\operatorname{CV}_k(p)=\\frac{1}{K}\\sum_{k=1}^K\\frac{1}{|I_k|} \\sum_{i\\in I_k}\\left( Y_i-\n{\\hat m}_{p,-I_k}(X_i)\\right)^2.\n\\]\n\n\n4.3.3 Generalized Cross-Validation (GCV)\n\\[\n\\operatorname{GCV}(p)=\\frac{1}{n\\left(1-\\frac{p}{n}\\right)^2}\\sum_{i=1}^n \\left( Y_i-\n{\\hat m}_p(X_i)\\right)^2\n\\] Motivation: The average residual sum of squares are given by \\[\n\\operatorname{ARSS}(p):=\\frac{1}{n}\\sum_{i=1}^n\\biggl( Y_i-\n\\hat{m}_{p}(X_i)\\biggr)^2\n\\tag{4.4}\\] which allows us to rewrite \\(\\operatorname{GCV}(p)\\) as \\[\n\\operatorname{GCV}(p)=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\operatorname{ARSS}(p)\n\\] Some lengthy derivations show that the expected value of \\(\\operatorname{ARSS}(p)\\) is \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2,\n\\end{align*}\n\\tag{4.5}\\] where\n\n\\(\\operatorname{MASE}(\\hat m_p)\\) denotes the mean average squared error,\n\\(-2\\sigma^2\\frac{p}{n}\\) denotes the optimism term, and\n\\(\\sigma^2\\) denotes the irreducible component of the (prediction-)error.\n\nMoreover, a Taylor expansion of \\(f(p/n)=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\) around \\(p/n=0\\) yields that \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation becomes precise as \\(\\frac{p}{n}\\to 0.\\) Thus \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{GCV}(p))\n&\\approx \\left(1 + 2\\frac{p}{n}\\right)\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p)) \\\\[2ex]\n&=\\left(1 + 2\\frac{p}{n}\\right) \\left(\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\\right)\\\\[2ex]\n&= \\operatorname{MASE}(\\hat m_p) +\\sigma^2 +\\\\[2ex]\n&+ \\underbrace{2\\frac{p}{n} \\operatorname{MASE}(\\hat m_p)}_{=O_p\\left(\\frac{p}{n}\\right)} - \\underbrace{4\\frac{p^2}{n^2} \\sigma^2}_{=O\\left(\\frac{p^2}{n^2}\\right)=o\\left(\\frac{p}{n}\\right)}\\\\[2ex]\n&= \\operatorname{MASE}(\\hat m_p) +\\sigma^2 + O_p\\left(\\frac{p}{n}\\right)\n\\end{align*}\n\\]\nThus, at least on average, minimizing \\(\\operatorname{GCV}(p)\\) is approximately (for \\(p_n/n\\to 0\\) as \\(n\\to\\infty\\)) equivalent to minimizing \\(\\operatorname{MASE}(\\hat m_p),\\) since \\(\\sigma^2\\) does not depend on \\(p.\\)\n\n\nR-Code to Compute an Estimate of the Optimal Smoothing Parameter using GCV\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\nThen, compute the GCV scores for different numbers of basis functions \\(p\\) and plot them to select an estimate for the optimal value of the smoothing parameter \\(p\\).\n\np_vec &lt;- 6:12 \nGCV_p &lt;- numeric(length(p_vec))\n\nfor(j in 1:length(p_vec)){\n\np         &lt;- p_vec[j] # number of basis functions\nq         &lt;- p - 2    # number of equidistant knots \nknot_seq  &lt;- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nS_p      &lt;- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) \nm_hat_p  &lt;- S_p   %*% y_vec\nARSS     &lt;- mean(c(y_vec - m_hat_p)^2)\nGCV_p[j] &lt;- ARSS/((1-p/n)^2)\n}\nplot(y = GCV_p, x = p_vec, type=\"o\")\n\n\n\n\n\n\n\n\nCompute the nonparametric regression estimate using the GCV optimal smoothing parameter \\(p=8.\\)\n\np         &lt;- 8\nq         &lt;- p - 2    # number of equidistant knots \nknot_seq  &lt;- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nS_p      &lt;- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) \nm_hat_p  &lt;- S_p   %*% y_vec\n\nLet’s plot the results:\n\nplot(y=y_vec, x=x_vec, xlab=\"X\", ylab=\"Y\", \n     main=\"Regression Splines\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"red\", lty=2, lwd=1.5)\nlines(y=m_hat_p, x=x_vec, col=\"blue\", lwd=1.5)\nlegend(\"bottomleft\", \n       c(\"(Unknown) Regression Function m\", \n         \"Regr.-Spline Fit with GCV optimal p=8\"), \n       col=c(\"red\",\"blue\"), \n       lty=c(2,1,1), lwd=c(2,2,2))\n\n\n\n\n\n\n\n\n\n\n4.3.4 Over-Fitting and Optimism\nRemember that in our econometrics lecture, where we assumed to know the model apart from the model parameters, we used the average squared residuals \\(\\operatorname{ARSS}(p)\\) \\[\n\\operatorname{ARSS}(p)=\\frac{1}{n}\\sum_{i=1}^n\\biggl( Y_i-\n\\hat{m}_{p}(X_i)\\biggr)^2\n\\tag{4.6}\\] as an estimator for \\(\\sigma^2.\\) Moreover, \\(\\operatorname{ARSS}(p)\\) was the main component to compute the \\(R^2\\)-coefficient \\[\nR^2(p) = 1- \\frac{\\operatorname{ARSS}(p)}{\\frac{1}{n}\\sum_{i=1}^n(Y_i - \\bar{Y})^2}.\n\\]\nIn non-parametrics, however, we do not assume to know the model, but try to learn the model (including the smoothing parameter \\(p\\)) form the data. Therefore, Equation 4.6 cannot be used as an estimator for \\(\\sigma^2\\), firstly, since we do not know \\(p\\), and secondly, since we expect \\(\\hat{m}_{p}\\) to be biased.\nIn fact, if we would mimimize \\(\\operatorname{ARSS}(p)\\) with respect to \\(p,\\) we would minimize the so-called in-sample prediction error. On average, this would result in minimizing \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\] which leads to a too large choice of \\(p\\) due to the distorting optimism term \\[\n-2\\sigma^2\\frac{p}{n}.\n\\]\nIn fact, for a too large \\(p,\\) the non-parametric estimate over-fits the data; i.e. \\(\\hat{m}_{p}\\) becomes so flexible such that \\[\n\\begin{align*}\nY_i & \\approx \\hat{m}_{p}(X_i)\\;\\;\\text{ for all}\\; i=1,\\dots,n\\\\[2ex]\n\\Rightarrow\\;\\operatorname{ARSS}(p)&\\approx 0\\;(\\neq\\sigma^2)\\\\[2ex]\n\\Rightarrow\\;R^2&\\approx 1.\n\\end{align*}\n\\]\nHowever, an over-fitted estimate \\(\\hat{m}_{p}\\) typically has fitted the noise component \\(\\varepsilon\\)—additionally to the signal component \\(m.\\) Therefore, an over-fitted estimate \\(\\hat{m}_{p}\\) will typically perform very poorly when used to predict new out-of-sample data. I.e. for a new outcome \\[\nY_{new} = m(X_{new}) + \\varepsilon_{new},\n\\] we’ll have that \\[\n|m(X_{new}) - \\hat{m}_{p}(X_{new})| \\gg 0;\n\\] see Figure 4.2.\n\n\n\n\n\n\nFigure 4.2: Over-fitting can lead to unusable out-of-sample predictions.\n\n\n\n\n\n\n\n\n\nOptimism\n\n\n\nThe term \\(2\\sigma^2\\frac{p}{n}\\) in \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\] is called the optimism of the fit and quantifies the amount by which the in-sample average residual sum of squares (ARSS) systematically under-estimates the true mean average squared error (MASE) of \\(\\hat m_p.\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html#exercises",
    "href": "Ch4_NPRegression.html#exercises",
    "title": "4  Nonparametric Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nShow that \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation becomes precise as \\(\\frac{p}{n}\\to 0.\\)\n\n\nExercise 2.\nShow that \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\]\n\n\nExercise 3.\nShow that the core-part of the optimism term, i.e.  \\[\n\\sigma^2\\frac{p}{n},\n\\] quantifies the average covariance between the error terms \\(\\varepsilon_i\\) and the fits \\(\\hat{m}_p(X_i)\\) over \\(i=1,\\dots,n.\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html#references",
    "href": "Ch4_NPRegression.html#references",
    "title": "4  Nonparametric Regression",
    "section": "References",
    "text": "References\n\n\n\n\nDe Boor, Carl, and Carl De Boor. 1978. A Practical Guide to Splines. Vol. 27. springer.\n\n\nEubank, Randall L. 1999. Nonparametric Regression and Spline Smoothing. CRC press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch3_Bootstrap.html#bootstrap-and-linear-regression-analysis",
    "href": "Ch3_Bootstrap.html#bootstrap-and-linear-regression-analysis",
    "title": "3  The Bootstrap",
    "section": "3.5 Bootstrap and Linear Regression Analysis",
    "text": "3.5 Bootstrap and Linear Regression Analysis\nIn this chapter we consider two different bootstrap resampling procedures that can be applied in linear regression analysis:\n\nSection 3.5.1 considers the Bootstrapping Pairs algorithm\n\nSection 3.5.2 considers the Residual Bootstrap algorithm.\n\nSetup: Linear regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or “dependent”) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\n\n\n\n\n\n\nDefinition 3.6 (Random and Fixed Design) \nRandom Design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0,\\) \\(M=\\mathbb{E}(X_iX_i^T)\\) non-singular, and with either\n\nhomoskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0\\), \\(i=1,\\dots,n\\), for a constant \\(\\sigma^2&lt;\\infty\\) or\nheteroskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)&lt;\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed Design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean, \\(\\mathbb{E}(\\varepsilon_i)=0,\\) and homoskedastic errors, \\(\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0,\\) for all \\(i=1,\\dots,n.\\)\n\n\n\n\nThe least squares estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i.\n\\end{align*}\n\\]\nUsing that \\(Y_i=X_i^\\top\\beta_0+\\varepsilon_i,\\) one can derive that \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\beta_0+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\n\n3.5.1 Bootstrap under Random Design: Bootstrapping Pairs\nUnder a random design (Definition 3.6), we assume that there exists a non-singular (thus invertible) matrix \\(M\\) \\[\nM=\\mathbb{E}(X_iX_i^T).\n\\] This implies that the following matrix \\(Q\\) is also non-singular: \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\varepsilon_i^2X_iX_i^T)\\\\[2ex]\n&=\\mathbb{E}(\\mathbb{E}(\\varepsilon_i^2X_iX_i^T|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T\\mathbb{E}(1|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIn case of homoskedastic errors, we have that \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\\\\\n&=\\sigma^2_0\\;\\mathbb{E}(X_iX_i^T)\\\\[2ex]\n&=\\sigma^2_0\\;M.\n\\end{align*}\n\\]\n\n\nThe law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta_0)\\rightarrow_d\\mathcal{N}_p(0,M^{-1}QM^{-1}),\\quad n\\to\\infty,\n\\] where \\(\\mathcal{N}_p(0,M^{-1}QM^{-1})\\) denotes the \\(p\\)-dimensional normal distribution with \\((p\\times 1)\\)-dimensional mean \\(0\\) and \\((p\\times p)\\)-dimensional variance-covariance matrix \\(M^{-1}QM^{-1}.\\)\nThe idea of bootstraping pairs is very simple: The procedure builds upon the assumption that \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. which suggests a bootstrap based on resampling the pairs \\((Y_i,X_i),\\) \\(i=1,\\dots,n.\\)\nBootstraping Pairs Algorithm:\n\nGenerate bootstrap samples \\[\n  (Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n  \\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*):\\) \\[\n\\hat\\beta^*_n=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nRepeating Steps 1-2 \\(m\\)-many times yields \\(m\\) (e.g. \\(m=10,000\\)) bootstrap estimators \\[\n\\hat\\beta^*_{n,1},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution of \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\nIt can be shown that bootstrapping pairs is consistent; i.e. that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\\approx\\mathcal{N}_p(0,M^{-1}QM^{-1}).\n\\]\n\n\n3.5.2 Bootstrap under Fixed Design: The Residual Bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure (Section 3.5.1) proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoskedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] under the fixed design (Definition 3.6), where \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\\overset{\\text{i.i.d.}}{\\sim}\\varepsilon\n\\] are i.i.d. with zero mean \\[\n\\mathbb{E}(\\varepsilon)=0\n\\] and homoskedastic variance \\[\n\\mathbb{E}(\\varepsilon^2)=\\sigma^2_0.\n\\]\n\n\n\n\n\n\nApplicability of the Residual Bootstrap under Random Designs\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs—even when the \\(X\\)-variables are correlated (e.g. time-series).\nIn such cases, the following arguments are meant conditionally on the observed predictors \\(X_1,\\dots,X_n\\).\nThe above assumptions on the error terms then, of course, also have to be satisfied conditionally on \\(X_1,\\dots,X_n.\\)\n\n\nThe idea of the residual bootstrap is very simple: The procedure builds upon the assumption that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\\overset{\\text{i.i.d.}}{\\sim}\\varepsilon\n\\] are i.i.d which suggests a bootstrap based on resampling the (homoskedastic) error terms.\nThese errors are, of course, unobserved, but they can be approximated by their corresponding residuals \\[\n\\hat \\varepsilon_i=Y_i-X_i^T\\hat\\beta_n, \\quad i=1,\\dots,n,\n\\] where \\[\n\\hat\\beta_n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator based on the original sample \\(\\mathcal{S}_n\\).\nIt is well known that \\[\n\\hat\\sigma^2_n= \\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides a consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\n\\hat\\sigma^2_n\\rightarrow_p \\sigma_0^2\n\\] as \\(n\\to\\infty.\\)\nResidual Bootstrap Algorithm:\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta_n\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta_n + \\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*_n = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) (e.g. \\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\hat\\beta^*_{n,2},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\n\nIt can be shown that the residual bootstrap is consistent; i.e. that\n\\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta_n-\\beta_0))}_{\\mathcal{N}_p\\left(0,\\sigma^2_0\\, M^{-1}\\right)}\n\\] for large \\(n.\\)\n\n\n3.5.3 Bootstrap under Fixed Design: The Wild Bootstrap\nThe wild bootstrap is a method for generating bootstrap samples that do not consist of resampling the original data (bootstrapping pairs in Section 3.5.1) or residuals (bootstrapping residuals in Section 3.5.2). Rather, the wild bootstrap combines the data with random variables drawn from a known distribution to form a bootstrap sample.\nThe wild bootstrap provides a way to deal with issues such as heteroskedasticity of unknown form in fixed-design regression models or random-design models in which one conditions on the predictors \\(X_1,\\dots,X_n.\\)\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] where the \\(X_i\\)’s are fixed in repeated samples (fixed design), and where the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are independent across \\(i=1,\\dots,n,\\) with\n\\[\n\\mathbb{E}(\\varepsilon_i)=0\\quad\\text{for all}\\quad i=1,\\dots,n,\n\\] and with possibly heteroskedastic variances \\[\n0&lt;\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_{0,i}&lt;\\infty\\quad\\text{for all}\\quad i=1,\\dots,n.\n\\]\nThat is, the data generating process is here independent across \\(i=1,\\dots,n,\\) but not necessarily identically distributed across \\(i=1,\\dots,n.\\)\n\n\n\n\n\n\nApplicability of the Wild Bootstrap under Random Designs\n\n\n\nThough we will formally rely on a fixed design assumption. However, as in the case of the residual bootstrap, the wild bootstrap is also applicable for random designs.\nIn random designs, the following arguments are meant conditionally on the observed predictors \\(X_1,\\dots,X_n\\).\nThe above assumptions on the error terms then, of course, also have to be satisfied conditionally on \\(X_1,\\dots,X_n.\\)\n\n\nAs the residual bootstrap (Section 3.5.2), the wild bootstrap uses the \\(X_i\\)’s from the original data. I.e., the \\(X_i\\)’s are not resampled. The wild bootstrap generates bootstrap samples \\[\n\\{(\\underbrace{X_1^\\top\\hat\\beta_n + \\varepsilon^\\ast_1}_{=Y_1^\\ast},X_1),\\dots,(\\underbrace{X_n^\\top\\hat\\beta_n + \\varepsilon^\\ast_n}_{=Y_n^\\ast},X_n)\\},\n\\] where \\(\\hat\\beta_n\\) is computed from the original sample. Repeatedly (\\(m\\) times) generating such bootstrap samples allows generating \\(m\\) realizations of the bootstrap estimator \\[\n\\hat\\beta^\\ast_{n,j} = \\left(X^\\top X\\right)^{-1}X^\\top Y^\\ast,\\quad j=1,\\dots,m.\n\\]\nThe bootstrap errors \\(\\varepsilon_i^\\ast\\)’s are generated by either of the following two methods:\n\nThe wild bootstrap (Mammen (1993)) uses \\[\n\\varepsilon_i^\\ast = W_i\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] to generate the bootstrap samples \\[\n\\{(\\underbrace{X_1^\\top\\hat\\beta_n + \\varepsilon^\\ast_1}_{=Y_1^\\ast},X_1),\\dots,(\\underbrace{X_n^\\top\\hat\\beta_n + \\varepsilon^\\ast_n}_{=Y_n^\\ast},X_n)\\},\n\\] where \\(W_i\\) is a discrete random variable taking values \\[\nW_i\\in\\left\\{\\left(1-\\sqrt{5}\\right)\\hat\\varepsilon_i,\\;\\left(1+\\sqrt{5}\\right)\\frac{\\hat\\varepsilon_i}{2}\\right\\}\n\\] with \\[\n\\hat\\varepsilon_i=Y_i - X_i^\\top\\hat\\beta_n,\\quad i=1,\\dots,n,\n\\]\ndenoting the original OLS residuals computed, and with \\[\n\\begin{align*}\nP\\left(W_i = \\left(1-\\sqrt{5}\\right)\\hat\\varepsilon_i\\right) &= \\frac{1+\\sqrt{5}}{2\\sqrt{5}}\\\\[2ex]\nP\\left(W_i = \\left(1+\\sqrt{5}\\right)\\frac{\\hat\\varepsilon_i}{2}\\right)&=1 - \\frac{1+\\sqrt{5}}{2\\sqrt{5}}.\n\\end{align*}\n\\] Under this construction, we have that \\[\n\\begin{align*}\n\\mathbb{E}(W_i)  &=0\\\\[2ex]\n\\mathbb{E}(W_i^2)&=\\hat\\varepsilon_i^2\\\\[2ex]\n\\mathbb{E}(W_i^3)&=\\hat\\varepsilon_i^3.\n\\end{align*}\n\\] See Mammen (1993) for a detailed discussion of the properties of this method.\nThe second method is an example of the multiplier bootstrap, which uses \\[\n\\varepsilon_i^\\ast = U_i\\,f(\\hat{\\varepsilon}_i)\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] to generate the bootstrap samples \\[\n\\{(\\underbrace{X_1^\\top\\hat\\beta_n + \\varepsilon^\\ast_1}_{=Y_1^\\ast},X_1),\\dots,(\\underbrace{X_n^\\top\\hat\\beta_n + \\varepsilon^\\ast_n}_{=Y_n^\\ast},X_n)\\},\n\\] where\n\n\\(f(\\hat{\\varepsilon}_i)\\) is a transformation of the residuals with \\(f(\\hat{\\varepsilon}_i)=\\hat{\\varepsilon}_i\\) being one possibility\n\\(U_1,\\dots,U_n\\overset{\\text{i.i.d.}}{\\sim}U\\) are real valued i.i.d. random variables, independent of the residuals \\(\\hat{\\varepsilon}_i,\\) and with \\(\\mathbb{E}(U)=0\\) and \\(\\mathbb{E}(U^2)=1.\\) One possibility would be \\(U\\sim\\mathcal{N}(0,1).\\) Another often used possibility is to use a Rademacher distributed random variable \\(U.\\)\n\n\nSee Davidson and Flachaire (2008) for a detailed discuss of the properties of this method.\n\n\n3.5.4 Bootstrap Confidence Intervals for the \\(\\boldsymbol{j}\\)th Component of the Regression Coefficient \\(\\boldsymbol{\\beta_{0}}\\)\nThis chapter introduces two confidence intervals. The first uses the basic bootstrap method (Section 3.3); the second uses the bootstrap-\\(t\\) method (Section 3.4).\nBoth confidence intervals can be constructed either via bootstrapping pairs (Section 3.5.1) or via bootstrapping residuals (Section 3.5.2).\n\n\n\n\n\n\nTip\n\n\n\nWhile the bootstrap confidence intervals based on bootstrapping pairs (Section 3.5.1) are heteroskedasticity robust, the bootstrap confidence intervals based on bootstrapping residuals are only valid for homoskedastic errors.\n\n\n\n3.5.4.1 Basic Bootstrap Confidence Intervals for \\(\\boldsymbol{\\beta_{0,j}}\\)\nLet \\[\n\\beta_{0,j}\\in\\mathbb{R},\n\\] \\(j=1,\\dots,p\\), denote the \\(j\\)th component of \\(\\beta_0\\in\\mathbb{R}^p,\\) and let \\[\n\\hat{\\beta}_{j,n}\\in\\mathbb{R}\n\\] denote the \\(j\\)th component of the estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p.\\)\nThe basic bootstrap confidence interval for \\(\\beta_{0,j}\\in\\mathbb{R}\\) can be constructed as following:\n\nUse either bootstrapping pairs (Section 3.5.1) or bootstrapping residuals (Section 3.5.2) or the wild bootstrap (Section 3.5.3) to generate \\(m\\) (e.g. \\(m=10,000\\)) bootstrap realizations \\[\n\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast.\n\\]\nDetermine the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) from the bootstrap realizations \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast\\) using Equation 3.9.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval as in Equation 3.10: \\[\n\\left[2\\hat\\beta_{nj}-\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j},\n   2\\hat\\beta_{nj}-\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\right],\n\\] where\n\n\\(\\hat\\beta_{nj}\\) denotes the \\(j\\)th component of \\(\\hat\\beta_{n}\\) computed from the original sample \\(\\mathcal{S}_n,\\) and\n\\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) are the empirical quantiles computed from the bootstrap estimators \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast.\\)\n\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval for homoskedastic errors, but also for heteroskedastic errors. In case of heteroskedastic errors, one needs to use an appropriate boostrap such as the pairs boostrap (random design) or the wild bootstrap (fixed design).\nNote that standard confidence intervals usually provided by statistical software packages are for homoskedastic errors. For instance, the confint(object) function in R for an object returned by the lm() function uses the standard error formula for homoskedastic errors.\n\n\n\n\n\n3.5.4.2 Bootstrap-\\(\\boldsymbol{t}\\) Confidence Intervals for \\(\\boldsymbol{\\beta_{0,j}}\\)\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_{0,j}\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nConsider the statistic \\[\nT_n = \\frac{\\hat\\beta_{j,n} -\\beta_{0,j}}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})},\n\\] where\n\n\\(\\beta_{0,j}\\) denotes the \\(j\\)th element of \\(\\beta_0\\in\\mathbb{R}^p,\\)\n\\(\\hat\\beta_{j,n}\\) denotes the \\(j\\)th element of the OLS estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p.\\)\n\nIn the case of homoskedastic error terms \\[\n\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\n=\\frac{\\hat{\\sigma}_n\\sqrt{\\hat{\\gamma}_{jj,n}}}{\\sqrt{n}},\n\\tag{3.12}\\] where \\(\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\\) and where \\[\n\\hat{\\gamma}_{jj,n}\n=\\left[\\widehat{M}_n^{-1}\\right]_{jj}\n=\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^\\top\\right)^{-1}\\right]_{jj}\n\\] denotes the \\(j\\)-th diagonal element of the \\((p\\times p)\\)-dimensional matrix \\(\\widehat{M}_n^{-1}=(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}.\\)\nIn the case of heteroskedastic errors, one can use \\[\n\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\n=\\frac{\\sqrt{\\left[\\widehat{M}_n^{-1}\\widehat{Q}_n\\widehat{M}_n^{-1}\\right]_{jj}}}{\\sqrt{n}}\n\\tag{3.13}\\] where\n\n\\(\\widehat{M}_n^{-1}=(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\)\n\\(\\widehat{Q}_n=\\widehat{\\mathbb{E}}(\\varepsilon_i^2X_iX_i^\\top)\\) denotes a Heteroskedasticity Consistent (HC) estimators of \\(\\mathbb{E}(\\varepsilon_i^2X_iX_i^\\top);\\) e.g. the HC2-estimator \\(\\widehat{\\mathbb{E}}(\\varepsilon_i^2X_iX_i^\\top)=\\frac{1}{n-p}\\sum_{i=1}^n\\hat\\varepsilon_i^2X_iX_i^\\top.\\)\n\nNote that \\(T_n\\) is an asymptotically pivotal statistic; i.e., \\[\nT_n= \\frac{(\\hat\\beta_{n,j}-\\beta_{0,j})}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_{0,j}\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\n\nUse either bootstrapping pairs (Section 3.5.1) for random designs, or bootstrapping residuals (Section 3.5.2) for fixed designs and homoskedastic errors, or the wild bootstrap (Section 3.5.3) for fixed desgins and heteroskedastic errors, to generate \\(m\\) (e.g. \\(m=10,000\\)) bootstrap realizations \\[\nT^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*,\n\\] with \\[\nT^*_{n,k}=\\frac{\\hat\\beta_{n,j}^*-\\hat\\beta_{0,j}}{\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})},\\quad k=1,\\dots,m,\n\\] where\n\n\\(\\hat\\beta_{0,j}\\) is computed from the original sample\n\\(\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})\\) is an appropriate estimate of the standard error (e.g. Equation 3.12 or Equation 3.13)\n\nthe residual components in \\(\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})\\) are resampled\nthe \\(X\\)-components in \\(\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})\\) are resampled only in random designs, but kept fix in fixed desgins.\n\n\nCompute the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) (see Equation 3.9) from the bootstrap estimates \\(T^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*.\\)\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation 3.11: \\[\n\\left[\n  \\hat\\beta_{j,n}-\\hat q^\\ast_{1-\\frac{\\alpha}{2},n,j}\\;\\left(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\right),\\;\n  \\hat\\beta_{j,n}-\\hat q^\\ast_{\\frac{\\alpha}{2},n,j}\\;\\left(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\right)\n\\right],\n\\] where\n\n\\(\\hat\\beta_{n,j}\\) and \\(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\) are computed from the original sample \\(\\mathcal{S}_n=((Y_1,X_1),\\dots,(Y_n,X_n)),\\) and\n\\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap realizations \\(T^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*.\\)\n\n\n\n\n\n3.5.5 Statistical Hypothesis Testing\nIn the following, we consider a fixed design, where one can use the residual bootstrap (Section 3.5.2) or the wild bootstrap (Section 3.5.3).\nSuppose we want to test the hypothesis \\[\n\\begin{align*}\nH_0                    &: \\beta_{0,j}    = 0\\\\[2ex]\n\\text{against}\\quad H_1&: \\beta_{0,j} \\neq 0\n\\end{align*}\n\\] using the test statistic \\[\nT_n = \\frac{\\hat\\beta_{j,n} - 0}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})},\n\\] where\n\n\\(\\beta_{0,j}\\) denotes the \\(j\\)th element of \\(\\beta_0\\in\\mathbb{R}^p,\\)\n\\(\\hat\\beta_{j,n}\\) denotes the \\(j\\)th element of the OLS estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p.\\)\n\\(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\) is an appropriate estimator of the standard error (e.g. Equation 3.12 or Equation 3.13)\n\nThe \\(p\\)-value is defined as \\[\n\\begin{align*}\n&p_{obs} = \\\\[2ex]\n&2\\,\\min\\left\\{P(T_n \\geq T_{n,obs}|H_0\\;\\text{is true}),\\;P(T_n \\leq T_{n,obs}|H_0\\;\\text{is true})\\right\\}\n\\end{align*}\n\\] where \\(T_{n,obs}\\) is the value of the test statistic computed from the original sample \\[\n\\mathcal{S}_n=\\left\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\right\\}.\n\\]\nTo conduct the test using the bootstrap, we have to estimate \\(p_{obs}\\) using the bootstrap.\nCentral question: How to generate bootstrap samples under \\(\\boldsymbol{H_0}\\)?\nTo estimate \\(\\beta_0\\in\\mathbb{R}^p\\) under \\(H_0,\\) we need to estimate all elements in \\(\\beta_0\\) that are not specified/fixed by \\(H_0\\) leaving the other elements at their \\(H_0\\)-values.\nLet \\(\\beta^{H_0}_0\\in\\mathbb{R}^{(p-1)}\\) denote the parameter vector that contains all elements of \\(\\beta_0\\in\\mathbb{R}^p\\) that are not specified by \\(H_0.\\)  The estimator of \\(\\beta^{H_0}_0\\in\\mathbb{R}^{(p-1)}\\) is then given by \\[\n\\underset{((p-1)\\times 1)}{\\hat{\\beta}^{H_0}_{n}}=\\left(\\tilde{X}^\\top\\tilde{X}\\right)^{-1}\\tilde{X}^\\top Y\n\\] where the \\((n\\times (p-1))\\)-matrix \\(\\tilde{X}\\) is the matrix \\(X\\) with the \\(j\\)th column removed.\nUsing \\(\\hat{\\beta}^{H_0}_{n},\\) we can compute the \\((n\\times 1)\\)-vector of residuals under \\(\\boldsymbol{H_0}\\) as \\[\n\\left(\\begin{matrix}\\hat{\\varepsilon}^{H_0}_1\\\\ \\vdots\\\\\\hat{\\varepsilon}^{H_0}_n\\end{matrix}\\right)=\\hat{\\varepsilon}^{H_0}=Y-\\tilde{X}\\hat{\\beta}^{H_0}_{n}.\n\\]\nBootstrap algorithm:\nStep 1 Residual Bootstrap Option: Draw independently and with replacement \\(n\\) values from \\[\n\\{\\hat{\\varepsilon}^{H_0}_1, \\dots, \\hat{\\varepsilon}^{H_0}_n\\}\n\\] to generate bootstrap realizations under \\(H_0\\) \\[\n\\{\\hat{\\varepsilon}^{H_0\\ast}_1, \\dots, \\hat{\\varepsilon}^{H_0\\ast}_n\\}.\n\\] These allow us to generate the bootstrap samples under \\(H_0,\\) \\[\n\\left\\{(Y_1^{H_0\\ast},X_1),\\dots,(Y_n^{H_0\\ast},X_n)\\right\\},\n\\] where \\[\nY_i^{H_0\\ast} = \\tilde{X}_i^\\top \\hat{\\beta}^{H_0}_{n} + \\hat{\\varepsilon}^{H_0\\ast}_i,\\quad i=1,\\dots,n.\n\\]\nStep 1 Wild Bootstrap Option: Use\n\\[\n\\{\\hat{\\varepsilon}^{H_0}_1, \\dots, \\hat{\\varepsilon}^{H_0}_n\\}\n\\] to generate wild bootstrap errors under \\(H_0\\) \\[\n\\varepsilon^{H_0\\ast}_i=\\left\\{\n  \\begin{array}{ll}\n  (1-\\sqrt{5})\\hat{\\varepsilon}^{H_0}_i&\\text{with propability }(1+\\sqrt{5})/2\\sqrt{5}\\\\\n  (1+\\sqrt{5})\\hat{\\varepsilon}^{H_0}_i/2&\\text{with propability }1-(1+\\sqrt{5})/2\\sqrt{5}\n  \\end{array}\n\\right.\n\\] These allow us to generate the bootstrap samples under \\(H_0,\\) \\[\n\\left\\{(Y_1^{H_0\\ast},X_1),\\dots,(Y_n^{H_0\\ast},X_n)\\right\\},\n\\] where \\[\nY_i^{H_0\\ast} = \\tilde{X}_i^\\top \\hat{\\beta}^{H_0}_{n} + \\varepsilon^{H_0\\ast}_i,\\quad i=1,\\dots,n.\n\\]\nStep 2. Based on the bootstrap sample \\[\n\\left\\{(Y_1^{H_0\\ast},X_1),\\dots,(Y_n^{H_0\\ast},X_n)\\right\\}\n\\] we can compute the bootstrap realization of the OLS estimator under \\(H_0,\\) \\[\n\\hat{\\beta}^{\\ast}_{n}=\\left(X^\\top X\\right)^{-1} X^\\top Y^{H_0\\ast},\n\\] which allows us to generate the corresponding realization of the test statistic \\[\nT_n^{\\ast}\n\\] under \\(\\boldsymbol{H_0}\\).\nStep 3. Repeating Steps 1-2 leads to \\(m\\)-many (e.g. \\(m=10,000\\)) bootstrap realizations of the test statistic\n\\[\nT_{n,1}^{\\ast},\\dots,T_{n,m}^{\\ast}\n\\] under \\(\\boldsymbol{H_0}\\).\nTo estimate the unknown \\(p_{obs},\\) we can use now the following estimator \\[\n\\begin{align*}\n&\\hat p_{obs} = \\\\[2ex]\n&=2\\,\\min\\left\\{\\hat{P}(T_n \\geq T_{n,obs}|H_0\\;\\text{is true}),\\;\\hat{P}(T_n \\leq T_{n,obs}|H_0\\;\\text{is true})\\right\\}\\\\[2ex]\n&=2\\,\\min\\left\\{\n  \\frac{1}{m}\\sum_{j=1}^m 1_{\\left(T_{n,j}^{\\ast} \\geq T_{n,obs}\\right)},\\;\n  \\frac{1}{m}\\sum_{j=1}^m 1_{\\left(T_{n,j}^{\\ast} \\leq T_{n,obs}\\right)}\n  \\right\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIn case of heteroskedasticity, the wild bootstrap and a corresponding formula for \\(\\widehat{\\operatorname{SE}}(\\hat{\\beta}_j)\\) has to be used.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html",
    "href": "Ch5_FunctionalDataAnalysis.html",
    "title": "5  Functional Data Analysis",
    "section": "",
    "text": "Literature:\nR-packages used in this chapter:"
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html#from-raw-data-to-functional-data",
    "href": "Ch5_FunctionalDataAnalysis.html#from-raw-data-to-functional-data",
    "title": "5  Functional Data Analysis",
    "section": "5.1 From Raw Data to Functional Data",
    "text": "5.1 From Raw Data to Functional Data\nThe simplest raw data set encountered in functional data analysis is a sample of the form: \\[\n\\begin{align}\nX_i(t_j),\\quad t_j\\in[a,b],\\quad i=1,\\dots,n \\quad j=1,\\dots,J.\n\\end{align}\n\\] The theoretical objects we wish to study are smooth curves: \\[\n\\begin{align}\nX_i(t),\\quad t\\in[a,b],\\quad i=1,\\dots,n \\quad j=1,\\dots,J.\\\\[4ex]\n\\end{align}\n\\]\n\npar(mfrow=c(1,2), mar=c(5.1, 4.1, 2.1, 2.1))\nmatplot(x=growth$age, y=growth$hgtf[,1:5], type=\"p\", lty=1, pch=21, \n        xlab=\"Age\", ylab=\"Height (cm)\", cex.lab=1.2,col=1:5,bg=1:5,\n        main=expression(X[i](t[j])))\nmatplot(x=growth$age, y=growth$hgtf[,1:5], type=\"l\", lty=1, pch=1, \n        xlab=\"Age\", ylab=\"Height (cm)\", cex.lab=1.2,\n        main=expression(X[i](t)))\npar(mfrow=c(1,1), mar=c(5.1, 4.1, 4.1, 2.1))\n\n\n\n\nFigure 5.5: Actually observed data points \\(X_i(t_j)\\) versus the theoretical objects \\(X(t).\\)\n\n\n\n\nPre-Processing the Data using Basis Expansions:\nTypically, the first step in working with functional data is to express the functional data by means of a basis expansion \\[\nX_i(t)\\approx\\sum_{m=1}^M c_{im} B_m(t),\\quad t\\in[a,b],\n\\] where \\(B_1(t),\\dots,B_M(t)\\) are some standard collection of basis functions like:\n\nsplines\nwavelets\nsine and cosine functions\netc.\n\nExample: B-spline basis functions \\(B_1(t),\\dots,B_M(t)\\):\n\nbspl_bf <- fda::create.bspline.basis(rangeval = c(0,1), \n                                     norder   = 3, \n                                     breaks   = seq(0,1,len=7))\nplot(bspl_bf, \n     main = \"B-spline Basis Functions\", \n     xlab = \"[a,b]=[0,1]\")\n\n\n\n\n\n\n\n\nExample: Fourier basis functions \\(B_1(t),\\dots,B_M(t)\\):\n\nfourier_bf <- fda::create.fourier.basis(rangeval = c(0,1), \n                                        nbasis   = 5)\nplot(fourier_bf, \n     main = \"Fourier Basis Functions\", \n     xlab = \"[a,b]=[0,1]\")\n\n\n\n\n\n\n\n\nExample: Pre-processing the data of the Berkeley growth study\n\nSmObj <- fda::smooth.basisPar(growth$age, \n                              y = growth$hgtf[,1:5],lam=0.1)\n\nresult <- plot(SmObj$fd, \n     xlab = \"Age (years)\", \n     ylab = \"Height (cm)\", cex.lab=1.2,\n     main = \"5 Girls in Berkeley Growth Study\", lty=1)\n\n\n\n\n\n\n\n\nExample: 1st derivative of the functional data from the Berkeley growth study\n\nresult <- plot(fda::deriv.fd(SmObj$fd, 1), \n     xlab = \"Age (years)\", \n     ylab = \"Growth Rate (cm / year)\", cex.lab=1.2,\n     main = \"5 Girls in Berkeley Growth Study (1st Derivative)\", lty=1)\n\n\n\n\n\n\n\n\nExample: 2nd derivative of the functional data from the Berkeley growth study\n\nresult <- plot(fda::deriv.fd(SmObj$fd, 2), \n     xlab = \"Age (years)\", \n     ylab = \"Growth Rate (cm / year)\", cex.lab=1.2,\n     main = \"5 Girls in Berkeley Growth Study (1st Derivative)\", lty=1)"
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html#sample-mean-and-covariance",
    "href": "Ch5_FunctionalDataAnalysis.html#sample-mean-and-covariance",
    "title": "5  Functional Data Analysis",
    "section": "5.2 Sample Mean and Covariance",
    "text": "5.2 Sample Mean and Covariance\nPointwise mean: \\[\n\\bar{X}_n(t)=\\frac{1}{n}\\sum_{i=1}^n X_i(t)\n\\]\nPointwise standard deviation: \\[\nS_n(t)=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\Big(X_i(t)-\\bar{X}_n(t)\\Big)^2}\n\\]\nPointwise covariance function: \\[\n\\hat{c}_n(t,s)=\\frac{1}{n-1}\\sum_{i=1}^n\\Big(X_i(t)-\\bar{X}_n(t)\\Big)\\Big(X_i(s)-\\bar{X}_n(s)\\Big)\n\\]\nThe sample covariance function is extensively used in Functional Data Analysis. The interpretation of the values of \\(\\hat{c}_n(t,s)\\) is the same as for the usual variance-covariance matrix.\nExample Data: Brownian motion trajectories\n\nset.seed(109)\n\nn       <- 50   # sample size\nJ       <- 500  # number of evaluation points\n\nBM.mat  <- matrix(0, ncol=n, nrow=J)\n\nfor(i in 1:n){\n     BM.mat[,i] <- cumsum(rnorm(J, sd = 1/100))\n}\n\nComputing and plotting the pointwise mean and standard deviations in R:\n\nBM.Mean <- rowMeans(BM.mat)\nBM.SD   <- apply(BM.mat,1,sd)\n\n\nxx      <- seq(0,1,len=J) \n\npar(mfrow=c(1,2))\nmatplot(x    = xx, \n        y    = BM.mat, \n        xlab = \"\", ylab = \"\", \n        type =\"l\", \n        col  = gray(.7), lty=1, \n        main = \"Brownian Motions\")\nlines(x = xx, \n      y = BM.Mean)\nlines(x = xx, \n      y = BM.SD, lty=2)\nlegend(\"topleft\", lty = c(1,2), \n       legend = c(\"Sample Mean\",\"Sample SD\"))\n##\nmatplot(x    = xx, \n        y    = BM.mat, \n        xlab = \"\", ylab = \"\", \n        type = \"l\", \n        col  = gray(.7), lty=1, \n        main = \"Brownian Motion\")\nlines(x = c(0,1), \n      y = c(0,0), lty=1)\nlines(x = c(0,1), \n      y = c(0,sqrt(J*(1/100)^2)), lty=2)\nlegend(\"topleft\", lty = c(1,2), \n       legend = c(\"True Mean\",\"True SD\"))\n\n\n\n\n\n\n\n\nComputing and plotting the pointwise covariance function in R:\n\nBM.cov <- var(t(BM.mat))\n\nslct   <- c(seq.int(1,500,by=20),500)\n\npar(mfrow=c(1,2), mar=c(1, 1.1, 1.2, 1.1))\npersp(x = xx[slct], \n      y = xx[slct], \n      z = BM.cov[slct,slct], \n      xlab = \"s\", ylab = \"t\", zlab = \"\", \n      main = \"Sample Covariance Function\",\n      theta = -40, phi = 20, expand = .75, col = \"lightblue\", shade = 1.05)\n\nx <- seq(0, 1, length= 30)\ny <- x\nf <- function(x, y){min(x,y)}\nf <- Vectorize(f)\nz <- outer(x, y, f)\n\npersp(x = x, y = y, z = z, xlab=\"s\", ylab=\"t\", zlab=\"\",\n      main  = \"True Covariance Function\",\n      theta = -40, phi = 20, expand = .75, col = \"lightblue\", shade = 1.05)"
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html#functional-principal-component-analysis",
    "href": "Ch5_FunctionalDataAnalysis.html#functional-principal-component-analysis",
    "title": "5  Functional Data Analysis",
    "section": "5.3 Functional Principal Component Analysis",
    "text": "5.3 Functional Principal Component Analysis\nPrincipal Component Analysis (PCA) is effectively an eigendecomposition of the empirical covariance function \\(\\hat{c}_n(t,s)\\) \\[\n\\begin{align*}\n\\hat{c}_n(t,s)\n&=\\sum_{j=1}^n\\hat{\\lambda}_j\\hat{v}_j(t)\\hat{v}_j(s),\\\\[2ex]\n&\\approx\\sum_{j=1}^{p<n}\\hat{\\lambda}_j\\hat{v}_j(t)\\hat{v}_j(s),\n\\end{align*}\n\\] where \\[\n\\hat{\\lambda}_1>\\hat{\\lambda}_2>\\dots \\geq 0\n\\] denote the estimated eigenvalues and where \\[\n\\hat{v}_1(t),\\dots,\\hat{v}_p(t)\n\\] denote the estimated principal component (or eigen) functions.\nThe estimated eigenfunctions are orthonormal, i.e.  \\[\n\\int_a^b\\hat{v}_j(t)\\hat{v}_k(t)dt=\n\\begin{cases}\n1, & j=k \\\\\n0, &j\\neq k.\n\\end{cases}\n\\]\nIn FDA, functional PCA is very prominent since the Estimated Functional Principal Components (EFPC’s) \\(\\hat{v}_j\\) are very well suited as basis functions for \\(X_i\\): \\[\n\\begin{align*}\nX_i(t)\n&=\\bar{X}_n(t) + \\sum_{j=1}^n\\hat{\\xi}_{ij}\\hat{v}_j(t)\\\\[2ex]\n&\\approx\\bar{X}_n(t) + \\sum_{j=1}^{p<n}\\hat{\\xi}_{ij}\\hat{v}_j(t),\n\\end{align*}\n\\] where \\(\\hat{\\xi}_{ij}\\) denote the estimated scores \\[\n\\hat{\\xi}_{ij}=\\int_a^b (X_i(t)-\\bar{X}_n(t))\\hat{v}_j(t)dt.\n\\]\nNote that \\[\n\\frac{1}{n}\\sum_{i=1}^n\\hat{\\xi}_{ij} = 0,\\quad\\text{for all }j=1,2,\\dots\n\\] and that \\[\n\\frac{1}{n}\\sum_{i=1}^n\\hat{\\xi}_{ij}^2 = \\hat{\\lambda}_j,\\quad\\text{for all }j=1,2,\\dots\n\\]\nExample: Computing the Functional PCA in R:\n\nBSPL.basis <- create.bspline.basis(rangeval=c(0,1), nbasis=200)\nBM.fd      <- smooth.basis(argvals=xx, y=BM.mat, fdParobj=BSPL.basis)\nBM.pca     <- pca.fd(BM.fd$fd, nharm=3)\npar(mfrow=c(1,3), mar=c(2.1, 1.1, 4.1, 1.1))\npersp(xx[slct], xx[slct], BM.cov[slct,slct], xlab=\"s\", ylab=\"t\", zlab=\"\",\n      main=\"Sample Covariance Function\", theta = -40, phi = 20, expand = .75, col = \"blue\", shade = 1.05)\ninvisible(plot(BM.pca$values[1:3], type=\"o\", ylab=\"\", main=\"Estimated Eigenvalues (p=3)\"))\ninvisible(plot(BM.pca$harmonics, lwd=3, ylab=\"\", main=\"Estimated FPC's (p=3)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Quantities\n\n\n\n\\[\n\\begin{align*}\nCov(X(t),X(s)) = c(t,s)\n&=\\sum_{j=1}^n \\lambda_j v_j(t) v_j(s),\\\\[2ex]\n&\\approx\\sum_{j=1}^{p<n} \\lambda_j v_j(t) v_j(s),\n\\end{align*}\n\\] where \\[\n\\lambda_1 > \\lambda_2>\\dots \\geq 0\n\\] denote the true eigenvalues and where \\[\nv_1(t),\\dots,v_p(t)\n\\] denote the true principal component (or eigen) functions.\nThe eigenfunctions are orthonormal, i.e.  \\[\n\\int_a^b v_j(t)v_k(t)dt=\n\\begin{cases}\n1, & j=k \\\\\n0, &j\\neq k.\n\\end{cases}\n\\]\nEigenfunctions as basis functions \\[\n\\begin{align*}\nX_i(t)\n&=\\mathbb{E}(X(t)) + \\sum_{j=1}^n \\xi_{ij}v_j(t)\\\\[2ex]\n&\\approx\\mathbb{E}(X(t)) + \\sum_{j=1}^{p<n}\\xi_{ij}v_j(t),\n\\end{align*}\n\\] where \\(\\xi_{ij}\\) denote the true scores \\[\n\\xi_{ij}=\\int_a^b (X_i(t)-\\mathbb{E}(X(t)) )v_j(t)dt.\n\\]\nNote that \\[\n\\mathbb{E}(\\xi_{ij}) = 0,\\quad\\text{for all }j=1,2,\\dots\n\\] and that \\[\nVar(\\xi_{ij}) = \\lambda_j,\\quad\\text{for all }j=1,2,\\dots.\n\\]\n\n\nBest basis property: The EFPC’s are the best basis; i.e. they have the smallest approximation error (in the mean square sense).\n\\[\n\\begin{align}\nX_i(t)&\\approx\\bar{X}_n(t) + \\sum_{j=1}^p\\hat{\\xi}_{ij}\\hat{v}_j(t)\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFerraty, Frédéric, and Philippe Vieu. 2006. Nonparametric Functional Data Analysis: Theory and Practice. Springer.\n\n\nHsing, Tailen, and Randall Eubank. 2015. Theoretical Foundations of Functional Data Analysis, with an Introduction to Linear Operators. John Wiley & Sons.\n\n\nKokoszka, Piotr, and Matthew Reimherr. 2017. Introduction to Functional Data Analysis. Chapman; Hall/CRC.\n\n\nRamsay, J. O., and B. W. Silverman. 2005. Functional Data Analysis. 2. ed. Springer."
  },
  {
    "objectID": "Ch3_Bootstrap.html#solutions",
    "href": "Ch3_Bootstrap.html#solutions",
    "title": "3  The Bootstrap",
    "section": "Solutions",
    "text": "Solutions\n\nSolution of Exercise 1.\n\n(a)\nThe exact point-wise distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\n\\[\n\\begin{align*}\nF_n(x)\n& = \\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\\\\n\\Rightarrow nF_n(x)\n& = \\sum_{i=1}^n 1_{(X_i\\leq x)} \\sim \\mathcal{Binom}\\left(n,p=F(x)\\right),\n\\end{align*}\n\\] since \\(1_{(X_i\\leq x)}\\) is a Bernoulli random variable with parameter \\[\n\\begin{align*}\np\n& = P(1_{(X_i\\leq x)} = 1)\\\\[2ex]\n& = P(X_i \\leq x)\\\\[2ex]\n& = F(x).\n\\end{align*}\n\\] Note that this holds for any distribution of \\(X_i.\\) Therefore, one says that \\(nF_n(x)\\) is distribution free.\n\n\n(b)\nFrom (a), we have that (using the standard mean and variance expressions for Binomial distributed random variables): \\[\n\\begin{align*}\n\\mathbb{E}(nF_n(x))\n&= n p\\\\[2ex]\n&= nF(x)\\\\[2ex]\n\\Leftrightarrow\\quad  \\mathbb{E}(F_n(x))\n&= p \\\\[2ex]\n&= F(x)\n\\end{align*}\n\\] and that \\[\n\\begin{align*}\nVar(nF_n(x))\n&= n p (1-p)\\\\[2ex]\n&= nF(x)(1-F(x))\\\\[2ex]\n\\Leftrightarrow \\quad Var(F_n(x))\n&= \\frac{p (1- p)}{n}\\\\[2ex]\n&= \\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\]\nMoreover, since \\(F_n(x)  = \\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\) is an average over i.i.d. random variables \\[\n1_{(X_1\\leq x)},\\dots,1_{(X_n\\leq x)},\n\\] the standard CLT (Lindeberg-Lévy) implies that \\[\n\\frac{F_n(x)-F(x)}{\\sqrt{\\frac{F(x)(1-F(x))}{n}}}\\to_d\\mathcal{N}(0,1)\n\\] as \\(n\\to\\infty.\\) Or equivalently, with a slight abuse of notation: \\[\nF_n(x)\\overset{a}{\\sim}\\mathcal{N}\\left(F(x),\\frac{F(x)(1-F(x))}{n}\\right).\n\\]\n\n\n(c)\nThe mean squared error between \\(F_n(x)\\) and \\(F(x)\\) is given by \\[\n\\begin{align*}\n\\operatorname{MSE}(F_n(x))\n&= \\mathbb{E}\\left((F_n(x)-F(x))^2\\right)\\\\[2ex]\n&= Var(F_n(x)) + \\left(\\mathbb{E}(F_n(x))-F(x)\\right)^2.\n\\end{align*}\n\\] It follows from our previous results that for each \\(x\\in\\mathbb{R}\\) \\[\nVar(F_n(x)) = \\frac{F(x)(1-F(x))}{n} \\to 0\n\\] as \\(n\\to\\infty,\\) and that \\[\n\\mathbb{E}(F_n(x)) -F(x) = 0\n\\] for all \\(n.\\) Therefore, \\[\n\\operatorname{MSE}(F_n(x)) = Var(F_n(x)) \\to 0\n\\] as \\(n\\to\\infty.\\)\nThus we can conclude that \\(F_n(x)\\) converges in the mean-square sense to \\(F(x)\\) for each \\(x\\in\\mathbb{R},\\) \\[\nF_n(x)\\to_{ms} F(x)\n\\] as \\(n\\to\\infty.\\)\nSince convergence in the mean square sense implies convergence in probability, we also have that for each \\(x\\in\\mathbb{R}\\) \\[\nF_n(x)\\to_{p} F(x)\n\\] as \\(n\\to\\infty,\\) which shows that \\(F_n(x)\\) is weakly consistent for \\(F(x)\\) for each \\(x\\in\\mathbb{R}.\\)\n\n\n\nSolution of Exercise 2.\n\n\n\n\n\n\nTip\n\n\n\nAnother, equivalent way to define uniform convergence:\n\\(g_n(\\cdot)\\) converges uniformly to \\(g(\\cdot)\\) if for every \\(\\varepsilon&gt;0,\\) there exists an \\(N_\\varepsilon\\) such that \\[\\begin{align*}\n&\\sup_{x\\in\\mathcal{X}}|g_n(x) - g(x)| &lt; \\varepsilon\\quad \\text{for all}\\quad n\\geq N_\\varepsilon\\\\[2ex]\n\\Leftrightarrow\\qquad & g_n(x)\\in[g(x)-\\varepsilon,g(x)+\\varepsilon]\\quad\\text{for all}\\quad x\\in\\mathcal{X}\\quad\\text{and all}\\quad n\\geq N_\\varepsilon.\n\\end{align*}\\]\nI.e., \\(g_n(\\cdot)\\) converges uniformly to \\(g(\\cdot)\\) if it is possible to draw an \\(\\varepsilon\\)-band around the graph of \\(g(x)\\) that contains all of the graphs of \\(g_n(x)\\) for large enough \\(n\\geq N_\\varepsilon.\\)\n\n\nExample 1: \\(\\mathcal{X}=\\mathbb{R}\\) The function \\[\ng_n(x) = x\\left(1+\\frac{1}{n}\\right)\n\\] converges point-wise (for each given \\(x\\in\\mathbb{R}\\)) to \\[\ng(x)=x,\n\\] since \\[\n|g_n(x)-g(x)|=\\frac{|x|}{n}\\to 0\\quad \\text{as}\\quad n\\to\\infty.\n\\] for each given \\(x\\in\\mathcal{X}.\\)\nHowever, \\(g_n\\) does not converge uniformly to \\(g\\) since \\[\n\\sup_{x\\in\\mathbb{R}}|g_n(x)-g(x)|=\\sup_{x\\in\\mathbb{R}}\\frac{|x|}{n}=\\infty\\neq 0\n\\] for each \\(n.\\)\nExample 2: \\(\\mathcal{X}=(0,1)\\) The function \\[\ng_n(x) = x^n\n\\] converges point-wise (for each given \\(x\\in(0,1)\\)) to \\[\ng(x)=0,\n\\] since \\[\n|g_n(x)-g(x)|=x^n\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\] for each given \\(x\\in(0,1).\\)\nHowever, \\(g_n\\) does not converge uniformly to \\(g\\) since \\[\n\\sup_{x\\in(0,1)}|g_n(x)-g(x)|=\\sup_{x\\in(0,1)}x^n=1\\neq 0\n\\] for each \\(n.\\)\n\n\nSolution of Exercise 3.\n\n(a) Part 1:\nSetup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(Var(X_i)=\\sigma^2&lt;\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\nIf \\(F\\) is a normal distribution:\n\\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\sim \\mathcal{N}(0,1)\\quad\\text{exactly for all}\\;n.\n\\end{array}\n\\]\nFor non-normal distributions \\(F\\) we have by the classic CLT: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nUsually, we do not know \\(\\sigma\\) and have to estimate this parameter using a consistent estimator such as \\(s^2=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), where \\(s\\to_p\\sigma\\) as \\(n\\to\\infty\\).\nThen by Slusky’s Theorem (allows to combine \\(\\to_d\\) and \\(\\to_p\\)-statements) we have that: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{s}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nThe classic confidence interval is then based on the above (asymptotic) normality result: \\[\n\\operatorname{CI}_{\\operatorname{classic},n}=\\left[\\bar{X}_n\\,-\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}},\\bar{X}_n\\,+\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}\\right],\n\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of the standard normal distribution. Alternatively, one can apply a “small-sample correction” by using the \\((1-\\alpha/2)\\)-quantile \\(t_{n-1, 1-\\alpha/2}\\) of the \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nFrom the above arguments it follows that: \\[\nP\\left(\\mu\\in \\operatorname{CI}_{\\operatorname{classic},n}\\right)\\to 1-\\alpha\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nLet us consider the finite-\\(n\\) (with \\(n=20\\)) performance of the classic confidence interval for the case where \\(F\\) is a normal distribution with mean \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\):\n\n##  Setup:\nn     &lt;-   20 # Sample Size\nmean  &lt;-    1 # Mean\nsdev  &lt;-    2 # Standard Deviation\nalpha &lt;- 0.05 # Level\n\nset.seed(123)\nB          &lt;- 1500 # MC repetitions\nCI.lo.vec  &lt;- rep(NA, B)\nCI.up.vec  &lt;- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     &lt;- rnorm(n=n, mean = mean, sd = sdev) \n  ## Estimates:\n  X.bar.MC     &lt;- mean(X.sample)\n  sd.hat.MC    &lt;- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] &lt;- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] &lt;- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n\n  ## Possible alternative version: \n  #CI.lo.vec[b] &lt;- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] &lt;- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      &lt;- CI.lo.vec &lt;= mean  &  mean &lt;= CI.up.vec\nfreq.non.cover &lt;- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\n\n\n\n(a) Part 2: Classic Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nNow, we consider the finite-\\(n\\) performance of the classic confidence interval under the same setup as above, but for the case where \\(F\\) is a non-normal distribution, namely, a \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom:\n\n## Setup:\nn     &lt;-   20  # Sample Size\ndf    &lt;-    1  # (=&gt; mean==1)\nalpha &lt;- 0.05 # Level\n\nset.seed(123)\nB          &lt;- 1500 # MC repetitions\nCI.lo.vec  &lt;- rep(NA, B)\nCI.up.vec  &lt;- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     &lt;- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC     &lt;- mean(X.sample)\n  sd.hat.MC    &lt;- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] &lt;- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] &lt;- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  \n  ## Possible alternative version: \n  #CI.lo.vec[b] &lt;- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] &lt;- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      &lt;- CI.lo.vec &lt;= mean  &  mean &lt;= CI.up.vec\nfreq.non.cover &lt;- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\n\n\n\n(b) Basic Bootstrap Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nLet’s generate an iid random sample \\(S_n\\) with \\(X_i\\sim\\chi^2_1\\) and the corresponding estimate \\(\\bar X_n\\):\n\n## Setup:\nn     &lt;-   20  # Sample Size\ndf    &lt;-    1  # (=&gt; mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  &lt;- rchisq(n, df=df)\n\n## Empirical mean:\n(X.bar &lt;- mean(S_n))\n\n[1] 0.6737282\n\n\nThe standard bootstrap confidence interval is given by (see lecture script): \\[\n\\left[2\\bar{X}_n - \\hat{q}^\\ast_{n,1-\\alpha/2}, 2\\bar{X}_n - \\hat{q}^\\ast_{n,\\alpha/2}\\right],\n\\] where \\(\\bar{X}_n\\) denotes the estimate computed from the original sample, and \\(\\hat{q}^\\ast_{\\alpha/2}\\) and \\(\\hat{q}^\\ast_{1-\\alpha/2}\\) denote the \\((\\alpha/2)\\) and \\((1-\\alpha/2)\\)-quantiles of the conditional distribution of \\(\\bar{X}_n^\\ast\\) given \\(\\mathcal{S}_n=\\left\\{X_1,\\dots,X_n\\right\\}.\\)\nIn the following we first generate the \\(m\\) bootstrap realizations \\[\n\\bar{X}_{n,1}^\\ast,\\dots,\\bar{X}_{n,m}^\\ast,\n\\] compute their quantiles \\(\\hat{q}^\\ast_{n,\\alpha/2}\\) and \\(\\hat{q}^\\ast_{n,1-\\alpha/2},\\) and plot all of this:\n\n## Bootstr-Setup:\nalpha            &lt;- 0.05\nn.Bootsrap.draws &lt;- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  &lt;- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] &lt;- sample(x=S_n, size=n, replace = TRUE)\n}\n## Boostrap draws of \\bar{X}_n^*:\nX.bar.bootstr.vec &lt;- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\n\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\nq.1 &lt;- quantile(X.bar.bootstr.vec, probs = 1-alpha/2)\nq.2 &lt;- quantile(X.bar.bootstr.vec, probs = alpha/2)\n## plot\nplot(ecdf(X.bar.bootstr.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-Distr. of \",bar(X)[n]^{\" *\"})))\nabline(v=c(q.1,q.2),col=\"red\")\n\n\n\n\n\n\n\n\nUsing our preparatory work above, the basic bootstrap confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Basic.Bootstr.lo &lt;- 2*X.bar - q.1\nCI.Basic.Bootstr.up &lt;- 2*X.bar - q.2\n\n## Re-labeling of otherwise false names:\nattr(CI.Basic.Bootstr.lo, \"names\") &lt;- c(\"2.5%\")\nattr(CI.Basic.Bootstr.up, \"names\") &lt;- c(\"97.5%\")\n##\nc(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)\n\n     2.5%     97.5% \n0.1545224 1.0425228 \n\n\nNow, we can investigate the finite-\\(n\\) performance of the standard bootstrap confidence interval:\n\n## Setup:\nn     &lt;-   20  # Sample Size\ndf    &lt;-    1  # (=&gt; mean==1)\nmean  &lt;-   df\nalpha &lt;- 0.05 # Level\nn.Bootsrap.draws &lt;- 1500\n\n## MC-Setup:\nset.seed(123)\nB          &lt;- 1500 # MC repetitions\nCI.Basic.Bstr.lo.vec &lt;- rep(NA, B)\nCI.Basic.Bstr.up.vec &lt;- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        &lt;- rchisq(n, df=df)\n  ## Estimate:\n  X.bar.MC      &lt;- mean(S_n.MC)\n  ## \n  Bootstr.Samples.MC  &lt;- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] &lt;- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec &lt;- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  ## (1-alpha/2)-quantile:\n  q.1.MC &lt;- quantile(X.bar.bootstr.MC.vec, probs = 1-alpha/2)\n  q.2.MC &lt;- quantile(X.bar.bootstr.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Basic.Bstr.lo.vec[b] &lt;- 2*X.bar.MC - q.1.MC\n  CI.Basic.Bstr.up.vec[b] &lt;- 2*X.bar.MC - q.2.MC\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      &lt;- CI.Basic.Bstr.lo.vec&lt;=mean & mean&lt;=CI.Basic.Bstr.up.vec\nfreq.non.cover &lt;- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Basic Bootrap 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==TRUE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==FALSE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\n\n\n\n(c) Bootstrap-\\(t\\) Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nThe bootstrap-\\(t\\) confidence interval is given by (see lecture script): \\[\n\\left[\n  \\bar{X}_n-\\hat{q}^\\ast_{n,1-\\alpha/2}\\left(\\frac{s_n}{\\sqrt{n}}\\right),  \n  \\bar{X}_n-\\hat{q}^\\ast_{n,\\alpha/2}  \\left(\\frac{s_n}{\\sqrt{n}}\\right)\n\\right],\n\\] where \\(\\bar{X}_n\\) and \\(s_n=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) are computed from the original sample, and where \\(\\hat{q}^\\ast_{n,\\alpha/2}\\) and \\(\\hat{q}^\\ast_{n,1-\\alpha/2}\\) denote the empirical \\((\\alpha/2)\\) and the \\((1-\\alpha/2)\\)-quantiles compute from the bootstrap estimates: \\[\n\\sqrt{n}\\frac{\\bar{X}_{n,j}^\\ast-\\bar{X}_n}{s_{n,j}^\\ast}\\quad j=1,\\dots,m.\n\\]\nIn the following we first generate the \\(m\\) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar{X}_{n,j}^\\ast-\\bar{X}_n}{s_{n,j}^\\ast}\\quad j=1,\\dots,m,\n\\] compute their quantiles \\(\\hat{q}^\\ast_{n,\\alpha/2}\\) and \\(\\hat{q}^\\ast_{n,1-\\alpha/2}\\), and plot all of this:\n\n## Setup:\nn     &lt;-   20  # Sample Size\ndf    &lt;-    1  # (=&gt; mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  &lt;- rchisq(n, df=df)\n\n## Empirical mean and sd:\nX.bar   &lt;- mean(S_n)\nsd.hat  &lt;- sd(S_n)\n\n## Bootstr-Setup:\nalpha            &lt;- 0.05\nn.Bootsrap.draws &lt;- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  &lt;- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] &lt;- sample(x=S_n, size=n, replace = TRUE)\n}\n## Compute boostrap draws of (\\bar{X}_n^*-\\bar{X}_n)/\\hat{\\sigma}^\\ast:\nX.bar.bootstr.vec    &lt;- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\nsd.bootstr.vec       &lt;- apply(X = Bootstr.Samples, MARGIN = 2, FUN = sd)\n##\nBootstr.t.sample.vec &lt;- sqrt(n)*(X.bar.bootstr.vec - X.bar)/sd.bootstr.vec\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\nq.1 &lt;- quantile(Bootstr.t.sample.vec, probs = 1-alpha/2)\nq.2 &lt;- quantile(Bootstr.t.sample.vec, probs = alpha/2)\n## plot\nplot(ecdf(Bootstr.t.sample.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-t-Distr. of \",\n          sqrt(n)(bar(X)[n]^{\" *\"}-bar(X)[n])/s[n]^{\"*\"})))\nabline(v=c(q.1,q.2),col=\"red\")\n\n\n\n\n\n\n\n\nUsing our preparatory work above, the bootstrap-\\(t\\) confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Bstr.t.lo &lt;- X.bar - q.1 * sd.hat/sqrt(n)\nCI.Bstr.t.up &lt;- X.bar - q.2 * sd.hat/sqrt(n)\n\n## Re-labeling of otherwise false names:\nattr(CI.Bstr.t.lo, \"names\") &lt;- c(\"2.5%\")\nattr(CI.Bstr.t.up, \"names\") &lt;- c(\"97.5%\")\n##\nc(CI.Bstr.t.lo, CI.Bstr.t.up)\n\n     2.5%     97.5% \n0.3052027 2.0241321 \n\n\nLet us investigate the finite-\\(n\\) performance of the bootstrap-t confidence interval:\n\n## Setup:\nn     &lt;-   20  # Sample Size\ndf    &lt;-    1  # (=&gt; mean==1)\nmean  &lt;-   df\nalpha &lt;- 0.05 # Level\nn.Bootsrap.draws &lt;- 1500\n\n## MC-Setup:\nset.seed(123)\nB          &lt;- 1500 # MC repetitions\nCI.Bstr.t.lo.vec &lt;- rep(NA, B)\nCI.Bstr.t.up.vec &lt;- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        &lt;- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC      &lt;- mean(S_n.MC)\n  sd.MC         &lt;- sd(S_n.MC)\n  ## \n  Bootstr.Samples.MC  &lt;- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] &lt;- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec &lt;- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  sd.bootstr.MC.vec    &lt;- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = sd)\n  ## Make it a \"Bootstrap-t\" sample:\n  Bootstr.t.MC.vec     &lt;- sqrt(n)*(X.bar.bootstr.MC.vec - X.bar.MC)/sd.bootstr.MC.vec\n  ## (1-alpha/2)-quantile:\n  q.1.MC &lt;- quantile(Bootstr.t.MC.vec, probs = 1-alpha/2)\n  q.2.MC &lt;- quantile(Bootstr.t.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Bstr.t.lo.vec[b] &lt;- X.bar.MC - q.1.MC * sd.MC/sqrt(n)\n  CI.Bstr.t.up.vec[b] &lt;- X.bar.MC - q.2.MC * sd.MC/sqrt(n)\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      &lt;- CI.Bstr.t.lo.vec&lt;=mean & mean&lt;=CI.Bstr.t.up.vec\nfreq.non.cover &lt;- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Bootrap-t 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==TRUE], \n       x1=CI.Bstr.t.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==FALSE], \n       x1=CI.Bstr.t.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\n\n\n\n\nSolution of Exercise 4.\n\n(a)\n\\[\n\\begin{align*}\n\\mathbb{E}^*(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\left.\\bar{Y}^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.\\frac{1}{n}\\sum_{i=1}^n Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(\\left.Y^*_i\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.Y^*\\right|\\mathcal{S}_n\\right)\n\\end{align*}\n\\] where we used that \\[\n\\left(\\left.Y^*_1\\right|\\mathcal{S}_n\\right),\\dots,\\left(\\left.Y^*_n\\right|\\mathcal{S}_n\\right)\\overset{\\text{i.i.d.}}{\\sim}\\left(\\left.Y^*\\right|\\mathcal{S}_n\\right)\n\\] with \\[\n(Y^*|\\mathcal{S}_n)\\in\\mathcal{S}_n=\\{Y_1,\\dots,Y_n\\}\n\\] and \\[\\begin{align*}\nP(Y^*=Y_1|\\mathcal{S}_n)&=\\frac{1}{n}\\\\[2ex]\nP(Y^*=Y_2|\\mathcal{S}_n)&=\\frac{1}{n}\\\\[2ex]\n\\qquad \\vdots & \\\\[2ex]\nP(Y^*=Y_n|\\mathcal{S}_n)&=\\frac{1}{n}.\n\\end{align*}\\] Thus, we can proceed by computing the mean \\(\\mathbb{E}\\left(\\left.Y^*\\right|\\mathcal{S}_n\\right)\\) of a discrete random variable \\(\\left(\\left.Y^*\\right|\\mathcal{S}_n\\right)\\) such that \\[\\begin{align*}\n\\mathbb{E}^*(\\bar{Y}^*)\n& = [\\dots \\text{Steps above}\\dots]\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.Y^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\sum_{i=1}^n Y_i P(Y^*=Y_1|\\mathcal{S}_n)\\\\[2ex]\n& = \\sum_{i=1}^n Y_i \\frac{1}{n} \\\\[2ex]\n& = \\bar{Y}.\n\\end{align*}\\]\n\n\n(b)\n\\[\n\\begin{align*}\n\\mathbb{E}(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\mathbb{E}(\\bar{Y}^*|\\mathcal{S}_n)\\right)\\quad[\\text{Iterated Law of Expectations}]\\\\[2ex]\n& = \\mathbb{E}(\\bar{Y})\\hspace{2cm}[\\text{Result from (a): }\\mathbb{E}(\\bar{Y}^*|\\mathcal{S}_n) = \\bar{Y}]\\\\[2ex]\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}(Y)\\\\[2ex]\n& = \\mathbb{E}(Y)=\\mu\n\\end{align*}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Bootstrap</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html#solutions",
    "href": "Ch4_NPRegression.html#solutions",
    "title": "4  Nonparametric Regression",
    "section": "Solutions",
    "text": "Solutions\n\nSolution for Exercise 1.\nLet \\(x=\\frac{p}{n}\\).\nA Taylor expansion of \\[\nf(x)=\\frac{1}{\\left(1-x\\right)^2}\n\\] around some point \\(a\\) yields\n\\[\n\\begin{align*}\nf(x)\n&=f(a)+\\frac{1}{1!}\\;(x-a)\\;\\; f'(a)\\\\[2ex]\n&\\hspace{1.5cm} + \\frac{1}{2!}\\;(x-a)^2\\;f''(a)\\\\[2ex]\n&\\hspace{1.5cm} + o\\left((x-a)^2\\right)\n\\end{align*}\n\\]\nThus, for \\(a = 0,\\) we have that \\[\n\\begin{align*}\nf(x)\n&=\\overbrace{1}^{f(0)}+\\frac{1}{1!}\\;x\\;\\;\\overbrace{\\;\\cdot\\;(-2)\\;\\cdot\\left(1-0\\right)^{-3}\\cdot\\left(-1\\right)}^{=f'(0)}\\\\[2ex]\n&\\hspace{1.5cm} + \\frac{1}{2!}\\;x^2\\;\\overbrace{\\;\\cdot\\; 6\\cdot\\left(1-0\\right)^{-4}\\cdot\\left(-1\\right)^2}^{=f''(0)}\\\\[2ex]\n&\\hspace{1.5cm} + o\\left(x^2\\right)\\\\[2ex]\n&=1+ 2 x +O\\left(x^2\\right)+ o\\left(x^2\\right)\\\\[2ex]\n&=1+ 2 x +O\\left(x^2\\right)\\\\[2ex]\n\\end{align*}\n\\] With \\(x=\\frac{p}{n},\\) we thus have that \\[\n\\begin{align*}\nf(p/n)\n&=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\\\[2ex]\n&=1 + 2\\,\\frac{p}{n}+O\\left(\\left(\\frac{p}{n}\\right)^2\\right).\\\\[2ex]\n\\end{align*}\n\\] Thus, \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation error goes to zero with the rate \\(\\left(\\frac{p}{n}\\right)^2\\to 0.\\)\n\n\nSolution for Exercise 2.\n\\[\n\\begin{align*}\n\\operatorname{ARSS}(p)&=\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i - \\hat{m}_p(X_i)\\right)^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\left((Y_i - m(X_i)) - (\\hat{m}_p(X_i) - m(X_i))\\right)^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\varepsilon_i^2 - 2\\cdot \\varepsilon_i (\\hat{m}_p(X_i) - m(X_i)) + (\\hat{m}_p(X_i)-m(X_i))^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\varepsilon_i^2 - 2\\cdot \\varepsilon_i \\cdot \\hat{m}_p(X_i) + 2\\cdot \\varepsilon_i\\cdot m(X_i) + (\\hat{m}_p(X_i)-m(X_i))^2\\right)\\\\[2ex]\n\\mathbb{E}_\\varepsilon\\big(\\operatorname{ARSS}(p)\\big)\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\mathbb{E}_\\varepsilon(\\varepsilon_i^2) - 2\\cdot \\mathbb{E}_\\varepsilon(\\varepsilon_i \\cdot \\hat{m}_p(X_i)) + 0 + \\mathbb{E}_\\varepsilon((\\hat{m}_p(X_i)-m(X_i))^2)\\right)\\\\[2ex]\n&=\\sigma^2 - \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon(\\varepsilon_i \\cdot \\hat{m}_p(X_i)) + \\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}_\\varepsilon((\\hat{m}_p(X_i)-m(X_i))^2)\\\\[2ex]\n&=\\sigma^2 - \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon(\\varepsilon_i \\cdot \\hat{m}_p(X_i)) + \\operatorname{MASE}(\\hat{m}_p)\\\\[2ex]\n&=\\sigma^2 - \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\sum_{j=1}^p\\hat\\beta_j b_{j,k}(X_i)\\right) + \\operatorname{MASE}(\\hat{m}_p)\\\\[2ex]\n\\end{align*}\n\\]\nIn the following, we focus on the second term: \\[\n\\begin{align*}\n&-\\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\sum_{j=1}^p\\hat\\beta_j b_{j,k}(X_i)\\right)\\\\[2ex]\n&=- \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\hat\\beta^\\top \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\right)\\\\[2ex]\n&=- \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot (\\beta+(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\varepsilon)^\\top \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\beta^\\top \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right) + \\varepsilon_i \\cdot((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\varepsilon)^\\top \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n 2\\cdot 0 - \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot ((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\varepsilon)^\\top \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\right)\\\\[2ex]\n&=- \\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\varepsilon^\\top \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1} \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n  e_i^\\top \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1} \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n  \\left(b_{1,k}(X_i),\\dots,b_{p,k}(X_i)\\right) (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n  \\operatorname{trace}\\left(\\left(b_{1,k}(X_i),\\dots,b_{p,k}(X_i)\\right) (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n  \\operatorname{trace}\\left( (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\left(b_{1,k}(X_i),\\dots,b_{p,k}(X_i)\\right)\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\operatorname{trace}\\left( (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\sum_{i=1}^n\\left(\\begin{matrix}b_{1,k}(X_i)\\\\\\vdots\\\\ b_{p,k}(X_i)\\end{matrix}\\right)\\left(b_{1,k}(X_i),\\dots,b_{p,k}(X_i)\\right)\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\operatorname{trace}\\left( (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{X}\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\cdot \\frac{1}{n}\\operatorname{trace}\\left(I_p\\right)\\\\[2ex]\n&=- 2\\sigma^2 \\frac{p}{n},\n\\end{align*}\n\\] where \\(e_i^\\top\\) is a \\((1\\times p)\\) vector with a \\(1\\) at the \\(i\\)th element and \\(0\\)s else.\nThus, \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\]\n\n\nSolution for Exercise 3.\nAlternative view on the second term:\n\\[\n\\begin{align*}\n&-\\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\sum_{j=1}^p\\hat\\beta_j b_{j,k}(X_i)\\right)\\\\[2ex]\n&-\\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\mathbb{E}_\\varepsilon\\left(\\varepsilon_i \\cdot \\hat{m}_p(X_i)\\right)\\\\[2ex]\n&-\\frac{1}{n}\\sum_{i=1}^n 2\\cdot \\overbrace{\\Big[\\mathbb{E}_\\varepsilon(\\varepsilon_i \\cdot \\hat{m}_p(X_i)) - \\underbrace{\\mathbb{E}_\\varepsilon(\\varepsilon_i)}_{=0} \\cdot \\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\Big]}^{=Cov_\\varepsilon(\\varepsilon_i, \\hat{m}_p(X_i))}\\\\[2ex]\n&-2\\cdot \\underbrace{\\frac{1}{n}\\sum_{i=1}^n  Cov_\\varepsilon\\left(\\varepsilon_i, \\hat{m}_p(X_i)\\right)}_{=\\sigma^2\\frac{p}{n}}\\\\[2ex]\n\\end{align*}\n\\] Thus \\[\n\\sigma^2\\frac{p}{n} = \\frac{1}{n}\\sum_{i=1}^n  Cov_\\varepsilon\\left(\\varepsilon_i, \\hat{m}_p(X_i)\\right).\n\\]\nThat is, the core-part of the second term, namely \\(\\sigma^2\\frac{p}{n},\\) equals the average covariance between the noise \\(\\varepsilon_i\\) and the fitted value \\(\\hat{m}_p(X_i)\\) across \\(i=1,\\dots,n.\\)\nThis observation explains the name optimism term. Of course, the fits \\(\\hat{m}_p(X_i)\\) should only be driven by the signal-components \\(m(X_i)\\) and not by the noise-components \\(\\varepsilon_i.\\)"
  },
  {
    "objectID": "Ch4_NPRegression.html#introduction",
    "href": "Ch4_NPRegression.html#introduction",
    "title": "4  Nonparametric Regression",
    "section": "",
    "text": "\\(Y_{i}\\in \\mathbb{R}\\) real response variable\n\\(X_{i}\\in [a,b]\\subset \\mathbb{R}\\) real explanatory variable\n\\(n\\) sufficiently large sample size (e.g., \\(n\\geq 40\\))\n\n\nThe Nonparametric Regression Model\n\\[\nY_i=m(X_i)+\\varepsilon_i\n\\]\n\n\\(m(X)=\\mathbb{E}(Y_i|X=X_i)\\) regression function\n\\(\\mathbb{E}(\\varepsilon_i)=0\\)\n\\(Var(\\varepsilon_i|X_i) = Var(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) and \\(X_i\\) are independent or at least mean-independent, i.e., \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\)\n\nSpecial cases of parametric regression models:\n\nLinear regression: \\(m\\) is a straight line \\[\nm(X)=\\beta_0+\\beta_1 X\n\\]\nPolynomial generalizations: \\(m\\) is a quadratic or cubic polynomial \\[\n\\begin{align*}\n              m(X)&=\\beta_0 +\\beta_1 X+\\beta_2 X^2\\\\\n\\text{or} \\quad m(X)&=\\beta_0+\\beta_1 X+\\beta_2 X^2+\\beta_3 X^3\n\\end{align*}\n\\]\n\nMany important applications lead to regression functions possessing a complicated structure. Standard models then are “too simple” and do not provide useful approximations of \\(m(x)\\).\n\n\n\n\n\n\nAs George Box is saying it:\n\n\n\n“All models are false, but some are useful” (G. Box)\n\n\nAn important point in theoretical analysis is the way how the observations \\(X_1,\\dots,X_n\\) have been generated. One distinguishes between fixed and random design.\n\nFixed design: The observation points \\(X_1,\\dots,X_n\\) are fixed (non stochastic) values.\n\nExample: Crop yield (\\(Y\\)) in dependence of the amount of fertilizer (\\(X\\)) used, when the amount is determined deterministically by the experimenter.\nEquidistant Design: (Most important special case of fixed design)\n\\[\nX_{i+1}-X_i=\\frac{b-a}{n}.\n\\]\n\nRandom design: The observation points \\(X_1,\\dots,X_n\\) are (realizations of) i.i.d. random variables with density \\(f\\). The density \\(f\\) is called “design density”. Throughout this chapter it will be assumed that \\(f(x)&gt;0\\) for all \\(x\\in [a,b]\\).\n\nExample: Sample \\((Y_1,X_1),\\dots,(Y_n,X_n)\\) of log-wages (\\(Y\\)) and age (\\(X\\)) of randomly selected individuals.\n\n\nIn the case of random design, \\(m(x)\\) is the conditional expectation of \\(Y\\) given \\(X=x\\), \\[\nm(x)=\\mathbb{E}(Y|\\ X=x)\n\\]\nand \\(Var(\\varepsilon_i|X_i)=\\sigma^2\\).\n\n\n\n\n\n\nNote\n\n\n\nFor random design all expectations (as well as variances) have to be interpreted as conditional expectations (variances) given \\(X_1,\\dots,X_n\\).\n\n\nExample: Canadian cross-section wage data consisting of a random sample taken from the 1971 Canadian Census Public Use Tapes for male individuals having common education (grade 13); see Figure 4.1.\n\nsuppressPackageStartupMessages(library(\"np\"))\ndata(\"cps71\")\nplot(cps71$age, cps71$logwage, xlab=\"Age\", ylab=\"log(wage)\")\n\n\n\n\n\n\n\nFigure 4.1: Canadian cross-section wage data.\n\n\n\n\n\nWhat would be a good/reasonable model assumption for \\(m(x)\\) to estimate the conditional mean function for the data shown in Figure 4.1?\n\n\n\n\n\n\nNo specific model assumption\n\n\n\nIn nonparametric regression analysis, we do not make assumptions about the specific structure of the regression function \\(m(x).\\) We only make the qualitative assumption that \\(m\\) is a sufficiently smooth function, i.e. that \\(m(t)\\) is sufficiently often differentiable at every \\(t\\in(a,b)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch4_NPRegression.html#footnotes",
    "href": "Ch4_NPRegression.html#footnotes",
    "title": "4  Nonparametric Regression",
    "section": "",
    "text": "The degree \\(k\\) of a polynomial \\(s(x)=s_0+s_1x+s_2x^2+\\dots+s_kx^{k}\\) refers to the highest exponent. The order \\(k+1\\) of a polynomial refers to the number coefficients \\((s_0,\\dots,s_k).\\)↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#likelihood-principle",
    "href": "Ch1_MaximumLikelihood.html#likelihood-principle",
    "title": "1  Maximum Likelihood",
    "section": "",
    "text": "Properties of Maximum Likelihood Estimators\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta_n\\) of some parameter, e.g. \\(\\theta_0\\in\\mathbb{R}\\), is\n\nConsistent:\n\\[\n\\hat\\theta_n\\rightarrow_p\\theta_0,\\quad n\\to\\infty\n\\]\nAsymptotically normal: \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta_0) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\n\\]\nAsymptotically efficient: This means that no other consistent estimator has a lower asymptotic mean squared error than the maximum likelihood estimator.\n\nLikewise for multivariate parameter \\(\\theta_0\\in\\mathbb{R}^p.\\)\nThus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.\n\n\n\n\n\n\nML-estimation requires fixing the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nLet \\[\nX_1,\\dots,X_n\n\\] denote a (i.i.d.) random sample, such that \\(X_i\\overset{\\text{i.i.d.}}{\\sim} f\\), for all \\(i=1,\\dots,n.\\)\nClassic ML-estimation requires us to fix the general family of density functions or probability mass functions \\(f,\\) where \\(f\\) is known up to an unknown parameter \\(\\theta,\\) and where \\(\\theta\\in\\mathbb{R}^K\\) is a finite (\\(1\\leq K&lt;\\infty\\)) dimensional parameter vector.\nExamples:\n\n\\(f\\) being the probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta)\\) with \\[\nf(x|\\theta)=\n\\left\\{\n\\begin{array}{ll}\n\\theta,   & \\text{if } x=1\\\\\n1-\\theta, & \\text{if } x=0\n\\end{array}\n\\right.\n\\] and unknown parameter \\(0\\leq \\theta\\leq 1.\\)\nThe density function of the exponential distribution \\[\nf(x;\\theta)=\\left\\{\n  \\begin{matrix}\n  \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n  0                     & \\text{for }x &lt; 0\\\\\n  \\end{matrix}\\right.\n\\] with unknown rate parameter \\(\\theta&gt;0\\) and \\(x\\in\\mathbb{R}.\\)\n\\(f\\) is the normal density \\[\nf(x|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\n\\] with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T\\) and \\(x\\in\\mathbb{R}.\\)\n\nThis requirement (fixing the family of density functions) can be overly restrictive. In many applications we typically do not know the family of \\(f.\\) To address this issue, the quasi maximum likelihood theory generalizes classic maximum likelihood estimation to cases where \\(f\\) is misspecified (see White (1982)).\n\n\n\n\nExample: Coin Flipping (Bernoulli Trial)\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair \\(\\text{Coin}\\) can take the value \\(H\\) (Head) or \\(T\\) (Tail), \\[\n\\text{Coin}\\in\\{H,T\\}.\n\\] Such coin-flips can be modeled using Bernoulli random variables \\[\nX\\sim\\mathcal{Bern}(\\theta_0)\n\\] where \\[\nX=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\n    \\end{matrix}\n    \\right.\n\\] The probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta_0),\\) with unknown probability of success parameter \\(0&lt;\\theta_0&lt;1,\\) is given by \\[\nf(x|\\theta_0)=\n\\left\\{\n  \\begin{array}{ll}\n  \\theta_0,&\\text{if } x=1\\\\\n  1-\\theta_0, & \\text{if } x=0\n  \\end{array}\n\\right.\n\\] I.e., the probability that we get Head \\(H\\) is \\[\n\\theta_0 = f(1|\\theta_0) = P(X=1) = P(\\text{Coin}=H),\n\\] and the probability that we get Tail \\(T\\) is \\[\n1-\\theta_0 = f(0|\\theta_0) = P(X=0) = P(\\text{Coin}=T).\n\\]\nOur goal is to estimate the unknown \\(\\theta_0\\) using a random (i.i.d.) sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\text{ in $i$th coin flip}\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\\text{ in $i$th coin flip}\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bern}(\\theta_0),\\quad i=1,\\dots,n.\n\\] \nA given observed realization of the random sample \\[\n\\{X_{1,obs},X_{2,obs},\\dots,X_{n,obs}\\}=\\{0,1,\\dots,0\\}\n\\] consists of \\(0\\leq N_{H,obs}\\leq n\\) \\[\nN_{H,obs}=\\sum_{i=1}^n X_{i,obs}\n\\] many heads and \\[\n0\\leq n-N_{H,obs} \\leq n\n\\] many tails.\n\nThe (Log-)Likelihood Function\nHow do we combine the information from the \\(n\\) observations \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] to estimate the unknown \\(\\theta_0\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta)\n&=\\prod_{i=1}^nf(X_{i,obs}|\\theta)\\\\[2ex]\n%&=\\left(P(X=1)\\right)^{N_{H,obs}}\\left(P(X=0)\\right)^{n-N_{H,obs}}\\\\[2ex]\n%&= \\theta^{N_{H,obs}}(1-\\theta)^{n-N_{H,obs}}  \\\\[2ex]\n&= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}},\n\\end{align*}\n\\] where \\(f(\\,\\cdot\\,|\\theta)\\) denotes here the probability mass function of the Bernoulli distribution with parameter candidate \\(p=\\theta.\\)\nThe function \\(\\mathcal{L}_n(\\theta)\\) is called the likelihood function.\n\n\n\n\n\n\n\nDefinition 1.1 (Likelihood Function) Let \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] denote a realization of a (i.i.d.) random sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\overset{\\text{i.i.d.}}{\\sim} f\\equiv f(\\,\\cdot\\,|\\theta)\\) for all \\(i=1,\\dots,n,\\) where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of \\(f.\\) Then, we call \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\] the likelihood function.\n(Note: A definition for dependent data (e.g., time series) is also possible.)\n\n\n\n\n\n\nEstimation Idea\nWe estimate the unknown parameter \\(\\theta_0\\) by maximizing the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) over the range of possible parameter values. The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}_n(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator\n\n\n\n\n\n\n\nDefinition 1.2 (Maximum Likelihood (ML) Estimator) \\[\n\\begin{align*}\n\\hat{\\theta}_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\end{align*}\n\\] where \\(\\Theta\\) denotes the parameter space.\n\n\n\n\nIn our coin flip example this means to estimate the unknown \\(\\theta_0\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) is maximal, \\[\n\\hat\\theta_{ML} = \\arg\\max_{\\theta\\in[0,1]} \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\]\nUsually it’s easier to work with sums rather than products—also for doing the asymptotics in Section 1.4. So we apply a monotonic transformation by taking the logarithm of the likelihood which leads to the log-likelihood function: \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\ln\\prod_{i=1}^n f(X_{i,obs}|\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n \\ln f(X_{i,obs}|\\theta).\n\\end{align*}\n\\] Since this is only a monotonic transformation, we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\ell_n(\\theta).\n\\end{align*}\n\\] \n\n\n\n\n\n\nLog-likelihoods instead of the likelihoods\n\n\n\n\nFrom a standpoint of computational complexity, you can imagine that summing is less expensive than multiplication (although nowadays these are almost equal).\nMore important: likelihoods would become very small and you will run out of your floating point precision very quickly, yielding an underflow. That’s why it is way more convenient to use the logarithm of the likelihood. Simply try to calculate the likelihood by hand, using pocket calculator—it’s almost impossible.\n\n\n\nIn our coin flipping example, taking the natural logarithm yields, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta) &= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}} \\\\[2ex]\n\\Rightarrow\\quad \\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n\\left( X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\right).\n\\end{align*}\n\\]\nThe coin flip example is actually so simple that we can maximize \\(\\ell_n(\\theta)\\) analytically. Computing the first derivative yields \\[\n\\begin{align*}\n\\ell'_n(\\theta)&=\\sum_{i=1}^n \\left(X_{i,obs}\\dfrac{1}{\\theta} - (1-X_{i,obs})\\dfrac{1}{1-\\theta}\\right)\\\\[2ex]\n&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta}\n\\end{align*}\n\\] Setting the first derivative to zero determines the maximum likelihood estimator (MLE) \\(\\hat\\theta_{ML}\\): \\[\n\\begin{array}{rrcl}\n&\\ell_n'(\\hat\\theta_{ML})&\\overset{!}{=}&0\\\\[2ex]\n\\Leftrightarrow&\\dfrac{N_{H,obs}}{\\hat\\theta_{ML}} &=& \\dfrac{n-N_{H,obs}}{1-\\hat\\theta_{ML}} \\\\[2ex]\n\\Leftrightarrow&N_{H,obs}-N_{H,obs}\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-N_{H,obs}\\hat\\theta_{ML}\\\\[2ex]\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{N_{H,obs}}{n}\n\\end{array}\n\\tag{1.1}\\]\nUsually, however, the log-likelihood function is way more complicated and one needs to apply numeric optimization algorithms to find the MLE, \\(\\hat\\theta_{ML}.\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#cramérrao-lower-bound",
    "href": "Ch1_MaximumLikelihood.html#cramérrao-lower-bound",
    "title": "1  Maximum Likelihood",
    "section": "1.5 Cramér–Rao Lower Bound",
    "text": "1.5 Cramér–Rao Lower Bound\nHarald Cramér and Calyampudi Radhakrishna Rao showed that for any unbiased estimator \\(\\hat\\theta\\), its asymptotic variance-covariance matrix cannot be smaller than \\[\n\\mathcal{I}^{-1}(\\theta_0),\n\\] where \\(\\mathcal{I}(\\theta_0)\\) is the Fisher information matrix \\[\n\\mathcal{I}(\\theta_0) = -\\frac{1}{n}\\mathbb{E}\\left(H_{\\ell_n}(\\theta_0)\\right)\n\\] evaluated at the true parameter value \\(\\theta_0.\\)\nThus, maximum likelihood estimators attain the Cramer-Rao lower bound and will therefore be asymptotically efficient.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#section",
    "href": "Ch1_MaximumLikelihood.html#section",
    "title": "1  Maximum Likelihood",
    "section": "",
    "text": "Definition 1.1 (Likelihood Function) More generally, when the observations \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\] where \\(f(X_{i,obs} | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_{i,obs},\\) and where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of the density function. (A definition for dependent data (e.g. time series) is also possible.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#introduction-the-likelihood-principle",
    "href": "Ch1_MaximumLikelihood.html#introduction-the-likelihood-principle",
    "title": "1  Maximum Likelihood",
    "section": "",
    "text": "1.1.1 Properties of Maximum Likelihood Estimators\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta_n\\) of some parameter, e.g. \\(\\theta_0\\in\\mathbb{R}\\), is\n\nConsistent:\n\\[\n\\hat\\theta_n\\rightarrow_p\\theta_0,\\quad n\\to\\infty\n\\]\nAsymptotically normal: \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta_0) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\n\\]\nAsymptotically efficient: This means that no other consistent estimator has a lower asymptotic mean squared error than the maximum likelihood estimator.\n\nLikewise for multivariate parameter \\(\\theta_0\\in\\mathbb{R}^p.\\)\nThus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.\n\n\n\n\n\n\nML-estimation requires fixing the family of distributions \\(f(\\cdot;\\theta)\\)\n\n\n\nLet \\[\nX_1,\\dots,X_n\n\\] denote a (i.i.d.) random sample, such that \\(X_i\\overset{\\text{i.i.d.}}{\\sim} f\\), for all \\(i=1,\\dots,n.\\)\nClassic ML-estimation requires us to fix the general family of density functions or probability mass functions \\(f,\\) where \\(f\\) is known up to an unknown parameter value \\(\\theta_0,\\) and where \\(\\theta_0\\in\\mathbb{R}^K\\) is a finite (\\(1\\leq K&lt;\\infty\\)) dimensional parameter vector.\nExamples:\n\n\\(f\\) being the probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta)\\) with \\[\nf(x;\\theta_0)=\n\\left\\{\n\\begin{array}{ll}\n\\theta_0,   & \\text{if } x=1\\\\\n1-\\theta_0, & \\text{if } x=0\n\\end{array}\n\\right.\n\\] and unknown parameter \\(0\\leq \\theta\\leq 1.\\)\nThe density function of the exponential distribution \\[\nf(x;\\theta_0)=\\left\\{\n  \\begin{matrix}\n  \\theta_0\\exp(- \\theta_0 x)& \\text{for }x\\geq 0\\\\\n  0                     & \\text{for }x &lt; 0\\\\\n  \\end{matrix}\\right.\n\\] with unknown rate parameter \\(\\theta_0&gt;0\\) and \\(x\\in\\mathbb{R}.\\)\n\\(f\\) is the normal density \\[\nf(x;\\theta_0)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_0}{\\sigma_0}\\right)^2\\right)\n\\] with unknown parameter vector \\(\\theta_0=(\\mu_0,\\sigma_0^2)^T\\) and \\(x\\in\\mathbb{R}.\\)\n\nThis requirement (fixing the family of density functions) can be overly restrictive. In many applications we typically do not know the family of \\(f.\\) To address this issue, the quasi maximum likelihood theory generalizes classic maximum likelihood estimation to cases where \\(f\\) is misspecified (see White (1982)).\n\n\n\n\n1.1.2 Example: Coin Flipping (Bernoulli Trial)\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair \\(\\text{Coin}\\) can take the value \\(H\\) (Head) or \\(T\\) (Tail), \\[\n\\text{Coin}\\in\\{H,T\\}.\n\\] Such coin-flips can be modeled using Bernoulli random variables \\[\nX\\sim\\mathcal{Bern}(\\theta_0)\n\\] where \\[\nX=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\n    \\end{matrix}\n    \\right.\n\\] The probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta_0),\\) with unknown probability of success parameter \\(0&lt;\\theta_0&lt;1,\\) is given by \\[\nf(x;\\theta_0)=\n\\left\\{\n  \\begin{array}{ll}\n  \\theta_0,&\\text{if } x=1\\\\\n  1-\\theta_0, & \\text{if } x=0\n  \\end{array}\n\\right.\n\\] I.e., the probability that we get Head \\(H\\) is \\[\n\\theta_0 = f(1;\\theta_0) = P(X=1) = P(\\text{Coin}=H),\n\\] and the probability that we get Tail \\(T\\) is \\[\n1-\\theta_0 = f(0;\\theta_0) = P(X=0) = P(\\text{Coin}=T).\n\\]\nOur goal is to estimate the unknown \\(\\theta_0\\) using a random (i.i.d.) sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\text{ in $i$th coin flip}\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\\text{ in $i$th coin flip}\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\overset{\\text{i.i.d.}}{\\sim}\\mathcal{Bern}(\\theta_0),\\quad i=1,\\dots,n.\n\\] \nA given observed realization of the random sample \\[\n\\{X_{1,obs},X_{2,obs},\\dots,X_{n,obs}\\}=\\{0,1,\\dots,0\\}\n\\] consists of \\(0\\leq N_{H,obs}\\leq n\\) \\[\nN_{H,obs}=\\sum_{i=1}^n X_{i,obs}\n\\] many heads and \\[\n0\\leq n-N_{H,obs} \\leq n\n\\] many tails.\n\nThe (Log-)Likelihood Function\nHow do we combine the information from the \\(n\\) observations \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] to estimate the unknown \\(\\theta_0\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}_{n,obs}(\\theta)\n&=\\prod_{i=1}^nf(X_{i,obs};\\theta)\\\\[2ex]\n%&=\\left(P(X=1)\\right)^{N_{H,obs}}\\left(P(X=0)\\right)^{n-N_{H,obs}}\\\\[2ex]\n%&= \\theta^{N_{H,obs}}(1-\\theta)^{n-N_{H,obs}}  \\\\[2ex]\n&= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}},\n\\end{align*}\n\\] where \\(f(\\,\\cdot\\,;\\theta)\\) denotes here the probability mass function of the Bernoulli distribution with parameter candidate \\(p=\\theta.\\)\nThe function \\(\\mathcal{L}_n(\\theta)\\) is called the likelihood function. The likelihood function depends on the random sample \\(\\{X_{1},\\dots,X_{n}\\}\\) and is thus itself random. If we want to emphasize that we look at a given realization of the random sample \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\), we write \\(\\mathcal{L}_{,obs}(\\theta)\\).\n\n\n\n\n\n\n\nDefinition 1.1 (Likelihood Function) Let \\[\n\\{X_{1},\\dots,X_{n}\\}\n\\] denote a random sample with \\(X_i\\overset{\\text{i.i.d.}}{\\sim} f\\equiv f(\\,\\cdot\\,;\\theta)\\) for all \\(i=1,\\dots,n,\\) where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of \\(f.\\) Then, we call \\[\n\\mathcal{L}_{n}(\\theta)=\\prod_{i=1}^n f(X_{i};\\theta),\n\\] the (random) likelihood function. Let \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] denote a (observed) realization of \\(\\{X_1,\\dots,X_n\\}\\). Then, we call \\[\n\\mathcal{L}_{n,obs}(\\theta)=\\prod_{i=1}^n f(X_{i,obs};\\theta),\n\\] a (observed) realization of the likelihood function.\n(Note: A definition for dependent data (e.g., time series) is also possible.)\n\n\n\n\n\n\n\n1.1.3 Estimation Idea\nWe estimate the unknown parameter \\(\\theta_0\\) by maximizing the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) over the range of possible parameter values \\(\\theta\\in\\Omega.\\)\n\nThe value \\(\\hat\\theta_{ML}\\) at which the likelihood function \\(\\mathcal{L}_n(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator\nThe value \\(\\hat\\theta_{ML,obs}\\) at which the observed likelihood function \\(\\mathcal{L}_{n,obs}(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimate; i.e., \\(\\hat\\theta_{ML,obs}\\) is a specific realization of the ML-estimator computed from the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}.\\)\n\n\n\n\n\n\n\nCaution\n\n\n\nFor the rest of this chapter, I follow the usual convention and generally write \\[\n\\{X_{1},\\dots,X_{n}\\}\\quad\\text{and}\\quad\n\\mathcal{L}_{n}(\\theta)=\\prod_{i=1}^n f(X_{i};\\theta)\n\\] to convey both the random variables point of view and the realization point of view (\\(obs\\)), since often both points of view can make sense in the same object. Only if I want to stress that the realization point of view applies, I write “\\(obs.\\)”\nTake, for instance, the ML-estimator \\[\n\\hat\\theta_{ML}.\n\\] Of course, we can only compute a specific value \\((\\hat\\theta_{ML,obs})\\) given an observed realization of the random sample. Using the random variable type of notation \\(\\hat\\theta_{ML}\\) to denote both the random variables point of view and the realization point of view reminds ourselves that any specific value \\((\\hat\\theta_{ML,obs})\\) is just a possible realization of the estimator.\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Maximum Likelihood (ML) Estimator) Let \\[\n\\{X_{1},\\dots,X_{n}\\}\n\\] denote a random sample with \\(X_i\\overset{\\text{i.i.d.}}{\\sim} f\\equiv f(\\,\\cdot\\,;\\theta)\\) for all \\(i=1,\\dots,n,\\) where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of \\(f.\\) Then, we call \\[\n\\begin{align*}\n\\hat{\\theta}_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\prod_{i=1}^n f(X_{i};\\theta),\n\\end{align*}\n\\] the Maximum Likelihood (ML) Estimator, where \\(\\Theta\\) denotes the parameter space.\n(Note: A definition for dependent data (e.g., time series) is also possible.)\n\n\n\n\nThus, to derive the estimator for the unknown \\(\\theta_0\\) in our coin flip example, we need to maximize the likelihood function, \\[\n\\begin{align*}\n\\hat{\\theta}_{ML}\n&=\\arg\\max_{\\theta\\in[0,1]} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in[0,1]} \\prod_{i=1}^n f(X_{i};\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in[0,1]} \\prod_{i=1}^n \\theta^{X_{i}}(1-\\theta)^{1-X_{i}}.\n\\end{align*}\n\\]\nUsually it’s easier to work with sums rather than products—also for doing the asymptotics in Section 1.4. So we apply a monotonic transformation by taking the logarithm of the likelihood which leads to the log-likelihood function: \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\ln\\prod_{i=1}^n f(X_{i};\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n \\ln f(X_{i};\\theta).\n\\end{align*}\n\\] Since this is only a monotonic transformation, we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\ell_n(\\theta).\n\\end{align*}\n\\] \n\n\n\n\n\n\nLog-likelihoods instead of the likelihoods\n\n\n\n\nFrom a standpoint of computational complexity, you can imagine that summing is less expensive than multiplication (although nowadays, these are almost equal).\nMore important: likelihoods would become very small and you will run out of your floating point precision very quickly, yielding an underflow. That’s why it is way more convenient to use the logarithm of the likelihood. Simply try to calculate the likelihood by hand, using pocket calculator—it’s almost impossible.\n\n\n\nIn our coin flipping example, taking the natural logarithm yields, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta) &= \\prod_{i=1}^n \\theta^{X_{i}}(1-\\theta)^{1-X_{i}} \\\\[2ex]\n\\Rightarrow\\quad \\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n\\left( X_{i} \\ln(\\theta) + (1-X_{i})\\ln(1-\\theta)\\right).\n\\end{align*}\n\\]\nFigure 1.1 shows some random realizations of the log-likelihood function based on different random realizations of the \\(n=5\\) coin tosses, i.e. realizations of the random sample \\(\\{X_1,\\dots,X_n\\}\\in\\{0,1\\}^n.\\)\n\n\n\n\n\n\n\n\nFigure 1.1: Random realizations of the log-likelihood function \\(\\ell_{n}\\) of our coin flipping example based on different realizations of the random sample \\(X_1,\\dots,X_n\\) with \\(n=5\\) and (usually unknown) \\(\\theta_0=0.2.\\)\n\n\n\n\n\nThe coin flip example is actually so simple that we can maximize \\(\\ell_n(\\theta)\\) analytically. Computing the first derivative yields \\[\n\\begin{align*}\n\\ell'_n(\\theta)&=\\sum_{i=1}^n \\left(X_{i}\\dfrac{1}{\\theta} - (1-X_{i})\\dfrac{1}{1-\\theta}\\right)\\\\[2ex]\n&=\\dfrac{N_{H}}{\\theta} - \\dfrac{n-N_{H}}{1-\\theta}\n\\end{align*}\n\\] Setting the first derivative to zero determines the maximum likelihood estimator (MLE) \\(\\hat\\theta_{ML}\\): \\[\n\\begin{array}{rrcl}\n&\\ell_n'(\\hat\\theta_{ML})&\\overset{!}{=}&0\\\\[2ex]\n\\Leftrightarrow&\\dfrac{N_{H}}{\\hat\\theta_{ML}} &=& \\dfrac{n-N_{H}}{1-\\hat\\theta_{ML}} \\\\[2ex]\n\\Leftrightarrow&N_{H}-N_{H}\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-N_{H}\\hat\\theta_{ML}\\\\[2ex]\n\\Leftrightarrow&\\hat\\theta_{ML}\n&=&\\dfrac{N_{H}}{n}\\\\[2ex]\n&&=&\\dfrac{1}{n}\\sum_{i=1}^n X_i\n\\end{array}\n\\tag{1.1}\\]\nGiven observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\},\\) a specific estimate of \\(\\theta_0\\) can thus be computed as \\[\n\\begin{align*}\n\\hat{\\theta}_{ML,obs}=\\dfrac{1}{n}\\sum_{i=1}^n X_{i,obs}.\n\\end{align*}\n\\]\nUsually, however, the log-likelihood function is way more complicated such that it is impossible to derive an explicit expression for the ML-estimator \\(\\hat\\theta_{ML}.\\) In such cases, one needs to apply numeric optimization algorithms to compute the ML-estimates, \\(\\hat\\theta_{ML,obs}.\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html",
    "href": "Ch4_NP_Density_Estimation.html",
    "title": "4  Nonparametric Density Estimation",
    "section": "",
    "text": "4.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#introduction",
    "href": "Ch4_NP_Density_Estimation.html#introduction",
    "title": "4  Nonparametric Density Estimation",
    "section": "",
    "text": "4.1.1 Example: Income Data Analysis\n\nc(inc76$income)[1:6]\n\n[1]  66.49  14.40  43.54  36.50  18.34 117.23\n\n\nTypical aim: Characterizing the income distribution (density function) \\(f\\) given a random sample \\[\n\\{X_1,\\dots,X_n\\}\n\\] with \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\quad\\text{and}\\quad X\\sim f.\n\\]\nTraditional statistical key figures:\n\nEmpirical mean\nEmpirical median\nEmpirical variance\nEmpirical interquartile distance, etc.\n\nOnly summarize single aspects of a distribution.\nEstimating the total density function \\(f\\) can provide more detailed, more wholistic information.\nFigure 4.1 shows a histogram (i.e., a very simple density estimator) of the income data.\n\nhist(inc76$income, \n     freq = FALSE,\n     xlab = \"Income\", \n     ylab = \"Density\", main = \"\")\n\n\n\n\n\n\n\nFigure 4.1: Histrogram of the income data.\n\n\n\n\n\nDisadvantages of the histogram:\n\nChoice of the bin-width (and of the start point)\nDiscontinuous, locally constant. Thus, a histogram generally cannot be a very efficient estimator for a (continuous) density function \\(f(x).\\)\n\nThe choice of the bin-width is crucial. Figure 4.2 shows examples for a too small and a too large bin-width choice.\n\n\n\n\n\n\n\n\nFigure 4.2: Histrograms of income data for different choices of the bin-width.\n\n\n\n\n\nFigure 4.3 shows a comparison of the histogram versus the density estimate from a kernel density estimator.\n\n\n\n\n\n\n\n\nFigure 4.3: Histogram versus a nonparametric kernel density estimation (green solid line).\n\n\n\n\n\nAdvantages of the kernel density estimator:\n\nChoice of the bandwidth (similar to bin-width) can be done using statistical theory\nContinuous estimate for a (continuous) density function \\(f.\\)\n\n\n\n4.1.2 From the Histogram to the Kernel Density Estimator\nConsider a histogram with \\(J\\) (e.g., \\(J=8\\) as in Figure 4.3) bins all having the same bin-width \\(2h,\\) defined by equidistant intervals \\[\n(x_{j-1},x_j]\n\\] with \\[\nx_j-x_{j-1}=2h\\quad \\text{for all} \\quad j=1,\\dots,J.\n\\] The bin-height is determined locally at the \\(j\\)th interval mid-point \\[\n\\bar{x}_j=(x_{j-1}+x_j)/2\n\\] by the relative frequency of data points \\(X_1,\\dots,X_n\\) that fall within the \\(j\\)th interval \\((x_{j-1},x_j],\\)\n\\[\n\\begin{align*}\n\\hat f_{hist}(\\bar{x}_j)\n& =\\frac{\\hbox{Number of } X_{i}\\hbox{ in } (x_{j-1},x_j]}{2hn}\n\\end{align*}\n\\tag{4.1}\\] Note: The scaling by \\(2hn\\) is necessary to guarantee that the area of each bin equals the relative frquency of data points \\(X_1,\\dots,X_n\\) that fall into the interval \\((x_{j-1},x_j],\\) \\[\n\\begin{align*}\n\\text{Area of $j$th Bin}\n& = (\\text{bin-width}) \\cdot (\\text{bin-height of $j$th bin})\\\\[2ex]\n& = \\qquad 2h\\quad\\, \\cdot \\frac{\\hbox{Number of } X_{i}\\hbox{ in } (x_{j-1},x_j]}{2hn}\\\\[2ex]\n& = \\frac{\\hbox{Number of } X_{i}\\hbox{ in } (x_{j-1},x_j]}{n}\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\; 1_{(X_i\\in(x_{j-1},x_j])},\n\\end{align*}\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function, i.e., \\(1_{(\\text{TRUE})}=1\\) and \\(1_{(\\text{FALSE})}=0.\\)\nThus, the scaling guarantees that the areas of the single bins of the histogram sum up to one: \\[\n\\begin{align*}\n\\sum_{j=1}^J\\text{Area of $j$th Bin}\n&=\\sum_{j=1}^J \\frac{1}{n}\\sum_{i=1}^n \\; 1_{(X_i\\in(x_{j-1},x_j])}\\\\[2ex]\n&=\\frac{1}{n} \\sum_{j=1}^J \\sum_{i=1}^n \\; 1_{(X_i\\in(x_{j-1},x_j])}\\\\[2ex]\n&=\\frac{n}{n}=1.\n\\end{align*}\n\\]\nDoing some rearrangements of Equation 4.1, allows writing the histogram as a type of kernel density estimator:\n\\[\n\\begin{align*}\n\\hat f_{hist}(\\bar{x}_j)\n& =\\frac{\\hbox{Number of } X_{i}\\hbox{ in } (x_{j-1},x_j]}{2hn}\\\\[2ex]\n&=\\frac{1}{nh}\\sum_{i=1}^n\\frac{1}{2}1_{\\left(\\bar{x}_j-h &lt; X_i\\leq \\bar{x}_j+h\\right)}\\\\[2ex]\n&=\\frac{1}{nh}\\sum_{i=1}^n\\frac{1}{2}1_{\\left(-1&lt;\\left(\\frac{X_i-\\bar{x}_j}{h}\\right)\\leq 1\\right)}\\\\[2ex]\n& =\\frac{1}{nh}\\sum_{i=1}^n K\\left(\\frac{X_{i}-\\bar{x}_j}{h}\\right),\n\\end{align*}\n\\] where \\(K(z)\\) denotes the symmetric \\((K(z)=K(-z))\\) kernel function \\[\nK(z)=\\left\\{\n\\begin{array}{ll}\n1/2 & \\hbox{ if } z\\in (-1,1] \\\\\n0   & \\hbox{ else}.\n\\end{array}\\right.\n\\]\nA kernel density estimator generalizes the histogram by estimating the unknown density \\(f\\) not only at the mid-points \\(\\bar{x}_j,\\) but at every \\(x,\\) which yields the moving histogram \\[\n\\begin{align*}\n{\\hat{f}}_{nh}(x)=\\frac{1}{nh}\\sum_{i=1}^nK\\left(\n\\frac{x-X_{i}}{h}\\right)\n\\end{align*}\n\\tag{4.2}\\] where\n\n\\(K\\) denotes the kernel function and\n\n\\(h&gt;0\\) the bandwidth",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#a-more-formal-motivation-of-the-kernel-density-estimator",
    "href": "Ch4_NP_Density_Estimation.html#a-more-formal-motivation-of-the-kernel-density-estimator",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.2 A More Formal Motivation of the Kernel Density Estimator",
    "text": "4.2 A More Formal Motivation of the Kernel Density Estimator\nLet \\[\n\\{X_1, \\ldots, X_n\\}\n\\] denote a random sample with \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\quad\\text{and}\\quad X\\sim f.\n\\]\nAim: Find a density estimator \\[\n\\hat{f}\n\\] for the true, unknown density \\(f\\) without making a parametric assumption on \\(f\\) such as assuming that \\(f\\) is the density of a normal distribution with unknown mean and unknown variance.\nThe only assumption on \\(f\\) is the following qualitative smoothness assumption: The density function \\(f(x)\\) is assumed to be sufficiently smooth—i.e., to be sufficiently often differentiable for all \\(x.\\)\nStarting point: We use the connection between density functions \\(f\\) and distribution functions \\(F(x) = P(X \\leq x);\\) namely,\n\\[\n\\begin{equation*}\n  f(x) = \\frac{d}{dx} F(x) = F'(x), \\qquad x \\in \\mathbb{R}.\n\\end{equation*}\n\\]\nIdea: Approximate the derivative of the distribution function using the difference quotient. For small \\(h &gt; 0,\\) we have that \\[\n\\begin{align*}\nf(x) =   F'(x) &\\approx \\frac{F(x+h) - F(x)}{h}%\\\\[2ex]\n%  F'(x) &= \\frac{F(x+h) - F(x)}{h} + O(h)\n\\end{align*}\n\\tag{4.3}\\] or \\[\n\\begin{align*}\nf(x) = F'(x) &\\approx \\frac{F(x) - F(x-h)}{h}.%\\\\[2ex]\n%F'(x)      &= \\frac{F(x) - F(x-h)}{h} + O(h),\n\\end{align*}\n\\tag{4.4}\\] \nIn both the right-hand (Equation 4.3) and the left-hand difference quotient (Equation 4.4) have an approximation error of the order \\(O(h)\\) for \\(h\\to 0\\) with \\(h&gt;0.\\)\n\nIn his seminal work on the kernel density estimator, Rosenblatt (1956) uses an even better derivative approximation based on the symmetric difference quotient \\[\n\\begin{equation*}\n    f(x) = F'(x) = \\frac{F(x+h) - F(x-h)}{2h} + O(h^2),\n\\end{equation*}\n\\tag{4.5}\\] where the approximation error is of the order \\(O(h^2)\\) for \\(h\\to 0\\) with \\(h&gt;0.\\) This means that the approximation error goes, in absolute values, to zero as fast as \\(h^2\\to 0\\) or faster; i.e. \\[\n\\begin{align*}\n& O\\big(h^2\\big)=\\\\[2ex]\n=&\\left\\{\\text{Any function}\\;g(h)\\text{ such that }\\frac{|g(h)|}{h^2}\\to c\\;\\text{ as }\\;h\\to 0\\text{, where }0\\leq c&lt;\\infty\\right\\}.\n\\end{align*}\n\\]\n\n\n\n\n\n\nDeriving the approximation error in Equation 4.5\n\n\n\n\n\nLet \\(F\\) be three times continuously differentiable; i.e. \\(F^{(3)}(x)\\) is a continuous function for all \\(x.\\) \\[\n\\begin{align*}\n&\\frac{F(x+h) - F(x-h)}{2h}=\\\\[2ex]\n&[\\text{Taylor polynomial approximations for $F(x+h)$ and $F(x-h)$ around $x\\colon$}]\\\\[2ex]\n=&\\frac{\\left(F(x)+F^{(1)}(x)h + \\frac{1}{2}\\,F^{(2)}(x) h^2 +\\frac{1}{6}\\,F^{(3)}(x)h^3 + o(h^3)\\right)}{2h}-\\\\[2ex]\n&\\frac{\\left(F(x)-F^{(1)}(x)h+ \\frac{1}{2}\\,F^{(2)}(x) h^2-\\frac{1}{6}\\,F^{(3)}(x)h^3+o(h^3)\\right)}{2h}\\\\[2ex]\n&[\\text{Applying straight forward simplifications:}]\\\\[2ex]\n=&\\frac{2\\,F^{(1)}(x)h+\\frac{2}{6}\\,F^{(3)}(x)h^3+o(h^3)}{2h}\\\\[2ex]\n=&\\frac{2\\,F^{(1)}(x)h}{2h}+\\frac{\\frac{1}{3}\\,F^{(3)}(x)h^3}{2h}+\\frac{o(h^3)}{2h}\\\\[2ex]\n&[\\text{Using that $h^{-1}o(h^3)=o(h^2)\\colon$}]\\\\[2ex]\n=&F'(x)\n+\\frac{1}{6}\\,F^{(3)}(x)h^2\n+\\frac{1}{2}\\,o(h^2)\\\\[2ex]\n&[\\text{Using that $\\texttt{constant}\\times h^2=O(h^2)$ and that $\\texttt{constant}\\times o(h^2)=o(h^2)\\colon$}]\\\\[2ex]\n=&F'(x)+O(h^2) + o(h^2)\\\\[2ex]\n&[\\text{Using that $o(h^2)=O(h^2)$ and that $2O(h^2)=O(h^2)\\colon$}]\\\\[2ex]\n=&F'(x)+O(h^2)\n\\end{align*}\n\\]\n\n\n\nUsing the definition of the distribution function \\(F(x)=P(X\\leq x)\\) we get \\[\n\\begin{align*}\n    f(x) & = \\frac{F(x+h) - F(x-h)}{2h} + O(h^2) \\\\[2ex]\n    & = \\frac{P(X \\leq x+h) - P(X \\leq x-h)}{2h} + O(h^2)\\\\[2ex]\n    & =\\frac{1}{2h} P(x-h &lt; X \\leq x+h) + O(h^2)\\\\[2ex]\n\\Rightarrow\\quad        f(x) & \\approx \\frac{1}{2h} P(x-h &lt; X \\leq x+h) ,\n\\end{align*}\n\\] where the approximation is very good for smallish \\(h&gt;0.\\)\nEstimating the unknown probability \\(P(x-h &lt; X \\leq x+h)\\) by its empirical counterpart (relative frequency) yields the moving histogram estimator as in Equation 4.2: \\[\n\\begin{align*}\n\\hat{f}_{nh}(x)\n&=\\frac{1}{2h}\\hat{P}\\left(x-h &lt; X \\leq x+h\\right)\\\\[2ex]\n&=\\frac{1}{2h}\\frac{\\text{Number of $X_i$ in $(x-h, x+h]$}}{n}\\\\[2ex]\n&=\\frac{1}{nh}\\sum_{i=1}^n\\frac{1}{2}1_{\\left(x-h &lt; X_i\\leq x+h\\right)}\\\\[2ex]\n&=\\frac{1}{nh}\\sum_{i=1}^n\\frac{1}{2}1_{\\left(-1&lt;\\left(\\frac{X_i-x}{h}\\right)\\leq 1\\right)}\\\\[2ex]\n&=\\frac{1}{nh}\\sum_{i=1}^nK\\left(\\frac{x-X_{i}}{h}\\right)\n\\end{align*}\n\\] where \\(K(z)\\) denotes the symmetric \\((K(z)=K(-z))\\) kernel function \\[\nK(z)=\\left\\{\n\\begin{array}{ll}\n1/2 & \\hbox{ if } z\\in (-1,1] \\\\\n0   & \\hbox{ else}.\n\\end{array}\\right.\n\\]\n\nmy_movhist &lt;- function(x, data, h){\n      n      &lt;- length(data)\n      dat_x  &lt;- data[data &gt; x-h & data &lt;= x+h]\n      result &lt;- length(dat_x)/(n*2*h)\n      return(result)\n}\nmy_movhist &lt;- Vectorize(my_movhist, \"x\")\n\n\ndata  &lt;- inc76$income\nh     &lt;- 12  # bandwidth\nxx    &lt;- seq(from=(min(data)), to=(max(data)), len=750)\nyy    &lt;- my_movhist(x = xx, data = data, h = h)                \n\nplot(x = xx, y = yy, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Density Estimation using the Moving Histogram\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.4: Income density estimate based on the moving histogram density estimator.\n\n\n\n\n\nDensity estimates of the moving histrogram estimator are discontinuous—even though, we aim to estimate a smooth (e.g. three times continuously differentiable) denstiy function \\(f.\\)\nKernel Density Estimator:\nReplacing the symmetric, discontinuous rectengular kernel function of the naive moving histogram by a symmetric, continuous and differentiable function, such as the Epanechnikov kernel: \\[\nK(z) =\\left\\{\n\\begin{array}{ll}\n\\frac{3}{4}(1-z^2)&\\text{ for } z \\in [-1,1]\\\\\n0&\\text{ else}\n\\end{array}\n\\right.\n\\] or the density function of the standard normal distribution: \\[\nK(z) = \\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp( -z^2 / 2 )\n\\] yields to the kernel density estimator with kernel \\(K\\) and bandwidth \\(h\\colon\\) \\[\n\\begin{equation*}\n  \\hat{f}_{nh}(x)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h} K\\left(\\frac{x - X_i}{h} \\right),\n  \\quad\\text{for}\\quad x \\in \\mathbb{R}.\n\\end{equation*}\n\\]\nThe kernel density estimator yields a smooth density estimate. It is more efficient than the (moving) histrogram and provides a flexible adjustment to the data.\nBandwidth \\(h\\) is also called smoothing parameter. The bandwidth (and the kernel \\(k\\)) need to be selected by the user (or autmatically by the computer).\nOften, we use a more compact notation for the scaled kernel function: \\[\nK_h(u) := K(u / h) / h\n\\] such that \\[\n\\begin{align*}\n\\hat{f}_{nh}(x)\n&=  n^{-1} \\sum_{i=1}^{n} \\frac{1}{h}  K\\left(\\frac{x - X_i}{h}\\right)\\\\[2ex]\n&=  n^{-1} \\sum_{i=1}^{n} K_h(x - X_i)\n\\end{align*}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\n\n\\(\\hat{f}_{nh}:\\) the index \\(nh\\) means here that the estimator depends on the sample size \\(n\\) and the bandwidth \\(h\\).\n\\(K_h:\\) the index \\(h\\) is here an abbreviation for \\(K( \\cdot / h) / h\\).\n\n\n\nFigure 4.5 shows the construction of the kernel density estimator which uses a mixture of the chosen kernel function \\(K\\) (here the Biweight kernel) centered at the observed data \\(X_1,\\dots,X_n\\) for estimating the unknown density \\(f.\\)\n\n\n\n\n\n\n\n\nFigure 4.5: The kernel density estimator uses a mixture of kernel functions for estimating the unknown density \\(f.\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#properties-of-the-kernel-density-estimator",
    "href": "Ch4_NP_Density_Estimation.html#properties-of-the-kernel-density-estimator",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.3 Properties of the Kernel Density Estimator",
    "text": "4.3 Properties of the Kernel Density Estimator\n\n4.3.1 Choice of the Kernel\n\n\n\n\n\n\nReminder: Definition of a Density Function\n\n\n\nWe call a function \\(f\\) a density function if it fulfilles the following properties:\n\nNon-negative: \\(f(x)\\geq 0\\) for all \\(x\\in\\mathbb{R}\\)\nNormed to 1: \\(\\int f(x)dx = 1\\)\n\n\n\nIt can be shown that if \\(K\\) is a density function, then also \\(\\hat{f}_{nh}\\) is a density function:\n\nNon-negative: \\[\nK(x) \\geq 0\\quad\\text{for all}\\quad x\\in\\mathbb{R}\\quad \\Rightarrow\\quad \\hat{f}_{nh}(x) \\geq 0\\quad\\text{for all}\\quad x\\in\\mathbb{R}\n\\]\nNormed to 1: \\[\n\\int K(x)dx = 1\\quad \\Rightarrow\\quad \\int \\hat{f}_{nh}(x)dx = 1\n\\]\n\n(See Exercises.)\nThis inheritance of the properties of \\(K\\) to the properties of \\(f_{nh}\\) also holds for the smoothness properties:\n\nSmoothness of \\(\\boldsymbol{K}\\): If \\(K\\) is continuously differentiable, then also \\(\\hat{f}_{nh}\\) is continuously differentiable.\n\nTypical kernel functions are smooth (continuous) density functions that are symmetric around zero.\n\nSymmetric around zero: \\[\nK(x)=K(-x)\\quad\\text{such that}\\quad \\int xK(x)dx=0\n\\]\n\nExamples:\n\nThe family of the symmetric beta density functions; for \\(p = 0,1,2,\\ldots\\) \\[\nK(u; p) =\\left\\{\n  \\begin{array}{ll}\n  \\mathrm{Const}_p \\left(1 - u^2 \\right)^p&\\text{for }u \\in [-1,1]\\\\\n  0 &\\text{else}\n  \\end{array}\\right.,\n\\] where the \\(p\\)-specific constant is chosen such that the kernel integrates to one.\n\n\\(p=0\\) yields the uniform kernel: \\[\n\\mathrm{Const}_p \\left(1 - u^2 \\right)^p = \\frac{1}{2}\n\\]\n\\(p=1\\) yields the Epanechnikov kernel: \\[\n   \\mathrm{Const}_p \\left(1 - u^2 \\right)^p = \\frac{3}{4}\\,(1-u^2)\n   \\]\n\\(p=2\\) yields the biweight kernel: \\[\n   \\mathrm{Const}_p \\left(1 - u^2 \\right)^p = \\frac{15}{16}\\,(1-u^2)^2\n   \\]\n\\(p=3\\) yields the triweight kernel: \\[\n   \\mathrm{Const}_p \\left(1 - u^2 \\right)^p = \\frac{35}{32}\\,(1-u^2)^3\n   \\]\n\nNormal (Gaussian) kernel: \\[\nK(u) = \\phi(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2 / 2)\n\\] for \\(u \\in\\mathbb{R}.\\)\nTriangular kernel: \\[\nK(u) = 1-|u|\n\\] for \\(u\\in[-1,1]\\) and \\(0\\) else.\n\n\n\n4.3.2 Choice of the Bandwidth\nUsing a too small bandwidth (e.g. here \\(h=\\) 2.14):\n\ndata  &lt;- inc76$income\nh     &lt;- bw.SJ(inc76$income) * .25  # too small bandwidth\nKDE   &lt;- density(data, bw = h, from = min(data), to = max(data))\n\nplot(x = KDE$x, y = KDE$y, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Kernels Density Estimtion With a too Small Bandwidth\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.6: Kernel density estimate with a too small bandwidth.\n\n\n\n\n\nToo a too large bandwidth (e.g. here \\(h=\\) 47):\n\ndata  &lt;- inc76$income\nh     &lt;- bw.SJ(inc76$income) * 2.5  # too large bandwidth\nKDE   &lt;- density(data, bw = h, from = min(data), to = max(data))\n\nplot(x = KDE$x, y = KDE$y, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Kernels Density Estimtion With a Too Large Bandwidth\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.7: Kernel density estimate with a too large bandwidth.\n\n\n\n\n\n\nUsing a well estimated bandwidth (e.g. here \\(h=\\) 8.55) based on the method proposed by Sheather and Jones (1991):\n\ndata  &lt;- inc76$income\nh     &lt;- bw.SJ(inc76$income) # good bandwidth\nKDE   &lt;- density(data, bw = h, from = min(data), to = max(data))\n\nplot(x = KDE$x, y = KDE$y, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Kernels Density Estimtion With a Good Bandwidth Choice\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.8: Kernel density estimate with a good bandwidth choice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html",
    "href": "Ch5_NPRegression.html",
    "title": "5  Nonparametric Regression",
    "section": "",
    "text": "5.1 Introduction\nLet us consider the case of univariate nonparametric regression, i.e., with one single explanatory variable \\(X\\in\\mathbb{R}\\).\nData: \\[\n(Y_{1},X_{1}),\\dots,(Y_{n},X_{n})\\overset{\\text{i.i.d}}{\\sim}(Y,X)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html#introduction",
    "href": "Ch5_NPRegression.html#introduction",
    "title": "5  Nonparametric Regression",
    "section": "",
    "text": "\\(Y_{i}\\in \\mathbb{R}\\) real response variable\n\\(X_{i}\\in [a,b]\\subset \\mathbb{R}\\) real explanatory variable\n\\(n\\) sufficiently large sample size (e.g., \\(n\\geq 40\\))\n\n\nThe Nonparametric Regression Model\n\\[\nY_i=m(X_i)+\\varepsilon_i\n\\]\n\n\\(m(X)=\\mathbb{E}(Y_i|X=X_i)\\) regression function\n\\(\\mathbb{E}(\\varepsilon_i)=0\\)\n\\(Var(\\varepsilon_i|X_i) = Var(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) and \\(X_i\\) are independent or at least mean-independent, i.e., \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\)\n\nSpecial cases of parametric regression models:\n\nLinear regression: \\(m\\) is a straight line \\[\nm(X)=\\beta_0+\\beta_1 X\n\\]\nPolynomial generalizations: \\(m\\) is a quadratic or cubic polynomial \\[\n\\begin{align*}\n              m(X)&=\\beta_0 +\\beta_1 X+\\beta_2 X^2\\\\\n\\text{or} \\quad m(X)&=\\beta_0+\\beta_1 X+\\beta_2 X^2+\\beta_3 X^3\n\\end{align*}\n\\]\n\nMany important applications lead to regression functions possessing a complicated structure. Standard models then are “too simple” and do not provide useful approximations of \\(m(x)\\).\n\n\n\n\n\n\nAs George Box is saying it:\n\n\n\n“All models are false, but some are useful” (G. Box)\n\n\nAn important point in theoretical analysis is the way how the observations \\(X_1,\\dots,X_n\\) have been generated. One distinguishes between fixed and random design.\n\nFixed design: The observation points \\(X_1,\\dots,X_n\\) are fixed (non stochastic) values.\n\nExample: Crop yield (\\(Y\\)) in dependence of the amount of fertilizer (\\(X\\)) used, when the amount is determined deterministically by the experimenter.\nEquidistant Design: (Most important special case of fixed design)\n\\[\nX_{i+1}-X_i=\\frac{b-a}{n}.\n\\]\n\nRandom design: The observation points \\(X_1,\\dots,X_n\\) are (realizations of) i.i.d. random variables with density \\(f\\). The density \\(f\\) is called “design density”. Throughout this chapter it will be assumed that \\(f(x)&gt;0\\) for all \\(x\\in [a,b]\\).\n\nExample: Sample \\((Y_1,X_1),\\dots,(Y_n,X_n)\\) of log-wages (\\(Y\\)) and age (\\(X\\)) of randomly selected individuals.\n\n\nIn the case of random design, \\(m(x)\\) is the conditional expectation of \\(Y\\) given \\(X=x\\), \\[\nm(x)=\\mathbb{E}(Y|\\ X=x)\n\\]\nand \\(Var(\\varepsilon_i|X_i)=\\sigma^2\\).\n\n\n\n\n\n\nNote\n\n\n\nFor random design all expectations (as well as variances) have to be interpreted as conditional expectations (variances) given \\(X_1,\\dots,X_n\\).\n\n\nExample: Canadian cross-section wage data consisting of a random sample taken from the 1971 Canadian Census Public Use Tapes for male individuals having common education (grade 13); see Figure 5.1.\n\nsuppressPackageStartupMessages(library(\"np\"))\ndata(\"cps71\")\nplot(cps71$age, cps71$logwage, xlab=\"Age\", ylab=\"log(wage)\")\n\n\n\n\n\n\n\nFigure 5.1: Canadian cross-section wage data.\n\n\n\n\n\nWhat would be a good/reasonable model assumption for \\(m(x)\\) to estimate the conditional mean function for the data shown in Figure 5.1?\n\n\n\n\n\n\nNo specific model assumption\n\n\n\nIn nonparametric regression analysis, we do not make assumptions about the specific structure of the regression function \\(m(x).\\) We only make the qualitative assumption that \\(m\\) is a sufficiently smooth function, i.e. that \\(m(t)\\) is sufficiently often differentiable at every \\(t\\in(a,b)\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html#basis-function-expansions",
    "href": "Ch5_NPRegression.html#basis-function-expansions",
    "title": "5  Nonparametric Regression",
    "section": "5.2 Basis Function Expansions",
    "text": "5.2 Basis Function Expansions\nSome frequently used approaches to nonparametric regression rely on expansions of the form \\[\nm(x)\\approx \\sum_{j=1}^p \\beta_j b_j(x),\n\\] where \\(b_1(x),b_2(x),\\dots\\) are suitable basis functions \\[\nb_j:\\mathbb{R}\\to\\mathbb{R},\\quad j=1,\\dots,p.\n\\]\nThe basis functions \\(b_1,b_2,\\dots\\) have to be chosen in such a way that for any possible smooth function \\(m\\) the squared approximation error tends to zero as \\(p\\rightarrow\\infty,\\)\n\\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\left(m(x)-\\sum_{j=1}^p \\vartheta_j b_j(x)\\right)^2\\to 0,\\quad p\\to\\infty.\n\\] However, for every fixed value \\(p,\\) we typically have that \\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\left(m(x)-\\sum_{j=1}^p \\vartheta_j b_j(x)\\right)^2 \\neq 0\n\\] which leads to a biased estimation procedure.\nFor a fixed value \\(p,\\) an estimator \\(\\hat m_p\\) of \\(m\\) is determined by \\[\n\\hat m_p(x)=\\sum_{j=1}^p \\hat\\beta_j b_j(x),\n\\] where the coefficients \\(\\hat\\beta_j\\) are obtained by ordinary least squares \\[\n\\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\hat\\beta_j \\underbrace{b_j(X_i)}_{X_{ij}}\\right)^2\n=\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\vartheta_j \\underbrace{b_j(X_i)}_{X_{ij}}\\right)^2\n\\] with \\[\n\\hat\\beta = \\left(\\begin{matrix}\\hat\\beta_1\\\\ \\vdots\\\\\\hat\\beta_p\\end{matrix}\\right)=\\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top Y,\n\\] where \\(\\mathbf{X}\\) denotes the \\((n\\times p)\\)-dimensional matrix with elements \\[\nX_{ij}=b_j(X_i),\n\\] \\(i=1,\\dots,n\\) and \\(j=1,\\dots,p,\\) and \\(Y=(Y_1,\\dots,Y_n)^\\top.\\)\nExamples of basis functions \\(b_1(x),\\,b_2(x)\\,\\dots\\):\n\npolynomials (monomial basis)\nspline functions\nwavelets\nFourier expansions (for periodic functions)\n\n\n5.2.1 Polynomial Regression\nTheoretical Justification: Every smooth function can be well approximated by a polynomial of sufficiently high degree.\nApproach:\n\nMonomial basis functions \\(b_j(x) = x^{j-1}\\), \\(\\;\\;j=1,\\dots,p.\\)\nChoose \\(p\\) and fit a polynomial of degree \\(p-1\\) (order \\(p\\)): \\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p}\\sum_{i=1}^n \\left(Y_i-\\sum_{j=1}^p \\vartheta_{j} X_i^{j-1}\\right)^2\n\\] \\[\n\\Rightarrow\\quad {\\hat m}_p(X)={\\hat \\beta}_{1}+\\sum_{j=2}^{p}\n{\\hat \\beta}_{j} X_i^{j-1}\n\\]\nThis corresponds to an approximation with basis functions \\[\n\\begin{align*}\nb_1(x)&=1\\\\[2ex]\nb_2(x)&=x\\\\[2ex]\nb_3(x)&=x^2\\\\[2ex]\n\\vdots\\\\[2ex]\nb_{p}(x)&=x^{p-1}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIt is only assumed that \\(m(x)\\) can be approximated by a polynomial \\(\\hat{m}_p(x)\\) as \\(p\\to\\infty.\\) However, for a given choice of \\(p,\\) there will usually still exist an approximation error.\nTherefore, \\(\\hat{m}_p(x)\\) is typically a biased estimator for given values of \\(p,\\) and a given value of \\(x\\in[a,b],\\) \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat{m}_p(x))\\quad & \\neq 0\\\\\n\\mathbb{E}(\\hat{m}_p(x)) - m(x)        & \\neq 0.\n\\end{align*}\n\\]\n\n\nR-Code to Compute Polynomial Regressions:\nGenerate some artificial data, where the usually unknown \\[\nm(x)=\\sin(5 x)\n\\] with \\(x\\in[0,1]\\):\n\nset.seed(1)\n# Generate some data: \nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n\n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\n# Save all in a dataframe\ndb     &lt;-  data.frame(x=x_vec,y=y_vec)\n\nCompute the ordinary least squares regressions of different polynomial regression models:\n\n# Fitting of polynomials to the data (parametric models):\n# Constant line fit: (Basis function x^0)\nreg_p1 &lt;- lm(y ~ 1, data=db)\n\n# Basis functions: x^0 + ... + x^3\nreg_p4 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)\n\n# Basis functions: x^0 + ... + x^6\nreg_p7 &lt;- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)\n\nTake a look at the fits:\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(db, main=\"Truth\")\n# True (usually unknown) regression function\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n\n## Fit by degree 0 polynomial\nplot(db, main=\"Degree 0\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p1, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n## Fit by degree 3 polynomial\nplot(db, main=\"Degree 3\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p4, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n## Fit by degree 6 polynomial\nplot(db, main=\"Degree 6\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p7, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n\n\n\n\n\n\n\nThe quality of the approximation obviously depends on the choice of the model selection parameter \\(p\\) which serves as a smoothing parameter.\nLet’s look at the fits across 200 Monte Carlo replications:\n\nm_true    &lt;- sin(x_vec * 5)\n\nn_MCrepl  &lt;- 200 # MC-replications\n\nm_hat_p1  &lt;- matrix(NA, n, n_MCrepl)\nm_hat_p4  &lt;- matrix(NA, n, n_MCrepl)\nm_hat_p7  &lt;- matrix(NA, n, n_MCrepl)\n\nfor(r in 1:n_MCrepl){\n    # Generate some data: \n    e_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n    y_vec  &lt;-  sin(x_vec * 5) + e_vec\n    db     &lt;-  data.frame(x = x_vec,y = y_vec)\n    # Estimations\n    reg_p1 &lt;- lm(y ~ 1, data=db)\n    reg_p4 &lt;- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)\n    reg_p7 &lt;- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)\n    # Save predictions (y hat)\n    m_hat_p1[,r] &lt;- predict(reg_p1, newdata = db)\n    m_hat_p4[,r] &lt;- predict(reg_p4, newdata = db)\n    m_hat_p7[,r] &lt;- predict(reg_p7, newdata = db)\n}\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(db, main=\"Truth\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nsubSelect &lt;- 25\nmatplot(y = m_hat_p1[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",  \n        ylim = range(m_hat_p1[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=0.5, main = \"Degree p=0\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nmatplot(y = m_hat_p4[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",\n        ylim = range(m_hat_p4[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=.5, main = \"Degree p=3\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nmatplot(y = m_hat_p7[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",\n        ylim = range(m_hat_p7[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=.5, main = \"Degree p=6\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\n\nUsing the 200 fits from above, we can approximate the\n\nsquared bias \\(\\left(\\operatorname{Bias}(\\hat{m}_p(x))\\right)^2\\) and the\nvariance \\(Var(\\hat{m}_p(x))\\)\n\npoint-wise at each \\(x\\)-value in x_vec.\n\n## Pointwise (for each x) biases of \\hat{m}(x): \nPt_Bias_p1 &lt;- rowMeans(m_hat_p1) - m_true\nPt_Bias_p4 &lt;- rowMeans(m_hat_p4) - m_true\nPt_Bias_p7 &lt;- rowMeans(m_hat_p7) - m_true\n\n## Pointwise squared biases\nPt_BiasSq_p1 &lt;- Pt_Bias_p1^2\nPt_BiasSq_p4 &lt;- Pt_Bias_p4^2\nPt_BiasSq_p7 &lt;- Pt_Bias_p7^2\n\n## Pointwise (for each x) variances \\hat{m}(x):\nPt_Var_p1  &lt;- apply(m_hat_p1, 1, var)\nPt_Var_p4  &lt;- apply(m_hat_p4, 1, var)\nPt_Var_p7  &lt;- apply(m_hat_p7, 1, var)\n\npar(mfrow=c(1,2))\nmatplot(y = cbind(Pt_BiasSq_p1, Pt_BiasSq_p4, Pt_BiasSq_p7), \n        x = x_vec, type = \"l\", col = c(1,2,\"darkgreen\"), \n        main = \"Pointwise Squared Bias\", ylab=\"\", xlab=\"x\")\nlegend(\"topleft\", col = c(1,2,\"darkgreen\"), lty = c(1,2,3), \nlegend = c(\"Degree p=0\", \"Degree p=3\", \"Degree p=6\"))\nmatplot(y = cbind(Pt_Var_p1, Pt_Var_p4, Pt_Var_p7), \n        x = x_vec, type = \"l\", col = c(1,2,\"darkgreen\"), \n        main = \"Pointwise Variance\", ylab=\"\", xlab=\"x\")\nlegend(\"top\", col = c(1,2,\"darkgreen\"), lty = c(1,2,3), \nlegend = c(\"Degree p=0\", \"Degree p=3\", \"Degree p=6\"))\n\n\n\n\n\n\n\n\nThe Bias-Variance Trade-Off:\n\n\\(p\\) small: Variance of the estimator is small, but (squared) bias is large.\n\\(p\\) large: Variance of the estimator is large, but (squared) bias is small.\n\n\n\n\n\n\n\nRemark\n\n\n\nPolynomial regression is not very popular in practice. Reasons are numerical problems in fitting high dimensional polynomials. Furthermore, high order polynomials often posses an erratic, difficult to interpret behavior at the boundaries.\n\n\n\n\n5.2.2 Regression Splines\nThe practical disadvantages of global basis functions (like polynomials), explain the success of local basis functions. A frequently used system of basis functions are local polynomials, i.e., so-called spline functions.\nA spline function is a piece wise polynomial function. They are defined with respect to a pre-specified sequence of \\(q\\) knots \\[\na=\\tau_1&lt;\\tau_2\\leq\\dots\\leq \\tau_{q-1}&lt;\\tau_q=b.\n\\] Different specifications of the knot sequence lead to different splines.\nMore precisely, for a given knot sequence, a spline function \\(s(x)\\) of degree \\(k\\) is defined by the following properties:\n\n\\(s(x)\\) is a polynomial of degree \\(k\\) (i.e. of order1 \\(k+1\\)) in every interval \\([\\tau_j,\\tau_{j+1}]\\), i.e. \\[\ns(x)=s_0+s_1x+s_2x^2+\\dots+s_kx^{k},\\quad x\\in[\\tau_j,\\tau_{j+1}]\n\\] with \\(s_0,\\dots,s_k\\in\\mathbb{R}.\\)\n\n\\(s(x)\\) is called a linear spline if \\(k=1\\)\n\\(s(x)\\) is a quadratic spline if \\(k=2\\)\n\\(s(x)\\) is a cubic spline if \\(k=3\\)\n\n\\(s(x)\\) is \\(k-1\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\nIn practice, the most frequently used splines are cubic spline functions based on an equidistant sequence of \\(q\\) knots, i.e., \\[\n\\tau_{j+1}-\\tau_j=\\tau_j-\\tau_{j-1}\n\\] for all \\(j=2,\\dots,q-1.\\)\nThe space of all spline functions of degree \\(k\\) defined with respect to a given knot sequence \\[\na=\\tau_1&lt;\\tau_2\\leq\\dots\\leq \\tau_{q-1}&lt;\\tau_q=b\n\\] is a \\[\n\\begin{array}{lccccc}\np & = & \\text{number of knots} &+& \\text{polynomial degree} & -\\;\\;1\\\\\n& = & q & + & k & -\\;\\;1\n\\end{array}\n\\] dimensional linear function space \\[\n{\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}=\\operatorname{span}(b_{1,k},\\dots,b_{p,k}),\n\\] where \\(b_{1,k},\\dots,b_{p,k}\\) denote the basis-functions.\n\n\nConstruction of B-Spline Basis Functions\nThe so-called B-spline basis functions are almost always used in practice, since they possess a number of advantages from a numerical point of view.\nThe B-spline basis functions for splines of degree \\(k\\) are defined with respect to a given knot sequence \\[\n\\underbrace{a=\\tau_1}_{\\text{lower boundary knot}}\\quad {\\color{red}&lt;}\\quad\\overbrace{\\tau_2\\leq\\dots\\leq \\tau_{q-1}}^{\\text{interior knots}}\\quad{\\color{red}&lt;}\\quad\\underbrace{\\tau_q=b}_{\\text{upper boundary knot}}.\n\\]\nTo construct the B-spline basis functions, one augments the knot sequence by repeating each of the boundary knots \\(k+1\\) times: \\[\n\\underbrace{\\tau_{-(k-1)}=\\dots=\\tau_0=\\tau_1}_{\\text{$k+1$ lower boundary knots}}\\quad {\\color{red}&lt;}\\quad\\overbrace{\\tau_2\\leq\\dots\\leq \\tau_{q-1}}^{\\text{interior knots}}\\quad{\\color{red}&lt;}\\quad\\underbrace{\\tau_q=\\tau_{q+1}=\\dots=\\tau_{q+k}}_{\\text{$k+1$ upper boundary knots}}.\n\\] Let \\[\n\\tau^\\ast_{1}, \\dots,\\tau^\\ast_{q+2k}\n\\] denote the augmented knot sequence after resetting the index to start at \\(1.\\)\nThe spline basis functions are calculated by a recursive (over \\(l=0,1,\\dots,k\\)) procedure.\nThe initial level (\\(l=0\\)) are piece-wise constant functions \\[\nb_{j,0}(x)=\\left\\{\n\\begin{matrix}  \n1 & \\text{ if } \\tau_{j}^*\\leq x &lt;\\tau_{j+1}^*\\\\\n0 & \\text{ else}\n\\end{matrix}\n\\right.,\n\\tag{5.1}\\] for \\(j=1,\\dots,q+2k,\\) and \\(x\\in [a,b].\\)\nFor \\(l=1,\\dots,k\\) the recursion is defined by \\[\n\\begin{align*}\nb_{j,l}(x)\n& =\\frac{x-\\tau_j^*}{\\tau_{l+j}^*-\\tau_j^*}b_{j,l-1}(x)\\\\\n& +\\frac{\\tau_{l+j+1}^*-x}{\\tau_{l+j+1}^*-\\tau_{j+1}^*}b_{j+1,l-1}(x),\n\\end{align*}\n\\tag{5.2}\\] \\(j=1,\\dots,q+2k\\), and \\(x\\in [a,b].\\)\nNote: The definitions in Equation 5.1 and Equation 5.2 are understood that if the denominator is 0, then the function is defined to be 0. The remaining non-degenerated basis functions are then the \\[\nb_{j,k},\\quad j=1,\\dots,p=q+k-1,\n\\] B-spline basis functions.\n\n\nR-Code to Compute B-Spline Basis Functions:\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=1} - 1=7\n\\] linear (\\(k=1\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         &lt;- 1\nknots          &lt;- seq(from = 0, to = 1, len = 7)\ninternal_knots &lt;- knots[-c(1, length(knots))]\nboundary_knots &lt;- knots[ c(1, length(knots))]\n\n## evaluation points (for plotting)\nx_vec &lt;- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree1 &lt;- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree1, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 1\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree 1\n\n\n\nHere, the basis functions are piecewise linear (\\(k=1\\)) functions with local support over at most \\(k+1=1+1=2\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns &\\in\\mathcal{S}_{k=1,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=7)\\\\[2ex]\ns(x)&= \\sum_{j=1}^{7}\\vartheta_j b_{j,1}(0)\n\\end{align*}\n\\] is \\(k-1=1-1=0\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=2} - 1=8\n\\] quadratic (\\(k=2\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         &lt;- 2\nknots          &lt;- seq(from = 0, to = 1, len = 7)\ninternal_knots &lt;- knots[-c(1, length(knots))]\nboundary_knots &lt;- knots[ c(1, length(knots))]\n\n## evaluation points (for plotting)\nx_vec &lt;- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree2 &lt;- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree2, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 2\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree 2\n\n\n\nHere, the basis functions are piecewise quadratic (\\(k=2\\)) functions with local support over at most \\(k+1=2+1=3\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns&\\in\\mathcal{S}_{k=2,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=7)\\\\[2ex]\ns(x)&= \\sum_{j=1}^{8}\\vartheta_j b_{j,1}(x)\n\\end{align*}\n\\] is \\(k-1=2-1=1\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=3} - 1=9\n\\] cubic (\\(k=3\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         &lt;- 3\nknots          &lt;- seq(from = 0, to = 1, len = 7)\ninternal_knots &lt;- knots[-c(1, length(knots))]\nboundary_knots &lt;- knots[ c(1, length(knots))]\n\n\n## evaluation points (for plotting)\nx_vec &lt;- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree3 &lt;- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree3, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 3\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree 3 (usual case)\n\n\n\nHere, the basis functions are piecewise cubic (\\(k=3\\)) functions with local support over at most \\(k+1=3+1=4\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns &\\in\\mathcal{S}_{k=3,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=7)\\\\[2ex]\ns(x) &= \\sum_{j=1}^{9} \\vartheta_j b_{j,1}(x)\n\\end{align*}\n\\] is \\(k-1=3-1=2\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nNormalized: The B-spline basis system has a property that is often useful: the sum of the B-spline basis function values at any point \\(x\\) is equal to one. Note, for example, that the first and last basis functions are exactly one at the boundaries. This is because all the other basis functions go to zero at these end points.\nCompact Support: Basis functions are positive only over at most \\(k+1\\) intervals and zero over the remaining intervals. This compact support property is important for computational efficiency. \nMultiple knots: A multiple interior knot (\\(\\tau_j=\\tau_{j+1}\\)) reduces the degree of continuity at that knot value. At a normal interior knot, a spline function is \\(k-1\\) times continuously differentiable. Each extra knot with the same value reduces continuity at that knot by one. This is the only way to reduce the continuity of the curve at the knot values. If there are \\(k\\) (or more) equal knots, then you get a discontinuity in the curve at this knot-location.\n\n\n\n\n5.2.2.1 Regression Splines with Equidistant Knots\nRemember the nonparametric regression model setup:\n\\[\nY_i=m(X_i)+\\varepsilon_i\n\\]\n\n\\(m(X)=\\mathbb{E}(Y_i|X=X_i)\\) regression function\n\\(\\mathbb{E}(\\varepsilon_i)=0\\)\n\\(Var(\\varepsilon_i|X_i) = Var(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) and \\(X_i\\) are independent or at least mean-independent, i.e., \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\)\n\nThe so-called regression spline (or B-spline) approach to estimating a regression function \\(m(x)\\) is based on fitting a set of spline basis functions to the data.\nTypically, cubic splines (\\(k=3\\)) with equidistant knots are applied:\n\n\\(k=3\\) (cubic splines)\n\\(\\tau_1=a\\)\n\\(\\tau_{j+1}=\\tau_j + (b-a)/(q-1),\\quad j=1,\\dots,q-1\\)\n\nsuch that \\(\\tau_q=b\\)\n\nIn this case the number of knots \\(q,\\) or more precisely the total number of basis functions \\(p\\) \\[\n\\begin{align*}\np\n&=q+k-1\\\\[2ex]\n&=q+2\\qquad (\\text{using that}\\quad k=3)\n\\end{align*}\n\\] serves as the smoothing parameter which has to be selected by the statistician.\nFor a given choice of \\(p,\\) let \\[\n\\underset{(n\\times p)}{\\mathbf{X}}\n\\] denote the \\(n\\times p\\) matrix with elements \\[\nX_{ij}=b_{j,k}(X_i),\\quad i=1,\\dots,n,\\quad j=1,\\dots,p,\n\\] and let \\[\nY=(Y_1,\\dots,Y_n)^\\top\n\\] denote the vector of response variables.\nAn estimator \\(\\hat{m}_p(x)\\) of \\(m(x)\\) is then given by \\[\n\\hat m_p(x)=\\sum_{j=1}^p \\hat\\beta_j b_{j,k}(x),\n\\] where the coefficients \\(\\hat\\beta_j\\) are determined by ordinary least squares \\[\n\\hat\\beta_1,\\dots,\\hat\\beta_p=\\arg\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\vartheta_j \\underbrace{b_{j,k}(X_i)}_{X_{ij}}\\right)^2.\n\\] That is, the vector of coefficients \\[\n\\hat \\beta=(\\hat\\beta_1,\\dots,\\hat\\beta_p)^\\top\n\\] can be written as \\[\n\\hat\\beta=(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top Y.\n\\] The fitted values are given by \\[\n\\left(\\begin{array}{c}\n{\\hat m}_p(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ {\\hat m}_p(X_n)\n\\end{array}\\right)=\\mathbf{X}\\hat\\beta=\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top}_{=:S_p}Y = S_p Y\n\\]\nThe matrix \\[\nS_p = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n\\] is referred to as the smoothing matrix.\n\n\n\n\n\n\nRemark\n\n\n\nQuite generally, the most important nonparametric regression procedures are linear smoothing methods. This means that in dependence of some smoothing parameter (here \\(p\\)), estimates of the vector \\[\n(m(X_1),\\dots,m(X_n))^\\top\n\\] are obtained by multiplying a smoother matrix \\(S_p\\) with \\(Y\\).\nThat is, \\[\n\\left(\\begin{array}{c}\nm(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ m(X_n)\n\\end{array}\\right)\\approx\n\\left(\\begin{array}{c}\n{\\hat m}_p(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ {\\hat m}_p(X_n)\n\\end{array}\\right)=S_p Y\n\\]\n\n\nR Code to Compute Regression Splines:\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\nThen, we generate cubic B-spline basis functions with equidistant knot sequence (different to x_vec) and evaluate them at x_vec:\n\ndegree      &lt;- 3 # piecewise cubic splines\n\nknot_seq_5  &lt;- seq(from = 0, to = 1, len = 5)# knots\n\nX_mat_p7    &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq_5[-c(1, length(knot_seq_5))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq_5[ c(1, length(knot_seq_5))]\n    )\n\nknot_seq_15  &lt;- seq(from = 0, to = 1, len = 15)# knots\n\nX_mat_p17    &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq_15[-c(1, length(knot_seq_15))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq_15[ c(1, length(knot_seq_15))]\n    )    \n\nComputing the smoothing matrices \\(S_p\\) for \\(p=7\\) and \\(p=17\\):\n\nS_p7  &lt;- X_mat_p7  %*% solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) \nS_p17 &lt;- X_mat_p17 %*% solve(t(X_mat_p17) %*% X_mat_p17) %*% t(X_mat_p17) \n\nComputing the estimates \\(\\hat{m}_p(X_1),\\dots,\\hat{m}_p(X_n)\\) for \\(p=7\\) and \\(p=17\\):\n\nm_hat_p7  &lt;- S_p7  %*% y_vec\nm_hat_p17 &lt;- S_p17 %*% y_vec\n\nPlotting the estimation results:\n\nplot(y=y_vec, x=x_vec, xlab=\"X\", ylab=\"Y\", \n     main=\"Regression Splines\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"red\", lty=2, lwd=1.5)\nlines(y=m_hat_p7, x=x_vec, col=\"blue\", lwd=1.5)\nlines(y=m_hat_p17, x=x_vec, col=\"darkorange\", lwd=1.5)\nlegend(\"bottomleft\", \n       c(\"(Unknown) Regression Function m\", \n         \"Regr.-Spline Fit with p=7\", \n         \"Regr.-Spline Fit with p=17\"), \n       col=c(\"red\",\"blue\", \"darkorange\"), \n       lty=c(2,1,1), lwd=c(2,2,2))\n\n\n\n\n\n\n\n\nThe following plot shows the regression spline fit (with \\(p=7\\)) \\[\n\\hat{m}_{7}(x)=\\sum_{j=1}^{7}\\hat\\beta_j b_{j,3}(x)\n\\] along with the \\(p=7\\) basis functions each multiplied by the fitted linear coefficient \\[\n\\hat\\beta_j b_{j,3}(x),\\quad j=1,\\dots,7.\n\\]\n\nbeta_hat_p7 &lt;- solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) %*% y_vec\nplot(y=m_hat_p7, x=x_vec, ylim = c(-3.5,1.5),\n     col=\"blue\", lwd=1.5, type = \"l\", xlab=\"x\", ylab=\"\")\nabline(v = knot_seq_5, col = gray(0.5))     \nmatlines(X_mat_p7 * matrix(rep(beta_hat_p7, each = n), ncol=7), \n         x = x_vec, type=\"l\", lty = 2, col = c(1,2,3,4,5,6,7))\nlegend(\"topright\", \n       c(\"Regr.-Spline Fit with p=7\"), \n       col = \"blue\", lty= 1, lwd= 2)\nlegend(\"bottomleft\", \n       c(expression(paste(\"1. basis function times \",hat(beta)[1])),\n         expression(paste(\"2. basis function times \",hat(beta)[2])),\n         expression(paste(\"3. basis function times \",hat(beta)[3])),\n         expression(paste(\"4. basis function times \",hat(beta)[4])),\n         expression(paste(\"5. basis function times \",hat(beta)[5])),\n         expression(paste(\"6. basis function times \",hat(beta)[6])),\n         expression(paste(\"7. basis function times \",hat(beta)[7]))\n       ), \n       col = c(1,2,3,4,5,6,7), lty = 2, lwd = 1)       \n\n\n\n\n\n\n\n\n\n\n\n5.2.3 Mean Average Squared Error of Regression Splines\nIn a nonparametric regression context we do not assume that the unknown true regression function \\(m(x)\\) exactly corresponds to a spline function. Thus, \\[\n\\hat m_p=(\\hat{m}_p(X_1),\\dots,\\hat{m}_p(X_n))^\\top\n\\] typically possesses a systematic estimation error (bias). That is, \\[\n\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\neq m(X_i).\n\\]\nTo simplify notation, we will in the following write \\[\n\\mathbb{E}_\\varepsilon(\\cdot)\\quad\\text{and}\\quad Var_\\varepsilon(\\cdot)\n\\] to denote expectation and variance “with respect to the random variable \\(\\varepsilon\\), only”.\nIn the case of random design, \\[\n\\mathbb{E}_\\varepsilon(\\cdot)\\quad\\text{and}\\quad Var_\\varepsilon(\\cdot)\n\\] thus denote the conditional expectation \\[\n\\mathbb{E}(\\cdot|X_1,\\dots,X_n)\n\\] and the conditional variance \\[\nVar(\\cdot|X_1,\\dots,X_n)\n\\] given the observed \\(X\\)-values.\nFor random design, these conditional expectations depend on the observed sample, and thus are random. For fixed design, such expectations are of course fixed values.\nIt will always be assumed that the matrix \\[\n\\mathbf{X}^\\top \\mathbf{X},\n\\] with \\(\\mathbf{X}=(b_{j,k}(X_i))_{i,j}\\), is invertible (under our conditions on the design density this holds with probability 1 for the random design).\nThe behavior of nonparametric function estimates is usually evaluated with respect to quadratic risk (i.e. mean squared error).\nA commonly used global measure of accuracy of a spline estimator \\(\\hat m_p\\) is the Mean Average Squared Error (MASE), which averages the local (at each \\(X_i\\)) mean squared errors over all \\(X_i,\\) \\(i=1,\\dots,n:\\) \\[\n\\begin{align*}\n&\\operatorname{MASE}(\\hat m_p):=\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left(m(X_i)-\\hat{m}_p(X_i)\\right)^2\\\\\n= &\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\left(\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))-m(X_i)\\right)^2}_{(\\operatorname{Bias}_\\varepsilon(\\hat{m}_p(X_i)))^2} + \\\\[2ex]\n&\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\mathbb{E}_\\varepsilon\\left((\\hat{m}_p(X_i)-\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\right)^2}_{Var_\\varepsilon(\\hat{m}_p(X_i))}\n\\end{align*}\n\\]\nAnother frequently used measure is the Mean Integrated Squared Error (MISE): \\[\n\\begin{align*}\n\\operatorname{MISE}(\\hat m_p):=\\int_a^b \\mathbb{E}_\\varepsilon\\left(m(x)-\\hat m_p(x)\\right)^2dx\n\\end{align*}\n\\]\n\nMASE versus MISE:\n\nEquidistant design: \\[\n(b-a)\\operatorname{MASE}(\\hat m_p)=\\operatorname{MISE}(\\hat m_p) + O(n^{-1})\n\\]\nMISE and MASE are generally not asymptotically equivalent in the case of random design \\[\n(b-a)\\operatorname{MASE}(\\hat m_p)=\\int_a^b \\mathbb{E}_\\varepsilon\\left(m(x)-\\hat m_p(x)\\right)^2 f(x)dx + O_P(n^{-1}).\n\\]\n\n\n\n\n\n\n\nLandau symbol “Big Oh” \\(O(r_n)\\)\n\n\n\n\\[\nO(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all sequences \\(x_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\dfrac{|x_n|}{r_n}\\to c\\) as \\(n\\to\\infty,\\) where \\(c\\) is a constant with \\(0\\leq c &lt; \\infty.\\)\n\nNote: This includes the case \\(\\dfrac{|x_n|}{r_n}\\to 0.\\)\nExamples:\n\n\\(-\\dfrac{1}{n}=O(n^{-1})\\)\n\\(\\dfrac{1}{n^2}=O(n^{-2})\\)\n\\(\\dfrac{1}{n^2}=O(n^{-1})\\)\n\\(\\displaystyle\\sum_{j=1}^{m} g\\left(x_j\\right)(x_{j}-x_{j-1}) = \\int_a^b g(x)dx + O(m^{-1})\\),  for every sufficiently smooth function \\(g\\) (\\(g\\) being continuously differentiable over \\((a,b)\\)), where \\(x_0=a\\) and \\(x_j=x_{0}+j(b-a)/m,\\) for \\(j=1,\\dots,m\\) such that \\(x_{j}-x_{j-1}=(b-a)/m.\\)\n\n\n\n\n\n\n\n\n\nLandau symbol “Small oh” \\(o(r_n)\\)\n\n\n\n\\[\no(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all sequences \\(x_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\dfrac{|x_n|}{r_n}\\to 0\\) as \\(n\\to\\infty.\\)\n\nExamples:\n\n\\(\\dfrac{1}{n^2}=o(n^{-1})\\)\n\\(n^{-a}=o(n^{-b})\\) for all \\(a&gt;b&gt;0.\\)\n\nNote: For every sequence \\(x_n=o(r_n)\\) it holds that \\(x_n=O(r_n),\\) but not the other way round.\n\n\n\n\n\n\n\n\nSpecial Cases \\(O(1)\\) and \\(o(1)\\)\n\n\n\n\\[\nO(1)\\quad \\text{and}\\quad o(1)\n\\]\nExamples:\n\n\\(1 + \\dfrac{1}{n} = O(1)\\)\n\\(\\dfrac{1}{n^2}=o(1)\\)\n\n\n\n\n\n\n\n\n\nStochastic Landau symbol “Big Oh P” \\(O_P(r_n)\\)\n\n\n\n\\[\nO_P(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all stochastic sequences \\(X_n\\), \\(n=1,2,\\dots,\\) for which\n\nthere exists for each (small) \\(\\epsilon&gt;0\\) a sufficiently large threshold \\(\\delta&gt;0\\) such that \\(\\displaystyle P\\left(\\frac{|X_n|}{r_n}&gt;\\delta\\right)&lt;\\epsilon\\) for all sufficiently large \\(n\\).  Plain English: “such that \\(\\frac{|X_n|}{r_n}\\) is bounded in probability for all large enough \\(n\\)”\n\nExample:\n\nIf \\(\\displaystyle \\sqrt{n}(\\bar{X}_n-\\mu)\\to_d\\mathcal{N}(0,\\sigma^2),\\) then \\[\n\\begin{align*}\n\\sqrt{n}(\\bar{X}_n-\\mu) &= O_P(1)\\\\[2ex]\n(\\bar{X}_n-\\mu) &= O_P(n^{-1/2})\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nStochastic Landau symbol “Small oh P” \\(o_P(r_n)\\)\n\n\n\n\\[\no_P(r_n)\\quad\\text{with}\\quad r_n&gt;0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all stochastic sequences \\(X_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\displaystyle \\frac{|X_n|}{r_n}\\to_P 0\\quad\\) as \\(\\quad n\\to\\infty\\)\n\nExample:\n\nIf \\(\\displaystyle \\sqrt{n}(\\bar{X}_n-\\mu)\\to_d\\mathcal{N}(0,\\sigma^2),\\) then \\[\n\\begin{align*}\n(\\bar{X}_n-\\mu)         &= o_P(1)\n\\end{align*}\n\\]\n\n\n\nIn the following we focus on the MASE which has the advantage that we can use matrix algebra.\nFirst, we look at the local bias of \\(\\hat{m}_p(X_i)\\) at a single evaluation point \\(X_i:\\) \\[\n\\operatorname{Bias}_\\varepsilon(\\hat{m}_p(X_i))=\\mathbb{E}_\\varepsilon(\\hat m_p(X_i))-m(X_i),\n\\] where \\[\n\\begin{align*}\n  \\mathbb{E}_\\varepsilon(\\hat m_p(X_i))&=\\mathbb{E}_\\varepsilon\\Big(\\sum_{j=1}^p \\hat{\\beta}_j b_{j,k}(X_i)\\Big)\\\\\n&=\\sum_{j=1}^p\\mathbb{E}_\\varepsilon(\\hat{\\beta}_j) b_{j,k}(X_i),\n\\end{align*}\n\\] with \\[\n\\hat{\\beta}=\\left(\\begin{matrix}\\hat{\\beta}_1\\\\\\vdots\\\\\\hat{\\beta}_p\\end{matrix}\\right)=(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top Y\n\\] and with \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\hat\\beta)\n&=\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  (\\overbrace{m+\\varepsilon}^{=Y})\\Big)\\\\[2ex]\n&=\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m\\Big)\\;+\\;\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\varepsilon\\Big)\\\\[2ex]\n&=\\overbrace{(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m}^{=(\\beta_1,\\dots,\\beta_p)^\\top=\\beta}\\;+\\;\\underbrace{(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\overbrace{\\mathbb{E}_\\varepsilon(\\varepsilon)}^{=0}}_{=0}\\\\[2ex]\n&=(\\beta_1,\\dots,\\beta_p)^\\top=\\beta\n\\end{align*}\n\\] where \\[\nm=\\left(\\begin{matrix}m(X_1)\\\\\\vdots\\\\m(X_n)\\end{matrix}\\right)\n\\] denotes the vector of true function values, and \\[\n\\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\\\vdots\\\\\\varepsilon_n\\end{matrix}\\right)\n\\] denotes the vector of error terms.\nThat is, the mean of the spline regression estimator evaluated at \\(X_i\\) is given by \\[\n\\mathbb{E}_\\varepsilon(\\hat m_p(X_i)) = \\sum_{j=1}^p\\beta_j b_{j,k}(X_i),\n\\] where \\[\n\\beta=\\left(\\begin{matrix}\\beta_1\\\\\\vdots\\\\\\beta_p\\end{matrix}\\right)=(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m\n\\] is a least squares solution; namely of the following least squares problem that tries to approximate the unknown vector \\(m=(m(X_1),\\dots,m(X_n))^\\top\\) using a spline function \\(s\\in {\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}:\\) \\[\n\\begin{align*}\n&\\sum_{i=1}^n \\left(m(X_i)-\\sum_{j=1}^p \\beta_j b_{j,k}(X_i)\\right)^2\\\\[2ex]\n&=\\min_{\\vartheta_1,\\dots,\\vartheta_p}\\sum_{i=1}^n \\left(m(X_i)-\\sum_{j=1}^p \\vartheta_j  b_{j,k}(X_i)\\right)^2\\\\[2ex]\n&=\\min_{s\\in {\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}} \\sum_{i=1}^n \\left(m(X_i)-s(X_i)\\right)^2.\n\\end{align*}\n\\]\nThat is, the mean of the spline regression estimator evaluated at \\(X_i\\) \\[\n\\mathbb{E}_\\varepsilon(\\hat m_p(X_i))=\\sum_{j=1}^p \\beta_j b_j(X_i)=:\\tilde m_p(X_i)\n\\] is the best least squares (\\(L_2\\)) approximation of the true, but unknown, regression function vector \\(m=(m(X_1),\\dots,m(X_n))^\\top\\) by means of a spline function vector \\(s=(s(X_1),\\dots,s(X_n))^\\top\\) with \\(s\\in{\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}.\\)\n\n\n\n\n\n\nImportant\n\n\n\nIf the true, but unknown, regression function \\(m\\) happens to be an element of the space of spline functions \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q},\\) then \\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) = 0,\\quad i=1,\\dots,n.\n\\] However, generally we do not expect that \\(m\\) is actually an element of \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q},\\) such that\n\\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) \\neq  0,\\quad i=1,\\dots,n.\n\\] For consistency, however, we need that \\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) \\to 0,\\quad i=1,\\dots,n.\n\\] as \\(n\\to\\infty\\) and \\(p\\equiv p_n\\to\\infty;\\) i.e. that \\(m\\) becomes eventually an element of the then very large space \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_{q_n}}\\) as \\(q_n\\to\\infty\\) with \\(n\\to\\infty.\\)\n\n\nFrom the general approximation properties of cubic splines (\\(k=3\\)) with \\(q=p-2\\) equidistant knots (De Boor and De Boor (1978) or Eubank (1999)), we know that:\n\nIf \\(m\\) is twice continuously differentiable over \\((a,b)\\), then \\[\n\\begin{align*}\n&(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n \\left(\\tilde m_p(X_i) - m(X_i)\\right)^2\n=\\left\\{\\begin{array}{ll}\nO(p^{-4})&\\quad\\text{fixed design}\\\\\nO_p(p^{-4})&\\quad\\text{random design}\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\]\nIf \\(m\\) is four times continuously differentiable, then \\[\n\\begin{align*}\n&(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n (\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\tilde m_p(X_i) - m(X_i)\\right)^2\n=\\left\\{\\begin{array}{ll}\nO(p^{-8})&\\quad\\text{fixed design}\\\\\nO_p(p^{-8})&\\quad\\text{random design}\\\\\n\\end{array}\n\\right.\n\\end{align*}\n\\]\n\nThe next step is to compute the (average) variance of the estimator, which can be obtained by the usual type of arguments applied in parametric regression:  \\[\n\\begin{align*}\nVar_\\varepsilon(\\hat m_p)\n&=\\frac{1}{n}\\sum_{i=1}^nVar_\\varepsilon(\\hat{m}_p(X_i))\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}_\\varepsilon\\big((\\hat{m}_p(X_i)-\\overbrace{\\tilde{m}(X_i)}^{=\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))})^2\\big)\\\\[2ex]\n%\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y-\n% \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\tilde m_p\\Vert_2^2\\right)\\\\\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\left\\Vert \\left(\\begin{matrix}\\hat{m}_p(X_1)\\\\\\vdots\\\\\\hat{m}_p(X_n)\\end{matrix}\\right)-\\left(\\begin{matrix}\\tilde{m}_p(X_1)\\\\\\vdots\\\\\\tilde{m}_p(X_n)\\end{matrix}\\right)\\right\\Vert_{2}^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y-\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top m\\Vert_2^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top (Y-m)\\Vert_2^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\Vert_2^2\\right)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  (\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top )^\\top}_{(1\\times n)}\\;\\;\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(n\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  (\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top )}_{(1\\times n)}\\;\\;\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(n\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(1\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\operatorname{trace}\\left(\\varepsilon^\\top  \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\right)\\right)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\operatorname{trace}\\left(  (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\varepsilon^\\top\\mathbf{X}\\right)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\operatorname{trace}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\mathbb{E}_\\varepsilon(\\varepsilon\\varepsilon^\\top ) \\mathbf{X}\\right)\\quad(\\text{with }\\mathbb{E}_\\varepsilon(\\varepsilon\\varepsilon^\\top )=I_n\\sigma_\\varepsilon)\\\\[2ex]\n&=\\frac{1}{n} \\sigma^2 \\text{trace}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\mathbf{X}\\right)\\\\[2ex]\n&=\\frac{1}{n} \\sigma^2 \\text{trace}\\left(I_p\\right)\\\\[2ex]\n&=\\sigma^2  \\frac{p}{n}\n\\end{align*}\n\\]\n\n\n\n\n\n\nTrace-Trick\n\n\n\nFor any \\((m\\times n)\\) matrix \\(A\\) and any \\((n\\times m)\\) matrix \\(B\\) we have the identity \\[\\text{trace}(AB)=\\text{trace}(BA)\\]\n\n\n\n\nSummary\nFor cubic (\\(k=3\\)) splines with \\(q\\) equidistant knots and a twice differentiable function \\(m\\) we have that:\n\\[\n\\begin{align*}\n\\displaystyle(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2&=\\left\\{\\begin{array}{ll}\nO(p^{-4})&\\quad\\text{fixed design}\\\\\nO_p(p^{-4})&\\quad\\text{random design}\\\\\n\\end{array}\n\\right.\\\\[2ex]\n\\displaystyle Var_\\varepsilon(\\hat m_p)&= \\sigma^2\\frac{p}{n},\n\\end{align*}\n\\tag{5.3}\\] where \\(p=q+2\\) is the number of basis functions (i.e. the smoothing parameter).\nThis leads to the classical trade-off between (average) squared bias and (average) variance that is typical for nonparametric statistics:\n\n\\(p\\) small: \\(\\displaystyle Var_\\varepsilon(\\hat m_p)\\) is small, but squared bias \\(\\displaystyle (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\) is large.\n\\(p\\) large: \\(\\displaystyle Var_\\varepsilon(\\hat m_p)\\) is large, but squared bias \\(\\displaystyle (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\) is small.\n\n\nLet us focus now on the case of a fixed design. From Equation 5.3 we have that \\[\n\\begin{align*}\n\\operatorname{MASE}(\\hat m_{p})\n&= (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2 + Var_\\varepsilon(\\hat m_p)\\\\[2ex]\n&=O(p^{-4}) + \\sigma^2\\frac{p}{n}\n\\end{align*}\n\\] I.e., using the definition of \\(O(\\cdot)\\), we have that \\[\n\\begin{align*}\n\\underbrace{\\operatorname{MASE}(\\hat m_{p})}_{\\geq 0}\n&\\leq \\underbrace{\\texttt{const}}_{&gt;0}\\;\\; p^{-4} + \\sigma^2\\frac{p}{n}\n\\end{align*}\n\\]\nThus, if \\[\np\\equiv p_n\\to\\infty\\quad\\text{as}\\quad n\\to\\infty,\n\\] but sufficiently slow, such that \\[\n\\dfrac{p_n}{n}\\to 0,\n\\] then \\[\n\\begin{align*}\n\\operatorname{MASE}(\\hat m_{p}) \\to 0\n\\end{align*}\n\\] as \\(n\\to 0.\\)\nAn optimal smoothing parameter \\(p_{opt}\\) that minimizes \\(\\operatorname{MASE}(\\hat m_{p})\\) will balance the squared bias and variance: \\[\n\\begin{align*}\n\\frac{d}{d\\,p}\\operatorname{MASE}(\\hat m_{p})\n&=\\frac{d}{d\\,p}\\left(\\texttt{const}\\cdot p^{-4}+\\sigma^2\\frac{p}{n}\\right)\\\\[2ex]\n&=-4\\cdot\\texttt{const}\\cdot p^{-5}+\\sigma^2\\frac{1}{n}.\n\\end{align*}\n\\] Setting zero and solving for \\(p_{opt}\\) yields \\[\np_{opt}=\\texttt{const}\\cdot n^{1/5},\n\\] where \\(\\texttt{const}\\) denotes here a generic factor collecting all factors that do not depend on \\(p_n\\) or \\(n.\\)\nThus \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O(n^{-4/5}).\n\\]\nIn the case of a random design \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O_p(n^{-4/5}).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nFor an estimator \\(\\hat m\\) based on a valid (!) parametric model we have \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O_p(n^{-1}),\n\\] since parametric models have no bias—provided, the model assumption is correct.\n\n\nSimilar results can be obtained for the mean integrated squared error (MISE): If \\(m\\) is twice continuously differentiable, and \\(p_{opt} \\sim n^{1/5}\\), then \\[\n\\operatorname{MISE}(\\hat m_{p_{opt}})=\\mathbb{E}_\\varepsilon\\left(\\int_a^b(m(x)-\\hat m_{p_{opt}}(x))^2dx\\right)=O_p(n^{-4/5}).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html#selecting-the-smoothing-parameter-p",
    "href": "Ch5_NPRegression.html#selecting-the-smoothing-parameter-p",
    "title": "5  Nonparametric Regression",
    "section": "5.3 Selecting the Smoothing Parameter \\(p\\)",
    "text": "5.3 Selecting the Smoothing Parameter \\(p\\)\nAim: We need to choose the smoothing parameter \\(p\\) in an (somehow) optimal and objective manner.\nProblem: Since \\(m\\) is unknown, we cannot directly compute the \\(\\operatorname{Bias}(\\hat{m}_p)\\) and thus not \\(\\operatorname{MASE}(\\hat{m}_p).\\) This renders a direct computation of the optimal smoothing parameter \\(p_{opt}\\) impossible.\nApproach: Determine an estimate \\(\\hat p_{opt}\\) of the unknown optimal smoothing parameter \\(p_{opt}\\) by minimizing a suitable error criterion with the following properties:\n\nFor every possible \\(p\\) the error criterion function can be calculated from the data.\nFor every possible \\(p\\) the error criterion provides “information” about the respective \\(\\operatorname{MASE}(\\hat{m}_p)\\).\n\n \nRecall: We have \\[\n\\hat m_p=\n\\left(\\begin{matrix}\n\\hat m_p(X_1)\\\\ \\vdots\\\\\\hat m_p(X_n)\n\\end{matrix}\\right)=\\mathbf{X}\\hat\\beta=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y=S_pY,\n\\] where\n\\[\n\\begin{align*}\n\\operatorname{trace}(S_p)\n&=\\operatorname{trace}\\big(\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\big)\\\\[2ex]\n&=\\operatorname{trace}\\big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{X}\\big)\\\\[2ex]\n&=\\operatorname{trace}\\big(I_p\\big)=p\n\\end{align*}\n\\]\nThat is, for given \\(p\\), the number of parameters to estimate by the spline method (one also speaks of the degrees of freedom of the smoothing procedure) is equal to \\(p\\) which corresponds to the trace of the smoother matrix \\(S_p=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top.\\)\nMost frequently used error criteria are Cross-Validation (CV) and Generalized Cross-Validation (GCV).\n\n5.3.1 Leave One Out Cross-Validation (LOOCV)\nFor a given value \\(p,\\) cross-validation tries to approximate the out-of-sample prediction error \\[\n\\operatorname{LOOCV}(p)=\\frac{1}{n} \\sum_{i=1}^n\\biggl( Y_i-\n{\\hat m}_{p,-i}(X_i)\\biggr)^2\n\\] Here, for any \\(i=1,\\dots,n\\), \\({\\hat m}_{p,-i}(\\cdot)\\) is the “leave-one-out” estimator of \\(m(\\cdot)\\) to be obtained when a spline function is fitted using the \\(n-1\\) observations: \\[\n(Y_1,X_1),\\dots,(Y_{i-1},X_{i-1}),(Y_{i+1},X_{i+1}),\\dots,(Y_{n},X_{n}).\n\\] Motivation: We have \\[\n\\begin{align*}\n&\\mathbb{E}_\\varepsilon(\\operatorname{LOOCV}(p))\\\\[2ex]\n= & \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\biggl( \\overbrace{m(X_i)+\\varepsilon_i}^{=Y_i}-\n{\\hat m}_{p,-i}(X_i)\\biggr)^2\\right]\\\\[2ex]\n= & \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\left(\\left( m(X_i)-\n{\\hat m}_{p,-i}(X_i) \\right)^2 +2\\left( m(X_i)-\n{\\hat m}_{p,-i}(X_i) \\right)\\varepsilon_i +\\varepsilon_i^2\\right)\\right]\\\\[2ex]\n= &\\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\left(m(X_i)-\n{\\hat m}_{p,-i}(X_i)\\right)^2\\right]}_{\\approx\\operatorname{MASE}(\\hat m_p)} \\\\[2ex]\n&+ 2\\frac{1}{n} \\sum_{i=1}^n\n\\underbrace{\\mathbb{E}_\\varepsilon\\left[( m(X_i)-\n{\\hat m}_{p,-i}(X_i))\\varepsilon_i\\right]}_{=0}+\\sigma^2\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{LOOCV}(p)) = \\operatorname{MASE}(\\hat m_p) + \\sigma^2,\n\\end{align*}\n\\] such that \\[\n\\begin{align*}\np_{opt}\n&= \\arg\\min_p\\mathbb{E}_\\varepsilon(\\operatorname{LOOCV}(p))\\\\[2ex]\n&= \\arg\\min_p\\operatorname{MASE}(\\hat m_p).\n\\end{align*}\n\\] That is, at least on average, minimizing \\(CV(p)\\) is equivalent to minimizing \\(\\operatorname{MASE}(\\hat m_p)\\).\nR-Code to Compute an Estimate of the Optimal Smoothing Parameter using LOOCV\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\nThen, compute the CV scores for different numbers of basis functions \\(p\\) and plot them to select an estimate for the optimal value of the smoothing parameter \\(p\\).\n\np_vec &lt;- 6:12 \nLOOCV_p &lt;- numeric(length(p_vec))\n\nfor(j in 1:length(p_vec)){\n\np         &lt;- p_vec[j] # number of basis functions\nq         &lt;- p - 2    # number of equidistant knots \nknot_seq  &lt;- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nLOOCV_scoreSq &lt;- numeric(n)\n\nfor(i in 1:n){\nm_hat_p_i  &lt;- X_mat[i,] %*% \n              solve(t(X_mat[-i,])  %*% X_mat[-i,])  %*% t(X_mat[-i,]) %*% y_vec[-i] \n\nLOOCV_scoreSq[i] &lt;- (y_vec[i] - m_hat_p_i)^2\n}\nLOOCV_p[j] &lt;- mean(LOOCV_scoreSq)\n}\nplot(y = LOOCV_p, x = p_vec, type=\"o\")\n\n\n\n\n\n\n\n\n\n\n5.3.2 \\(k\\)-fold Cross-Validation\nIn practice, one usually works with \\(k\\)-fold CV (\\(k=5\\) or \\(k=10\\)). For this the index set \\(I=\\{1,\\dots,n\\}\\) is partitioned into \\(k\\) disjoint index sets \\(I_1,\\dots,I_k\\) of (roughly) equal sizes, i.e. \\(|I_1|\\approx|I_2|\\approx\\dots\\approx|I_k|\\), such that \\(I_1\\cup I_1\\cup \\dots \\cup I_k=I\\) and \\(I_j\\cap I_k=\\emptyset\\) for all \\(j\\neq k\\). \\[\n\\operatorname{CV}_k(p)=\\frac{1}{K}\\sum_{k=1}^K\\frac{1}{|I_k|} \\sum_{i\\in I_k}\\left( Y_i-\n{\\hat m}_{p,-I_k}(X_i)\\right)^2.\n\\]\n\n\n5.3.3 Generalized Cross-Validation (GCV)\n\\[\n\\operatorname{GCV}(p)=\\frac{1}{n\\left(1-\\frac{p}{n}\\right)^2}\\sum_{i=1}^n \\left( Y_i-\n{\\hat m}_p(X_i)\\right)^2\n\\] Motivation: The average residual sum of squares are given by \\[\n\\operatorname{ARSS}(p):=\\frac{1}{n}\\sum_{i=1}^n\\biggl( Y_i-\n\\hat{m}_{p}(X_i)\\biggr)^2\n\\tag{5.4}\\] which allows us to rewrite \\(\\operatorname{GCV}(p)\\) as \\[\n\\operatorname{GCV}(p)=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\operatorname{ARSS}(p)\n\\] Some lengthy derivations show that the expected value of \\(\\operatorname{ARSS}(p)\\) is \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2,\n\\end{align*}\n\\tag{5.5}\\] where\n\n\\(\\operatorname{MASE}(\\hat m_p)\\) denotes the mean average squared error,\n\\(-2\\sigma^2\\frac{p}{n}\\) denotes the optimism term, and\n\\(\\sigma^2\\) denotes the irreducible component of the (prediction-)error.\n\nMoreover, a Taylor expansion of \\(f(p/n)=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\) around \\(p/n=0\\) yields that \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation becomes precise as \\(\\frac{p}{n}\\to 0.\\) Thus \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{GCV}(p))\n&\\approx \\left(1 + 2\\frac{p}{n}\\right)\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p)) \\\\[2ex]\n&=\\left(1 + 2\\frac{p}{n}\\right) \\left(\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\\right)\\\\[2ex]\n&= \\operatorname{MASE}(\\hat m_p) +\\sigma^2 +\\\\[2ex]\n&+ \\underbrace{2\\frac{p}{n} \\operatorname{MASE}(\\hat m_p)}_{=O_p\\left(\\frac{p}{n}\\right)} - \\underbrace{4\\frac{p^2}{n^2} \\sigma^2}_{=O\\left(\\frac{p^2}{n^2}\\right)=o\\left(\\frac{p}{n}\\right)}\\\\[2ex]\n&= \\operatorname{MASE}(\\hat m_p) +\\sigma^2 + O_p\\left(\\frac{p}{n}\\right)\n\\end{align*}\n\\]\nThus, at least on average, minimizing \\(\\operatorname{GCV}(p)\\) is approximately (for \\(p_n/n\\to 0\\) as \\(n\\to\\infty\\)) equivalent to minimizing \\(\\operatorname{MASE}(\\hat m_p),\\) since \\(\\sigma^2\\) does not depend on \\(p.\\)\n\n\nR-Code to Compute an Estimate of the Optimal Smoothing Parameter using GCV\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      &lt;- 100     # Sample Size\nx_vec  &lt;- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  &lt;- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  &lt;-  sin(x_vec * 5) + e_vec\n\nThen, compute the GCV scores for different numbers of basis functions \\(p\\) and plot them to select an estimate for the optimal value of the smoothing parameter \\(p\\).\n\np_vec &lt;- 6:12 \nGCV_p &lt;- numeric(length(p_vec))\n\nfor(j in 1:length(p_vec)){\n\np         &lt;- p_vec[j] # number of basis functions\nq         &lt;- p - 2    # number of equidistant knots \nknot_seq  &lt;- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nS_p      &lt;- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) \nm_hat_p  &lt;- S_p   %*% y_vec\nARSS     &lt;- mean(c(y_vec - m_hat_p)^2)\nGCV_p[j] &lt;- ARSS/((1-p/n)^2)\n}\nplot(y = GCV_p, x = p_vec, type=\"o\")\n\n\n\n\n\n\n\n\nCompute the nonparametric regression estimate using the GCV optimal smoothing parameter \\(p=8.\\)\n\np         &lt;- 8\nq         &lt;- p - 2    # number of equidistant knots \nknot_seq  &lt;- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     &lt;- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nS_p      &lt;- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) \nm_hat_p  &lt;- S_p   %*% y_vec\n\nLet’s plot the results:\n\nplot(y=y_vec, x=x_vec, xlab=\"X\", ylab=\"Y\", \n     main=\"Regression Splines\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"red\", lty=2, lwd=1.5)\nlines(y=m_hat_p, x=x_vec, col=\"blue\", lwd=1.5)\nlegend(\"bottomleft\", \n       c(\"(Unknown) Regression Function m\", \n         \"Regr.-Spline Fit with GCV optimal p=8\"), \n       col=c(\"red\",\"blue\"), \n       lty=c(2,1,1), lwd=c(2,2,2))\n\n\n\n\n\n\n\n\n\n\n5.3.4 Over-Fitting and Optimism\nRemember that in our econometrics lecture, where we assumed to know the model apart from the model parameters, we used the average squared residuals \\(\\operatorname{ARSS}(p)\\) \\[\n\\operatorname{ARSS}(p)=\\frac{1}{n}\\sum_{i=1}^n\\biggl( Y_i-\n\\hat{m}_{p}(X_i)\\biggr)^2\n\\tag{5.6}\\] as an estimator for \\(\\sigma^2.\\) Moreover, \\(\\operatorname{ARSS}(p)\\) was the main component to compute the \\(R^2\\)-coefficient \\[\nR^2(p) = 1- \\frac{\\operatorname{ARSS}(p)}{\\frac{1}{n}\\sum_{i=1}^n(Y_i - \\bar{Y})^2}.\n\\]\nIn non-parametrics, however, we do not assume to know the model, but try to learn the model (including the smoothing parameter \\(p\\)) form the data. Therefore, Equation 5.6 cannot be used as an estimator for \\(\\sigma^2\\), firstly, since we do not know \\(p\\), and secondly, since we expect \\(\\hat{m}_{p}\\) to be biased.\nIn fact, if we would mimimize \\(\\operatorname{ARSS}(p)\\) with respect to \\(p,\\) we would minimize the so-called in-sample prediction error. On average, this would result in minimizing \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\] which leads to a too large choice of \\(p\\) due to the distorting optimism term \\[\n-2\\sigma^2\\frac{p}{n}.\n\\]\nIn fact, for a too large \\(p,\\) the non-parametric estimate over-fits the data; i.e. \\(\\hat{m}_{p}\\) becomes so flexible such that \\[\n\\begin{align*}\nY_i & \\approx \\hat{m}_{p}(X_i)\\;\\;\\text{ for all}\\; i=1,\\dots,n\\\\[2ex]\n\\Rightarrow\\;\\operatorname{ARSS}(p)&\\approx 0\\;(\\neq\\sigma^2)\\\\[2ex]\n\\Rightarrow\\;R^2&\\approx 1.\n\\end{align*}\n\\]\nHowever, an over-fitted estimate \\(\\hat{m}_{p}\\) typically has fitted the noise component \\(\\varepsilon\\)—additionally to the signal component \\(m.\\) Therefore, an over-fitted estimate \\(\\hat{m}_{p}\\) will typically perform very poorly when used to predict new out-of-sample data. I.e. for a new outcome \\[\nY_{new} = m(X_{new}) + \\varepsilon_{new},\n\\] we’ll have that \\[\n|m(X_{new}) - \\hat{m}_{p}(X_{new})| \\gg 0;\n\\] see Figure 5.2.\n\n\n\n\n\n\nFigure 5.2: Over-fitting can lead to unusable out-of-sample predictions.\n\n\n\n\n\n\n\n\n\nOptimism\n\n\n\nThe term \\(2\\sigma^2\\frac{p}{n}\\) in \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\] is called the optimism of the fit and quantifies the amount by which the in-sample average residual sum of squares (ARSS) systematically under-estimates the true mean average squared error (MASE) of \\(\\hat m_p.\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html#exercises",
    "href": "Ch5_NPRegression.html#exercises",
    "title": "5  Nonparametric Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nShow that \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation becomes precise as \\(\\frac{p}{n}\\to 0.\\)\n\n\nExercise 2.\nShow that \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\]\n\n\nExercise 3.\nShow that the core-part of the optimism term, i.e.  \\[\n\\sigma^2\\frac{p}{n},\n\\] quantifies the average covariance between the error terms \\(\\varepsilon_i\\) and the fits \\(\\hat{m}_p(X_i)\\) over \\(i=1,\\dots,n.\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html#references",
    "href": "Ch5_NPRegression.html#references",
    "title": "5  Nonparametric Regression",
    "section": "References",
    "text": "References\n\n\n\n\nDe Boor, Carl, and Carl De Boor. 1978. A Practical Guide to Splines. Vol. 27. springer.\n\n\nEubank, Randall L. 1999. Nonparametric Regression and Spline Smoothing. CRC press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch5_NPRegression.html#footnotes",
    "href": "Ch5_NPRegression.html#footnotes",
    "title": "5  Nonparametric Regression",
    "section": "",
    "text": "The degree \\(k\\) of a polynomial \\(s(x)=s_0+s_1x+s_2x^2+\\dots+s_kx^{k}\\) refers to the highest exponent. The order \\(k+1\\) of a polynomial refers to the number coefficients \\((s_0,\\dots,s_k).\\)↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Nonparametric Regression</span>"
    ]
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#sec-CA",
    "href": "Ch2_EMAlgorithmus.html#sec-CA",
    "title": "2  EM Algorithm & Cluster Analysis",
    "section": "2.4 Cluster Analysis",
    "text": "2.4 Cluster Analysis\nThe problem of predicting a discrete (categorial) random variable \\(Y\\) (i.e. the group label) from a possibly multivariate predictor \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\\overset{\\text{i.i.d.}}{\\sim}(Y,X)\n\\] where\n\n\\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and\n\\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n\n\nNote: Above in Section 2.3, we used the dummy variables \\(Z_{ig},\\) to encode the discrete (categorial) \\(Y\\) group labels.\n\n\n\n\n\n\n\nExample\n\n\n\nPredict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g. passing the exam (\\(Y=1\\)) vs. failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g. previous gradings, number of hours studied, etc.)\n\n\nA classification rule \\(h\\) is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\n\n\n\n\n\n\n(Un-)Supervised Classification\n\n\n\n\nSupervised Classification: If there are learning/training data with group-labels \\[\n({\\color{red}Y_1},X_1),\\dots,({\\color{red}Y_n},X_n)\n\\] that can be used to estimate \\(h,\\) it’s called a supervised classification (computer science: supervised learning) problem.\nUnsupervised Classification/Cluster Analysis: If there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it’s called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n\n\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e. the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] That is, the conditonal mean \\(m(x)\\) equals the posterior probability \\(P(Y=1|X=x),\\) i.e. the conditional probability of \\(Y=1\\) given \\(X=x.\\)\nFrom Bayes’ theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) }\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)}\n\\end{align*}\n\\tag{2.8}\\] where\n\n\\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\n\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density or a Gaussian mixture.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\n\nEquivalent forms of the Bayes’ classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)&gt;P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)&gt;\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition 2.1.\n\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown \\[\nm(x)=P(Y=1|X=x)\n\\] and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests, neural nets, etc.\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation 2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)&gt;\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G&gt;1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), and \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard — particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian (mixture) density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x_i|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure 2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\n\n\n\nFigure 2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\n\nThe final estimation result replicates Figure 2.2.\nThe average penguin probably doesn’t care about our EM Algorithm.\n\n\n\n\n\n\nFigure 2.5: Penguin research on the limit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EM Algorithm & Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#references",
    "href": "Ch4_NP_Density_Estimation.html#references",
    "title": "4  Nonparametric Density Estimation",
    "section": "References",
    "text": "References\n\n\n\n\nHall, Peter, and James Stephen Marron. 1987. “Extent to Which Least-Squares Cross-Validation Minimises Integrated Square Error in Nonparametric Density Estimation.” Probability Theory and Related Fields 74 (4): 567–81.\n\n\nParzen, Emanuel. 1962. “On Estimation of a Probability Density Function and Mode.” The Annals of Mathematical Statistics 33 (3): 1065–76.\n\n\nRosenblatt, Murray. 1956. “Remarks on Some Nonparametric Estimates of a Density Function.” The Annals of Mathematical Statistics 27 (3): 832—837.\n\n\nSheather, Simon J, and Michael C Jones. 1991. “A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation.” Journal of the Royal Statistical Society: Series B (Methodological) 53 (3): 683–90.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#accuracy-of-the-kernel-density-estimator",
    "href": "Ch4_NP_Density_Estimation.html#accuracy-of-the-kernel-density-estimator",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.4 Accuracy of the Kernel Density Estimator",
    "text": "4.4 Accuracy of the Kernel Density Estimator\nThe accuracy of kernel density estimator depends on\n\nthe choice of the kernel function \\(K,\\)\nthe choice of the bandwidth \\(h,\\) and\nthe complexity of the unknown density \\(f\\) to be estimated.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe choice of the kernel function \\(K\\) is (by far) less important/critical for the accuracy of the kernel density estimator than than the choice of the bandwidth \\(h.\\)\n\n\nMost common goal: Automatic data-dependent bandwidth selection that is “globally optimal” ; i.e. optimal for all relevant \\(x\\) in \\(f(x).\\)\nWe call a bandwidth choice method optimal, if it minimizes a loss-function, which quantifies the estimation errors.\nCommonly used loss-functions:\n\nIntegrated Squared Error (ISE) \\[\n\\mathrm{ISE}(\\hat{f}_{nh}(x))=\\int (\\hat{f}_{nh}(x)-f(x))^2\\,dx\n\\] Caution: The \\(\\mathrm{ISE}(\\hat{f}_{nh}(x))\\) is a random quantity.\nMean Integrated Squared Error (MISE): \\[\n\\mathrm{MISE}(\\hat{f}_{nh})=\\int\\mathrm{MSE}(\\hat{f}_{nh}(x))\\,dx=\\int \\mathbb{E}(\\hat{f}_{nh}(x)-f(x))^2\\,dx\n\\]\nAsymptotic Mean Integrated Squared Error (AMISE): \\[\n\\mathrm{AMISE}(\\hat{f}_{nh}) = \\mathrm{MISE}(\\hat{f}_{nh}) + o_P(1)\n\\] I.e. the asymptotic approximation of the \\(\\mathrm{MISE}.\\)\n\nThe minimum requirement on \\(\\hat{f}_{nh}\\) is consistency. That is, the estimated density function \\(\\hat{f}_{nh}\\) should approach the true (unknown!) density \\(f\\), under the hypothetical assumption that the sample size grows \\((n \\to \\infty).\\)\nRemember: We’ll call an estimator \\(\\hat\\theta_n\\equiv\\hat\\theta(X_1,\\dots,X_n)\\) consistent, if \\[\n\\hat\\theta_n\\to_p\\theta\\quad\\text{as}\\quad n\\to\\infty.\n\\] Unless a different notion of convergence (such as MSE or “almost surely”) has been specified.\nWhen estimating functions, such as density functions \\(f(x),\\) a distinction is made between the following concepts of consistency:\n\nPointwise consistency: \\[\n\\hat{f}_{nh}(x)\\to_P f(x),\\quad \\text{as}\\quad n\\to\\infty\n\\] for any given \\(x\\in\\mathbb{R}.\\)\nUniform consistency: \\[\n\\sup_{x}\\left|\\hat{f}_{nh}(x) - f(x)\\right|\\to_P 0,\\quad \\text{as}\\quad n\\to\\infty.\n\\]\n\nFor pointwise consistency the following Assumptions are needed:\n\n\n\n\n\n\nAssumptions for Showing Pointwise Consistency and Asymptotic Normality\n\n\n\nUnder the following assumptions:\n\nSequence of bandwidth parameters \\(h\\equiv h_n:\\)\n\n\\(h_n \\to 0\\) as \\(n\\to\\infty\\),\n\\(n h_n \\to \\infty\\) as \\(n \\to \\infty\\)\n\nTrue density function \\(f:\\)\n\n\\(f\\) continuouse and sufficiently often differentiable for all \\(x\\)\n\nKernel function \\(K\\):\n\ncontinuouse and non-negative \\(K(z)\\geq 0\\) for all \\(z\\),\n\\(K\\) symmetric around \\(0\\)\n\\(\\lim_{|y| \\to \\infty} | y K(y) | = 0\\)\n\\(K\\) bounded (\\(\\int |K(z)|dz &lt; \\infty\\)),\n\\(\\int K(z)dz = 1\\)\n\n\none can show pointwise consistency for a given \\(x,\\)\n\\[\n\\hat{f}_{nh}(x)\\to_P f(x),\\quad \\text{as}\\quad n\\to\\infty\n\\]\n\n\n\n\n\n\n\nNote: Under the additional, more restrictive bandwidth assumption that \\[\nh_n \\to 0\\quad\\text{and}\\quad n h_{n}^{\\textcolor{red}{2}} \\to \\infty,\\quad\\text{as}\\quad n\\to\\infty,\n\\] one can also show uniform consistency (see Parzen (1962)), \\[\n\\sup_{x}\\left|\\hat{f}_{nh}(x) - f(x)\\right|\\to_P 0,\\quad \\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\nProof-Strategy for Showing Pointwise Consistency\nUnder the above assumptions, show that the pointwise Mean Squared Error (MSE) converges to zero as \\(n\\to\\infty\\) for a given \\(x,\\) \\[\n\\mathrm{MSE}(\\hat{f}_{nh}(x))\\to 0,\\quad \\text{as}\\quad n\\to\\infty.\n\\] by showing that both, the variance \\[\nVar\\left(\\hat{f}_{nh}(x)  \\right)\n\\] and the squared bias \\[\n\\left(\\mathrm{Bias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\n\\] converge to zero as \\(n\\to\\infty,\\) \\[\n\\begin{align*}\n\\mathrm{MSE}\\left(\\hat{f}_{nh}(x) \\right)\n&=\n\\mathbb{E}\\left(\\hat{f}_{nh}(x) - f(x) \\right)^2\\\\[2ex]\n&=\n\\underbrace{Var\\left(\\hat{f}_{nh}(x)  \\right)}_{\\underset{n\\to\\infty}\\longrightarrow 0}\n+\n\\underbrace{\\left(\\mathrm{Bias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2.}_{\\underset{n\\to\\infty}\\longrightarrow 0}\n\\end{align*}\n\\]\nRemember: Convergence in quadratic mean implies convergence in probability.\n\n\n\n\n\n\nPointwise Asymptotic Normality\n\n\n\nUnder the above pointwise consistency assumptions one can also show pointwise asymptotic normality: \\[\n\\begin{equation*}\n\\frac{ \\sqrt{n\\,h_n}\\left(\\hat{f}_{nh}(x) - \\mathbb{E} \\left( \\hat{f}_{nh}(x) \\right)\\right)}{\n     \\sqrt{ Var\\left( \\hat{f}_{nh}(x) \\right) }}\n   \\overset{a}{\\sim}\\mathcal{N}(0,1)\n\\end{equation*}\n\\] for \\(n\\to\\infty,\\) which implies that we can construct pointwise confidence intervals (etc.)\nCaution: Generally, we have for finite \\(n\\) that: \\[\n\\mathbb{E}\\left( \\hat{f}_{nh}(x) \\right)\\neq f(x);\n\\] i.e. for finite \\(n\\) there is generally a non-negligible estimation bias.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#globally-optimal-bandwidth-choice",
    "href": "Ch4_NP_Density_Estimation.html#globally-optimal-bandwidth-choice",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.5 Globally Optimal Bandwidth Choice",
    "text": "4.5 Globally Optimal Bandwidth Choice\nOne distinguishes locally optimal bandwidth choices for estimating \\(f(x)\\) at a given \\(x,\\) and globally optimal bandwidth choices that are optimal with respect to a global loss function.\nTypically, one determines a globally optimal bandwidth by minimizing the Mean Integrated Squared Error loss function \\[\n\\begin{align*}\n\\mathrm{MISE}(\\hat{f}_{nh})\n&=\\int\\mathrm{MSE}(\\hat{f}_{nh}(x))\\,dx\\\\[2ex]\n&=\\int \\mathbb{E}\\left[(\\hat{f}_{nh}(x)-f(x))^2\\right]\\,dx\\\\[2ex]\n&=\\int Var\\left(\\hat{f}_{nh}(x)  \\right) \\,dx +\n  \\int \\left(\\mathrm{Bias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\\,dx.\n\\end{align*}\n\\]\nProblem: The exact computation of \\(\\mathrm{MISE}(\\hat{f}_{nh})\\) is only possible under very restrictive, simplifying assumptions such as assuming that \\(f\\) is a specific parametric density function—an assumption we do not want to make!\nSolution: Therefore, we compute an asymptotic approximation to \\(\\mathrm{MISE}(\\hat{f}_{nh}),\\) \\[\n\\begin{align*}\n\\mathrm{MISE}(\\hat{f}_{nh})&\\approx \\mathrm{AMISE}(\\hat{f}_{nh})\\\\[2ex]\n&=\\int AVar\\left(\\hat{f}_{nh}(x)  \\right) \\,dx +\n  \\int \\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\\,dx\\\\[2ex]\n\\end{align*}\n\\] which becomes good as \\(n\\to\\infty,\\) and which can be done without making restrictive parametric assumptions on \\(f.\\)\nThe asymptotic approximation \\(\\mathrm{AMISE}(\\hat{f}_{nh})\\) for \\(\\mathrm{MISE}(\\hat{f}_{nh})\\) is the most often used loss function for determining a global choice of the bandwidth \\(h.\\)\n\nAsymptotic Approximation of \\(\\mathrm{MISE}(\\hat{f}_{nh})\\)\nComputing \\(\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\\colon\\)\nLet \\[\nX_1,\\dots,X_n\\overset{\\text{iid}}{\\sim}X,\\quad\\text{where}\\quad X\\sim f.\n\\]\nComputing the pointwise mean: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\hat{f}_{nh}(x)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{h}K\\left(\\frac{x-X_i}{h}\\right)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{h}\\mathbb{E}\\left(K\\left(\\frac{x-X_i}{h}\\right)\\right)\\\\[2ex]\n&\\left[\\text{since }X_1,\\dots,X_n\\overset{\\text{iid}}{\\sim}X\\right]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{1}{h} K\\left(\\frac{x - X}{h}\\right)\\right)\\\\[2ex]\n&=\\int_{-\\infty}^\\infty \\frac{1}{h} K\\left(\\frac{x - u}{h}\\right)f(u)du\\\\[2ex]\n&\\left[\\text{Substitution: $u=x+yh\\;\\Rightarrow \\frac{du}{dy}=h$}\\right]\\\\[2ex]\n&=\\int_{-\\infty}^\\infty \\frac{h}{h} K(y) f(x + y h ) dy\\\\[2ex]\n&\\left[\\text{Taylor expansion of $f(x+yh)$ around $f(x)\\colon$}\\right]\\\\[2ex]\n&=\\int_{-\\infty}^\\infty K(y) \\left\\{ f(x) + f'(x) y h + \\frac{1}{2!} f''(x) y^2 h^2 + o(h^2) \\right\\} dy\\\\[2ex]\n&=f(x) \\underbrace{\\int_{-\\infty}^\\infty K(y)\\,dy}_{=1} +\n  f(x) h \\underbrace{\\int_{-\\infty}^\\infty y K(y)\\,dy}_{=0}\\\\[2ex]\n&\\;\\; + h^2 \\frac{1}{2} f''(x) \\underbrace{\\int_{-\\infty}^\\infty K(y)y^2 dy}_{\\nu_2(K)}+ o(h^2)  \\underbrace{\\int_{-\\infty}^\\infty K(y)\\,dy}_{=1}\\\\[3ex]\n&=f(x) + h^2 \\frac{1}{2} f''(x) \\nu_2(K) + o(h^2)\n\\end{align*}\n\\]\nThus, the pointwise squared bias is given by \\[\n\\begin{align*}\n  &\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2 \\\\[2ex]\n  &=\\left(\\mathbb{E}(\\hat{f}_{nh}(x))-f(x)\\right)^2 \\\\[2ex]\n  &=\\left(h^2 \\frac{1}{2} f''(x) \\nu_2(K) + o(h^2)\\right)^2\\\\[2ex]\n  &=\\left(h^2 \\frac{1}{2} f''(x) \\nu_2(K)\\right)^2 + \\underbrace{2\\,\\left({\\color{red}h^2} \\frac{1}{2} f''(x) \\nu_2(K)\\right)  o({\\color{red}h^2})}_{o({\\color{red}h^4})} + o(h^4)\\\\[2ex]\n  &=\\underbrace{h^4 \\frac{1}{4} \\nu_2(K)^2 f''(x)^2}_{=:\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2}  + o(h^4)\n\\end{align*}  \n\\]\nThis shows that the kernel estimator \\(\\hat{f}_{nh}(x)\\) is asymptotically unbiased, since \\[\n\\begin{align*}\n  \\mathrm{Bias}(\\hat{f}_{nh}(x))&=\\left(h^4 \\frac{1}{4} \\nu_2(K)^2 f''(x)^2 + o(h^4)\\right)^{1/2} \\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}  \n\\]\nThe squared asymptotic bias is given by neglecting the smaller order \\(o(h^4)\\) terms: \\[\n\\begin{align*}\n\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2=h^4 \\frac{1}{4} \\nu_2(K)^2 f''(x)^2\n\\end{align*}  \n\\]\nNote that \\[\n\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2\\quad\\text{and}\\quad\n\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\n\\] are asymptotically equivalent, since for \\(n\\to\\infty\\) \\[\\begin{align*}\n\\frac{\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2}{\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2}\n& =1+\\frac{o(h^4)}{\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2} \\\\[2ex]\n&=1+o(h^4) \\cdot O(h^{-4})\\\\[2ex]\n&=1+o(1\\cdot h^4) \\cdot O(1\\cdot h^{-4})\\\\[2ex]\n&=1+o(1) \\cdot O(1\\cdot h^{-4}\\cdot h^4)\\\\[2ex]\n&=1+o(1) \\cdot O(1)\\\\[2ex]\n&=1+o(1)\\\\\n\\Rightarrow\\quad\n\\lim_{n\\to\\infty}\\frac{\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2}{\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2} & =1\n\\end{align*}\\]\nComputing \\(AVar\\left( \\hat{f}_{nh}(x) \\right)\\colon\\) eWhiteboard\nComputing \\(\\operatorname{AMISE}\\left( \\hat{f}_{nh}(x) \\right)\\colon\\) eWhiteboard\n\\[\\begin{align*}\n\\mathrm{AMISE}(\\hat{f}_{nh}) &=\n\\frac{1}{nh} R(K) +\n\\frac{1}{4} h^4 \\nu_2(K)^2 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx\\\\[2ex]  \n\\end{align*}\\]\nMinimizing \\(\\mathrm{AMISE}(\\hat{f}_h)\\) with respect to \\(h\\) yields the (asymptotically) optimal bandwidth: \\[\\begin{align*}\n  h_{\\mathrm{opt}}&=\n  \\left\\{\n    \\frac{R(K)}{{n\\,\\nu_2(K)}^2 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx}\n  \\right\\}^{1 / 5}\\\\\n  &=\\texttt{Constant} \\cdot n^{-1/5}\n\\end{align*}\\]\nUnknown quantities: Kernel constants \\[\\begin{align*}\nR(K)\n&=\\int\\left(K(y)\\right)^2\\,dy\\\\\n&(=\\frac{3}{5}\\text{, in case of the Epanechnikov kernel})\\\\[2ex]\n\\nu_2(K)&=\\int y^2\\,K(y)\\,dy\\\\\n&(=\\frac{1}{5}\\text{, in case of the Epanechnikov kernel})\n\\end{align*}\\]\n\n\\(\\nu_2(K)\\colon\\) second moment of the kernel\n\\(R(K)\\colon\\) roughness of the kernel\n\nUnknown quantity: The global roughness of \\(\\boldsymbol{f}\\)\n\\[\n\\int \\left(f''(x)\\right)^2 dx\n\\]\nThe minimal value of the AMISE when using the optimal bandwidth is therefore given by:\n\\[\\begin{align*}\n\\mathrm{AMISE}(\\hat{f}_{nh_{\\mathrm{opt}}})\n  &=\\min_{h &gt; 0} \\mathrm{AMISE}(\\hat{f}_{nh} )\\\\[2ex]\n  &=\\frac{1}{nh_{\\mathrm{opt}}} R(K) +\n\\frac{1}{4} h_{\\mathrm{opt}}^4 \\nu_2(K)^2 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx\\\\[2ex]\n  &=\\frac{5}{4} \\left\\{\\nu_2(K)^2 R(K)^4 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx \\right\\}^{1/5} n^{-4/5}\\\\[2ex]\n  &=\\texttt{Constant} \\cdot n^{-4/5}\n\\end{align*}\\]\nThat is, \\(\\hat{f}_{nh_{\\mathrm{opt}}}\\) is AMISE (i.e. MSE) consistent with rate \\(n^{-4/5}.\\)\nThis is a non-parametric convergence rate.\n\n\n\n\n\n\nParametric MSE rate of \\(\\bar{X}_n\\) for \\(\\mu_0\\)\n\n\n\n\\[\\begin{align*}\n\\mathrm{MSE}(\\bar{X}_n)\n  &=(\\mathrm{Bias}(\\bar{X}_n))^2 + Var(\\bar{X}_n)\\\\[2ex]\n  &=Var(\\bar{X}_n)\\\\[2ex]\n  &=\\frac{Var(X)}{n}\\\\[2ex]\n  &=\\texttt{Constant} \\cdot n^{-1}\n\\end{align*}\\]\n\n\nOne immediately recognizes some key properties of \\(\\mathrm{AMISE}(\\hat{f}_{nh_{\\mathrm{opt}}})\\):\n\nIt decreases as the sample size \\(n\\) increases, with a rate \\(n^{-4/5}\\).\nThe influence of the kernel \\(K\\) appears through \\(\\nu_2(K)^2\\) and \\(R(K)\\).\nThe influence of the density \\(f\\) appears through \\(\\int \\left(f''(x)\\right)^2 dx\\).\n\nA density \\(f\\) with a high roughness, as measured by \\(\\int \\left(f''(x)\\right)^2 dx,\\) is more difficult to estimate than one with low roughness.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#exercises",
    "href": "Ch4_NP_Density_Estimation.html#exercises",
    "title": "4  Nonparametric Density Estimation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nLet \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\quad\\text{with}\\quad X\\sim f\n\\] The unknown density \\(f\\) shall be estimated using a kernel density estimator. Choose the Epanechnikov kernel as the kernel function \\(K.\\)\n\nDefine the kernel density estimator \\(\\hat f_{nh}(x)\\) of \\(f(x)\\) and sketch (qualitatively) the behavior of \\(\\mathrm{MISE}(\\hat f_h)\\) as a function of the bandwidth \\(h\\).\nShow that the estimator \\(\\hat f_h\\) is a density function.\nExplain (qualitatively) the concept of the Normal-Reference bandwidth \\(h_{NR}.\\)\nNow assume that the (unknown) true density \\(f\\) has a normal mixture structure as shown in Figure 4.9. What statement can be made in this case about a Normal-Reference bandwidth? Will it typically hold that \\(h_{NR}&lt;h_{opt}\\), \\(h_{NR}\\approx h_{opt}\\) or \\(h_{NR}&gt;h_{opt}\\)? Justify your answer.\nWhat is the convergence rate of \\(\\mathrm{MISE}(\\hat f_{h_{NR}})\\)? Justify your answer.\n\n\n\n\n\n\n\n\n\nFigure 4.9: A normal mixture density function.\n\n\n\n\n\n\n\nExercise 2.\nAssume that \\(X_1,\\dots,X_n\\overset{\\text{iid}}{\\sim}X\\) is an iid random sample from a continuous uniform distribution on \\([0,1]\\) (i.e.~\\(X\\sim U(0,1)\\)). Consider the asymptotic behavior of a kernel estimator \\(\\hat f_{nh}\\) of \\(f\\) using an Epanechnikov kernel and under an asymptotic setup, where \\(n\\rightarrow\\infty\\), \\(h\\equiv h_n\\rightarrow 0\\), and \\(\\frac{1}{nh_n}\\rightarrow 0.\\)\n\nWhat is the bias of the kernel density estimator \\(\\hat f_{nh}(x)\\) at the point \\(x=0.5\\)?\nShow that \\(\\hat f_{nh}(x)\\) is not a MSE-consistent estimator of \\(f(x)\\) at the point \\(x=0.\\)\n\n\n\nExercise 3 (lengthy exercise)\nLet \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\quad\\text{with}\\quad X\\sim f\n\\] We are now interested in estimating the derivative \\(f'(x)\\) at a point \\(x.\\) When using a continuously differentiable kernel function (use the Biweight kernel), the derivative \\(\\hat f_h'(x)\\) of a kernel estimator \\(\\hat f_{nh}(x)\\) provides an estimator of \\(f'(x).\\) Now consider the asymptotic behavior of \\(\\hat f_{nh}'(x)\\) for \\(n\\rightarrow\\infty\\), \\(h_n\\rightarrow 0\\), \\(\\frac{1}{nh_n^2}\\rightarrow 0,\\) assuming that \\(f\\) is three times continuously differentiable.\n\nShow that \\[\n\\mathbb{E}\\left( \\hat f_h'(x)\\right) = f'(x)+c(K)h^2f^{'''}(x)+o\\left(h^2\\right),\n\\] where \\(c(K)\\) is a constant depending on the kernel function.\nMoreover, show that \\[\nVar\\left( \\hat f_h'(x)\\right) = \\frac{f(x)d(K)}{nh^2}+o\\left(\\frac{1}{nh^2}\\right),\n\\] where \\(d(K)\\) is a constant depending on the kernel function. Can it be concluded from these results that \\(\\hat f_h'(x)\\) is a (weakly) consistent estimator of \\(f'(x)\\)? Justify your answer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#solutions",
    "href": "Ch4_NP_Density_Estimation.html#solutions",
    "title": "4  Nonparametric Density Estimation",
    "section": "Solutions",
    "text": "Solutions\n\nSolution of Exercise 1.\n\nSee solution of b. for the definition of \\(\\hat{f}_h(x).\\) A sketch of the graph of \\(\\mathrm{MISE}(h)\\) results from the sum of the graphs of \\(\\int \\operatorname{Var}(\\hat{f}_h(x))dx\\) and \\(\\int \\operatorname{Bias}(\\hat{f}_h(x))^2dx\\). (See eWhiteboard for the sketch.)\n\\(\\hat{f}_h(x)\\) is a density function, if (i) \\(\\hat{f}_h(x)\\geq 0,\\) for all \\(x\\in\\mathbb{R}\\) and (ii) \\(\\int\\hat{f}_h(x)dx=1.\\) Part (i) is fulfilled, since: \\[\\begin{align*}\n\\hat{f}_{h}(x)\n=\\frac{1}{nh}\\sum_{i=1}^n K\\left(\\frac{x-X_i}{h}\\right)\n\\geq \\frac{1}{nh}\\sum_{i=1}^n 0\n=0,\n\\end{align*}\\] where we use that \\(K(z)\\geq 0\\) for all \\(z\\in\\mathbb{R}.\\) See, for instance, the Epanechnikov kernel function: \\[\nK(z)=\\frac{3}{4}(1-z^2)\\mathbf{1}_{z\\in[-1,1]}\\quad\\Rightarrow\\quad 0\\leq K(z)\\leq\\frac{3}{4}\\quad\\text{for all }z\\in\\mathbb{R}.\n\\]\nPart (ii) is also fulfilled, since: \\[\\begin{align*}\n\\int\\hat{f}_{h}(x)\\,dx\n&=\\frac{1}{nh}\\sum_{i=1}^n \\int K\\left(\\frac{x-X_i}{h}\\right)\\,dx\\\\\n&[\\text{Subst:}\\frac{x-X_i}{h}=y,\\quad \\frac{dy}{dx}=\\frac{1}{h}\\Leftrightarrow dx=h dy]\\\\\n&=\\frac{h}{nh}\\sum_{i=1}^n \\int K\\left(y\\right)\\,dy\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n 1\n=1,\n\\end{align*}\\] where we use that \\(\\int K(y)\\,dy=1.\\)\nStarting point: AMISE-optimal bandwidth choice: \\[\\begin{align*}\nh_{\\textrm{opt}}&=\\left\\{\\frac{R(K)}{n\\,\\nu_2(K)^2\\,\\int f''(x)^2\\,dx}\\right\\}^{1/5}\n\\end{align*}\\] where \\(\\int f''(x)^2\\,dx\\) is the only unknown quantity. The Normal-Referenz bandwidth \\(h_{NR}\\) approximates this unknown quantity using a normal distribution. Idea: If \\(f\\approx\\phi_{\\mu,\\sigma},\\) then \\(\\int f''(x)^2\\,dx\\approx \\int \\phi_{\\mu,\\sigma}''(x)^2\\,dx,\\) where \\(\\sigma\\) can be estimated from the data. (The parameter \\(\\mu\\) has no influence on the value of the integral.)\nThe density shown in Figure 4.8 is a multi-modal denisty with four peaks. It is to be expected that: \\[\n\\int f''(x)^2\\,dx &gt; \\int \\phi_{\\mu,\\sigma}''(x)^2\\,dx \\quad\\Rightarrow\\quad h_{\\mathrm{opt}} &lt; h_{NR}.\n\\] Regardless of whether \\[\n\\int \\phi_{\\mu,\\sigma}''(x)^2\\,dx\\approx\\int f''(x)^2\\,dx\n\\] or\n\\[\n\\int \\phi_{\\mu,\\sigma}''(x)^2\\,dx\\not\\approx\\int f''(x)^2\\,dx\n\\] the convergence rate is \\[\\begin{align*}\n\\mathrm{MISE}(\\hat{f}_{NR})\n  &=\\frac{5}{4}\\left\\{\\nu_2(K)^2\\,R(K)^4\\,\\int \\phi_{\\mu,\\sigma}''(x)^2\\,dx\\right\\}^{1/5}\\,n^{-4/5}+o(n^{-4/5})\\\\[1ex]\n  &=\\mathrm{Const}\\cdot n^{-4/5}+o(n^{-4/5})\\\\[1ex]\n  &=\\mathcal{O}\\left(n^{-4/5}\\right)\n\\end{align*}\\] Of course, this result is only of limited use for finite sample sizes \\(n,\\) where the estimation result strongly depends on a good choice of the bandwidth \\(h.\\)\n\n\n\nSolution of Exercise 2.\n\nThe bias is defined as \\(\\mathrm{Bias}(\\hat{f}_h(x))=\\mathbb{E}\\left[\\hat{f}_h(x)\\right]-f_{\\mathrm{uni}}(x).\\) Thus, \\[\\begin{align*}\n  \\mathbb{E}\\left[\\hat{f}_h(x)\\right]\n  &=\\frac{1}{h}\\mathbb{E}\\left[K\\left(\\frac{x-X}{h}\\right)\\right]\\\\\n  &=\\frac{1}{h}\\int K\\left(\\frac{x-u}{h}\\right)f_{\\mathrm{uni}}(u)\\,du\\\\[2ex]\n  &=\\frac{1}{h}\\int K\\left(\\frac{u-x}{h}\\right)f_{\\mathrm{uni}}(u)\\,du\\quad \\text{($K$ symmetric)}\\\\[2ex]\n  &[\\text{Subst:}\\; y=\\frac{u-x}{h}\\Leftrightarrow u=x+yh\\quad\\Rightarrow du=h\\,dy]\\\\[2ex]\n  &=\\frac{h}{h}\\int K(y)f_{\\mathrm{uni}}(x+yh)\\,dy\\\\\n  &=           \\int_{-1}^1 K(y)f_{\\mathrm{uni}}(x+yh)\\,dy\\quad \\text{($K$ Epanechnikov)}\n\\end{align*}\\]\nFor \\(x=0.5\\) and sufficiently small bandwidth \\(h\\equiv h_n &lt; 0.5,\\) we have that \\(f_{\\mathrm{uni}}(0.5+yh)=1\\) for all \\(y\\in[-1,1],\\) such that \\[\\begin{align*}\n  \\mathbb{E}\\left[\\hat{f}_h(0.5)\\right]\n  &=\\int_{-1}^1 K(y)\\,1\\,dy = 1 = f_{\\mathrm{uni}}(0.5)\n\\end{align*}\\] \\(\\Rightarrow \\mathrm{Bias}(\\hat{f}_h(0.5))=\\mathbb{E}\\left[\\hat{f}_h(0.5)\\right]-f_{\\mathrm{uni}}(0.5)=1-1=0\\) for all \\(h\\equiv h_n &lt; 0.5.\\)\nTo be shown: \\(\\hat{f}_h(0)\\not\\to_{\\mathrm{MSE}} f_{\\textrm{uni}}(0),\\) where \\(f_{\\textrm{uni}}(0)=1\\). From a. we have that \\[\\begin{align*}\n  \\mathbb{E}\\left[\\hat{f}_h(0)\\right]\n  &=\\int_{-1}^1 K(y)f_{\\mathrm{uni}}(0+yh)\\,dy\\\\\n  &=\\int_{-1}^1 K(y)\\mathbf{1}_{0\\leq yh\\leq 1}\\,dy\\quad [\\text{Definition of }f_{\\mathrm{uni}}]\\\\\n  &=\\int_{{\\color{red}0}}^1 K(y)\\mathbf{1}_{0\\leq yh\\leq 1}\\,dy\\quad [\\text{Integrand is zero for all }y&lt;0]\\\\\n  &=\\int_{0}^1 K(y)\\mathbf{1}_{{\\color{red}yh\\leq 1}}\\,dy\\quad [\\text{Indicator function is here zero only for }yh&gt;1.]\\\\\n\\end{align*}\\] Thus, for all sufficiently large sample sizes \\(n\\) and corresponding sufficiently small bandwidth values \\(h=h_n\\) such that \\(yh_n\\leq 1\\) for all \\(0\\leq y\\leq 1,\\) we have that \\[\\begin{align*}\n  \\mathbb{E}\\left[\\hat{f}_h(0)\\right]=\\int_{0}^1 K(y)\\,{\\color{red}1}\\,dy = 0.5\n\\end{align*}\\]\nThus, at the boundary point \\(x=0.5,\\) we have for \\(n\\to\\infty,\\) \\(h_n\\to 0\\) that \\[\\begin{align*}\n&\\quad \\mathbb{E}\\left[\\hat{f}_h(0)\\right]\\to 0.5 \\neq  f_{\\textrm{uni}}(0)=1\\\\\n\\Rightarrow&\\quad (\\operatorname{Bias}(\\hat{f}_h(0)))^2\\not\\to 0\\\\\n\\Rightarrow&\\quad \\hat{f}_h(0)\\not\\to_{\\mathrm{MSE}} f_{\\textrm{uni}}(0)\n\\end{align*}\\]\n\n\n\nSolution of Exercise 3.\nPart a.\nThe mean: \\[\\begin{align*}\n  &\\mathbb{E}\\left(\\frac{d}{dx}\\hat{f}_h(x)\\right)\\\\\n  &=\\mathbb{E}\\left(\\frac{1}{nh}\\sum_{i=1}^n \\frac{d}{dx}K\\left(\\frac{X_i-x}{h}\\right)\\right)\\\\\n  &=\\frac{1}{nh}\\sum_{i=1}^n \\mathbb{E}\\left(\\frac{d}{dx}K\\left(\\frac{X_i-x}{h}\\right)\\right)\\\\\n  &=\\frac{1}{h}\\mathbb{E}\\left(\\frac{d}{dx}K\\left(\\frac{X_i-x}{h}\\right)\\right)\\\\[2ex]\n  &\\quad [\\text{see AC-0 for computing $\\frac{d}{dx}K\\big(\\frac{X_i-x}{h}\\big)$}]\\\\[2ex]\n  &=\\frac{1}{h}\\mathbb{E}\\left(\\frac{1}{h}\\tilde K\\left(\\frac{X_i-x}{h}\\right)\\right)\\quad{\\small\\text{($\\tilde K$ is defined in AC-0)}}\\\\\n  % &=\\frac{1}{h}\\mathbb{E}\\left(\\frac{1}{h}\\tilde K\\left(\\frac{X_i-x}{h}\\right)\\right)\\quad{\\small\\text{(see AC1)}}\\\\\n  &=\\frac{1}{h^2}\\int \\tilde K\\left(\\frac{u-x}{h}\\right)f(u)\\,du\\\\\n  &\\quad [\\text{(Subst: $y=(u-x)/h$, $du=h\\,dy$, $u=x+hy$)}]\\\\\n  &=\\frac{1}{h}\\int \\tilde K(y)f(x+hy)\\,dy\\\\\n  & \\quad\\text{[Taylorexpansion]}\\\\\n  &=\\frac{1}{h}\\int \\tilde K(y)\\left\\{f(x)+f'(x)hy+\\frac{1}{2}f^{''}(x)(hy)^2+\\frac{1}{6}f^{'''}(x)(hy)^3+o(h^3)\\right\\}\\,dy \\\\\n  &=\\frac{1}{h}f(x)\\underbrace{\\int \\tilde K(y)dy}_{=0\\,\\text{(see AC-1)}}\n  +\\frac{h}{h}f'(x)\\underbrace{\\int \\tilde K(y)ydy}_{=1\\,\\text{(see AC-3)}}\n  +\\frac{h^2}{h}\\frac{1}{2}f''(x)\\underbrace{\\int \\tilde K(y)y^2\\,dy}_{=0\\,\\text{(see AC-2)}}\\\\\n  &+\\frac{h^3}{h}f'''(x)\\underbrace{\\left(\\frac{1}{6}\\right) \\int \\tilde K(y)y^3\\,dy}_{=c(K)}+o(h^2)\\\\\n  &=f'(x)+h^2\\,f'''(x)\\,c(K)+o(h^2)\n\\end{align*}\\]\nAuxiliary calculation AC-0:\nDefintion of the Biweight kernel: \\[\nK\\left(\\frac{X_i-x}{h}\\right)=\\left\\{\n  \\begin{array}{ll}\n  \\frac{15}{16}\\left(1-\\left(\\frac{X_i-x}{h}\\right)^2\\right)^2&\\text{for } |(x-X_i)/h|&lt;1\\\\\n  0& \\text{else}\n  \\end{array}\n  \\right.\n\\] Derivative of \\(K((X_i-x)/h)\\) with respect to \\(x\\) (using chain rule twice): \\[\\begin{align*}\n\\frac{d}{dx}K\\left(\\frac{X_i-x}{h}\\right)=&2\\frac{15}{16}\\left(1-\\left(\\frac{X_i-x}{h}\\right)^2\\right)\\left(-2\\left(\\frac{X_i-x}{h}\\right)\\right)\\left(-\\frac{1}{h}\\right)\\\\\n&=\\left(\\frac{1}{h}\\right)\\underbrace{\\frac{60}{16}\\left(1-\\left(\\frac{X_i-x}{h}\\right)^2\\right)\\left(\\frac{X_i-x}{h}\\right)}_{=:\\tilde{K}\\left(\\frac{X_i-x}{h}\\right)}\\\\ &=\\left(\\frac{1}{h}\\right)\\tilde{K}\\left(\\frac{X_i-x}{h}\\right),\n\\end{align*}\\] for \\(|(x-X_i)/h|&lt;1\\), zero else.\nAlternatively, this tedious computation can be carried out using R.\n\nK_exp          &lt;- expression( (15/16)*(1- ((X-x)/h)^2 )^2 ) \n(K_Dx_exp      &lt;- D(K_exp, \"x\"))# derivatives\n\n(15/16) * (2 * (2 * (1/h * ((X - x)/h)) * (1 - ((X - x)/h)^2)))\n\nK_Dx_fun       &lt;- function(x){}\nbody(K_Dx_fun) &lt;- K_Dx_exp\n\nAuxiliary calculation AC-1:\nThe result that \\(\\int \\tilde K(y)dy=0\\) follows directly from the fact that \\(\\tilde K\\) is point-symmetric about the origin. Here is an illustration:\n\nK_tilde &lt;- function(y){(60/16)*(1-y^2)*y}\n\n# integrate(f=function(x){K_tilde(x)*x^3},lower = -1,upper = 1)\n\nplot(y=K_tilde(seq(-1,1,len=30)), \n     x=seq(-1,1,len=30), type=\"l\", xlab=\"\",ylab=\"\")\nabline(v=0,h=0,lty=2)\n\n\n\n\n\n\n\n\nFormally: Point-symmetry implies that \\[\n-\\tilde K(y)=\\tilde K(-y)\n\\] and thus that \\[\n\\tilde K(y)=-\\tilde K(-y).\n\\] Thus \\[\\begin{align*}\n\\int_{-1}^{0}\\tilde K(y)dy\n& = - \\int_{-1}^{0}\\tilde K(-y)dy\\\\\n&[\\text{Subst: } u = -y,\\quad \\frac{du}{dy}=-1, \\text{from }-(-1) \\text{ to }-0]\\\\\n& = \\int_{1}^{0}\\tilde K(u)du\\\\\n& = - \\int_{0}^{1}\\tilde K(u)du\n\\end{align*}\\] Consequently \\[\\begin{align*}\n\\underbrace{\\int_{-1}^{0}\\tilde K(y)dy + \\int_{0}^{1}\\tilde K(y)dy}_{=\\int_{-1}^1\\tilde{K}(y)dy} &=0\\\\\n\\end{align*}\\]\nAuxiliary calculation AC-2:\nAs in AC-1 it follows also here by the point-symmetriy \\(\\tilde K(-y) (-y)^2=-(\\tilde K(y) y^2)\\) that \\[\\begin{align*}\n%\\int_{-1}^0 \\tilde K(y) y^2 \\,dy=-\\int_{0}^{1} \\tilde K(y) y^2 \\,dy\\;\\Rightarrow\\;\n\\int_{-1}^1 \\tilde K(y) y^2 \\,dy=0.\n\\end{align*}\\]\n\n\n\n\n\n\nNote\n\n\n\nOne can show that all even moments of the kernel function \\(\\tilde{K}\\) are zero: \\(\\int \\tilde K(y) y^p \\,dy=0\\) for all \\(p=0,2,4,\\dots\\), since we have point-symmetry\n\\(\\tilde K(-y) (-y)^p=-(\\tilde K(y) y^p)\\) for all \\(p=0,2,4,\\dots\\).\nIn the case of the usual kernel function \\(K\\) all odd moments are zero.\n\n\nAuxiliary calculation AC-3:\nPartial integration: \\[\n\\int_a^b f'(y)\\,g(y)\\,dy=[f(y)\\,g(y)]_a^b-\\int_a^b f(y)\\,g'(y)\\,dy\n\\]\nNote: The antiderivative of \\(\\tilde K(y)=\\frac{60}{16}(1-y^2)\\,y\\) is the Biweight kernel multiplied by \\((-1)\\). That is: \\(-K(y)=-\\frac{15}{16}(1-y^2)^2\\).\nUsing this, we have \\[\\begin{align*}  \n\\int \\tilde K(y) y \\, dy=\\int_{-1}^1 \\tilde K(y) y \\, dy=\\underbrace{\\left[-K(y)\\,y\\right]_{-1}^1}_{=0-0}\\;-\\;\\int_{-1}^1 -K(y)\\,1\\,dy=1.\n\\end{align*}\\]\nPart b.\nYes, \\(\\hat f_h'(x)\\) is a (weakly) consistent estimator of \\(f'(x)\\).\nJustification:\nFrom the given information, it follows that \\[\\begin{align*}\n\\mathrm{AMSE}\\left(\\hat{f}'_h(x)\\right)=\\underbrace{h^4\\,f^{'''}(x)^2\\,c(K)^2}_{\\mathrm{ABias}\\left(\\hat{f}'_h(x)\\right)^2}+\\underbrace{\\frac{f(x)d(K)}{nh^2}}_{\\mathrm{\\mathrm{AVar}\\left(\\hat{f}'_h(x)\\right)}}.\n\\end{align*}\\] Under the assumptions that \\(n\\rightarrow\\infty\\), \\(h\\equiv h_n\\rightarrow 0\\), \\(\\frac{1}{nh_n^2}\\rightarrow 0\\), it follows that \\[\n\\mathrm{MSE}\\left(\\hat{f}'_h(x)\\right)\\xrightarrow[]{}0\\quad\\text{as}\\quad n\\to\\infty.\n\\] An equivalent notation: \\[\n\\hat{f}'_{nh}(x)\\xrightarrow[MSE]{}f'(x)\\quad\\text{as}\\quad n\\to\\infty.\n\\] Convergence in mean square implies convergence in probability (by Chebyshev’s inequality): \\[\\begin{align*}\n&\\hat{f}'_{nh}(x)\\xrightarrow[MSE]{}f'(x) \\quad\\text{as}\\quad n\\to\\infty\\\\\n\\Rightarrow\\quad & \\hat{f}'_{nh}(x)\\xrightarrow[p]{}f'(x)\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#bandwidth-selection",
    "href": "Ch4_NP_Density_Estimation.html#bandwidth-selection",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.6 Bandwidth Selection",
    "text": "4.6 Bandwidth Selection\n\n4.6.1 Normal Reference Bandwidth\nIn the asymptotic formula for the AMISE optimal bandwidth \\(h_{\\mathrm{opt}},\\) the unknown density \\(f\\) enters “only” through the functional \\(\\int \\left(f''(x)\\right)^2 dx.\\)\nIf, for example, one can assume that the underlying density \\(f\\) does not deviate significantly from a normal density, then a reasonable approximation of the optimal bandwidth for the true density \\(f\\) can be obtained via the optimal bandwidth for the normal density.\nReference bandwidths are often very good quick-and-dirty approaches.\nNormal density: \\[\n\\phi_{\\mu,\\sigma}(x)=\\frac{1}{\\sigma} \\phi (\\frac{x-\\mu}{\\sigma}),\n\\] where \\(\\phi\\) denotes the density of the standard normal distribution, \\(\\mu\\) the mean and \\(\\sigma^2\\) the variance of \\(X.\\)\nSome simple derivations yields that \\[\n\\int_{-\\infty}^\\infty \\phi_{\\mu,\\sigma}''(x)^2 dx=\\frac{3}{\\sqrt{\\pi} 8 \\sigma^5}.\n\\] which motivates the Normal-Reference Bandwidth: \\[\\begin{align*}\n  h_{\\mathrm{NR}}=\n  \\left\\{\n    \\frac{8 \\sqrt{\\pi} R(K)}{3 \\nu_2(K)^2\\; n}\n    \\right\\}^{1/5} \\hat{\\sigma},\n\\end{align*}\\] where \\(\\hat\\sigma\\) denotes an estimator of the standard deviation, such as, for instance, the empirical standard deviation \\[\n\\hat{\\sigma}_n = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2},\n\\] or the robust estimator for the standard deviation of normal distributed data \\[\n\\hat{\\sigma}_n = \\mathrm{IQR}_n / (\\Phi^{-1}(0.75) - \\Phi^{-1}(0.25)),\n\\] or the minimum of these two estimators.\n\n\n4.6.2 Bandwidth Selection by Cross Validation\nAim: Choose \\(h,\\) such that \\(\\textrm{ISE}(\\hat{f}_h)\\) is minimized.\n\\[\\begin{align*}\n\\mathrm{ISE}(\\hat{f}_{nh})\n&=\\int_{-\\infty}^{\\infty}\\left( \\hat{f}_{nh}(x) - f(x)\\right)^2 \\, dx \\\\[2ex]\n&= \\int_{-\\infty}^{\\infty} \\left(\\hat{f}_{nh}(x)\\right)^2dx\n-2\\int_{-\\infty}^{\\infty} \\hat{f}_{nh}(x)f(x)dx\n+\\int_{-\\infty}^{\\infty} \\left(f(x)\\right)^2dx.\n\\end{align*}\\]\nSince \\(\\int \\left(f(x)\\right)^2dx\\) does not depend on \\(h,\\), minimizing \\(\\textrm{ISE}(\\hat{f}_{nh})\\) with respect to \\(h\\) is equivalent to minimizing \\[\\begin{align*}\nh_{\\mathrm{ISE}}\n&=\\arg\\min_{h&gt;0}\\mathrm{ISE}(\\hat{f}_{nh})\\\\[2ex]\n&=\\arg\\min_{h&gt;0} \\left(\\int_{-\\infty}^{\\infty} \\left(\\hat{f}_{nh}(x)\\right)^2dx\n-2\\int_{-\\infty}^{\\infty} \\hat{f}_h(x)f(x)dx\\right).\n\\end{align*}\\]\nProblem: The second term contains the unknown quantity \\(f(x)\\).\nIdee: Estimate \\(\\int \\hat{f}_{nh}(x)f(x)dx\\) by \\[\n\\frac{1}{n}\\sum_{i=1}^n \\hat{f}_{nh,-i}(X_i),\n\\] where \\(\\hat f_{nh,-i},\\) for \\(i=1,\\dots,n,\\) denotes a kernel density estimator computed by leaving out the \\(i\\)th observation–i.e., using the reduced sample \\(X_1,\\dots, X_{i-1},X_{i+1},\\dots,X_n.\\)\nIt can be shown that \\[\n\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n \\hat{f}_{h,-i}(X_i)\\right)=\\mathbb{E}\\left(\\int \\hat{f}_h(x)f(x)dx\\right).\n\\] I.e. \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{f}_{h,-i}(X_i)\\) is an unbiased estimator of \\(\\mathbb{E}\\left(\\int \\hat{f}_h(x)f(x)dx\\right).\\)\nThis and further positive properties justifies the so-called Least Squares Cross-Validation (LSCV) criterion. \\[\n\\mathrm{LSCV}(h)=\\int_{-\\infty}^{\\infty} \\left(\\hat{f}_{h}(x)\\right)^2dx\n-2\\frac{1}{n}\\sum_{i=1}^n \\hat{f}_{h,-i}(X_i).\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\textrm{ISE}(\\hat{f}_h)\\) and therefore also the (unknown) ISE-optimal bandwidth \\(h_{\\textrm{ISE}}\\) are random variables. That is, the target to be estimated is itself a random variable.\nThis makes the estimation of \\(h_{ISE}\\) are very hard problem. In fact, one has that\n\\[\n\\frac{\\hat{h}_{\\mathrm{ISE}}}{h_{\\mathrm{ISE}}}-1 = \\mathcal{O}_p(n^{-1/10}),\n\\] where \\(\\hat{h}_{ISE}\\) denotes the best possible estimator of \\(h_{ISE}\\) (see Hall and Marron (1987))\n\n\nMinimizing \\(\\mathrm{LSCV}(h)\\) with respect to \\(h\\), formally \\[\n\\hat{h}_{\\mathrm{LSCV}}=\\arg\\min_{h&gt;0}\\mathrm{LSCV}(h),\n\\] yields the best possible estimator for the random (stochastic) bandwidth \\(h_{\\textrm{ISE}}\\).\nThat is, \\[\n\\frac{\\hat{h}_{\\mathrm{LSCV}}}{h_{\\textrm{ISE}}}-1=\\mathcal{O}_p(n^{-1/10})\n\\]\nLeast Squares Cross Validation is often also referred to as Unbiased Cross-Validation, because \\[\\begin{align*}\n\\mathbb{E}\\left(\\mathrm{LSCV}(h)\\right)\n&=\\mathbb{E}\\left(\\int_{-\\infty}^{\\infty}\\left( \\hat{f}_{nh}(x) - f(x)\\right)^2 \\, dx\\right) - \\int_{-\\infty}^{\\infty} \\left(f(x)\\right)^2dx \\\\\n&=\\mathrm{MISE} - \\int_{-\\infty}^{\\infty} \\left(f(x)\\right)^2dx\n\\end{align*}\\]\nThat is, on average, minimizing \\(\\mathrm{LSCV}(h)\\) yields the same result as minimizing the MISE criterion.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#theory-globally-optimal-bandwidth-choice",
    "href": "Ch4_NP_Density_Estimation.html#theory-globally-optimal-bandwidth-choice",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.5 Theory: Globally Optimal Bandwidth Choice",
    "text": "4.5 Theory: Globally Optimal Bandwidth Choice\nOne distinguishes locally optimal bandwidth choices for estimating \\(f(x)\\) at a given \\(x,\\) and globally optimal bandwidth choices that are optimal with respect to a global loss function.\nTypically, one determines a globally optimal bandwidth by minimizing the Mean Integrated Squared Error loss function \\[\n\\begin{align*}\n\\mathrm{MISE}(\\hat{f}_{nh})\n&=\\int\\mathrm{MSE}(\\hat{f}_{nh}(x))\\,dx\\\\[2ex]\n&=\\int \\mathbb{E}\\left[(\\hat{f}_{nh}(x)-f(x))^2\\right]\\,dx\\\\[2ex]\n&=\\int Var\\left(\\hat{f}_{nh}(x)  \\right) \\,dx +\n  \\int \\left(\\mathrm{Bias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\\,dx.\n\\end{align*}\n\\]\nProblem: The exact computation of \\(\\mathrm{MISE}(\\hat{f}_{nh})\\) is only possible under very restrictive, simplifying assumptions such as assuming that \\(f\\) is a specific parametric density function—an assumption we do not want to make!\nSolution: Therefore, we compute an asymptotic approximation to \\(\\mathrm{MISE}(\\hat{f}_{nh}),\\) \\[\n\\begin{align*}\n\\mathrm{MISE}(\\hat{f}_{nh})&\\approx \\mathrm{AMISE}(\\hat{f}_{nh})\\\\[2ex]\n&=\\int AVar\\left(\\hat{f}_{nh}(x)  \\right) \\,dx +\n  \\int \\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\\,dx\\\\[2ex]\n\\end{align*}\n\\] which becomes good as \\(n\\to\\infty,\\) and which can be done without making restrictive parametric assumptions on \\(f.\\)\nThe asymptotic approximation \\(\\mathrm{AMISE}(\\hat{f}_{nh})\\) for \\(\\mathrm{MISE}(\\hat{f}_{nh})\\) is the most often used loss function for determining a global choice of the bandwidth \\(h.\\)\n\nAsymptotic Approximation of \\(\\mathrm{MISE}(\\hat{f}_{nh})\\)\nComputing \\(\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\\colon\\)\nLet \\[\nX_1,\\dots,X_n\\overset{\\text{iid}}{\\sim}X,\\quad\\text{where}\\quad X\\sim f.\n\\]\nGenerally, the mean \\(\\mathbb{E}\\left(\\hat{f}_{nh}(x)\\right)\\) cannot be computed.\nHowever, using that \\(n\\to\\infty\\) and \\(h\\equiv h_n\\to 0,\\) we can approximate the mean \\(\\mathbb{E}\\left(\\hat{f}_{nh}(x)\\right)\\) by \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\hat{f}_{nh}(x)\\right) &=f(x) + h^2 \\frac{1}{2} f''(x) \\nu_2(K) + o(h^2)\n\\end{align*}\n\\tag{4.6}\\]\n\n\n\n\n\n\nDeriving Equation 4.6\n\n\n\n\n\n\\[\\begin{align*}\n\\mathbb{E}\\left(\\hat{f}_{nh}(x)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{h}K\\left(\\frac{x-X_i}{h}\\right)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{h}\\mathbb{E}\\left(K\\left(\\frac{x-X_i}{h}\\right)\\right)\\\\[2ex]\n&\\left[\\text{since }X_1,\\dots,X_n\\overset{\\text{iid}}{\\sim}X\\right]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{1}{h} K\\left(\\frac{x - X}{h}\\right)\\right)\\\\[2ex]\n&=\\int_{-\\infty}^\\infty \\frac{1}{h} K\\left(\\frac{x - u}{h}\\right)f(u)du\\\\[2ex]\n&\\left[\\text{Substitution: $u=x+yh\\;\\Rightarrow \\frac{du}{dy}=h$}\\right]\\\\[2ex]\n&=\\int_{-\\infty}^\\infty \\frac{h}{h} K(y) f(x + y h ) dy\\\\[2ex]\n&\\left[\\text{Taylor expansion of $f(x+yh)$ around $f(x)\\colon$}\\right]\\\\[2ex]\n&=\\int_{-\\infty}^\\infty K(y) \\left\\{ f(x) + f'(x) y h + \\frac{1}{2!} f''(x) y^2 h^2 + o(h^2) \\right\\} dy\\\\[2ex]\n&=f(x) \\underbrace{\\int_{-\\infty}^\\infty K(y)\\,dy}_{=1} +\n  f(x) h \\underbrace{\\int_{-\\infty}^\\infty y K(y)\\,dy}_{=0}\\\\[2ex]\n&\\;\\; + h^2 \\frac{1}{2} f''(x) \\underbrace{\\int_{-\\infty}^\\infty K(y)y^2 dy}_{\\nu_2(K)}+ o(h^2)  \\underbrace{\\int_{-\\infty}^\\infty K(y)\\,dy}_{=1}\\\\[3ex]\n&=f(x) + h^2 \\frac{1}{2} f''(x) \\nu_2(K) + o(h^2)\n\\end{align*}\\]\n\n\n\nThus, the pointwise squared bias is given by \\[\n\\begin{align*}\n  &\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2 \\\\[2ex]\n  &=\\left(\\mathbb{E}(\\hat{f}_{nh}(x))-f(x)\\right)^2 \\\\[2ex]\n  &=\\left(h^2 \\frac{1}{2} f''(x) \\nu_2(K) + o(h^2)\\right)^2\\\\[2ex]\n  &=\\left(h^2 \\frac{1}{2} f''(x) \\nu_2(K)\\right)^2 + \\underbrace{2\\,\\left({\\color{red}h^2} \\frac{1}{2} f''(x) \\nu_2(K)\\right)  o({\\color{red}h^2})}_{o({\\color{red}h^4})} + o(h^4)\\\\[2ex]\n  &=\\underbrace{h^4 \\frac{1}{4} \\nu_2(K)^2 f''(x)^2}_{=:\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2}  + o(h^4)\n\\end{align*}  \n\\]\nThis shows that the kernel estimator \\(\\hat{f}_{nh}(x)\\) is asymptotically unbiased, since \\[\n\\begin{align*}\n  \\mathrm{Bias}(\\hat{f}_{nh}(x))&=\\left(h^4 \\frac{1}{4} \\nu_2(K)^2 f''(x)^2 + o(h^4)\\right)^{1/2} \\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}  \n\\]\nThe squared asymptotic bias is given by neglecting the smaller order \\(o(h^4)\\) terms: \\[\n\\begin{align*}\n\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2=h^4 \\frac{1}{4} \\nu_2(K)^2 f''(x)^2\n\\end{align*}  \n\\]\nNote that the squared bias and the squared asymptotic bias \\[\n\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2\\quad\\text{and}\\quad\n\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2\n\\] are asymptotically equivalent, since for \\(n\\to\\infty\\) \\[\\begin{align*}\n\\frac{\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2}{\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2}\n& =1+\\frac{o(h^4)}{\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2} \\\\[2ex]\n&=1+o(h^4) \\cdot O(h^{-4})\\\\[2ex]\n&=1+o(1\\cdot h^4) \\cdot O(1\\cdot h^{-4})\\\\[2ex]\n&=1+o(1) \\cdot O(1\\cdot h^{-4}\\cdot h^4)\\\\[2ex]\n&=1+o(1) \\cdot O(1)\\\\[2ex]\n&=1+o(1)\\\\\n\\Rightarrow\\quad\n\\lim_{n\\to\\infty}\\frac{\\left(\\mathrm{Bias}(\\hat{f}_{nh}(x))\\right)^2}{\\left(\\mathrm{ABias} \\left( \\hat{f}_{nh}(x) \\right)\\right)^2} & =1\n\\end{align*}\\]\nComputing \\(AVar\\left( \\hat{f}_{nh}(x) \\right)\\colon\\) eWhiteboard\nComputing \\(\\operatorname{AMISE}\\left( \\hat{f}_{nh}(x) \\right)\\colon\\) eWhiteboard\n\\[\\begin{align*}\n\\mathrm{AMISE}(\\hat{f}_{nh}) &=\n\\frac{1}{nh} R(K) +\n\\frac{1}{4} h^4 \\nu_2(K)^2 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx\\\\[2ex]  \n\\end{align*}\\]\nMinimizing \\(\\mathrm{AMISE}(\\hat{f}_h)\\) with respect to \\(h\\) yields the (asymptotically) optimal bandwidth: \\[\\begin{align*}\n  h_{\\mathrm{opt}}&=\n  \\left\\{\n    \\frac{R(K)}{{n\\,\\nu_2(K)}^2 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx}\n  \\right\\}^{1 / 5}\\\\[2ex]\n  &=\\texttt{Constant} \\cdot n^{-1/5}\n\\end{align*}\\]\nKnown quantities: Kernel constants \\[\\begin{align*}\nR(K)\n&=\\int\\left(K(y)\\right)^2\\,dy\\\\\n&(=\\frac{3}{5}\\text{, in case of the Epanechnikov kernel})\\\\[2ex]\n\\nu_2(K)&=\\int y^2\\,K(y)\\,dy\\\\\n&(=\\frac{1}{5}\\text{, in case of the Epanechnikov kernel})\n\\end{align*}\\]\n\n\\(\\nu_2(K)\\colon\\) second moment of the kernel\n\\(R(K)\\colon\\) roughness of the kernel\n\nUnknown quantity: The global roughness of \\(\\boldsymbol{f}\\)\n\\[\n\\int \\left(f''(x)\\right)^2 dx\n\\]\nThe minimal value of the AMISE when using the optimal bandwidth is therefore given by:\n\\[\\begin{align*}\n\\mathrm{AMISE}(\\hat{f}_{nh_{\\mathrm{opt}}})\n  &=\\min_{h &gt; 0} \\mathrm{AMISE}(\\hat{f}_{nh} )\\\\[2ex]\n  &=\\frac{1}{nh_{\\mathrm{opt}}} R(K) +\n\\frac{1}{4} h_{\\mathrm{opt}}^4 \\nu_2(K)^2 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx\\\\[2ex]\n  &=\\frac{5}{4} \\left\\{\\nu_2(K)^2 R(K)^4 \\int_{-\\infty}^\\infty \\left(f''(x)\\right)^2 dx \\right\\}^{1/5} n^{-4/5}\\\\[2ex]\n  &=\\texttt{Constant} \\cdot n^{-4/5}\n\\end{align*}\\]\nThat is, \\(\\hat{f}_{nh_{\\mathrm{opt}}}\\) is AMISE (i.e. MSE) consistent with rate \\(n^{-4/5}.\\)\nThis is a non-parametric convergence rate.\n\n\n\n\n\n\nParametric MSE rate of \\(\\bar{X}_n\\) for \\(\\mu_0\\)\n\n\n\n\\[\\begin{align*}\n\\mathrm{MSE}(\\bar{X}_n)\n  &=(\\mathrm{Bias}(\\bar{X}_n))^2 + Var(\\bar{X}_n)\\\\[2ex]\n  &=Var(\\bar{X}_n)\\\\[2ex]\n  &=\\frac{Var(X)}{n}\\\\[2ex]\n  &=\\texttt{Constant} \\cdot n^{-1}\n\\end{align*}\\]\n\n\nOne immediately recognizes some key properties of \\(\\mathrm{AMISE}(\\hat{f}_{nh_{\\mathrm{opt}}})\\):\n\nIt decreases as the sample size \\(n\\) increases, with a rate \\(n^{-4/5}\\).\nThe influence of the kernel \\(K\\) appears through \\(\\nu_2(K)^2\\) and \\(R(K)\\).\nThe influence of the density \\(f\\) appears through \\(\\int \\left(f''(x)\\right)^2 dx\\).\n\nA density \\(f\\) with a high roughness, as measured by \\(\\int \\left(f''(x)\\right)^2 dx,\\) is more difficult to estimate than one with low roughness.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  },
  {
    "objectID": "Ch4_NP_Density_Estimation.html#choice-of-the-kernel",
    "href": "Ch4_NP_Density_Estimation.html#choice-of-the-kernel",
    "title": "4  Nonparametric Density Estimation",
    "section": "4.4 Choice of the Kernel",
    "text": "4.4 Choice of the Kernel\n\n\n\n\n\n\nReminder: Definition of a Density Function\n\n\n\nWe call a function \\(f\\) a density function if it fulfilles the following properties:\n\nNon-negative: \\(f(x)\\geq 0\\) for all \\(x\\in\\mathbb{R}\\)\nNormed: \\(\\int f(x)dx = 1\\)\n\n\n\nIt can be shown that if \\(K\\) is a density function, then also \\(f_{nh}\\) is a density function:\n\nNon-negative: \\[\nK(x) \\geq 0\\quad\\text{for all}\\quad x\\in\\mathbb{R}\\quad \\Rightarrow\\quad \\hat{f}_{nh}(x) \\geq 0\\quad\\text{for all}\\quad x\\in\\mathbb{R}\n\\]\nNormed to 1: \\[\n\\int K(x)dx = 1\\quad \\Rightarrow\\quad \\int \\hat{f}_{nh}(x)dx = 1\n\\]\n\n(See Exercises.)\nThis inheritance of the properties of \\(K\\) to the properties of \\(f_{nh}\\) also holds for the smoothness properties:\n\nSmoothness of \\(\\boldsymbol{K}\\): If \\(K\\) is continuously differentiable, then also \\(\\hat{f}_{nh}\\) is continuously differentiable.\n\nTypical kernel functions are smooth (continuous) density functions that are symmetric around zero.\n\nSymmetric around zero: \\[\nK(x)=K(-x)\\quad\\text{such that}\\quad \\int xK(x)dx=0\n\\]\n\nExamples:\n\nThe family of the symmetric beta density functions; for \\(p = 0,1,2,\\ldots\\) \\[\nK(u; p) =\\left\\{\n  \\begin{array}{ll}\n  \\mathrm{Const}_p \\left(1 - u^2 \\right)^p&\\text{for }u \\in [-1,1]\\\\\n  0 &\\text{else}\n  \\end{array}\\right.,\n\\] where the \\(p\\)-specific constant is chosen such that the kernel integrates to one.\n\n\\(p=0\\) yields the uniform kernel: \\[\nK(u) = \\frac{1}{2}\n\\]\n\\(p=1\\) yields the (important) Epanechnikov kernel: \\[\n   K(u) = \\frac{3}{4}\\,(1-u^2)\n   \\]\n\\(p=2\\) yields the biweight kernel: \\[\n   K(u) = \\frac{15}{16}\\,(1-u^2)^2\n   \\]\n\\(p=3\\) yields the triweight kernel: \\[\n   K(u) = \\frac{35}{32}\\,(1-u^2)^3\n   \\]\n\nNormal (Gaussian) kernel: \\[\nK(u) = \\phi(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2 / 2)\n\\] for \\(u \\in\\mathbb{R}.\\)\nTriangular kernel: \\[\nK(u) = 1-|u|\n\\] for \\(u\\in[-1,1]\\) and \\(0\\) else.\n\n\n4.4.1 Choice of the Bandwidth Parameter\nUsing a too small bandwidth (e.g. here \\(h=\\) 2.14):\n\ndata  &lt;- inc76$income\nh     &lt;- bw.SJ(inc76$income) * .25  # too small bandwidth\nKDE   &lt;- density(data, bw = h, from = min(data), to = max(data))\n\nplot(x = KDE$x, y = KDE$y, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Kernels Density Estimtion With a too Small Bandwidth\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.6: Kernel density estimate with a too small bandwidth.\n\n\n\n\n\nToo a too large bandwidth (e.g. here \\(h=\\) 47):\n\ndata  &lt;- inc76$income\nh     &lt;- bw.SJ(inc76$income) * 2.5  # too large bandwidth\nKDE   &lt;- density(data, bw = h, from = min(data), to = max(data))\n\nplot(x = KDE$x, y = KDE$y, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Kernels Density Estimtion With a Too Large Bandwidth\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.7: Kernel density estimate with a too large bandwidth.\n\n\n\n\n\n\nUsing a well estimated bandwidth (e.g. here \\(h=\\) 8.55) based on the method proposed by Sheather and Jones (1991):\n\ndata  &lt;- inc76$income\nh     &lt;- bw.SJ(inc76$income) # good bandwidth\nKDE   &lt;- density(data, bw = h, from = min(data), to = max(data))\n\nplot(x = KDE$x, y = KDE$y, \n     ylab=\"Density\", xlab=\"Income\", \n     main = \"Kernels Density Estimtion With a Good Bandwidth Choice\", type=\"l\")\n\n\n\n\n\n\n\nFigure 4.8: Kernel density estimate with a good bandwidth choice.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Nonparametric Density Estimation</span>"
    ]
  }
]