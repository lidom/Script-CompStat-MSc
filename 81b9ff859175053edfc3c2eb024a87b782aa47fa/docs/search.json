[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics (M.Sc.)",
    "section": "",
    "text": "Day \n    Time \n    Lecture Hall \n  \n \n\n  \n    Monday \n    14:15-15:45 \n    Jur / H√∂rsaal K \n  \n  \n    Thursday \n    14:15-15:45 \n    Jur / RS 0.017 \n  \n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\nEM Algorithm & Cluster Analysis\nBootstrap\nNonparametric Regression\nFunctional Data Analysis\n\n\n\n\n\nThis online script available at: https://www.dliebl.com/Script-CompStat-MSc/ (pwd: compstat)\nWe‚Äôll use an eWhiteboard for derivations and some extra explanations.\nBasic material from our econometrics course:\n\nIntroduction to R\nProbability\n\n\n\n\n\n\n\n\n\nConsider using git/github for your personal course notes.\n\nhttps://happygitwithr.com/"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html",
    "href": "Ch1_MaximumLikelihood.html",
    "title": "1¬† Maximum Likelihood",
    "section": "",
    "text": "The basic idea behind maximum likelihood estimation is very simple: Assume that the data is generated by some distribution with a certain (finite) set of unknown distribution parameters (e.g.¬†the normal distribution with unknown mean and variance). Then find the distribution parameters for which it is most likely that the distribution has generated the data we actually observed.\nIn (classical) maximum likelihood estimation we must be rather specific about the process that generated the data. This is a trade off ‚Äì by imposing a fair amount of structure on the data, we get in return a very desirable estimator. The question remains, however, whether we have made the right decision about the specific distribution/density function.\n\n\nWhy do we like maximum likelihood as an estimation method? The answer is that: A maximum likelihood estimator \\(\\hat\\theta_n\\) of some parameter \\(\\theta_0\\in\\mathbb{R}\\) is\n\nConsistent: \\(\\hat\\theta_n\\rightarrow_p\\theta_0\\) as \\(n\\to\\infty\\)\nAsymptotically normal: \\(\\sqrt{n}(\\hat\\theta_n-\\theta_0) \\stackrel{a}{\\sim} \\mathcal{N}(0, \\sigma^2)\\)\nAsymptotically efficient: This means that no consistent estimator has lower asymptotic mean squared error than the maximum likelihood estimator.\n\nLikewise for multivariate parameter \\(\\theta_0\\in\\mathbb{R}^p.\\)\nThus, maximum likelihood estimators can be very appealing, provided that the assumption on the general distribution family is correct.\n\n\n\n\n\n\nML-estimation requires to fix the family of distributions \\(f(\\cdot|\\theta)\\)\n\n\n\nClassic ML-estimation requires us to fix the general family of density/probability mass functions \\(f\\) of the random variables in the (i.i.d.) random sample \\(X_1,\\dots,X_n\\) such that \\(X_i\\sim f\\), \\(i=1,\\dots,n,\\) where \\(f\\) is known up to an unknown parameter \\(\\theta,\\) where \\(\\theta\\in\\mathbb{R}^K\\) is allowed to be a finite (\\(1\\leq K<\\infty\\)) dimensional vector.\nExamples:\n\n\\(f\\) being the probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta)\\) with \\[\nf(x_i|\\theta)=\n\\left\\{\n\\begin{array}{ll}\n\\theta,&\\text{if } x_i=1\\\\\n1-\\theta, & \\text{if } x_i=0\n\\end{array}\n\\right.\n\\] and unknown parameter \\(0\\leq \\theta\\leq 1.\\)\n\\(f\\) is the normal density \\[\nf(x_i|\\theta)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)\\right)\n\\] with unknown parameter vector \\(\\theta=(\\mu,\\sigma^2)^T.\\)\n\nThis requirement (fixing the family of density functions) can be overly restrictive. In many applications we typically do not know the family of \\(f.\\) To address this issue, the quasi maximum likelihood theory generalizes classic maximum likelihood estimation to cases where \\(f\\) is misspecified (see White (1982)).\n\n\n\n\n\nTo introduce the main idea of maximum likelihood estimation, we use the simple example of a coin flipping experiment, where a possibly unfair \\(\\text{Coin}\\) can take the value \\(H\\) (Head) or \\(T\\) (Tail), \\[\n\\text{Coin}\\in\\{H,T\\}.\n\\] Such coin-flips can be modeled using Bernoulli random variables \\[\nX\\sim\\mathcal{Bern}(\\theta_0)\n\\] where \\[\nX=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\n    \\end{matrix}\n    \\right.\n\\] The probability mass function of the Bernoulli distribution \\(\\mathcal{Bern}(\\theta_0)\\) with unknown probability of success parameter \\(0<\\theta_0<1,\\) is given by \\[\nf(x|\\theta_0)=\n\\left\\{\n  \\begin{array}{ll}\n  \\theta_0,&\\text{if } x=1\\\\\n  1-\\theta_0, & \\text{if } x=0\n  \\end{array}\n\\right.\n\\] I.e. \\[\n\\theta_0 = f(1|\\theta_0) = P(X=1) = P(\\text{Coin}=H),\n\\] which implies that the probability that we get a tail \\(T\\) is \\[\n1-\\theta_0 = f(0|\\theta_0) = P(X=0) = P(\\text{Coin}=T).\n\\]\nOur goal is to estimate the unknown \\(\\theta_0\\) using a random (i.i.d.) sample of size \\(n\\) \\[\n\\{X_1,\\dots,X_n\\}\n\\] with \\[\nX_i=\\left\\{\n    \\begin{matrix}\n    1 & \\text{if } \\text{Coin}=H\\text{ in $i$th coin flip}\\\\[2ex]\n    0 & \\text{if } \\text{Coin}=T\\text{ in $i$th coin flip}\n    \\end{matrix}\n    \\right.\n\\] such that \\[\nX_i\\sim\\mathcal{Bern}(\\theta_0),\\quad i=1,\\dots,n.\n\\] \nA given observed realization of the random sample \\[\n\\{X_{1,obs},X_{2,obs},\\dots,X_{n,obs}\\}=\\{0,1,\\dots,0\\}\n\\] consists of \\(0\\leq N_{H,obs}\\leq n\\) \\[\nN_{H,obs}=\\sum_{i=1}^n X_{i,obs}\n\\] many heads and of \\[\n0\\leq n-N_{H,obs} \\leq n\n\\] many tails.\n\n\nHow do we combine the information from the \\(n\\) observations \\[\n\\{X_{1,obs},\\dots,X_{n,obs}\\}\n\\] to estimate the unknown \\(\\theta_0\\)?\nIf the observations are realizations of an i.i.d. sample, then the joint probability of observing \\(h\\) heads \\(H\\) and \\(n-h\\) tails \\(T\\) in \\(n\\) coin flips is: \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta)\n&=\\prod_{i=1}^nf(X_{i,obs}|\\theta)\\\\[2ex]\n%&=\\left(P(X=1)\\right)^{N_{H,obs}}\\left(P(X=0)\\right)^{n-N_{H,obs}}\\\\[2ex]\n%&= \\theta^{N_{H,obs}}(1-\\theta)^{n-N_{H,obs}}  \\\\[2ex]\n&= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\end{align*}\n\\]\nThe function \\(\\mathcal{L}_n(\\theta)\\) is called the likelihood function.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.1 (Likelihood Function) More generally, when the observations \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) are a realization of an i.i.d. sample \\(\\{X_1,\\dots,X_n\\}\\) with \\(X_i\\sim f\\) for all \\(i=1,\\dots,n\\), we have that \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\] where \\(f(X_{i,obs} | \\theta)\\) is the density function of the random variable \\(X_i\\) evaluated at the realization \\(X_{i,obs},\\) and where \\(\\theta\\) denotes the unknown finite dimensional parameter vector of the density function. (A definition for dependent data (e.g.¬†time series) is also possible.)\n\n\n\n\n\n\nWe estimate the unknown parameter \\(\\theta_0\\) by maximizing the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) over the range of possible parameter values. The value \\(\\hat\\theta\\) at which the likelihood function \\(\\mathcal{L}_n(\\cdot)\\) is maximized is called the maximum likelihood (ML) estimator\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.2 (Maximum Likelihood (ML) Estimator) \\[\n\\begin{align*}\n\\hat{\\theta}_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\prod_{i=1}^n f(X_{i,obs}|\\theta),\n\\end{align*}\n\\] where \\(\\Theta\\) denotes the parameter space.\n\n\n\nIn our coin flip example this means to estimate the unknown \\(\\theta_0\\) by the value \\(\\hat\\theta\\) at which the likelihood of the observed data \\(\\{X_{1,obs},\\dots,X_{n,obs}\\}\\) is maximal, \\[\n\\hat\\theta_{ML} = \\arg\\max_{\\theta\\in[0,1]} \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}}.\n\\]\nUsually it‚Äôs easier to work with sums rather than products‚Äîalso for doing the asymptotics in Section¬†1.4. So we apply a monotonic transformation by taking the logarithm of the likelihood which leads to the log-likelihood function: \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\ln\\prod_{i=1}^n f(X_{i,obs}|\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n \\ln f(X_{i,obs}|\\theta).\n\\end{align*}\n\\] Since this is only a monotonic transformation, we have that \\[\n\\begin{align*}\n\\hat\\theta_{ML}\n&=\\arg\\max_{\\theta\\in\\Theta} \\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\arg\\max_{\\theta\\in\\Theta} \\ell_n(\\theta).\n\\end{align*}\n\\] \nIn our coin flipping example, taking the natural logarithm (\\(\\ln\\)) yields, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\theta) &= \\prod_{i=1}^n \\theta^{X_{i,obs}}(1-\\theta)^{1-X_{i,obs}} \\\\[2ex]\n\\Rightarrow\\quad \\ell_n(\\theta)\n&=\\ln\\mathcal{L}_n(\\theta)\\\\[2ex]\n&=\\sum_{i=1}^n\\left( X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\right).\n\\end{align*}\n\\]\nThe coin flip example is actually so simple that we can maximize \\(\\ell_n(\\theta)\\) analytically. Computing the first derivative yields \\[\n\\begin{align*}\n\\ell'_n(\\theta)&=\\sum_{i=1}^n \\left(X_{i,obs}\\dfrac{1}{\\theta} - (1-X_{i,obs})\\dfrac{1}{1-\\theta}\\right)\\\\[2ex]\n&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta}\n\\end{align*}\n\\] Setting the first derivative to zero determines the maximum likelihood estimator (MLE): \\[\n\\begin{array}{rrcl}\n&\\ell_n'(\\hat\\theta_{ML})&\\overset{!}{=}&0\\\\[2ex]\n\\Leftrightarrow&\\dfrac{N_{H,obs}}{\\hat\\theta_{ML}} &=& \\dfrac{n-N_{H,obs}}{1-\\hat\\theta_{ML}} \\\\[2ex]\n\\Leftrightarrow&N_{H,obs}-N_{H,obs}\\hat\\theta_{ML}  &=& n\\hat\\theta_{ML}-N_{H,obs}\\hat\\theta_{ML}\\\\[2ex]\n\\Leftrightarrow&\\hat\\theta_{ML}&=&\\dfrac{N_{H,obs}}{n}\n\\end{array}\n\\tag{1.1}\\]\nUsually, however, the log-likelihood function is way more complicated and one needs to apply numeric optimization algorithms to find the MLE, \\(\\hat\\theta_{ML}.\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "href": "Ch1_MaximumLikelihood.html#numeric-optimization",
    "title": "1¬† Maximum Likelihood",
    "section": "1.2 Numeric Optimization",
    "text": "1.2 Numeric Optimization\nUsually we are not so fortunate as to have an analytical solution for the MLE, and must rely on the computer to find the maximizing arguments of the log-likelihood function. Various methods exist for finding the maximum (or minimum) of a function.\n\nGeneral idea:\n\nStart at some value, \\(\\theta_{(0)},\\) in the parameter space \\(\\Theta.\\)\nSearch across the parameter space \\(\\Theta\\) using a step-wise procedure until a updated parameter value \\(\\ell'(\\theta_{(m)})\\) is found that yield a derivative of the log likelihood that is effectively zero (i.e.¬†smaller than some convergence/stopping criterion), \\(\\ell'(\\theta_{(m)})\\approx 0.\\)\n\n\n1.2.1 Newton-Raphson Optimization\nOne of the most-used methods for optimization is the Newton-Raphson method (or a variant of it). The Newton-Raphson method relies on Taylor-series approximations of the log-likelihood function.\nIn the following, we consider the univariate case \\(\\theta\\in\\mathbb{R}.\\) However, the multivariate case \\(\\theta\\in\\mathbb{R}^K\\) is treated likewise, but requires substituting first derivatives by gradients, second derivatives by the Hessian, etc.\n\n\n\n\n\n\nNote\n\n\n\n\nMinimization and maximization are essentially the same problems since minimizing a function \\(f(x)\\) with respect to \\(x\\) is equivalent to maximizing \\(-f(x)\\) with respect to \\(x.\\)\n\n\nLet \\(f\\) be a two times differentiable function to be optimized (here maximized). The first- and second-order Taylor-series approximations of \\(f\\) around the point \\(\\theta\\) are: \\[\n\\begin{align*}\n\\text{First-order:}\\quad &f(\\theta+h)\\approx \\overbrace{f(\\theta)+f'(\\theta)h}^{\\text{Taylor Polynomial of order 1}} \\\\\n\\text{Second-order:}\\quad& f(\\theta+h)\\approx \\underbrace{f(\\theta)+f'(\\theta)h + \\frac{1}{2} f''(\\theta)h^2}_{\\text{Taylor Polynomial of order 2}},\n\\end{align*}\n\\] Locally, i.e.¬†for \\(h\\approx 0,\\) (e.g.¬†\\(h=\\pm 0.04\\)) the Taylor polynomials are very good approximations of \\(f(\\theta + h);\\) see Figure¬†1.1.\n\n\n\n\n\nFigure¬†1.1: First- and second-order Taylor approximations of a function \\(f\\) around \\(\\theta=1.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.1 (Taylor‚Äôs Theorem)  Today, there are many different versions of Taylor‚Äôs theorem. We consider the following two:\n1. Peano form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k\\) times differentiable at \\(x\\in\\mathbb{R}\\) and let \\(h\\in\\mathbb{R}.\\) Then there exists a function \\(P_{k,x}:\\mathbb{R}\\to\\mathbb{R}\\) such that \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ P_{k,x}(h)\\;(h)^k\n\\end{align*}\n\\] with \\[\nP_{k,x}(h)\\to 0\\quad\\text{as}\\quad h\\to 0,\n\\] where \\(f^{(\\ell)}(x)\\) denotes the \\(\\ell\\)th derivative of \\(f\\) at \\(x.\\)\n2. Lagrange or Mean-value form of the remainder term: Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be \\(k+1\\) times differentiable on the open interval between \\(x\\) and \\(x+h,\\) with \\(h\\in\\mathbb{R},\\) and let \\(f^{(k)}\\) be continuous on the closed interval between \\(x\\) and \\(x+h,\\). Then \\[\n\\begin{align*}\nf(x+h) &=\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ M_{k,x}(h)\n\\end{align*}\n\\] with \\[\nM_{k,x}(h)=\\frac{f^{(k+1)}(\\xi)}{(k+1)!}(h)^{k+1}\n\\] for some real number \\(\\xi\\) between \\(x\\) and \\(x+h.\\) (This form of Taylor‚Äôs theorem is based on the mean-value Theorem¬†1.2.)\n\n\n\n\n\n\nQualitative version using the small-\\(o\\) notation:\n\n\n\n\\[\n\\begin{align*}\nf(x + h) & =\nf(x)\n+ \\sum_{\\ell=1}^k \\frac{f^{(\\ell)}(x)}{\\ell!}(h)^\\ell\n+ o\\big(|h|^k\\big),\n\\end{align*}\n\\] where \\(o\\big(|h|^k\\big)\\) denotes the family of real-valued functions, \\(g(h)\\) say, that are of a strictly smaller \\(o\\)rder of magnitude than the function \\(|h|^k\\) as \\(h\\to 0;\\) i.e.\n\\[\no\\big(|h|^k\\big)=\\left\\{g(h)\\to 0\\;\\text{ as }\\; h\\to 0:\\frac{|g(h)|}{|h|^k}\\to 0\\quad\\text{as}\\quad h\\to 0\\right\\}.\n\\]\n1. Note: Peano form of the remainder term: \\(P_{k,x}(h)\\;(h)^k=o\\big(|h|^k\\big)\\) since \\[\n\\frac{|P_{k,x}(h)\\;(h)^k|}{|h|^k}\n%=\\frac{|P_k(x+h)|\\cdot |h|^k|}{|h|^k}\n=|P_{k,x}(h)|\\to 0\\quad\\text{as}\\quad h\\to 0.\n\\]\n2. Note: Mean-value form of the remainder term: \\(M_{k,x}(h)=o\\big(|h|^k\\big)\\) since \\[\n\\frac{|M_{k,x}(h)|}{|h|^k}=\n\\left|\\frac{f^{(k+1)}(\\xi)}{(k+1)!}\\right|\\cdot|h|\\to 0\\quad\\text{as}\\quad h\\to 0.\n\\]\n\n\n\n\n\n\nOptimization Idea\nLet \\(\\ell_n\\) be a log-likelihood function with continuous first, \\(\\ell_n',\\) and second, \\(\\ell_n'',\\) derivative.\nTo optimize the log-likelihood function \\(\\ell_n,\\) we try to find the root of \\(\\ell_n',\\) i.e.¬†the value of \\(\\theta\\in\\Theta\\) such that \\[\n\\ell_n'(\\theta)=0.\n\\] That is, we try to find the value of \\(\\theta\\) that fulfills the first order condition of the optimization problem. We do so using a step-wise optimization approach, where each step has a smallish size \\(h.\\)\nInitialization: Let \\(\\theta_{(0)}\\in\\Theta\\) be our first guess of the root of \\(\\ell'_n.\\)\n\\(h\\)-Steps: Typically, our guess is not perfect and thus \\(\\ell_n'(\\theta_{(0)})\\neq 0.\\) Therefore, we want to move from \\(\\theta_{(0)}\\) to a new root-candidate \\(\\theta_{(1)}\\) by doing an \\(h\\)-step update \\[\n\\theta_{(1)} = \\theta_{(0)} + h.\n\\]\n\n\nThe first-order Taylor-series approximation of \\(\\ell_n'\\) around our first guess \\(\\theta_{(0)}\\) gives \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(0)} + h) & \\approx \\ell_n'(\\theta_{(0)}) + \\ell_n''(\\theta_{(0)})h\n\\end{align*}\n\\] Thus, to find the \\(h\\)-step that brings us closer to the root of \\(\\ell_n',\\) we can (approximatively) use the \\(h\\)-step that brings us to the root of its first-order approximation, i.e. \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(0)}) + \\ell_n''(\\theta_{(0)}) h_{(0)} = 0\\\\[2ex]\n\\Rightarrow h_{(0)} = -\\frac{\\ell_n'(\\theta_{(0)})}{\\ell_n''(\\theta_{(0)})}.\n\\end{align*}\n\\] Based on this \\(h\\)-step, the new root-candidate is \\[\n\\begin{align*}\n\\theta_{(1)}\n& = \\theta_{(0)} + h_{(0)}\\\\[2ex]\n& = \\theta_{(0)} - \\frac{\\ell_n'(\\theta_{(0)})}{\\ell_n''(\\theta_{(0)})}.\n\\end{align*}\n\\] Likewise, the \\(m\\)th root-candidate is \\[\n\\begin{align*}\n\\theta_{(m)}\n& = \\theta_{(m-1)} + h_{(m-1)}\\\\[2ex]\n& = \\theta_{(m-1)} - \\frac{\\ell_n'(\\theta_{(m-1)})}{\\ell_n''(\\theta_{(m-1)})};\n\\end{align*}\n\\] see also Figure¬†1.2.\n\n\n\n\n\nFigure¬†1.2: The \\(m\\)th update step in the Newton-Raphson root-finding algorithm.\n\n\n\n\n\n\n\n1.2.2 Convergence of the Newton-Raphson Algorithm\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_n';\\) i.e.¬† \\[\n\\ell_n'(\\theta_{root})=0.\n\\] We aim to find \\(\\theta_{root}\\) using the Newton-Raphson algorithm and call our best approximation of \\(\\theta_{root}\\) the maximum likelihood estimate; i.e.¬†\\(\\hat{\\theta}_{ML}\\approx\\theta_{root}.\\)\nLet \\[\ne_{(0)}=\\theta_{root}-\\theta_{(0)}\n\\] denote the start value error and let \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start value error neighborhood around \\(\\theta_{root}.\\)\nOne can shown that if \\(\\ell_n'\\) is ‚Äúwell behaved‚Äù over \\(I;\\) i.e.¬†\n\nif \\(\\ell_n''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\nif \\(\\ell_n'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I,\\)\n\nand if our first guess \\(\\theta_{(0)}\\) is ‚Äúclose enough;‚Äù i.e.¬†\n\nif \\(M|e_{(0)}|<1,\\) where \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_n'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_n''(\\theta)|}\\right)\\geq 0,\n\\]\n\nthen \\(\\theta_{(m)}\\) will converge to \\(\\theta_{root}\\) as \\(m\\to\\infty.\\)\n\n\n\n\n\n\nWarning\n\n\n\nUnfortunately, we typically don‚Äôt know if \\(\\ell_n'\\) is ‚Äúwell behaved‚Äù and we usually don‚Äôt know whether our first guess is ‚Äúclose enough‚Äù. So, typically we cannot guarantee convergence of the Newton-Raphson algorithm. üò≠ \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor problems that are globally concave, the starting value \\(\\theta_0\\) doesn‚Äôt matter. For more complex problems, however, the Newton-Raphson algorithm can get stuck into a local maximum. In such cases, it is usually a good idea to try multiple starting values.\nIn actual practice, implementation of the Newton-Raphson algorithm can be tricky. We may have \\(\\ell_n''(\\theta_{(m)})=0,\\) in which case the function looks locally like a straight line, with no solution to the Taylor series approximation \\[\n\\begin{align*}\n\\ell_n'(\\theta_{(m)} + h) & \\approx \\ell_n'(\\theta_{(m)}) + \\ell_n''(\\theta_{(m)})h.\n\\end{align*}\n\\] In this case a simple strategy is to move a small step in the direction which decreases the function value, based only on \\(\\ell_n'(\\theta_m).\\)\nIn other cases where \\(\\theta_{(m)}\\) is too far from the true maximizer \\(\\theta\\), the Taylor approximation may be so inaccurate that \\(\\ell_n(\\theta_{(m+1)})\\) is actually smaller than \\(\\ell_n(\\theta_{(m)}).\\) When this happens one may replace \\(\\theta_{(m+1)}\\) with \\((\\theta_{(m+1)}+\\theta_{(m)})/2\\) (or some other value between \\(\\theta_{(m)}\\) and \\(\\theta_{(m+1)}\\)) in the hope that a smaller step will produce better results.\n\n\n\nStopping Criterion: Since we are expecting that \\(\\ell_n'(\\theta_{(m)})\\to 0,\\) as \\(m\\to\\infty,\\) a good stopping condition for the Newton-Raphson algorithm is \\[\n|\\ell_n'(\\theta_{(m)})|\\leq \\varepsilon\n\\] for some (small) tolerance \\(\\varepsilon>0.\\)\n\n\n\n\n\n\nPseudo-Code: Newton-Raphson Algorithm\n\n\n\n\\[\n\\begin{array}{ll}\n\\texttt{\\textbf{select }} \\theta_{(0)}\\in\\Theta\\;\\;\\text{ and}&\\varepsilon>0 \\\\[2ex]\n\\texttt{\\textbf{let }} m=0         &  \\\\\n\\texttt{\\textbf{while }}  | \\ell_n'(\\theta_{(m)}) | >\\varepsilon & \\texttt{\\textbf{do}}\\\\\n&\\left[\n                                    \\begin{array}{l}\\texttt{\\textbf{let }} m = m+1 \\\\\n                                    \\texttt{\\textbf{let }} \\theta_{(m)} = \\theta_{(m-1)} - \\frac{\\ell_n'(\\theta_{(m-1)})}{\\ell_n''(\\theta_{(m-1)})} \\\\\n                                    \\end{array} \\right.\\\\\n\\texttt{\\textbf{let }}\\hat\\theta_{ML}=\\theta_{(m)} & \\\\\n\\texttt{\\textbf{return }} \\hat\\theta_{ML} &  \\\\\n\\end{array}\n\\]\n\n\n\n\n1.2.3 Newton-Raphson Algorithm: Coin-Flipping Example\nLet‚Äôs return to our earlier coin flipping example.\nIf we observe, for instance, only one head \\(N_{H,obs}=1\\) for a sample size of \\(n=5,\\) we already know from Equation¬†1.1 that \\[\n\\hat\\theta_{ML}=\\frac{N_{H,obs}}{n}=\\frac{1}{5}=0.2,\n\\] but let us, nevertheless, apply the Newton-Raphson algorithm.\nThe first and second derivatives of \\[\n\\ell_n(\\theta)=\\sum_{i=1}^n\\big(X_{i,obs} \\ln(\\theta) + (1-X_{i,obs})\\ln(1-\\theta)\\big)\n\\] are \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=\\dfrac{N_{H,obs}}{\\theta} - \\dfrac{n-N_{H,obs}}{1-\\theta} \\\\[2ex]\n\\ell_n''(\\theta) &= -\\dfrac{N_{H,obs}}{\\theta^2} + \\dfrac{n}{(1-\\theta)^2}(-1)-\\dfrac{N_{H,obs}}{(1-\\theta)^2}(-1)\\\\[2ex]\n&= -\\dfrac{N_{H,obs}}{\\theta^2} - \\dfrac{n-N_{H,obs}}{(1-\\theta)^2}.\n\\end{align*}\n\\]\nWe consider a sample size of \\(n=5\\) with the following observed outcome:\n\nOne Head: \\(\\quad N_{H,obs}=1\\)\nFour Tails: \\(\\quad n-N_{H,obs}=4\\)\n\nSetting \\(\\varepsilon=10^{-10}\\) as our stopping criterion and \\(\\theta_{(0)}=0.4\\) as our starting value allows us to run the Newton-Raphson algorithm which gives us the results shown in Table¬†1.1. The numeric optimization solution is \\(\\hat\\theta_{ML} = 0.2\\) which equals the analytic solution.\n\n\nTable¬†1.1: Result of applying the Newton Raphson optimization algorithm to our coin flipping example for given data with \\(N_{H,obs}=1,\\) sample size \\(n=5,\\) starting value \\(\\theta_{(0)}=0.4,\\) and convergence criterion \\(\\varepsilon=10^{-10}.\\)\n\n\n\n\n\n\n\n\n\\(m\\)\n\\(\\hat\\theta_{(m)}\\)\n\\(h_{{(m)}}=\\frac{-\\ell_n'(\\hat\\theta_{(m)})}{\\ell_n''(\\hat\\theta_{(m)})}\\)\n\\(\\ell_n'(\\hat\\theta_{(m)})\\gtrless \\varepsilon\\)\n\n\n\n\n\\(0\\)\n\\(0.40\\)\n\\(-2.4\\cdot 10^{-1}\\)\n\\({\\color{red}-4.2 > \\varepsilon}\\)\n\n\n\\(1\\)\n\\(0.16\\)\n\\(\\phantom{-}3.3\\cdot 10^{-2}\\)\n\\({\\color{red}\\phantom{-}1.5 > \\varepsilon}\\)\n\n\n\\(2\\)\n\\(0.19\\)\n\\(\\phantom{-}6.6\\cdot 10^{-3}\\)\n\\({\\color{red}\\phantom{-}2.2\\cdot 10^{-1} > \\varepsilon}\\)\n\n\n\\(3\\)\n\\(0.19\\)\n\\(\\phantom{-}1.7\\cdot 10^{-4}\\)\n\\({\\color{red}\\phantom{-}5.4\\cdot 10^{-3} > \\varepsilon}\\)\n\n\n\\(4\\)\n\\(0.19\\)\n\\(\\phantom{-}1.1\\cdot 10^{-7}\\)\n\\({\\color{red}\\phantom{-}3.5\\cdot 10^{-6} > \\varepsilon}\\)\n\n\n\\(5\\)\n\\(0.20\\)\n\\(\\phantom{-}4.8\\cdot 10^{-14}\\)\n\\({\\color{darkgreen}\\phantom{-}1.5\\cdot 10^{-12} < \\varepsilon}\\)"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "href": "Ch1_MaximumLikelihood.html#sec-LinRegNorm",
    "title": "1¬† Maximum Likelihood",
    "section": "1.3 Linear Regression under Normality",
    "text": "1.3 Linear Regression under Normality\nNow let‚Äôs return to the linear regression model \\[\nY_i=X_i'\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\tag{1.2}\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or ‚Äúdependent‚Äù) variable, \\[\n\\beta_0\\in\\mathbb{R}^K\n\\] denotes the vector of unknown slope parameters, and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})'\\in\\mathbb{R}^K\n\\] denotes the vector of predictor variables, where the (i.i.d.) random sample\n\\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] follows a random design with homoskedastic errors (see Definition¬†1.3).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.3 (Random Design (Regression Analysis)) \nA random desgin in regression analysis is given by the following setup:\nLet \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] or equivalently \\[\n(X_1,\\varepsilon_1), (X_2,\\varepsilon_2), \\dots, (X_n,\\varepsilon_n)\n\\] denote a (i.i.d.) random sample with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\), intertable \\((K\\times K)\\) matrix \\(\\mathbb{E}(X_iX_i')=\\Sigma_{X'X}\\), \\(i=1,\\dots,n,\\) and with either\n\nhomoskedastic errors: \\(0<\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0<\\infty\\)\n\nor\n\nheteroskedastic errors: \\(0<\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)<\\infty\\), for a strictly positive and finite variance function \\(\\sigma^2_0(\\cdot).\\)\n\n\n\n\nFor the following, it is convenient to write Equation¬†1.2 using matrix notation \\[\n\\begin{eqnarray*}\n  \\underset{(n\\times 1)}{Y}&=&\\underset{(n\\times K)}{X}\\underset{(K\\times 1)}{\\beta_0} + \\underset{(n\\times 1)}{\\varepsilon},\n\\end{eqnarray*}\n\\] where \\[\n\\begin{equation*}\nY=\\left(\\begin{matrix}Y_1\\\\ \\vdots\\\\Y_n\\end{matrix}\\right),\\quad X=\\left(\\begin{matrix}X_{11}&\\dots&X_{1K}\\\\\\vdots&\\ddots&\\vdots\\\\ X_{n1}&\\dots&X_{nK}\\\\\\end{matrix}\\right),\\quad\\text{and}\\quad \\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n\\end{matrix}\\right).\n\\end{equation*}\n\\]\nUnder normally distributed (and homoskedastic) error terms, \\(\\varepsilon_i,\\) we have that \\[\n\\begin{align}\n\\underset{(n\\times 1)}{\\varepsilon} &\\sim \\mathcal{N}_n\\left(0, \\sigma_0^2I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y-X\\beta_0)|X &\\sim \\mathcal{N}_n\\left(0, \\sigma^2_0I_n\\right)\\\\[2ex]\n\\Rightarrow\\quad\n(Y_i-X_i'\\beta_0)|X_i &\\sim \\mathcal{N}\\left(0, \\sigma^2_0\\right)\\\\[2ex]\n\\Rightarrow\\quad\nY_i|X_i &\\sim \\mathcal{N}\\left(X_i'\\beta_0, \\sigma^2_0\\right)\n\\end{align}\n\\tag{1.3}\\]\n\n\n\n\n\n\n\nUnder Equation¬†1.3, we have \\[\nf(Y_i|X_i;\\beta_0',\\sigma_0^2)=\n\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i'\\beta_0)^2}{2\\sigma_0^2}\\right),\n\\] where \\[\n\\theta_0=(\\beta_0',\\sigma_0^2)'\\in\\mathbb{R}^K\\times\\mathbb{R}_{>0}\n\\] denotes the unknown parameter vector.\nThis allows us to setup the likelihood function, \\[\n\\begin{align*}\n\\mathcal{L}_n(\\beta',\\sigma^2)\n& =\\prod_{i=1}^n f(Y_i|X_i;\\beta',\\sigma^2)\\\\[2ex]\n& =\\prod_{i=1}^n \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(Y_i-X_i'\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n& =\\left(\\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\right)^{n}\\exp\\left(-\\frac{\\sum_{i=1}^n (Y_i-X_i'\\beta)^2}{2\\sigma^2}\\right)\\\\[2ex]\n%& =\\dfrac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{\\varepsilon'\\varepsilon}{2\\sigma^2}\\right)\\\\[2ex]\n& =(2\\pi)^{-n/2} \\cdot (\\sigma^2)^{-n/2}\\cdot  \\exp\\left(-\\frac{(Y-X\\beta)'(Y-X\\beta)}{2\\sigma^2}\\right),\\\\[2ex]\n\\end{align*}\n\\]   and the log-likelihood function, \\[\n\\begin{align*}\n\\ell_n(\\beta',\\sigma^2)& =-\\dfrac{n}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln(\\sigma^2) - \\dfrac{1}{2 \\sigma^2}(Y-X\\beta)'(Y-X\\beta).\n\\end{align*}\n\\] \nTaking first derivatives gives \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)}\n%&= -\\dfrac{n}{2\\sigma^2}+ \\dfrac{1}{2\\sigma^4}(Y-X\\beta)'(Y-X\\beta)\n&=-\\frac{n}{2 \\sigma^{2}}+\\left[\\frac{1}{2}(Y-X\\beta)'(Y-X\\beta)\\right]\\frac{1}{\\left(\\sigma^{2}\\right)^{2}}\\\\\n%&=\\frac{1}{2 \\sigma^{2}}\\left[\\frac{1}{\\sigma^{2}} (Y-X\\beta)'(Y-X\\beta)-n\\right]\n\\end{align*}\n\\] Putting the above derivative functions into one column vector yields the \\(((K+1)\\times 1)\\)-dimensional gradient called score function in ML-theory: \\[\n\\nabla\\ell_n(\\theta')=\n\\left(\\begin{matrix}\n\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)\\\\\n\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)\n\\end{matrix}\\right)\n\\tag{1.4}\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.4 (Score Function) More generally, let \\(\\ell_n(\\theta)\\) denote the log-likelihood function evaluated at a \\(p\\)-dimensional parameter vector \\(\\theta=(\\theta_1,\\dots,\\theta_p)'.\\)\nThen the gradient vector \\[\\nabla\\ell_n(\\theta')=\\left(\\begin{matrix}\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_1}(\\theta')\\\\ \\vdots\\\\\n  \\dfrac{\\partial \\ell_n}{\\partial \\theta_p}(\\theta')\n  \\end{matrix}\n  \\right)\n\\] is called the score-function.\nThe score function is random, since it depends on the random sample.\nAt the true parameter vector \\(\\theta_0\\in\\mathbb{R}^p,\\) the score function satisfies \\[\n\\mathbb{E}\\left(\\dfrac{\\partial \\ell_n}{\\partial \\theta_j}(\\theta_0')\\right)=0\n\\] for all \\(j=1,\\dots,p.\\) We show this below in Section¬†1.4.\n\n\n\nSetting the score function in Equation¬†1.4 equal to zero yields a system of \\(K+1\\) equations with \\(K+1\\) unknowns, which we can solve for the maximum likelihood estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}:\\)\nSolving for \\(\\hat\\beta_{ML}:\\) \\[\n\\begin{align*}\n& \\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\hat\\beta_{ML}',\\hat\\sigma^2_{ML}) \\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow\\quad& - \\dfrac{1}{\\hat\\sigma^2_{ML}}(-X'Y + X'X\\hat\\beta_{ML})  \\overset{!}{=}0\\\\[2ex]\n\\Rightarrow\\quad & \\hat\\beta_{ML}=(X'X)^{-1}X'Y\\\\[2ex]\n\\end{align*}\n\\] Solving for \\(s^2_{ML}:\\) \\[\n\\begin{align*}\n& \\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\hat\\beta_{ML}',\\hat\\sigma^2_{ML}) \\overset{!}{=}0\\\\[2ex]\n\\Leftrightarrow\\quad&-\\frac{n}{2 s_{ML}^2}+\\left[\\frac{1}{2}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\right]\\frac{1}{\\left(s_{ML}^2\\right)^{2}}  \\overset{!}{=}0\\ \\\\[2ex]\n\\Rightarrow\\quad  &\ns_{ML}^2 =\\dfrac{1}{n}(Y-X\\hat\\beta_{ML})'(Y-X\\hat\\beta_{ML})\\\\[2ex]\n&\\phantom{s_{ML}^2}=\\dfrac{1}{n}\\sum_i^n \\hat\\varepsilon_i^2,\n\\end{align*}\n\\] where \\(\\hat\\varepsilon_i = Y_i - X_i'\\hat{\\beta}_{ML}.\\)\nObservations:\n\n\\(\\hat\\beta_{ML}\\) equals the OLS estimator \\(\\hat\\beta=(X'X)^{-1}X'Y.\\)  Since the ML estimator \\(\\hat\\beta_{ML}\\) is here equivalent to the OLS estimator we can use the classic inference machinery (\\(t\\)-test, \\(F\\)-test, confidence intervals) developed for the classic OLS estimator (see your econometrics class).\n\\(s_{ML}^2\\) differs from the unbiased variance estimator \\(s_{UB}^2=\\frac{1}{n-K}\\hat{\\varepsilon}_i^2.\\)\n\n\n1.3.1 Variance of ML-Estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML}\\)\n\n\n\n\n\n\nComputing the Asymptotic Variance\n\n\n\nTo compute the asymptotic variance of the ML-estimators \\(\\hat\\beta_{ML}\\) and \\(s^2_{ML},\\) we need to\n\ncompute the Hessian matrix (i.e.¬†all second partial derivatives) of \\(\\ell_n,\\)\n\ntake the expectation of this Hessian matrix and multiply it by \\(-1/n\\), which gives us the Fisher Information matrix.\nInverting the Fisher information matrix give the asymptotic variance expression.\n\n\n\nLet‚Äôs do this preliminary work in the following:\n\nPartial second derivatives with respect to \\(\\beta:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n\\Rightarrow\\quad\n\\underset{(K\\times K)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)}\n&= - \\dfrac{1}{\\sigma^2}(X'X)\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{1}{\\sigma^2} \\mathbb{E}(X'X)\\right)\\\\[2ex]\n&=  -\\frac{1}{n}\\cdot \\left(-\\dfrac{n}{\\sigma^2} \\Sigma_{X'X}\\right)\\\\[2ex]\n&=  \\dfrac{1}{\\sigma^2} \\Sigma_{X'X},\n\\end{align*}\n\\] where\n\\[\n\\mathbb{E}\\left(X'X\\right)\n=\\mathbb{E}\\left(\\sum_{i=1}^nX_iX_i'\\right)\n=n\\underbrace{\\mathbb{E}\\left(X_iX_i'\\right)}_{=:\\Sigma_{X'X}} = n\\Sigma_{X'X}.\n\\]\nSecond derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(1\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\sigma^2}(\\beta',\\sigma^2)}\n&=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2}\\frac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{2}}\\\\[2ex]\n\\Rightarrow\\quad\\underset{(1\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2)}\n&=\\frac{n}{2 \\left(\\sigma^{2}\\right)^2}-\\dfrac{(Y-X\\beta)'(Y-X\\beta)}{\\left(\\sigma^{2}\\right)^{3}} \\\\[2ex]\n&=\\frac{n}{2\\sigma^{4}}-\\frac{\\sum_{i=1}^n\\varepsilon_i^2}{\\sigma^{6}} \\\\[2ex]\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma^{4}}-\\frac{\\mathbb{E}\\left(\\sum_{i=1}^n\\varepsilon_i^2\\right)}{\\sigma^{6}} \\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot \\left(\\frac{n}{2\\sigma^{4}}-\\frac{n\\sigma^2}{\\sigma^{6}}\\right)\\\\[2ex]\n&=\\left(-\\frac{1}{2\\sigma^{4}}+\\frac{1}{\\sigma^{4}}\\right)\\\\[2ex]\n&=\\frac{1}{2\\sigma^{4}}\\\\[2ex]\n\\end{align*}\n\\]\nFirst derivative with respect to \\(\\beta,\\) second derivative with respect to \\(\\sigma^2:\\) \\[\n\\begin{align*}\n\\underset{(K\\times 1)}{\\dfrac{\\partial \\ell_n}{\\partial \\beta}(\\beta',\\sigma^2)}    \n&= - \\dfrac{1}{\\sigma^2}(-X'Y + X'X\\beta)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2}(X')(Y - X\\beta)\\\\[2ex]\n&= \\dfrac{1}{\\sigma^2}X'\\varepsilon\\\\[2ex]\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\Rightarrow\\quad\n\\underset{(K\\times 1)}{\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta \\partial \\sigma^2}(\\beta',\\sigma^2)}\n=   \\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)'\n& = -\\frac{X'\\varepsilon}{\\sigma^4}\\\\\n\\end{align*}\n\\] \\[\n\\begin{align*}\n\\Rightarrow\\quad\n&-\\frac{1}{n}\\cdot  \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\sigma^2}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\cdot\\left(\\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)\\right)'\\\\[2ex]\n&=\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X'\\varepsilon)}{\\sigma^4}\\\\[2ex]\n&=\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(\\mathbb{E}(X'\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=\\frac{1}{n}\\cdot\\frac{\\mathbb{E}(X'\\mathbb{E}(\\varepsilon|X))}{\\sigma^4}\\\\[2ex]\n&=\\frac{1}{n}\\cdot 0=0,\n\\end{align*}\n\\] since \\(\\mathbb{E}(\\varepsilon|X)=0\\) is an \\((n\\times 1)\\) zero vector.\nCollecting the above results, allows us to write down the expression for \\((-1/n)\\) times the expectation of the Hessian matrix of \\(\\ell_n\\) which yields the Fisher Information (Matrix):\n\\[\n\\begin{align*}\n&\\mathcal{I}(\\theta) :=\\; -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta',\\sigma^2)\\right)\\\\[2ex]\n&=\n-\\frac{1}{n}\\cdot \\mathbb{E}\n\\left[\\begin{array}{cc}\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta',\\sigma^2)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right)\\\\\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta',\\sigma^2)\\right) &\n\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta',\\sigma^2) \\right)\n\\end{array}\\right]\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2}\\Sigma_{X'X}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4}}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1.5 (Fisher Information Matrix) The matrix \\[\n\\mathcal{I}(\\theta) := -\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\theta)\\right)\n\\] is called Fisher Information Matrix.\n\n\n\n\nAsymptotic Variance and Fisher Information Matrix\nThe asymptotic variance of the MLE \\[\n\\hat{\\theta}_{ML}=\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n\\] is given by the inverse of the Fisher information matrix evaluated at the true parameter values \\(\\beta_0\\) and \\(\\sigma^2_0.\\) \\[\n\\begin{align*}\n&AVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n=\\lim_{n\\to\\infty} n Var\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\\\\[2ex]\n&=\\left(\\mathcal{I}(\\beta'_0,\\sigma^2_0)\\right)^{-1}\\\\[2ex]\n&=\\left(-\\frac{1}{n}\\cdot\\mathbb{E}\\left(H_{\\ell_n}(\\beta'_0,\\sigma^2_0)\\right)\\right)^{-1}\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\beta\\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right)\\\\\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2 \\partial \\beta}(\\beta'_0,\\sigma^2_0)\\right) &\n-\\frac{1}{n}\\cdot \\mathbb{E}\\left(\\dfrac{\\partial^2 \\ell_n}{\\partial \\sigma^2\\partial \\sigma^2}(\\beta'_0,\\sigma^2_0) \\right)\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\frac{1}{\\sigma^2_0}\\Sigma_{X'X}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{\\frac{1}{2\\sigma^4_0}}\n\\end{array}\\right]^{-1}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\underset{(K\\times K)}{\\sigma^2_0\\Sigma_{X'X}^{-1}}\n&\n\\underset{(K\\times 1)}{0}\\\\\n\\underset{(1\\times K)}{0} &\n\\underset{(1\\times 1)}{2\\sigma^4_0}\n\\end{array}\\right]\n\\end{align*}\n\\]\n\nThat is, \\[\n\\begin{align*}\nAVar\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\n&=\\lim_{n\\to\\infty} n Var\\left(\\begin{array}{c}\\hat\\beta_{ML} \\\\ s_{ML}^2\\end{array}\\right)\\\\[2ex]\n&=\n\\left[\\begin{array}{cc}\n\\sigma^2_0\\Sigma_{X'X}^{-1} & 0 \\\\[2ex]\n0 & \\ 2\\sigma^4_0\n\\end{array}\\right].\n\\end{align*}\n\\tag{1.5}\\]\n\n\nOf course, the variance expressions in Equation¬†1.5 contain unknown quantities and thus are not directly usable in practice. However, we can plug in estimates of the unknown quantities; namely \\[\ns_{ML}^2                         \\quad\\text{for}\\quad \\sigma^2_0\n\\] and \\[\nS_{X'X}^{-1}=\\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i'\\right)^{-1} \\quad \\text{for}\\quad \\Sigma_{X'X}^{-1}.\n\\]\nThis leads to estimators of the asymptotic variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\) \\[\n\\begin{align}\n\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=s_{ML}^2 S_{X'X}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\frac{1}{n}\\sum_{i=1}^nX_i X_i'\\right)^{-1}\\\\[2ex]\n\\widehat{AVar}(s^2_{ML})\n&=2\\left(s_{ML}^2\\right)^2\n\\end{align}\n\\] and thus to estimators of the variances of \\(\\hat{\\beta}_{ML}\\) and \\(s_{ML}^2:\\) \\[\n\\begin{align}\n\\widehat{Var}(\\hat{\\beta}_{ML})\n=\\frac{1}{n}\\widehat{AVar}(\\hat{\\beta}_{ML})\n&=s_{ML}^2 \\frac{1}{n}S_{X'X}^{-1}\\\\[2ex]\n&=s_{ML}^2 \\left(\\sum_{i=1}^nX_i X_i'\\right)^{-1}\\\\[2ex]\n\\widehat{Var}(s^2_{ML})\n=\\frac{1}{n}\\widehat{AVar}(s^2_{ML})\n&=\\frac{1}{n}2\\left(s_{ML}^2\\right)^2.\n\\end{align}\n\\]"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "href": "Ch1_MaximumLikelihood.html#sec-MLAsymp",
    "title": "1¬† Maximum Likelihood",
    "section": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators",
    "text": "1.4 Asymptotic Theory of Maximum-Likelihood Estimators\nIn the following, we consider the asymptotic distribution of ML-estimators.\nWe only consider the simplest situation: Assume a random sample\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X,\n\\] where \\(X\\in\\mathbb{R}\\) is a univariate random variable with density function \\[f(x;\\theta_0),\n\\] where the true (unknown, univariate) parameter \\(\\theta_0\\in\\Theta\\) is an interior point of a compact parameter interval \\[\\Theta=[\\theta_l,\\theta_u]\\subset\\mathbb{R}.\n\\] Note: \\(\\theta_0\\) is an ‚Äúinterior point‚Äù of \\(\\Theta\\) if \\(\\theta_l<\\theta_0<\\theta_u.\\)\nMoreover, we consider the following setup.\n\nLikelihood function: \\[\n\\mathcal{L}_n(\\theta)=\\prod_{i=1}^n f(X_i;\\theta)\n\\]\nLog-likelihood function: \\[\n\\ell_n(\\theta)=\\ln\\mathcal{L}(\\theta)=\\sum_{i=1}^n \\ln f(X_i;\\theta)\n\\]\nMaximum-likelihood estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_n=\\arg\\max_{\\theta\\in\\Theta}\\ell_n(\\theta)\n\\]\nThe maximum-likelihood estimator \\(\\hat{\\theta}_n\\) maximizes \\(\\ell_n(\\theta)\\) uniquely such that \\[\n\\ell_n'(\\hat\\theta_n)=0\\quad\\text{and}\\quad\\ell_n''(\\hat\\theta_n)<0\n\\]\nIt is assumed that the partial derivatives \\[\n\\frac{\\partial}{\\partial\\theta}f(x;\\theta)\\quad\\text{and}\\quad \\frac{\\partial^2}{\\partial\\theta^2}f(x;\\theta)\n\\] exist and that these partial derivatives can be passed under the integral such that \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial}{\\partial\\theta} f(x;\\theta)dx\\\\\n\\frac{\\partial^2}{\\partial\\theta^2}\\int f(x;\\theta)dx\n&=\\int\\frac{\\partial^2}{\\partial\\theta^2} f(x;\\theta)dx\n\\end{align*}\n\\] for all \\(\\theta\\in\\Theta.\\)\n\n\n\n\n\n\n\nExample\n\n\n\nAn example that fits into the above setup is the density of the exponential distribution \\[\nf(x;\\theta)=\\left\\{\n    \\begin{matrix}\n    \\theta\\exp(- \\theta x)& \\text{for }x\\geq 0\\\\\n    0                     & \\text{for }x < 0\\\\\n    \\end{matrix}\\right.\n\\] with unknown ‚Äúrate‚Äù parameter \\(\\theta>0.\\)\nOr, more generally, the densities of the one-parameter, \\(\\theta\\in\\Theta\\subset\\mathbb{R},\\) exponential family\n\\[\nf(x;\\theta)=h(x)\\exp(\\eta(\\theta) T(x) - B(\\theta))\n\\] where \\(h:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(T:\\) \\(\\mathbb{R}\\to\\mathbb{R},\\) \\(\\eta:\\) \\(\\Theta\\to\\mathbb{R},\\) and \\(B:\\) \\(\\Theta\\to\\mathbb{R}.\\)\n\n\nThe derivation of the asymptotic distribution of the ML estimator, \\(\\hat\\theta_n,\\) relies on a Taylor expansion of the derivative of the log-likelihood function, \\[\n\\ell_n'(\\cdot),\n\\] around \\(\\theta_0\\) (see Equation¬†1.6). To derive this expression, we use the mean value theorem (Theorem¬†1.2).\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1.2 (Mean Value Theorem) Let \\(f\\) be continuous over the closed interval \\([a,b]\\) and differentiable over the open interval \\((a,b).\\) Then, there exists at least one point \\(c\\in(a,b)\\) such that \\[\nf'(c) = \\frac{f(b)-f(a)}{b-a}\n\\] or equivalently \\[\nf(b)=f(a) + f'(c)(b-a).\n\\]\n\n\n\nBy the Mean Value Theorem (Theorem¬†1.2), we know that \\[\n\\ell_n'(\\hat{\\theta}_n)=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\tag{1.6}\\] for some \\(\\psi_n\\) between \\(\\hat{\\theta}_n\\) and \\(\\theta_0;\\) i.e.\n\n\\(\\psi_n\\in(\\theta_0,\\hat{\\theta}_n)\\quad\\) if \\(\\quad\\theta_0<\\hat{\\theta}_n\\)\n\\(\\psi_n\\in(\\hat{\\theta}_n,\\theta_0)\\quad\\) if \\(\\quad\\hat{\\theta}_n<\\theta_0\\)\n\n\nNote: Equation¬†1.6 is simply the first-order version of the mean-value form of Taylor‚Äôs theorem (Theorem¬†1.1).\n\nSince \\(\\hat{\\theta}_n\\) maximizes the log-Likelihood function it follows that \\[\n\\ell_n'(\\hat{\\theta}_n)=0.\n\\] Together with Equation¬†1.6, this implies that \\[\n\\overbrace{\\ell_n'(\\hat{\\theta}_n)}^{=0}=\\ell_n'(\\theta_0)+\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] \\[\n\\Rightarrow\\quad \\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0).\n\\tag{1.7}\\] Now, note that necessarily \\[\n\\int_{-\\infty}^{\\infty} f(x;\\theta)dx=1\n\\] for all possible values of \\(\\theta\\in\\Theta,\\) since \\(f\\) is a density function.\nTherefore, \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta}\\underbrace{\\int_{-\\infty}^{\\infty} f(x;\\theta)dx}_{=1}&=\\frac{\\partial}{\\partial \\theta}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}f(x;\\theta)dx\n=\\frac{\\partial}{\\partial \\theta}\\int_{-\\infty}^{\\infty} f(x;\\theta)dx\n=0\n\\tag{1.8}\\] for all \\(\\theta\\in\\Theta.\\)\nLikewise, \\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\theta^2}\\underbrace{\\int_{-\\infty}^{\\infty} f(x;\\theta)dx}_{=1}&=\\frac{\\partial^2}{\\partial \\theta^2}1 = 0,\\quad\\text{for all}\\quad\\theta\\in\\Theta.\n\\end{align*}\n\\] Using again that we can here pass the partial derivative under the integral sign, we thus have \\[\n\\int_{-\\infty}^{\\infty} \\frac{\\partial^2}{\\partial \\theta^2}f(x;\\theta)dx\n=\\frac{\\partial^2}{\\partial \\theta^2}\\int_{-\\infty}^{\\infty} f(x;\\theta)dx\n=0\n\\tag{1.9}\\] for all \\(\\theta\\in\\Theta.\\)\nUsing Equation¬†1.8 and Equation¬†1.9, we can now show that the average \\[\n\\frac{1}{n}\\ell_n'(\\theta_0)=\\frac{1}{n}\\underbrace{\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)}_{\\ell_n'(\\theta_0)}\n\\] is asymptotically normal. This is done in the following by checking the three conditions for applying the Lindeberg-L√©vy central limit theorem.\nFirstly, the average \\[\n\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\n\\] is taken over i.i.d. random variables: \\[\n\\frac{\\partial}{\\partial \\theta} \\ln f(X_1;\\theta_0),\\dots,\\frac{\\partial}{\\partial \\theta} \\ln f(X_n;\\theta_0)\\overset{\\text{i.i.d.}}{\\sim}\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\n\\]\nSecondly, for the mean one gets: \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=\\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\mathbb{E}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X;\\theta_0)}{f(X;\\theta_0)}\\right)\\quad[\\text{chain rule}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)}\n{f(x;\\theta_0)}f(x;\\theta_0)dx\\quad[\\text{Def. of $\\mathbb{E}$}]\\\\[2ex]\n&=\\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta}  f(x;\\theta_0)dx\\\\[2ex]\n&=0,\n\\end{align*}\n\\tag{1.10}\\] where the last step follows from Equation¬†1.8.\nThirdly, for the variance one gets: \\[\n\\begin{align*}\nVar\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)\n&=Var\\left(\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\ln f(X_i;\\theta_0)\\right)\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\partial}{\\partial \\theta} \\ln f(X;\\theta_0)\\right)\\quad[\\text{i.i.d.}]\\\\\n&=\\frac{1}{n}Var\\left(\\frac{\\frac{\\partial}{\\partial \\theta} f(X;\\theta_0)}{f(X|\\theta)}\\right)\\quad[\\text{chain rule}]\\\\\n&=\\frac{1}{n}\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\\\\\n&=\\frac{1}{n}\\mathcal{I}(\\theta_0),\n\\end{align*}\n\\] where the simplification of the variance expression to a second moment expression follows from Equation¬†1.10. \n\nWe can write the last expression using the Fisher Information \\(\\mathcal{I}(\\theta_0)-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0))\\) since below in Equation¬†1.12 we‚Äôll see that \\[\n\\begin{align*}\n\\mathbb{E}\\left(\\left(\\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n& =-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)) = \\mathcal{I}(\\theta_0).\n\\end{align*}\n\\]\n\n\nThus, we can apply the Lindeberg-L√©vy central limit theorem from which it follows that \\[\n\\frac{\\frac{1}{n}\\ell_n'(\\theta_0)-\\overbrace{\\mathbb{E}\\left(\\frac{1}{n}\\ell_n'(\\theta_0)\\right)}^{=0}}{\\sqrt{\\frac{1}{n}\\mathcal{I}(\\theta_0)} } = \\frac{\\ell_n'(\\theta_0)}{\\sqrt{n\\mathcal{I}(\\theta_0)} } \\to_d \\mathcal{N}(0,1)\n\\] as \\(n\\to\\infty.\\)\nBy our mean value expression in Equation¬†1.7 \\[\n\\ell_n'(\\theta_0)=-\\ell_n''(\\psi_n)(\\hat{\\theta}_n-\\theta_0)\n\\] we thus have \\[\n\\frac{-\\ell_n''(\\psi_n)}{\\sqrt{n \\mathcal{I}(\\theta_0)}}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1),\n\\] which is equivalent to \\[\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\;\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right) \\to_d \\mathcal{N}(0,1).\n\\tag{1.11}\\] The \\(\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\)-part in Equation¬†1.11 is our object of interest.\nThe further analysis requires us to study the asymptotic behavior of\n\\[\n-\\frac{1}{n}\\ell_n''(\\psi_n)\n\\] which will help us to understand the behavior of \\(\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)\\) in Equation¬†1.11.\n\n\n\n\n\n\nImportant\n\n\n\nBefore we consider \\(-\\frac{1}{n}\\ell_n''(\\psi_n),\\) we begin with studying the mean and the variance of the simpler statistic \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0).\n\\]\n\n\nFirst, the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\partial}{\\partial\\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta}\\left(\\frac{\\frac{\\partial}{\\partial \\theta}f(X_i;\\theta_0)}{f(X_i;\\theta_0)}\\right)\\quad[\\text{chain rule}]\n\\end{align*}\n\\] Applying the quotient rule yields \\[\n\\begin{align*}\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\left(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}f(X_i;\\theta_0)\\right) f(X_i;\\theta_0)-\\frac{\\partial}{\\partial\\theta}f(X_i;\\theta_0)\\frac{\\partial}{\\partial\\theta} f(X_i;\\theta_0)}{\\left(f(X_i;\\theta_0)\\right)^2}\\right)\\\\[2ex]\n&=-\\frac{1}{n}\\sum_{i=1}^n\n\\left(\n\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}-\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X_i;\\theta_0)}\n{f(X_i;\\theta_0)}\\right)^2  \n\\right).\n\\end{align*}\n\\] Taking the mean of \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) yields that \\[\n\\begin{align*}\n\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}\n{f(X;\\theta_0)}+\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}\n{f(X;\\theta_0)}\\right)^2\\right)\\quad[\\text{i.i.d.}]\\\\[2ex]\n&=\\frac{n}{n}\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)+\\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}\n{f(X;\\theta_0)}\\right)^2\\right)\n\\end{align*}\n\\] From Equation¬†1.10, we know that \\(\\mathbb{E}\\left(-\\frac{\\frac{\\partial^2}{\\partial \\theta^2} f(X;\\theta_0)}{f(X;\\theta_0)}\\right)=0\\) thus \\[\n\\begin{align*}\n\\underbrace{\\mathbb{E}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)}_{=\\mathcal{I}(\\theta_0)}\n&=0 + \\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n\\end{align*}\n\\] \\[\n\\Rightarrow \\qquad\n\\mathcal{I}(\\theta_0) =\n\\mathbb{E}\\left(\\left( \\frac{\\frac{\\partial}{\\partial \\theta}  f(X;\\theta_0)}{f(X;\\theta_0)}\\right)^2\\right)\n\\tag{1.12}\\]\nThis means that \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\n\\] is an unbiased estimator of the Fisher information \\(\\mathcal{I}(\\theta_0).\\)\nMoreover, Equation¬†1.12 provides an alternative expression for the Fisher information \\(\\mathcal{I}(\\theta_0).\\)\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nFor multivariate (\\(p\\)-dimensional) parameters \\(\\theta_0,\\) the Fisher information \\(\\mathcal{I}(\\theta_0)=(-1)\\cdot \\mathbb{E}\\left(\\ell_n''(\\theta_0)\\right)\\) becomes the (\\(p\\times p\\)) Fisher information matrix (see Section¬†1.3.1).\n\n\nSecond, the variance of variance of \\(-\\frac{1}{n}\\ell_n''(\\theta_0):\\) \\[\n\\begin{align*}\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\n&=Var\\left(-\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial \\theta\\partial \\theta}\\ln f(X_i;\\theta_0)\\right)\\\\[2ex]\n&=\\frac{n}{n^2}\n\\underbrace{Var\\left(\\frac{\\partial^2}{\\partial \\theta \\partial \\theta}  \\ln f(X;\\theta_0)\\right)}_{=\\texttt{constant}}\\\\[2ex]\n&=\\frac{1}{n}\\texttt{constant},\n\\end{align*}\n\\] which implies that \\[\nVar\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWith these mean and variance results for \\(-\\frac{1}{n}\\ell_n''(\\theta_0),\\) we can write down the Mean Squared Error (MSE) of the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) of \\(\\mathcal{I}(\\theta_0):\\) \\[\n\\begin{align*}\n&\\operatorname{MSE}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[2ex]\n&=\n\\mathbb{E}\\left(\\left(-\\frac{1}{n}\\ell_n''(\\theta_0) -\\mathcal{I}(\\theta_0)\\right)^2\\right)\\\\[2ex]\n&=\\underbrace{\\left(\\operatorname{Bias}\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\right)^2}_{=0}+Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\\\[3ex]\n&=Var\\left(-\\frac{1}{n}\\ell_n''(\\theta_0)\\right)\\to 0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\nThat is, the estimator \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) is a mean square consistent estimator, i.e. \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_{m.s.} \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] which implies that \\(\\frac{1}{n}\\ell_n''(\\theta_0)\\) is also a (weakly) consistent estimator, i.e.¬† \\[\n-\\frac{1}{n}\\ell_n''(\\theta_0)\\to_p \\mathcal{I}(\\theta_0)\\quad \\hbox{as}\\quad n\\to\\infty,\n\\] since mean square convergence implies convergence in probability.\n\n\n\n\n\n\nImportant\n\n\n\nü§î Remember, we wanted to study \\(-\\frac{1}{n}\\ell_n''(\\psi_n)\\) in Equation¬†1.11 not \\(-\\frac{1}{n}\\ell_n''(\\theta_0).\\) Studying \\(-\\frac{1}{n}\\ell_n''(\\theta_0)\\) was only the simpler thing to do.\nLuckily, we are actually close now.\n\n\nNext, we use that ML estimators \\(\\hat\\theta_n\\) are (weakly) consistent, i.e., \\[\n\\hat\\theta_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\nExample: Our results in Section¬†1.3 imply, for instance, that the ML estimator \\(\\hat{\\beta}_n\\) is consistent for \\(\\beta.\\)\n\nSince \\(\\psi_n\\) is a mean value between \\(\\theta_0\\) and \\(\\hat{\\theta}_n\\) (Equation¬†1.6), consistency of \\(\\hat{\\theta}_n\\) implies that \\[\n\\psi_n\\to_p\\theta_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nTherefore, we have by the continuous mapping theorem that \\[\n\\begin{align}\n-\\frac{1}{n}\\ell_n''(\\psi_n) & \\to_p \\phantom{-}\\mathcal{I}(\\theta_0)\\quad \\hbox{ as }\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow\\qquad\n\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)&\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} \\quad \\hbox{ as }\\quad n\\to\\infty.\n\\end{align}\n\\]\nNow, using Slutsky‚Äôs theorem, we can connect the above consistency result with the asymptotic normality result in Equation¬†1.11 such that \\[\n\\begin{align*}\n\\underbrace{\\left(\\frac{-\\frac{1}{n}\\ell_n''(\\psi_n)}{\\sqrt{\\mathcal{I}(\\theta_0)}}\\right)}_{\\to_p \\sqrt{\\mathcal{I}(\\theta_0)} }\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d\\mathcal{N}(0,1)\n\\end{align*}\n\\] or equivalently \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d N\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right),\n\\end{align*}\n\\tag{1.13}\\] where \\(1/\\mathcal{I}(\\theta_0)\\) is the asymptotic variance of the ML estimator \\(\\hat{\\theta}_n\\) and equals the inverse of the (here scalar valued) Fisher information \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell_n''(\\theta_0)).\n\\]\nEquation¬†1.13 is the asymptotic normality result we aimed for.\n\n\n\n\n\n\nMultivariate Settings\n\n\n\nThe above arguments can easily be generalized to multivariate (\\(p\\)-dimensional) parameter vectors \\(\\theta\\in\\mathbb{R}^p\\). In this case, \\(\\mathcal{I}(\\theta_0)\\) becomes a \\(p\\times p\\) matrix, and \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n-\\theta_0\\right)\\to_d \\mathcal{N}_p\\left(0, \\mathcal{I}(\\theta_0)^{-1}\\right),\n\\] where \\(\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}\\left(H_{\\ell_n}(\\theta_0)\\right)\\) is the \\((p\\times p)\\) Fisher information matrix with \\(H_{\\ell_n}(\\theta_0)\\) denoting the Hesse matrix of \\(\\ell_n(\\cdot)\\) evaluated at \\(\\theta_0.\\)\n\n\n\n\n\n\n\n\nML-Theory and Machine learning\n\n\n\nThe Fisher information is used in machine learning techniques such as elastic weight consolidation, which reduces catastrophic forgetting in artificial neural networks (Kirkpatrick et al. (2017)).\nFisher information can be used as an alternative to the Hessian of the loss function in second-order gradient descent network training (Martens (2020))."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#invariance-property-of-the-ml-estimator",
    "href": "Ch1_MaximumLikelihood.html#invariance-property-of-the-ml-estimator",
    "title": "1¬† Maximum Likelihood",
    "section": "1.5 Invariance Property of the ML-Estimator",
    "text": "1.5 Invariance Property of the ML-Estimator\nSuppose that a distribution has the parameter \\(\\theta_0,\\) but we are interested in finding an estimator of a function of \\(\\theta_0,\\) say \\[\n\\eta_0=\\tau(\\theta_0).\n\\] The invariance property of ML-estimators says that if \\(\\hat{\\theta}_n\\) is the ML-estimator of \\(\\theta_0,\\) then \\(\\tau(\\hat{\\theta}_n)\\) is the ML-estimator of \\(\\eta_0=\\tau(\\theta_0).\\)\n\nOne-to-One Functions\nLet the function \\[\n\\eta = \\tau(\\theta)\n\\] be a one-to-one function. That is, for each value of \\(\\theta\\) there is a unique value of \\(\\eta\\) and vice versa.\nImportant property of one-to-one functions: A one-to-one function \\(\\eta = \\tau(\\theta)\\) possesses a well-defined inverse \\[\n\\theta=\\tau^{-1}(\\eta).\n\\]\n\n\n\n\n\n\nExample:\n\n\n\nFor instance, the functions \\[\n\\begin{align*}\n\\eta = \\tau(\\theta) & = \\theta + 3\n\\quad\\Rightarrow\\quad \\theta = \\tau^{-1}(\\eta) = \\eta -3 \\\\[2ex]\n\\eta = \\tau(\\theta) & = \\theta/5  \n\\quad\\Rightarrow\\quad \\theta = \\tau^{-1}(\\eta) = 5 \\eta\n\\end{align*}\n\\] are one-to-one functions. However, for instance, the functions \\[\n\\begin{align*}\n\\tau(\\theta) & = \\sin(\\theta)\\\\[2ex]\n\\tau(\\theta) & = \\theta^2  \n\\end{align*}\n\\] are not one-to-one functions.\n\n\nIn this one-to-one case, it is easily seen that it makes no difference whether we maximize the likelihood function as a function of \\(\\theta\\) or as a function of \\(\\eta = \\tau(\\theta)\\)‚Äîin each case we get the same answer.\nThe likelihood function of \\(\\tau(\\theta),\\) written as a function of \\(\\eta,\\) is given by \\[\n\\begin{align*}\n\\mathcal{L}^*(\\eta)\n&= \\prod_{i=1}^n f\\big(X_i;\\tau^{-1}(\\eta)\\big)\n= \\mathcal{L}\\big(\\;\\overbrace{\\tau^{-1}(\\eta)}^{=\\theta}\\;\\big)\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n  \\sup_{\\eta}  \\mathcal{L}^*(\\eta)\n= \\sup_{\\eta}  \\mathcal{L}\\big(\\;\\overbrace{\\tau^{-1}(\\eta)}^{=\\theta}\\;\\big)\n= \\sup_{\\theta}\\mathcal{L}\\big(\\theta\\big).\n\\end{align*}\n\\] Thus, the maximum of \\(\\mathcal{L}^*(\\eta)\\) is attained at \\[\n\\eta=\\tau(\\theta)=\\tau(\\hat\\theta_n),\n\\] showing that the ML-estimator of \\(\\tau(\\theta_0)\\) is \\(\\tau(\\hat\\theta_n).\\)\n\n\n\n\n\n\n\nMore general (not one-to-one) functions\n\n\n\nIn many cases, however, this simply version of the invariance of ML-estimators is not useful since many functions of interest are not one-to-one.\nLuckily, the invariance property of the ML-estimator also holds for functions that are not one-to-one; see Chapter 7 in Casella and Berger (2001)."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#exercises",
    "href": "Ch1_MaximumLikelihood.html#exercises",
    "title": "1¬† Maximum Likelihood",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nProgram the Newton-Raphson algorithm for a numerical computation of the ML estimate \\(\\hat\\theta\\) of the parameter \\(\\theta=P(\\text{Coin}=\\texttt{HEAD})\\) in our coin toss example of this chapter. Replicate the results shown in Table¬†1.1.\n\n\nExercise 2.\nAssume an i.i.d. random sample \\(X_1,\\dots,X_n\\) from an exponential distribution, i.e.¬†the underlying density of \\(X_i\\) is given by \\[\nf(x;\\theta_0)=\n\\left\\{\\begin{array}{ll}\\theta_0\\exp(-\\theta_0 x),&x\\geq 0\\\\0,&x<0\\end{array}\\right.\n\\] with \\(\\theta_0>0,\\) where \\[\n\\mu:=\\mathbb{E}(X_i)=\\frac{1}{\\theta_0}\n\\] and \\[\nVar(X_i)=\\frac{1}{\\theta_0^2}.\n\\]\n\nWhat is the log-likelihood function for the i.i.d. random sample \\(X_1,\\dots,X_n\\)?\nDerive the maximum likelihood (ML) estimator \\(\\hat\\theta_n\\) of \\(\\theta_0.\\)\nFrom maximum likelihood theory we know that \\[\n\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\to_d \\mathcal{N}\\left(0,\\frac{1}{\\mathcal{I}(\\theta_0)}\\right).\n\\] Derive the expression for the Fisher information \\(\\mathcal{I}(\\theta_0).\\) Use the Fisher information to give the explicit formula for the asymptotic distribution of \\(\\hat\\theta_n\\).\n\n\n\nExercise 3.\n\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Unif}(0,\\theta_0).\\)\n\nWhat is the likelihood function?\nWhat is the maximum likelihood estimator of \\(\\theta_0\\)?\n\n\n\nExercise 4.\nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim\\mathcal{Poisson}(\\lambda_0).\\) That is \\(X\\sim f\\) with density function \\[\nf(x;\\lambda_0) = \\frac{\\lambda_0^x \\exp(-\\lambda_0)}{x!}.\n\\]\n\nFind the maximum likelihood estimator, \\(\\hat{\\lambda},\\) of \\(\\lambda_0.\\)\nLet \\(0<\\lambda_0\\leq 4.\\) Find the maximum likelihood estimator, \\(\\hat{P}(X=4),\\) of \\(P(X=4).\\)\n\n\n\nExercise 5.\nShow that the Newton-Raphson algorithm converges; i.e.¬†that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\] under the setup outlined in Section¬†1.2.2.\nTip: Use the first-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with explicit reminder term \\(R\\) given by \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + R,\n\\end{align*}\n\\] where \\[\nR=\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\] for a mean-value \\(\\xi_{(m)}\\) between \\(\\theta_{(m)}\\) and \\(\\theta_{root}\\). This is called the Lagrange form of the Taylor-Series reminder term and follows from the Mean-Value Theorem Theorem¬†1.2."
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#references",
    "href": "Ch1_MaximumLikelihood.html#references",
    "title": "1¬† Maximum Likelihood",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCasella, George, and Roger Berger. 2001. Statistical Inference. 2nd ed. Duxbury.\n\n\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, et al. 2017. ‚ÄúOvercoming Catastrophic Forgetting in Neural Networks.‚Äù Proceedings of the National Academy of Sciences 114 (13): 3521‚Äì26.\n\n\nMartens, James. 2020. ‚ÄúNew Insights and Perspectives on the Natural Gradient Method.‚Äù The Journal of Machine Learning Research 21 (1): 5776‚Äì5851.\n\n\nWhite, Halbert. 1982. ‚ÄúMaximum Likelihood Estimation of Misspecified Models.‚Äù Econometrica, 1‚Äì25."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html",
    "href": "Ch2_EMAlgorithmus.html",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "",
    "text": "The Expectation Maximization (EM) algorithm is often used to simplify or to facilitate complex maximum likelihood estimation problems. In this chapter, we present the EM algorithm for estimating Gaussian mixture distributions, as this is probably its most well-known application. Even the original work on the EM algorithm (Dempster, Laird, and Rubin 1977) already dealt with the estimation of Gaussian mixture distributions."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "href": "Ch2_EMAlgorithmus.html#motivation-cluster-analysis-using-gaussian-mixture-models",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models",
    "text": "2.1 Motivation: Cluster Analysis using Gaussian Mixture Models\nAs a data example we use the palmerpenguins data (Horst, Hill, and Gorman (2020)).\nThese data are from surveys of penguin populations on the Palmer Archipelago (Antarctic Peninsula). Penguins are often difficult to distinguish from one another (Figure¬†2.1). We will try to find groupings in the penguin data (fin length) using a Gaussian mixture distribution. To be able to estimate such mixing distributions, we introduce the EM algorithm.\n\n\n\nFigure¬†2.1: Cheeky penguin in action.\n\n\nThe following code chunk prepares the data\n\n\n\n\n\n\nCaution\n\n\n\nWe have the information about the different penguin species (penguin_species) but in the following we pretend not to know this information.\nWe want to determine the group memberships (species) by cluster analysis on the basis of the fin lengths (penguin_flipper) alone.\nAfterwards we can use the data in penguin_species to check how good our cluster analysis is.\n\n\n\n## Select a color palette\ncol_v <- RColorBrewer::brewer.pal(n = 3, name = \"Set2\")\n\n## Preparing the data:\npenguins <- palmerpenguins::penguins %>%  # penguin data\n  tidyr::as_tibble() %>%                  # 'tibble'-dataframe\n  dplyr::filter(species!=\"Adelie\") %>%    # remove penguin species 'Adelie' \n  droplevels() %>%                        # remove the non-used factor level\n  tidyr::drop_na() %>%                    # remove NAs\n  dplyr::mutate(species = species,        # rename variables \n                flipper = flipper_length_mm) %>% \n  dplyr::select(species, flipper)         # select variables \n\n##  \nn      <- nrow(penguins)                  # sample size (n=187)\n\n## Pulling out the variable 'penguin_species':\npenguin_species <- dplyr::pull(penguins, species)\n\n## Pulling out the variable 'penguin_flipper':\npenguin_flipper <- dplyr::pull(penguins, flipper)\n\n## Plot\n## Histogramm:\nhist(x = penguin_flipper, freq = FALSE, \n     xlab=\"Flipper-Length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039))\n## Stipchart hinzuf√ºgen:\nstripchart(x = penguin_flipper, method = \"jitter\", \n           jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[3],.5), \n           bg=alpha(col_v[3],.5), cex=1.3, add = TRUE)\n\n\n\n\n\n\n\n\n\nClustering using Gaussian Mixture Distributions\nAt the end of this chapter, we‚Äôll be able to\n\nEstimate the Gaussian mixture distribution using the EM algorithm\nAssign the predictors \\(x_i\\) (flipper length) to the group (penguine species) that maximizes the ‚Äúposterior probability‚Äù (see Figure¬†2.2 and Section¬†2.3.2)\n\n\n\n\n\n\nFigure¬†2.2: Cluster analysis based on a mixture distribution with two weighted normal distributions.\n\n\n\n\nFigure Figure¬†2.2 shows the result of a cluster analysis based on a mixture distribution of two weighted normal distributions. Cluster result: 95% of the penguins could be correctly assigned - based only on their flipper lengths.\nThe following R codes can be used to reproduce the above cluster analysis (using the R package mclust) and Figure¬†2.2:\n\n## mclust R package:\n## Cluster analysis using Gaussian mixture distributions\nsuppressMessages(library(\"mclust\"))\n\n## Number of Groups\nG <- 2 \n\n## Sch√§tzung des Gau√üschen Mischmodells (per EM Algorithmus)\n## und Clusteranalyse\nmclust_obj <- mclust::Mclust(data       = penguin_flipper, \n                             G          = G, \n                             modelNames = \"V\", \n                             verbose    = FALSE)\n\n# summary(mclust_obj)\n# str(mclust_obj)\n\n## estimated group assignment \nclass <- mclust_obj$classification\n\n## Fraction of correct group assignments:\n# cbind(class, penguin_species)\nround(sum(class == as.numeric(penguin_species))/n, 2)\n\n## estimated means of the two Gaussian distributions\nmean_m <- t(mclust_obj$parameters$mean)\n\n## estimated variances (and possibly covariances) \ncov_l  <- list(\"Cov1\" = mclust_obj$parameters$variance$sigmasq[1], \n               \"Cov2\" = mclust_obj$parameters$variance$sigmasq[2])\n\n## estimated mixture weights (prior-probabilities) \nprop_v <- mclust_obj$parameters$pro\n\n## evaluating the Gaussian mixture density function \nnp      <- 100 # number of evaluation points\nxxd     <- seq(min(penguin_flipper)-3, \n               max(penguin_flipper)+5, \n               length.out = np)\n## mixture density\nyyd     <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +\n           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n## single densities\nyyd1    <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]\nyyd2    <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]\n\n## Plot\nhist(x = penguin_flipper, xlab=\"Flipper length (mm)\", main=\"Penguins\\n(Two Groups)\",\n     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))\nlines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))\nlines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)\nlines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)\nabline(v=203.1, lty=3)\nstripchart(penguin_flipper[class==1], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)\nstripchart(penguin_flipper[class==2], \n           method = \"jitter\", jitter = .0005, at = .001,\n           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)\n\nBut coding is nothing without understanding. We‚Äôll learn the underlying statistical method it in this chapter."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "href": "Ch2_EMAlgorithmus.html#the-em-algorithm-for-maximum-likelihood-estimation-of-gaussian-mixture-distributions",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions",
    "text": "2.2 The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions\n\n2.2.1 Gaussian Mixture Models (GMM)\nWe denote a random variable \\(X\\) that follows a Gaussian mixture distribution as \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\]\nThe corresponding density function of a Gaussian mixture distribution is defined as follows: \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g \\varphi(x;\\mu_g,\\sigma_g)\n\\tag{2.1}\\]\n\nWeights: \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\) with \\(\\pi_g>0\\) and \\(\\sum_{g=1}^G\\pi_g=1\\)\nMeans: \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) with \\(\\mu_g\\in\\mathbb{R}\\)\nStandard deviations: \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) with \\(\\sigma_g>0\\)\nNormal density of group \\(g=1,\\dots,G\\): \\[\n\\varphi(x;\\mu_g,\\sigma_g)=\\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right)\n\\]\nUnknown parameters: \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) und \\(\\boldsymbol{\\sigma}\\)\n\n\n\n2.2.2 Maximum Likelihood (ML) Estimation\nWe could try to estimate the unknown parameters \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\), \\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\) using the maximum likelihood method.\n\nI‚Äôll say it right away: The attempt will fail.\n\n\nBasic Idea of ML Estimation\n\nAssumption: The data \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) is a realization of a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\n\\] with \\[\nX\\sim\\mathcal{N}_{\\mathcal{mix}}(G,\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}).\n\\]\n\n\n\nEstimation Idea: Choose \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\) such that \\(f_{GMM}(\\cdot;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) ‚Äúoptimally‚Äù fits the observed data \\(\\mathbf{x}\\).\nImplementation of the Estimation Idea: Maximize (with respect to \\(\\boldsymbol{\\pi}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\sigma}\\)) the likelihood function \\[\n\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})=\\prod_{i=1}^nf_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\n\\] Or, equivalently, maximize the log-likelihood function (simpler maximization) \\[\n\\begin{align*}\n%\\ln\\left(\\mathcal{L}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\\right)=\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n=&\\sum_{i=1}^n\\ln\\left(f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\right)\\\\\n=&\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\end{align*}\n\\tag{2.2}\\]\n\n\n\n\n\n\n\nMaximization constraints\n\n\n\nThe maximization must take into account the parameter constraints in Equation¬†2.1; namely,\n\n\\(\\sigma_g>0\\) and\n\\(\\pi_g>0\\) for all \\(g=1,\\dots,G\\) such that\n\\(\\sum_{g=1}^G\\pi_g=1\\).\n\n\n\nThe maximizing parameter values \\(\\hat{\\boldsymbol{\\pi}}\\), \\(\\hat{\\boldsymbol{\\mu}}\\) and \\(\\hat{\\boldsymbol{\\sigma}}\\) are the ML-Estimators:\n\\[\n(\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\sigma}})=\\arg\\max_{\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}}\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n\\]\nüòí Problems with singularities in numerical solutions: If one tries to solve the above maximization problem numerically with the help of the computer, one will quickly notice that the results are highly unstable, implausible and not very trustworthy. The reason for these unstable estimates are problems with singularities.\nFor real GMMs (i.e.¬†GMMs with more than one group \\(G>1\\)), problems with singularities occur very easily during a numerical maximization. This happens whenever one (or more) of the normal distribution component(s), say \\(\\varphi_g(x_i;\\mu_g,\\sigma_g),\\) tries to describe only single data points. This leads to a Gaussian density function centered around a single data point \\(x_i\\) such that\n\\[\n\\varphi(x_i;{\\color{red}\\mu_g=x_i},\\sigma_g),\n\\] where \\[\n\\sigma_g\\to 0.\n\\] This degenerating situation leads to very large density function values, \\[\n\\varphi(x_i;\\mu_g=x_i,\\sigma_g)\\to\\infty\\quad\\text{for}\\quad \\sigma_g\\to 0,\n\\] and thus maximize the log-likelihood in an undesirable way (see Figure¬†2.3).\n\n\n\n\n\nFigure¬†2.3: Gaussian density with \\(\\mu_g=x_i\\) for \\(\\sigma_g\\to 0\\).\n\n\n\n\nSuch undesirable, trivial maximization solutions typically lead to implausible, non-useful estimation results.\nü§ì Analytic solution: It is a bit tedious, but one can maximize the log-likelihood function of the GMM (see Equation¬†2.2) analytically. If you do this, you will get the following expressions: \\[\n\\begin{align*}\n\\hat\\pi_g&=\\frac{1}{n}\\sum_{i=1}^np_{ig},\\quad\n\\hat\\mu_g=\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}x_i\\\\[2ex]\n\\hat\\sigma_g&=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}}{\\left(\\sum_{j=1}^np_{jg}\\right)}\\left(x_i-\\hat\\mu_g\\right)^2},\n\\end{align*}\n\\tag{2.3}\\] where \\[\np_{ig}=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\tag{2.4}\\] for \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G\\).\n\n\n\n\n\n\nNote\n\n\n\nDeriving the expressions for \\(\\hat{\\mu}_g\\), \\(\\hat{\\sigma}_g\\) and \\(\\hat{\\pi}_g\\) in Equation¬†2.3 is really a bit tedious (multiple applications of the chain rule, product rule, etc., as well as an application of the Lagrange multiplier method for optimization under side-constraints) but in principle doable.\n\n\n\nüôà However: The above expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\) and \\(\\hat\\sigma_g\\) depend themselves on the unknown parameters\n\n\\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\),\n\\(\\boldsymbol{\\mu}=(\\mu_1,\\dots,\\mu_G)\\) and\n\\(\\boldsymbol{\\sigma}=(\\sigma_1,\\dots,\\sigma_G)\\),\n\nbecause the probabilities \\(0\\leq p_{ig}\\leq 1\\) (defined in Equation¬†2.4) depend on these unknown parameters.\nThus, the expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation¬†2.3 do not allow direct estimation of the unknown parameters \\(\\pi_g\\), \\(\\mu_g\\), and \\(\\sigma_g\\).\n\n\n\n\n\n\nPrior and Posterior Probabilities\n\n\n\n\nThe probability \\[\n\\pi_g = \\mathbb{P}\\left(\\text{Penguine $i$ belongs to group}\\;g\\right)\n\\]\nin Equation¬†2.4 is called the prior probability. The prior probability \\(\\pi_g\\) is the probability that a penguine \\(i\\), from which we know nothing about its flipper length, belongs to group \\(g\\).\nThe conditional probability \\[\np_{ig} = \\mathbb{P}\\left(\\text{Penguine $i$ belongs to group}\\;g|X_i = x_i\\right)\n\\] in Equation¬†2.4 is called the posterior probability. The posterior probability \\(p_{ig}\\) is the probability that penguine \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g.\\)\n\nWe‚Äôll discuss the prior and the posterior probability in more detail in Section¬†2.3.2.\n\n\nü•≥ Solution: The EM Algorithm\n\n\n\n2.2.3 The EM Algorithm for GMMs\nThe expressions for \\(\\hat\\pi_g\\), \\(\\hat\\mu_g\\), and \\(\\hat\\sigma_g\\) in Equation¬†2.3, however, suggest a simple iterative maximum likelihood estimation procedure: An alternating estimation of\n\nthe (unknown) posterior probabilities \\[\n\\begin{align*}\np_{ig}\n&= \\mathbb{P}\\left(\\text{Penguine $i$ belongs to group}\\;g|X_i = x_i\\right)\\\\[2ex]\n& = \\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\] for \\(i=1,\\dots,n,\\) and \\(g=1,\\dots,G,\\) and of\nthe (unknown) parameters \\[\n(\\pi_g,\\mu_g,\\sigma_g) \\quad\\text{for}\\quad g=1,\\dots,G\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nGiven an estimate \\(\\hat{p}_{ig},\\) you can compute \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\) using Equation¬†2.3.\nGiven the estimates \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g),\\) you can compute \\(\\hat{p}_{ig}\\) using Equation¬†2.4.\n\n\nThe EM Algorithm:\n\nInitialization:  Set starting values \\[\n\\begin{align*}\n&\\boldsymbol{\\hat{\\pi}}^{(0)}=(\\hat{\\pi}_1^{(0)},\\dots,\\hat{\\pi}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\mu}}^{(0)}=(\\hat{\\mu}_1^{(0)},\\dots,\\hat{\\mu}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\sigma}}^{(0)}=(\\hat{\\sigma}_1^{(0)},\\dots,\\hat{\\sigma}_G^{(0)})\n\\end{align*}\n\\]\nLoop:  For \\(r=1,2,\\dots\\)\n\n(Expectation) Compute: \\[\\hat{p}_{ig}^{(r-1)}=\\frac{\\hat{\\pi}_g^{(r-1)}\\varphi(x_i;\\hat{\\mu}^{(r-1)}_g,\\hat{\\sigma}_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\hat{\\pi}}^{(r-1)},\\boldsymbol{\\hat{\\mu}}^{(r-1)},\\boldsymbol{\\hat{\\sigma}}^{(r-1)})}\\]\n(Maximization) Compute:\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^n\\hat{p}_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{\\hat{p}_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^n\\hat{p}_{jg}^{(r-1)}\\right)}x_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{\\hat{p}_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^n\\hat{p}_{jg}^{(r-1)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence:  Stop if the value of the maximized log-likelihood function, \\(\\ell(\\boldsymbol{\\hat{\\pi}}^{(r)},\\boldsymbol{\\hat{\\mu}}^{(r)},\\boldsymbol{\\hat{\\sigma}}^{(r)};\\mathbf{x})\\), does not change anymore substantially.\n\nThe above pseudo code is implemented in the following code chunk:\n\nlibrary(\"MASS\")\nlibrary(\"mclust\")\n\n## data:\nx <- cbind(penguin_flipper) # data [n x d]-dimensional. \nd <- ncol(x)                # dimension (d=1: univariat)\nn <- nrow(x)                # sample size\nG <- 2                      # number of groups\n\n## further stuff \nllk       <- matrix(NA, n, G)\np         <- matrix(NA, n, G)  \nloglikOld <- 1e07\ntol       <- 1e-05\nit        <- 0\ncheck     <- TRUE \n\n## EM Algorithm\n\n## 1. Starting values for pi, mu and sigma:\npi    <- rep(1/G, G)              # naive pi \nsigma <- array(diag(d), c(d,d,G)) # varianz = 1\nmu    <- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )\n\nwhile(check){\n  \n  ## 2.a Expectation step\n  for(g in 1:G){\n    p[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  p <- sweep(p, 1, STATS = rowSums(p), FUN = \"/\")\n  \n  ## 2.b Maximization step \n  par   <- mclust::covw(x, p, normalize = FALSE)\n  mu    <- par$mean\n  sigma <- par$S\n  pi    <- colMeans(p)\n  \n  ## 3. Check convergence \n  for(g in 1:G) {\n    llk[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])\n  }\n  loglik <- sum(log(rowSums(llk))) # current max. log-likelihood value\n  ##\n  diff      <- abs(loglik - loglikOld)/abs(loglik) # rate of change\n  loglikOld <- loglik\n  it        <- it + 1\n  ## Check whether rate of change is still large enough (> tol)?\n  check     <- diff > tol\n}\n\n## Estimation results:\nresults <- matrix(c(pi, mu, sqrt(sigma)), \n                  nrow = 3, \n                  ncol = 2, \n                  byrow = TRUE,\n                  dimnames = list(c(\"weights\", \n                                    \"means\", \n                                    \"standard-deviations\"),\n                                  c(\"group 1\", \n                                    \"group 2\"))) \n##\nresults %>% round(., 2)\n\n                    group 1 group 2\nweights                0.69    0.31\nmeans                216.19  194.24\nstandard-deviations    7.32    6.25"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "href": "Ch2_EMAlgorithmus.html#the-true-view-on-the-em-algorithm-adding-unobserved-variables",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (‚Äúlatent‚Äù) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n  =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n2.3.1 Data Completion\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((z_{i1},z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G>2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) for each \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G.\\) However, it must hold true that each \\(i\\) belongs to only one group, i.e. \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g.¬†member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_1,\\dots,\\pi_G\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g\\varphi(x;\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot;\\mu_g,\\sigma_g)\\) in the total distribution \\(f_{GMM}\\). Therefore, we know that, on average, \\[\n\\pi_g\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(z_{ig}\\) as a unobserved realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] and with the restriction that \\[\n\\sum_{g=1}^GZ_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.$\n\\]\nNote that the condition \\(\\sum_{g=1}^GZ_{ig}=1\\) implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\[\n\\pi_g = P(Z_{ig}=1)\n\\] If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n‚ÄúWith probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).‚Äù\n\n\nPosterior Probability \\[\np_{ig}=P(Z_{ig}=1|X_i=x_i)\n\\] If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes‚Äô Theorem (see Equation¬†2.5) which leads to the posterior probability: \n\n‚ÄúWith probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).‚Äù\n\n\n\n\n\n\n\n\nBayes‚Äô Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere‚Äôs the Expectation  in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig}\\) is the Expectation-step of the EM algorithm (Section¬†2.2.3).\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e.¬†the predictors like the flipper lengths), \\[\n\\mathbf{x}=(x_1,\\dots,x_n),\n\\] we had also observed the group assignments, \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{g=1}^G\\left(\\sum_{i=1}^nz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\right)\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation¬†2.2), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize: We can effectively maximize separately for each group \\(g,\\) which then involves only a single normal density function and not a too flexible mixture of density functions. This simplifies the maximization problem considerably, since the normal density belongs to the exponential family (see Section¬†1.4) which is not the case for the normal mixture distribution.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version (in \\(\\mathbf{Z}\\)) of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\] From this, we can calculate the conditional expected value (using Equation¬†2.6), which motivates the ‚ÄúExpectation‚Äù-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathbb{E}_{\\boldsymbol{\\theta}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\end{align*}\n\\] where \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) is used to denote the parameter vector.\nThe following EM algorithm differs only in notation from the version already discussed in Section¬†2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step.\nThe chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. \n\nInitialization: Set starting values \\(\\boldsymbol{\\theta}^{(0)}=(\\pi^{(0)}, \\mu^{(0)}, \\sigma^{(0)})\\)\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n&=\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z}))\\\\\n&=\\sum_{i=1}^n\\sum_{k=1}^Kp_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\np_{ig}^{(r-1)} = \\frac{\\pi_g^{(r-1)} \\varphi(x_i;\\mu_g^{(r-1)},\\sigma_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\pi}^{(r-1)},\\boldsymbol{\\mu}^{(r-1)},\\boldsymbol{\\sigma}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\theta}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{(r-1)})\n\\end{align*}\n\\]\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\theta}^{(r)},\\boldsymbol{\\theta}^{(r-1)})\n\\] does not change anymore substantially."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "href": "Ch2_EMAlgorithmus.html#unsupervised-classification",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "2.4 (Unsupervised) Classification",
    "text": "2.4 (Unsupervised) Classification\nThe problem of predicting a discrete random variable \\(Y\\) (i.e.¬†the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] where \\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and \\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n(Note: Above we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.)\nExample: Predict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g.¬†passing the exam (\\(Y=1\\)) vs.¬†failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g.¬†previous gradings, number of hours studied, etc.)\nA classification rule is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\nIf there are learning/training data with group-labels \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] that can be used to estimate \\(h,\\) it‚Äôs called a supervised classification (computer science: supervised learning) problem.\nIf there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it‚Äôs called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e.¬†the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] From Bayes‚Äôs theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where \\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes‚Äô classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)>\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition¬†2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown quantities and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation¬†2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G>1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard ‚Äî particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure¬†2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\nFigure¬†2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure¬†2.2.\nBut well, the average penguin probably doesn‚Äôt care about the EM Algorithm.\n\n\n\nFigure¬†2.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#exercises",
    "href": "Ch2_EMAlgorithmus.html#exercises",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\n\nConsider \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\sim\\text{Bernoulli}(p).\\) Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nNow let \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}X,\n\\] where \\(X\\) is a Bernoulli mixture random variable with parameters \\(p_g\\) and prior probabilities \\(\\pi_g\\), \\(g=1,\\dots,G\\). Write the expressions of the (log) likelihood functions \\(\\mathcal{L}\\) and \\(\\ell\\).\nLet \\[\n(Z_{i1},\\dots,Z_{iG})\\in\\{0,1\\}^G\n\\] denote the vector of latent group indicator random variables with \\[\nZ_{i1}+\\dots + Z_{iG}=1\n\\] and \\[\nP(Z_{ig}=1)=\\pi_g,\\quad g=1,\\dots,G.\n\\] Thus, the realization \\(z_i=(0,1,0,\\dots,0)\\) means that the \\(i\\)th observation comes from the \\(2\\)nd Bernoulli distribution \\(\\text{Bernoulli}(p_2)\\). Write the expressions of the (log) likelihood functions \\(\\tilde{\\mathcal{L}}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x},\\mathbf{Z})\\) and \\(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x},\\mathbf{Z})\\) that take into account the latend group indicator random variables.\nWrite down the expression for the posterior probability \\[\n\\mathfrak{p}_{ig} = P(Z_{ig}=1 | X_i = x_i).\n\\]\nDerive the conditional expectation of \\(\\tilde\\ell,\\) given \\(\\mathbf{X}=\\mathbf{x},\\) \\[\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x},\\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right).\n\\]\nMaximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x},\\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\) with respect to \\(p_g\\) for \\(g=1,\\dots,G.\\)\nMaximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x},\\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\) with respect to \\(\\pi_g\\) for \\(g=1,\\dots,G\\) such that \\(\\sum_{g=1}^G\\pi_g=1.\\)\nSketch the EM-Algorithm"
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#references",
    "href": "Ch2_EMAlgorithmus.html#references",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. ‚ÄúMaximum Likelihood from Incomplete Data via the EM Algorithm.‚Äù Journal of the Royal Statistical Society: Series B 39 (1): 1‚Äì22.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nTraiberman, Sharon. 2019. ‚ÄúOccupations and Import Competition: Evidence from Denmark.‚Äù American Economic Review 109 (12): 4260‚Äì4301."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#cluster-analysis",
    "href": "Ch2_EMAlgorithmus.html#cluster-analysis",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "2.4 Cluster Analysis",
    "text": "2.4 Cluster Analysis\nThe problem of predicting a discrete random variable \\(Y\\) (i.e.¬†the group label) from a possibly multivariate predictor random variable \\(X\\) is called classification.\nConsider iid data \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\\overset{\\text{i.i.d.}}{\\sim}(Y,X)\n\\] where\n\n\\(X_i\\in\\mathbb{R}^p\\) is a \\(p\\)-dimensional vector and\n\\(Y_i\\) takes values in some finite set \\(\\mathcal{Y}.\\)\n\n\nNote: Above in Section¬†2.3, we used \\(Z,\\) here we use \\(Y\\) to denote the (unknown) group labels.\n\n\n\n\n\n\n\nExample\n\n\n\nPredict \\(Y\\in\\mathcal{Y}=\\{0,1\\}\\) (e.g.¬†passing the exam (\\(Y=1\\)) vs.¬†failing \\(Y=0\\)) using the observed predictor values \\(X\\in\\mathbb{R}^p\\) (e.g.¬†previous gradings, number of hours studied, etc.)\n\n\nA classification rule \\(h\\) is a function \\[\nh: \\mathbb{R}^p \\to \\mathcal{Y}.\n\\] That is, when we observe a new \\(X\\in\\mathbb{R}^p,\\) we predict \\(Y\\) to be \\(h(X)\\in\\mathcal{Y}.\\)\n\n\n\n\n\n\n(Un-)Supervised Classification\n\n\n\n\nSupervised Classification: If there are learning/training data with group-labels \\[\n({\\color{red}Y_1},X_1),\\dots,({\\color{red}Y_n},X_n)\n\\] that can be used to estimate \\(h,\\) it‚Äôs called a supervised classification (computer science: supervised learning) problem.\nUnsupervised Classification/Cluster Analysis: If there are learning/training data without group-labels \\[\nX_1,\\dots,X_n\n\\] it‚Äôs called a unsupervised classification (computer science: unsupervised learning) problem or cluster analysis.\n\n\n\n\n2.4.1 Bayes Classifier\nWe would like to find a classification rule \\(h\\) that makes accurate predictions. The most often used quantity to measure the accuracy of classification methods is the error rate.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Error rate) The true error rate of the classifier \\(h\\) is the loss function \\[\nL(h) = P(h(X)\\neq Y).\n\\tag{2.7}\\] The empirical error rate is \\[\n\\hat{L}_n(h)=\\frac{1}{n}\\sum_{i=1}^n 1_{(h(X_i)\\neq Y_i)},\n\\] where \\(1_{(\\cdot)}\\) denotes the indicator function with \\(1_{(\\texttt{TRUE})}=1\\) and \\(1_{(\\texttt{FALSE})}=0.\\)\n\n\n\nWe try to find a classifier \\(h\\) that minimizes \\(L(h)\\) and \\(\\hat{L}_n(h),\\) respectively.\nLet us focus on the special case of only two groups which can be coded, without loss of generality, as \\[\nY\\in\\{0,1\\}\n\\] For instance, \\[\nY_i=\\left\\{\\begin{array}{ll}\n1&\\text{if penguin $i$ belongs to species Chinstrap}\\\\\n0&\\text{if penguin $i$ belongs NOT to species Chinstrap}.\n\\end{array}\\right..\n\\]\nThe regression function (i.e.¬†the conditional mean function) is then given by \\[\n\\begin{align*}\nm(x)\n&:=\\mathbb{E}(Y|X=x)\\\\[2ex]\n&=1\\cdot P(Y=1|X=x) + 0\\cdot P(Y=0|X=x)\\\\[2ex]\n&=P(Y=1|X=x).\n\\end{align*}\n\\] That is, the conditonal mean \\(m(x)\\) is the posterior probability, i.e.¬†the probability of \\(Y=1\\) given \\(X=x.\\)\nFrom Bayes‚Äô theorem it follows that \\[\n\\begin{align*}\nm(x)\n&=P(Y=1|X=x)\\\\[2ex]\n&=\\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{\\pi_0\\;f_{X|Y}(x|Y=0)+\\pi_1\\;f_{X|Y}(x|Y=1)}\\\\[2ex]\n&=\\frac{\\pi_1\\; f_{X|Y}(x|Y=1)}{f_{X}(x)},\n\\end{align*}\n\\tag{2.8}\\] where\n\n\\[\n\\pi_0= P(Y=0)\\quad\\text{and}\\quad\\pi_1  = P(Y=1)\n\\] denote the prior probabilities with \\(\\pi_0 + \\pi_1 = 1,\\)\n\n\\[\nf_{X|Y}(x|Y=0)\\quad\\text{and}\\quad f_{X|Y}(x|Y=1)\n\\] denote the conditional density functions of \\(X\\) given \\(Y=0\\) and \\(Y=1,\\) respectively, and\n\\[\nf_X(x)=\\pi_1\\;\\; f_{X|Y}(x|Y=1) + \\pi_0\\;\\; f_{X|Y}(x|Y=0)\n\\] denotes the unconditional density function of \\(X.\\)\n\nNote: Here \\(f\\) denotes here some (unknown) density function, not necessarily the Gaussian density or a Gaussian mixture.\nThe Bayes classifier, \\(h^\\ast,\\) classifies data according to the Bayes classification rule\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2.2 (Bayes Classification Rule and Decision Boundary)  The Bayes classification rule \\(h^\\ast\\) is given by \\[\nh^\\ast(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] The decision boundary of a classifier \\(h\\) is given by the set \\[\n\\mathcal{D}(h)=\\{x : P(Y=1|X=x)=P(Y=0|X=x)\\}.\n\\]\n\n\n\nEquivalent forms of the Bayes‚Äô classification rule: \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }m(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\\\\[2ex]\n& = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\pi_1 f_{X|Y}(x|Y=1)>\\pi_0f_{X|Y}(x|Y=0)\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 2.1 (Optimality of the Bayes decision rule)  The Bayes decision rule is optimal. That is, if \\(h\\) is any other classification rule then \\[\nL(h^\\ast)\\leq L(h),\n\\] where \\(L(h)=P(h(X)\\neq Y)\\) denotes the error rate loss function defined in Definition¬†2.1.\n\n\n\nThe Bayes decision rule \\(h^\\ast(x)\\) depends on the unknown \\(m(x)=P(Y=1|X=x)\\) and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule.\nVery roughly, there are three main approaches:\n\nEmpirical Risk Minimization: Choose a set of classifiers \\(\\mathcal{H}\\) and try to find \\(\\hat{h}\\in\\mathcal{H}\\) such that \\[\n\\hat{h}:=\\arg\\min_{h\\in\\mathcal{H}}L(h)\n\\] Example: Random forests, neural nets, etc.\nRegression: Find an estimate \\(\\hat{m}(x)\\) of the regression function \\(m(x)=\\mathbb{E}(Y|X=x)\\) in Equation¬†2.8 and then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear regression, logistic regression, etc.\nDensity Estimation: Find density and probability estimates \\(\\hat{f}_{X|Y},\\) \\(\\hat{\\pi}_0=\\hat{P}(Y=0),\\) and \\(\\hat{\\pi}_1=\\hat{P}(Y=1)\\) and define \\[\n\\begin{align*}\n\\hat{m}(x)\n&=\\hat{P}(Y=1|X=x)\\\\[2ex]\n&=\\frac{\\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}{\\hat{\\pi}_0 \\hat{f}_{X|Y}(x|Y=0) + \\hat{\\pi}_1 \\hat{f}_{X|Y}(x|Y=1)}.\n\\end{align*}\n\\] Then use \\[\n\\hat{h}(x) = \\left\\{\\begin{array}{ll}\n1&\\text{if }\\hat{m}(x)>\\frac{1}{2}\\\\\n0&\\text{otherwise}.\n\\end{array}\\right.\n\\] Examples: Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.\n\n\nMore than two group labels\nOf course, we can generalize all this to the case where the discrete random variables \\(Y\\) takes on more than only two group-labels.\nLet \\[\nY\\in\\{1,\\dots,G\\}\n\\] for any \\(G>1.\\)\nThen, the (error rate optimal) Bayes classification rule is \\[\n\\begin{align*}\nh^\\ast(x)\n& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\pi_g f_{X|Y}(x|Y=g),\\\\[2ex]\n\\end{align*}\n\\] where \\[\nP(Y=g|X=x) = \\frac{\\pi_g f_{X|Y}(x|Y=g)}{\\sum_{g=1}^G\\pi_g f_{X|Y}(x|Y=g)}\n\\] denotes the posterior probability of group \\(g\\), and \\[\n\\pi_g = P(Y=g)\n\\] denotes the prior probability of group \\(g,\\) and \\(f_{X|Y}(x|Y=g)\\) denotes the conditional density function of \\(X\\) given \\(Y=g.\\)\n\n\n\n2.4.2 Synopsis: Penguin Example\nIn our penguin example, we use the density estimation approach.\nEstimating general densities \\(f\\) is hard ‚Äî particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as \\(f\\) being a Gaussian (mixture) density.\nIn our penguin example, we assume that the conditional density function of flipper length \\(X\\) given species \\(Y=g\\) can be modelled reasonably well using a Gaussian density, \\[\nf_{X|Y}(x|Y=g) = \\varphi(x|\\mu_g,\\sigma_g) = \\frac{1}{\\sqrt{2\\pi}\\sigma_g}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu_g}{\\sigma_g}\\right)^2\\right).\n\\] which leads to a Gaussian Mixture distribution.\nThe unknown parameters \\(\\pi_g,\\) \\(\\mu_g,\\) and \\(\\sigma_g,\\) \\(g=1,\\dots,G,\\) are estimated using the EM algorithm\nUnsupervised Classification: Assign the data points \\(x_i\\) to the group \\(g\\) according to the classification rule \\[\n\\begin{align*}\n\\hat{h}(x_i)\n%& = \\arg\\max_{g}P(Y=g|X=x) \\\\[2ex]\n& = \\arg\\max_{g}\\hat{\\pi}_g \\varphi(x_i|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\\\[2ex]\n\\end{align*}\n\\]\nFigure¬†2.4 shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:\n\nThe vertical line shows the decision boundary\nThe two Gaussian density functions (dashed lines) show the conditional densities \\(\\varphi(x|\\hat{\\mu}_g,\\hat{\\sigma}_g),\\) \\(g=1,2.\\)\nThe orange and green dots show the (unsupervised) classification results\n\n\n\n\n\n\nFigure¬†2.4: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.\n\n\n\n\nThe final estimation result replicates Figure¬†2.2.\nThe average penguin probably doesn‚Äôt care about our EM Algorithm.\n\n\n\nFigure¬†2.5: Penguin research on the limit."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#sec-TrueViewEM",
    "href": "Ch2_EMAlgorithmus.html#sec-TrueViewEM",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables",
    "text": "2.3 The True View on the EM Algorithm: Adding Unobserved Variables\nThe EM algorithm allows maximum likelihood problems to be simplified by adding unobserved (‚Äúlatent‚Äù) variables to the data. This idea is the actually original contribution of the EM Algorithm (Dempster, Laird, and Rubin (1977)). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.\n\n\n\n\n\n\nRemember:\n\n\n\nWe were note able to maximize the log-likelihood function \\[\n\\ell(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x})\n  =\\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\n\\] directly. In fact, the \\(\\ln(\\sum_{g=1}^G[\\dots])\\)-construction makes life difficult here.\n\n\n\n2.3.1 Data Completion\nIn our penguin data there are two groups \\(g\\in\\{1,2\\}.\\)\nThus, in principle (albeit unobserved) there are \\(G=2\\) dimensional dummy variable vectors \\((z_{i1},z_{i2}),\\) \\(i=1,\\dots,n,\\) which encode the group-labels, \\[\n(z_{i1},z_{i2})=\n\\left\\{\\begin{array}{ll}\n(1,0)&\\text{if penguin }i\\text{ belongs to group }g=1\\\\\n(0,1)&\\text{if penguin }i\\text{ belongs to group }g=2\\\\\n\\end{array}\\right.\n\\]\n\n\n\n\n\n\nCase of more than two \\(G>2\\) groups:\n\n\n\n\\[\n\\begin{align*}\n&(z_{i1},\\dots,z_{ig},\\dots,z_{iG})=\\\\[2ex]\n&=\\left\\{\\begin{array}{ll}\n(1,0,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=1\\\\\n(0,1,\\dots,0)&\\text{if data point }i\\text{ belongs to group }g=2\\\\\n\\quad\\quad\\vdots&\\\\\n(0,0,\\dots,1)&\\text{if data point }i\\text{ belongs to group }g=G\\\\\n\\end{array}\\right.\n\\end{align*}\n\\]\n\n\nThe group labels \\(z_{ig}\\) can take values \\(z_{ig}\\in\\{0,1\\},\\) for each \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G.\\) However, it must hold true that each \\(i\\) belongs to only one group, i.e. \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRequiring that \\[\n\\sum_{g=1}^Gz_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g.¬†member of a gender group and member of a religious group).\n\n\nUnfortunately, the true group labels \\(z_{ig}\\) are missing. However, we nevertheless know at least something about their group-assignments. The weights \\[\n\\pi_1,\\dots,\\pi_G\n\\] of the Gaussian mixture distribution \\[\nf_{GMM}(x;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})=\\sum_{g=1}^G\\pi_g\\varphi(x;\\mu_g,\\sigma_g),\n\\] give us the proportions of the individual distributions \\(\\varphi(\\cdot;\\mu_g,\\sigma_g)\\) in the total distribution \\(f_{GMM}\\). Therefore, we know that, on average, \\[\n\\pi_g\\cdot 100\\%\n\\] of the data points \\(i=1,\\dots,n\\) come from group \\(g.\\)\nThus, we can consider the missing group label \\(z_{ig}\\) as a unobserved realization of a binary random variable \\(Z_{ig}\\in\\{0,1\\}\\) with probabilities \\[\n\\begin{align*}\nP(Z_{ig}=1)&=\\pi_g\\\\[2ex]\nP(Z_{ig}=0)&=(1-\\pi_g)\\\\[2ex]\n\\end{align*}\n\\] and with the restriction that \\[\n\\sum_{g=1}^GZ_{ig}=1\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\]\nNote that the condition \\(\\sum_{g=1}^GZ_{ig}=1\\) implies that if  \\[\nZ_{ig}=1\n\\] then \\[\nZ_{ij}=0\\quad \\text{for all }j\\neq g.\n%\\quad \\Rightarrow\\quad Z_{i1}=0,\\dots,Z_{ig-1}=0,Z_{ig+1}=0,\\dots,Z_{iG}=0.\n\\]\n\n\n2.3.2 Prior and Posterior Probabilities\nPrior Probability \\[\n\\pi_g = P(Z_{ig}=1)\n\\] If we know nothing about the flipper length of penguin \\(i\\) then we are left with the prior probability: \n\n‚ÄúWith probability \\(\\pi_g=P(Z_{ig}=1)\\) penguin \\(i\\) belongs to group \\(g\\).‚Äù\n\n\nPosterior Probability \\[\np_{ig}=P(Z_{ig}=1|X_i=x_i)\n\\] If we know the flipper length of penguin \\(i\\) then we can update the prior probability using Bayes‚Äô Theorem (see Equation¬†2.5) which leads to the posterior probability: \n\n‚ÄúWith probability \\(p_{ig}=P(Z_{ig}=1|X_i=x_i)\\) penguin \\(i\\) with flipper length \\(x_i\\) belongs to group \\(g\\).‚Äù\n\n\n\n\n\n\n\n\nBayes‚Äô Theorem applied to the Gaussian mixture distribution\n\n\n\n\\[\n\\begin{align*}\np_{ig}\n=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\n&=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\\\\[2ex]\n&=\\frac{\\overbrace{P(Z_{ig}=1)}^{\\text{prior-prob}}\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}\n\\end{align*}\n\\tag{2.5}\\]\n\n\n\n\n\n\n\n\nWhere‚Äôs the Expectation  in the EM-Algorithm?\n\n\n\nThe posterior probabilities \\(p_{ig}\\) are conditional means: \\[\n\\begin{align*}\np_{ig}\n&= 1\\cdot P(Z_{ig}=1|X_i=x_i)+0\\cdot P(Z_{ig}=0|X_i=x_i)\\\\[2ex]\n&= \\mathbb{E}(Z_{ig}|X_i=x_i)\\\\\n\\end{align*}\n\\tag{2.6}\\] Thus, the computation of \\(p_{ig}\\) is the Expectation-step of the EM algorithm (Section¬†2.2.3).\n\n\n\n\n2.3.3 The Abstract Version of the EM-Algorithm\nIf, in addition to the data points (i.e.¬†the predictors like the flipper lengths), \\[\n\\mathbf{x}=(x_1,\\dots,x_n),\n\\] we had also observed the group assignments, \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] then we could establish the following alternative likelihood (\\(\\tilde{\\mathcal{L}}\\)) and log-likelihood (\\(\\tilde{\\ell}\\)) functions: \\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\prod_{i=1}^n\\prod_{g=1}^G\\left(\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)\\right)^{z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{z})\n&=\\sum_{i=1}^n\\sum_{g=1}^Gz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{g=1}^G\\left(\\sum_{i=1}^nz_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\right)\n\\end{align*}\n\\]\nUnlike the original log-likelihood function (Equation¬†2.2), the new log-likelihood function \\(\\tilde\\ell\\) would be easy to maximize: We can effectively maximize separately for each group \\(g,\\) which then involves only a single normal density function and not a too flexible mixture of density functions. This simplifies the maximization problem considerably, since the normal density belongs to the exponential family (see Section¬†1.4) which is not the case for the normal mixture distribution.\nHowever, we do not observe the realizations \\[\n\\mathbf{z}=(z_{11},\\dots,z_{nG}),\n\\] but only know the distribution of the random variables \\[\n\\mathbf{Z}=(Z_{11},\\dots,Z_{nG}).\n\\] This leads to a stochastic version (in \\(\\mathbf{Z}\\)) of the log-likelihood function: \\[\n\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})=\\sum_{i=1}^n\\sum_{g=1}^GZ_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\]\nWe do not know \\(Z_{ig},\\) but we can compute the best (in the mean square sense) predition of \\(Z_{ig}\\) using the conditional mean \\[\n\\mathbb{E}_{\\boldsymbol{\\theta}}\\left(Z_{ig}|\\mathbf{X}=\\mathbf{x}\\right)\n=\\mathbb{E}_{\\boldsymbol{\\theta}}\\left(Z_{ig}|X_i=x_i\\right)=p_{ig},\n\\] where \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) is used to denote the parameter vector, and where (by Equation¬†2.5) \\[\n\\begin{align*}\np_{ig}\n&=\\overbrace{P(Z_{ig}=1|X_i=x_i)}^{\\text{Posterior-prob}}\\\\[2ex]\n&=\\frac{\\pi_g\\varphi(x_i;\\mu_g,\\sigma_g)}{f_{GMM}(x_i;\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})}.\n\\end{align*}\n\\]\nFrom this, we can calculate the conditional expected value (using Equation¬†2.6), which motivates the ‚ÄúExpectation‚Äù-Step in the EM-algorithm: \\[\n\\begin{align*}\n&\\mathbb{E}_{\\boldsymbol{\\theta}}\\left(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})|\\mathbf{X}=\\mathbf{x}\\right)\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{k=1}^K\\mathbb{E}_{\\boldsymbol{\\theta}}\\left(Z_{ig}|\\mathbf{X}=\\mathbf{x}\\right)\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&\\quad =\\sum_{i=1}^n\\sum_{g=1}^Gp_{ig}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\},\n\\end{align*}\n\\] where \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma})\\) is used to denote the parameter vector.\n\n\n\n\n\n\nTip\n\n\n\nGiven the estimates \\(\\hat{p}_{ig},\\) for all \\(i=1,\\dots,n\\) and \\(g=1,\\dots,G,\\) we can compute ML-estimates \\((\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g)\\) for all \\(g=1,\\dots,G.\\) \nGiven ML-estimates \\(\\hat{\\theta}_g=(\\hat\\pi_g, \\hat\\mu_g,\\hat\\sigma_g),\\) for all \\(g=1,\\dots,G,\\) we can compute \\(\\hat{p}_{ig}\\) using\n\\[\n\\hat{p}_{ig}\n=\\frac{\\hat{\\pi}_g\\varphi(x_i;\\hat{\\mu}_g,\\hat{\\sigma}_g)}{f_{GMM}(x_i;\\boldsymbol{\\hat\\pi},\\boldsymbol{\\hat\\mu},\\boldsymbol{\\hat\\sigma})}.\n\\]\n\n\nThe following EM algorithm differs only in notation from the version already discussed in Section¬†2.2.3. The notation chosen here clarifies that the Expectation-step updates the log-likelihood function to be maximized in the Maximization-step.\nThe chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. \n\nInitialization: Set starting values \\(\\boldsymbol{\\hat{\\theta}}^{(0)}=(\\boldsymbol{\\hat{\\pi}}^{(0)}, \\boldsymbol{\\hat{\\mu}}^{(0)}, \\boldsymbol{\\hat{\\sigma}}^{(0)}),\\) where \\[\n\\begin{align*}\n&\\boldsymbol{\\hat{\\pi}}^{(0)}=(\\hat{\\pi}_1^{(0)},\\dots,\\hat{\\pi}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\mu}}^{(0)}=(\\hat{\\mu}_1^{(0)},\\dots,\\hat{\\mu}_G^{(0)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\sigma}}^{(0)}=(\\hat{\\sigma}_1^{(0)},\\dots,\\hat{\\sigma}_G^{(0)})\n\\end{align*}\n\\]\nLoop: For \\(r=1,2,\\dots\\)\n\n(Expectation)  Compute: \\[\n\\begin{align*}\n&\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n=\\mathbb{E}_{\\boldsymbol{\\hat{\\theta}}^{(r-1)}}\\left(\\tilde{\\ell}(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma};\\mathbf{x},\\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\\\[2ex]\n&=\\sum_{i=1}^n\\sum_{k=1}^K\\mathbb{E}_{\\boldsymbol{\\theta}^{(r-1)}}\\left(Z_{ig}\\big|X_i=x_i\\right)\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\\\\[2ex]\n&=\\sum_{i=1}^n\\sum_{k=1}^K\\hat{p}_{ig}^{(r-1)}\\left\\{\\ln\\left(\\pi_g\\right)+\\ln\\left(\\varphi(x_i;\\mu_g,\\sigma_g)\\right)\\right\\}\n\\end{align*}\n\\] where \\[\n\\hat{p}_{ig}^{(r-1)} = \\frac{\\hat{\\pi}_g^{(r-1)} \\varphi(x_i;\\hat{\\mu}_g^{(r-1)},\\hat{\\sigma}_g^{(r-1)})}{f_{GMM}(x_i;\\boldsymbol{\\hat{\\pi}}^{(r-1)},\\boldsymbol{\\hat{\\mu}}^{(r-1)},\\boldsymbol{\\hat{\\sigma}}^{(r-1)})}\n\\]\n(Maximization) Compute: \\[\n\\begin{align*}\n\\boldsymbol{\\hat{\\theta}}^{(r)}=\\arg\\max_{\\boldsymbol{\\theta}}\\mathcal{Q}(\\boldsymbol{\\theta},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n\\end{align*}\n\\] where \\(\\boldsymbol{\\hat{\\theta}}^{(r)}=(\\boldsymbol{\\hat{\\pi}}^{(r)},\\boldsymbol{\\hat{\\mu}}^{(r)},\\boldsymbol{\\hat{\\sigma}}^{(r)}),\\) \\[\n\\begin{align*}\n&\\boldsymbol{\\hat{\\pi}}^{(r)}=(\\hat{\\pi}_1^{(r)},\\dots,\\hat{\\pi}_G^{(r)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\mu}}^{(r)}=(\\hat{\\mu}_1^{(r)},\\dots,\\hat{\\mu}_G^{(r)})\\\\[2ex]\n&\\boldsymbol{\\hat{\\sigma}}^{(r)}=(\\hat{\\sigma}_1^{(r)},\\dots,\\hat{\\sigma}_G^{(r)}),\n\\end{align*}\n\\] with\n\n\\(\\hat\\pi_g^{(r)}=\\frac{1}{n}\\sum_{i=1}^np_{ig}^{(r-1)},\\quad\\quad\\hat\\mu_g^{(r)}=\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}x_i\\)\n\n\n\\(\\hat\\sigma_g^{(r)}=\\sqrt{\\sum_{i=1}^n\\frac{p_{ig}^{(r-1)}}{\\left(\\sum_{j=1}^np_{jg}^{(r-1)}\\right)}\\left(x_i-\\hat\\mu_g^{(r)}\\right)^2}\\)\n\n\nCheck Convergence: Stop if the value of the maximized log-likelihood function \\[\n\\mathcal{Q}(\\boldsymbol{\\hat{\\theta}}^{(r)},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n\\] does not change anymore substantially; i.e., if \\[\n\\mathcal{Q}(\\boldsymbol{\\hat{\\theta}}^{(r)},\\boldsymbol{\\hat{\\theta}}^{(r-1)})\n\\approx\n\\mathcal{Q}(\\boldsymbol{\\hat{\\theta}}^{(r+1)},\\boldsymbol{\\hat{\\theta}}^{(r-1+1)})\n\\]"
  },
  {
    "objectID": "Ch1_MaximumLikelihood.html#solutions",
    "href": "Ch1_MaximumLikelihood.html#solutions",
    "title": "1¬† Maximum Likelihood",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\nBelow I use the same data (one H, four T) that was used to produce the results in Table¬†1.1 of our script. However, you can produce new data by setting another seed-value\n\ntheta_true <- 0.2    # unknown true theta value\nn          <-  5     # sample size\n\nset.seed(1)\n\n# simulate data: n many (unfair) coin tosses\nx <- sample(x       = c(0,1), \n            size    = n, \n            replace = TRUE, \n            prob    = c(1-theta_true, theta_true)) \n\n## number of heads (i.e., the number of \"1\"s in x)\nN_H <- sum(x)\n\n## First derivative of the log-likelihood function\nLp_fct   <- function(theta, N_H = N_H, n = n){\n    (N_H/theta) - (n - N_H)/(1 - theta)    \n}\n## Second derivative of the log-likelihood function\nLpp_fct   <- function(theta, N_H = N_H, n = n){\n    - (N_H/theta^2) - (n - N_H)/(1 - theta)^2    \n}\n\nt     <- 1e-10   # convergence criterion\ncheck <- TRUE    # check object to stop the while-loop\ni     <- 0       # count iterations\n\n## Initializations \ntheta  <- 0.4     # starting value theta_{(0)}\nh_step <- NULL    # empty value \nLp     <- Lp_fct( theta, N_H=N_H, n=n)\nLpp    <- Lpp_fct(theta, N_H=N_H, n=n)\n\nwhile(check){\n    i         <- i + 1\n    ##\n    h_step_new <- -1 * (Lp_fct(theta[i], N_H=N_H, n=n) / Lpp_fct(theta[i], N_H=N_H, n=n))    \n    h_step     <- c(h_step, h_step_new)\n    theta_new  <- theta[i] + h_step_new\n    Lp_new     <- Lp_fct( theta_new, N_H=N_H, n=n)\n    Lpp_new    <- Lpp_fct(theta_new, N_H=N_H, n=n)\n    ##\n    theta      <- c(theta, theta_new) \n    Lp         <- c(Lp,    Lp_new) \n    Lpp        <- c(Lpp,   Lpp_new) \n    ##\n    if( abs(Lp_fct(theta_new, N_H=N_H, n=n)) < t ){\n      check <- FALSE\n    }\n}\n\nresults           <- cbind(1:length(theta)-1, theta, -Lp/Lpp, Lp)\ncolnames(results) <- c(\"m\", \"theta_m\", \"h_m\", \"Lp(theta_m)\")\nresults\n\n     m   theta_m           h_m   Lp(theta_m)\n[1,] 0 0.4000000 -2.400000e-01 -4.166667e+00\n[2,] 1 0.1600000  3.326733e-02  1.488095e+00\n[3,] 2 0.1932673  6.558924e-03  2.159084e-01\n[4,] 3 0.1998263  1.736356e-04  5.433195e-03\n[5,] 4 0.1999999  1.132731e-07  3.539786e-06\n[6,] 5 0.2000000  4.814638e-14  1.504574e-12\n\n\n\n\nSolutions of Exercise 2.\n\n(a) Log-Likelihood Function\nThe log-likelihood function is given by \\[\n\\begin{align*}\n\\ell_n(\\theta)\n&=\\sum_{i=1}^n \\ln (\\theta\\exp(-\\theta X_i))\\\\\n&=\\sum_{i=1}^n (\\ln \\theta -\\theta X_i)\\\\\n&=n \\ln \\theta -\\sum_{i=1}^n \\theta X_i\n\\end{align*}\n\\]\n\n\n(b) ML-Estimator\nThe ML estimator is defined as \\(\\hat{\\theta}_{n}=\\arg\\max_{\\theta}\\ell(\\theta)\\). Deriving the ML estimator \\(\\hat\\theta_n\\): \\[\n\\begin{align*}\n\\ell_n'(\\theta)&=n\\frac{1}{\\theta} - \\sum_{i=1}^n X_i\\\\\n\\ell_n'(\\hat\\theta_n)=0\\quad \\Leftrightarrow &\\quad 0=n\\frac{1}{\\hat\\theta_n} - \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad n\\frac{1}{\\hat\\theta_n} = \\sum_{i=1}^n X_i\\\\\n\\Leftrightarrow &\\quad \\hat\\theta_n = \\frac{1}{\\frac{1}{n}\\sum_{i=1}^n X_i}= \\frac{1}{\\bar{X}_n}\n\\end{align*}\n\\]\n\n\n(b) Fisher Information\nThe Fisher information is given by \\[\n\\mathcal{I}(\\theta_0)=-\\frac{1}{n}\\mathbb{E}(\\ell''(\\theta_0)).\n\\] The second derivative of \\(\\ell_n(\\theta)\\) evaluated at \\(\\theta_0\\) is given by \\[\n\\ell''_n(\\theta)=-n\\frac{1}{\\theta^2_0}.\n\\] Thus, \\[\n\\begin{align*}\n\\mathcal{I}(\\theta_0)\n&=-\\frac{1}{n}\\mathbb{E}(\\ell''_n(\\theta_0))\\\\[2ex]\n&=-\\frac{1}{n}\\left(-n\\frac{1}{\\theta^2_0}\\right)\\\\[2ex]\n&=\\frac{1}{\\theta^2_0}.\n\\end{align*}\n\\] Therefore, the asymptotic distribution of \\(\\hat\\theta_n\\) is \\[\n\\begin{align*}\n\\sqrt{n}(\\hat\\theta_n-\\theta)&\\to_d \\mathcal{N}\\left(0,\\theta^2_0\\right),\n\\end{align*}\n\\] \\(n\\to\\infty.\\)\nSince we do not know the asymptotic variance \\(\\theta_0^2,\\) we need to plug-in a consistent estimator; namely, \\[\n\\hat{\\theta}_n^2 = \\left(\\frac{1}{\\bar{X}_n}\\right)^2 \\approx \\theta^2_0.\n\\] This allows us to use the following Normal approximation to construct statistical hypothesis tests and confidence intervals, etc: \\[\n\\begin{align*}\n\\hat\\theta_n&\\overset{a}{\\sim}\\mathcal{N}\\left(\\theta,\\frac{\\hat{\\theta}_n^2}{n}\\right)\n\\end{align*}\n\\] This approximation is good for largish sample sizes (roughly \\(n\\geq 30\\)).\n\n\n\nSolutions of Exercise 3.\n\n(a) Likelihood Function\nRecall that the density function of \\(\\mathcal{Unif}(0,\\theta)\\) is \\[\nf(x;\\theta_0)\n=\\left\\{\n\\begin{array}{ll}\n\\frac{1}{\\theta} & 0\\leq x\\leq \\theta_0\\\\\n0                & \\text{otherwise}\\\\\n\\end{array}\n\\right.\n\\] Thus, the likelihood function is \\[\n\\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i;\\theta).\n\\] Note: If any \\(\\theta<X_i,\\) we have that \\[\n\\mathcal{L}_n(\\theta)=0.\n\\] Putting it differently, let \\[\nX_{(n)}=\\max\\{X_1,\\dots,X_n\\}\n\\] denote the \\(n\\)th order-statistic, then \\[\n\\mathcal{L}_n(\\theta)=0\\quad\\text{for all}\\quad \\theta<X_{(n)}.\n\\]\nHowever, for all values of \\(\\theta\\) with \\(\\theta \\geq X_{(n)}\\) we have that \\[\nf(X_i;\\theta)=\\frac{1}{\\theta}\\quad\\textbf{for all}\\quad i=1,\\dots,n.\n\\] Thus, for all values of \\(\\theta\\) with \\(\\theta \\geq X_{(n)},\\) \\[\n\\mathcal{L}_n(\\theta)=\\left(\\frac{1}{\\theta}\\right)^n.\n\\]\nSumming up, \\[\n\\mathcal{L}_n(\\theta)\n=\\left\\{\n\\begin{array}{ll}\n\\left(\\frac{1}{\\theta}\\right)^n & \\theta \\geq  X_{(n)}\\\\\n0                               & \\theta < X_{(n)}\\\\\n\\end{array}\n\\right.\n\\tag{1.14}\\]\n\n\n\n(b) Maximum Likelihood Estimator of \\(\\theta_0\\)\n\\(\\mathcal{L}_n(\\theta)\\) is strictly decreasing over the interval \\([X_{(n)},\\infty);\\) see Figure¬†1.3.\n\nn          <- 20   # sample size\nX_max      <- 0.25\n\ntheta_vec  <- seq(from = 0, \n                  to   = X_max * 1.5, \n                  len  = 100) \nlikelihood_fun <- function(theta, X_max, n){ \n    likelihood                <- 1/(theta^n)\n    likelihood[theta < X_max] <- 0 \n    return(likelihood) \n}\n\nlikelihood_vec <- likelihood_fun(theta = theta_vec,\n                                 X_max = X_max, \n                                 n     = n)\n\nplot(y = likelihood_vec, \n     x = theta_vec, \n     type = \"l\", \n     xlab = expression(theta),\n     ylab = \"Likelihood\", \n     main = \"\")            \naxis(1, at = X_max, labels = expression(X[(n)]))                  \n\n\n\n\nFigure¬†1.3: Graph of the likelihood function \\(\\mathcal{L}_n(\\theta)\\) given in Equation¬†1.14.\n\n\n\n\nThus, the maximum likelihood estimator of \\(\\theta_0\\) is \\[\n\\begin{align}\n\\hat{\\theta}_{ML}\n& =\\arg\\max_{\\theta>0}\\mathcal{L}_n(\\theta)\\\\\n& = X_{(n)}.\n\\end{align}\n\\]\n\n\nSolutions of Exercise 4.\n\n(a) Finding the Maximum Likelihood Estimator of \\(\\lambda_0\\)\n\\[\n\\begin{align}\n\\mathcal{L}_n(\\lambda)\n& = \\prod_{i=1}^n f(X_i;\\lambda)\\\\[2ex]\n& = \\prod_{i=1}^n \\frac{\\lambda^{X_i} \\exp(-\\lambda)}{X_i!} \\\\[2ex]\n& = \\frac{\\lambda^{\\sum_{i=1}^n X_i}  \\exp(-n \\lambda)}{\\prod_{i=1}^n (X_i!)} \\\\[4ex]\n\\ell(\\lambda)\n&= \\left(\\sum_{i=1}^n X_i\\right) \\ln(\\lambda) -n\\lambda\\cdot 1 - \\sum_{i=1}^n \\ln(X_i!)\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell'_n(\\lambda)\n&= \\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\lambda}  - n\n\\end{align}\n\\]\n\\[\n\\begin{align}\n\\ell''_n(\\lambda)\n&= -\\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\lambda^2} < 0  \n\\end{align}\n\\] since by the properties of the Poisson distribution \\(X_1,\\dots,X_n>0\\) and \\(\\lambda>0.\\)\nThus the maximum likelihood estimator of \\(\\lambda_0\\) is given by \\[\n\\begin{align}\n&\\frac{\\left(\\sum_{i=1}^n X_i\\right)}{\\hat\\lambda_n}  - n \\overset{!}{=} 0\\\\[2ex]\n\\Rightarrow & \\hat \\lambda_n = \\frac{1}{n}\\sum_{i=1}^n X_i.\n\\end{align}\n\\]\n\n\n(b) Finding the Maximum Likelihood Estimator of \\(P(X=4)\\)\n\\[\n\\begin{align}\nP(X=4) = \\frac{\\lambda_0^4 \\exp(-\\lambda_0)}{4!}\n\\end{align}\n\\]\nThus \\(P(X=4)\\) is a function of \\(\\lambda\\) \\[\n\\begin{align}\nP(X=4)\\equiv P(X=4|\\lambda) = \\frac{\\lambda^4 \\exp(-\\lambda)}{4!} = \\tau(\\lambda)\n\\end{align}\n\\]\n\nlambda_vec <- seq(from = .0001, to = 15, len = 100)\ng_vec      <- (lambda_vec^4 * exp(-1*lambda_vec))/( factorial(4) )\n\nplot(x = lambda_vec, y = g_vec, \n     type = \"l\", xlab=expression(lambda), ylab=expression(tau(lambda)))\nabline(v = 4)\naxis(1, at = 4)\n\n\n\n\n(For \\(0<\\lambda\\leq 4,\\) \\(g(\\lambda)\\) is even a one-to-one mapping.)\nBy the invariance property of the maximum likelihood estimator (which also applies to functions \\(\\tau\\) that are not one-to-one) we thus have \\[\n\\begin{align}\n\\hat{P}(X=4)\\equiv \\hat{P}(X=4|\\hat{\\lambda}_n) = \\frac{\\hat{\\lambda}^4_n \\exp(-\\hat{\\lambda}_n)}{4!}\n\\end{align}\n\\] with \\(\\hat{\\lambda}_n=\\frac{1}{n}\\sum_{i=1}^n X_i.\\)\n\n\n\nSolutions of Exercise 5.\nSetup of Section¬†1.2.2:\nLet \\(\\theta_{root}\\) denote the root of \\(\\ell_n';\\) i.e.¬† \\[\n\\ell_n'(\\theta_{root})=0.\n\\] Let \\[\n\\begin{align*}\ne_{(0)}&=\\theta_{root}-\\theta_{(0)}\\\\[2ex]\ne_{(m)}&=\\theta_{root}-\\theta_{(m)}\n\\end{align*}\n\\] denote the start-value error and the \\(m\\)th step error, respectively.\nLet \\[\nI=[\\theta_{root}-|e_{(0)}|, \\theta_{root}+|e_{(0)}|]\n\\] denote the start-value neighborhood around \\(\\theta_{root}.\\)\nLet \\(\\ell_n'\\) be ‚Äúwell behaved‚Äù over \\(I;\\) such that\n\n\\(\\ell_n''(\\theta)\\neq 0\\) for all \\(\\theta\\in I\\) and\n\\(\\ell_n'''(\\theta)\\) is finite and continuous for all \\(\\theta\\in I.\\)\n\nLet \\(\\theta_{(0)}\\) be ‚Äúclose enough;‚Äù i.e.¬†let\n\n\\(M|e_{(0)}|<1,\\)\n\nwhere \\[\nM=\\frac{1}{2}\\left(\\sup_{\\theta\\in I}|\\ell_n'''(\\theta)|\\right)\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell_n''(\\theta)|}\\right)\\geq 0,\n\\]\nIn the following, we show that the Newton-Raphson algorithm converges under this setup.\n\nBy the second-order Taylor expansion of \\(\\ell'(\\theta_{root})\\) around \\(\\theta_{(m)}\\) with the mean-value form of the remainder term, we have that \\[\n\\begin{align*}\n\\overset{\\theta_{(m)}+(\\theta_{root}-\\theta_{(m)})}{\\ell'\\big(\\;\\overbrace{\\theta_{root}}\\;\\big)}\n& = \\ell'(\\theta_{(m)}) +\n    \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) +\n    \\frac{1}{2}\\ell'''(\\xi)(\\theta_{root}-\\theta_{(m)})^2\n\\end{align*}\n\\] for some real-valued number \\(\\xi_{(m)}\\) between \\(\\theta_{root}\\) and \\(\\theta_{(m)}.\\)\nSince \\(\\ell'(\\theta_{root})=0,\\) we have that \\[\n\\begin{align*}\n0\n& = \\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)}) + \\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\n\\end{align*}\n\\] Some rearrangments lead \\[\n\\begin{align*}\n\\ell'(\\theta_{(m)}) + \\ell''(\\theta_{(m)})(\\theta_{root}-\\theta_{(m)})\n& = -\\frac{1}{2}\\ell'''(\\xi_{(m)})(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n{\\color{blue}\\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}} + (\\theta_{root}-\\theta_{(m)})\n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\qquad[\\text{dividing by}\\;\\ell''(\\theta_{(m)})]\\\\[2ex]\n\\end{align*}\n\\] Using the update steps of the alrorithm \\[\n\\theta_{(m+1)} = \\theta_{(m)} - \\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}\n\\quad\\Leftrightarrow\\quad\n\\theta_{(m)} - \\theta_{(m+1)}  = {\\color{blue}\\frac{\\ell'(\\theta_{(m)})}{\\ell''(\\theta_{(m)})}}\n\\] yields \\[\n\\begin{align*}\n\\theta_{(m)} - \\theta_{(m+1)} + (\\theta_{root}-\\theta_{(m)})\n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad\n\\theta_{root} - \\theta_{(m+1)}  \n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(\\theta_{root}-\\theta_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad\ne_{(m+1)}  \n&= \\frac{-\\ell'''(\\xi_{(m)})}{2\\ell''(\\theta_{(m)})}(e_{(m)})^2\n\\end{align*}\n\\] Taking absolute values, since we are not interested in the sign of the approximation errors, yields \\[\n\\begin{align*}\n|e_{(m+1)}|  \n&= \\frac{1}{2} \\; |\\ell'''(\\xi_{(m)})|\\;\\frac{1}{|\\ell''(\\theta_{(m)})|}(e_{(m)})^2\n\\end{align*}\n\\] Considering the worst case within \\(I,\\) leads to the following inequality \\[\n\\begin{align*}\n|e_{(m+1)}|  \n&\\leq  \\overbrace{\\frac{1}{2} \\; \\left(\\sup_{\\theta\\in I}|\\ell'''(\\theta)|\\right)\\;\\left(\\sup_{\\theta\\in I}\\frac{1}{|\\ell''(\\theta_{(m)})|}\\right)}^{=M}\\;(e_{(m)})^2\\\\[2ex]\n\\Leftrightarrow\\quad |e_{(m+1)}|  \n&\\leq  M\\;(e_{(m)})^2\\\\[-2ex]\n\\end{align*}\n\\tag{1.15}\\] To show that the Newton-Raphon algorithm converges, we need to show that \\[\n|e_{(m)}|\\to 0 \\quad\\text{as}\\quad m \\to\\infty.\n\\]\nFor \\(0\\leq M\\,|e_{(0)}|<1\\) the inequality in Equation¬†1.15 becomes a sharp inequality \\[\n\\begin{align*}\n&|e_{(1)}|\\leq \\overbrace{M\\;|e_{(0)}|}^{<1}\\,|e_{(0)}|\n\\quad\\Rightarrow\\quad |e_{(1)}|< \\,|e_{(0)}|\\\\[3ex]\n&{\\color{darkgreen}[\\text{Using that $M\\;|e_{(0)}|<1$ and that $|e_{(1)}|<|e_{(0)}|$}]}\\\\[2ex]\n\\Rightarrow\\quad\n& |e_{(2)}|\\leq {\\color{darkgreen}\\overbrace{M\\;|e_{(1)}|}^{<1}}\\,|e_{(1)}|\n\\quad\\Rightarrow\\quad |e_{(2)}|< \\,|e_{(1)}|\\\\[2ex]\n&\\phantom{|e_{(2)}|\\leq \\overbrace{M\\;|e_{(1)}|}^{<1}\\,|e_{(1)}|}\\vdots\\\\[2ex]\n\\Rightarrow\\quad\n& |e_{(m+1)}|< \\,|e_{(m)}| \\quad\\text{for all }m=0,1,2\\dots\n\\end{align*}\n\\] which shows the convergence of the Newton Raphson algorithm. (The inequality in Equation¬†1.15 implies that the convergence is even quadratic; i.e.¬†very fast.)\nSpecial Case \\(M=0\\):  For \\[\n\\sup_{\\theta\\in I}|\\ell'''(\\theta)|=0\n\\] we have that \\(M=0\\) which implies that we find the root, \\(\\theta_{root},\\) already in the first \\((m=1)\\) update step, since\n\\[\n\\begin{align*}\n|e_{(1)}|  \n&\\leq \\overbrace{\\left(M |e_{(0)}|\\right)}^{=0}\\;|e_{(0)}|\\\\[2ex]\n\\Rightarrow\\quad |e_{(1)}|&=0\\\\[2ex]\n\\Rightarrow\\quad \\theta_{(1)}&=\\theta_{root},\n\\end{align*}\n\\] even when \\(|e_{(0)}|\\gg 0.\\)\nThis makes sense, since \\(\\sup_{\\theta\\in I}|\\ell'''(\\theta)|=0\\) implies that \\(\\ell'(\\theta)\\) has no curvature; i.e.¬†\\(\\ell'(\\theta)\\) is a straight line for all \\(x\\in I\\) which implies that the Taylor approximation used in the update steps of the Newton-Raphson algorithm is just perfect (no approximation error)."
  },
  {
    "objectID": "Ch3_Bootstrap.html",
    "href": "Ch3_Bootstrap.html",
    "title": "3¬† The Bootstrap",
    "section": "",
    "text": "The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.\nSome literature:"
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-Illustration",
    "href": "Ch3_Bootstrap.html#sec-Illustration",
    "title": "3¬† The Bootstrap",
    "section": "3.1 Illustration: When are you happy about the Bootstrap?",
    "text": "3.1 Illustration: When are you happy about the Bootstrap?\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y.\\) These returns \\(X\\) and \\(Y\\) are random with\n\n\\(Var(X)=\\sigma^2_X\\)\n\\(Var(Y)=\\sigma^2_Y\\)\n\\(Cov(X,Y)=\\sigma_{XY}\\)\n\nWe want to invest a fraction \\(\\alpha\\in(0,1)\\) in \\(X\\) and invest the remaining \\(1-\\alpha\\) in \\(Y.\\)\nOur aim is to minimize the variance (risk) of our investment, i.e., we want to minimize \\[\nVar\\left(\\alpha X + (1-\\alpha)Y\\right).\n\\] One can show that the value \\(\\alpha\\) that minimizes this variance is \\[\n\\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n\\tag{3.1}\\] Using a data set that contains past measurements \\[\n((X_1,Y_1),\\dots,(X_n,Y_n))\n\\] for \\(X\\) and \\(Y,\\) we can estimate the unknown \\(\\alpha\\) by plugging in estimates of the variances and covariances \\[\n\\hat\\alpha = \\frac{\\hat\\sigma^2_Y - \\hat\\sigma_{XY}}{\\hat\\sigma^2_X + \\hat\\sigma^2_Y - 2\\hat\\sigma_{XY}}\n\\tag{3.2}\\] with \\[\n\\begin{align*}\n\\hat{\\sigma}^2_X&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2\\\\\n\\hat{\\sigma}^2_Y&=\\frac{1}{n}\\sum_{i=1}^n\\left(Y_i-\\bar{Y}\\right)^2\\\\\n\\hat{\\sigma}_{XY}&=\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)\\left(Y_i-\\bar{Y}\\right),\n\\end{align*}\n\\] where \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\) and \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^nY_i.\\)\nIt is natural to wish to quantify the accuracy of our estimator \\[\n\\hat\\alpha\\approx \\alpha.\n\\]\nFor instance, to construct a confidence interval we need to know the standard error of the estimator \\(\\hat\\alpha\\), \\[\n\\sqrt{Var(\\hat\\alpha)} = \\operatorname{SE}(\\hat\\alpha)=?\n\\] However, computing \\(\\operatorname{SE}(\\hat\\alpha)\\) is here difficult due to the definition of \\(\\hat\\alpha\\) in Equation¬†3.2 which contains variance estimates also in the denominator.\n\n\n\n\n\n\nConclusion: Why Bootstrap?\n\n\n\nIn cases as described above, we are happy to use the Basic Bootstrap Method (Section¬†3.3) which allows us to approximate \\(\\operatorname{SE}(\\hat\\alpha)\\) simply by resampling from the data observed; i.e.¬†without the need of an explicite formula of a consistent estimator of \\(\\operatorname{SE}(\\hat\\alpha).\\) The Basic Bootstrap Method is found to be as accurate as the standard asymptotic Normality results which, however, require an explicite formula of an estimator of \\(\\operatorname{SE}(\\hat\\alpha)\\) to become useful.\nIf we have a consistent estimator for the \\(\\operatorname{SE}(\\hat\\alpha),\\) then we can make use of this estimator by applying the Bootstrap-\\(t\\) Method (Section¬†3.4). The Bootstrap-\\(t\\) Method is found to be more accurate than the standard asymptotic Normality results."
  },
  {
    "objectID": "Ch3_Bootstrap.html#recap-the-empirical-distribution-function",
    "href": "Ch3_Bootstrap.html#recap-the-empirical-distribution-function",
    "title": "3¬† The Bootstrap",
    "section": "3.2 Recap: The Empirical Distribution Function",
    "text": "3.2 Recap: The Empirical Distribution Function\nThe distribution of a real-valued random variable \\(X\\) can be completely described by its (cumulative) distribution function\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.1 ((Cumulative) Distribution Function (CDF)) \\[\nF(x)=P(X \\leq x)\\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\]\n\n\n\nThe sample analogue of \\(F\\) is the so-called empirical distribution function, which is an important tool of statistical inference.\nLet\n\\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim X\n\\] denote a real-valued random sample with \\(X\\sim F,\\) and let \\(1_{(\\cdot)}\\) denote the indicator function, i.e., \\[\n\\begin{align*}\n1_{(\\text{TRUE})} &=1\\quad\\text{and}\\quad 1_{(\\text{FALSE})}=0.\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.2 (Empirical (Cumulative) Distribution Function (ECDF)) \\[\nF_n(x)=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\quad\\text{for all}\\quad x\\in\\mathbb{R}.\n\\] I.e \\(F_n(x)\\) is the proportion of observations with \\(X_i\\le x,\\) \\(i=1,\\dots,n.\\)\n\n\n\nProperties of the ECDF:\n\\(F_n\\) is a monotonically increasing right-continuous step function that is bounded between zero and one, \\[\n0\\le F_n(x)\\le 1,\n\\] where \\[\nF_n(x)=\\left\\{\n  \\begin{array}{ll}\n  0&\\text{ if }x  < X_{(1)}\\\\\n  1&\\text{ if }x\\ge X_{(n)}\\\\\n  \\end{array}\n\\right.\n\\] where \\[\nX_{(1)}\\leq X_{(2)}\\leq \\dots \\leq X_{(n)}\n\\] denotes the order-statistic.\n\\(F_n\\) is itself a distribution function according to Definition¬†3.1; namely, the distribution function of the discrete random variable \\(X^*,\\) where\n\\[\nX^*\\in\\{X_1,\\dots,X_n\\}\n\\] and \\[\nP(X^*=X_i)=\\frac{1}{n}\\quad\\text{for each}\\quad i=1,\\dots,n.\n\\] Thus \\[\n\\begin{align*}\nF_n(x)\n&=\\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\\\[2ex]\n&= P\\left(X^*\\leq x\\right).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExample 3.1 (Computing the empirical distribution function \\(F_n\\) in R) \nSome data (i.e.¬†an observed realization of an random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d}}{\\sim}F\\)):\n\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n5.20\n\n\n2\n4.80\n\n\n3\n5.40\n\n\n4\n4.60\n\n\n5\n6.10\n\n\n6\n5.40\n\n\n7\n5.80\n\n\n8\n5.50\n\n\n\nCorresponding empirical distribution function using R:\n\nobservedSample <- c(5.20, 4.80, 5.30, 4.60, \n                    6.10, 5.40, 5.80, 5.50)\n\nmyecdf_fun     <- ecdf(observedSample)\n\nplot(myecdf_fun, main=\"\")\n\n\n\n\nThe R function ecdf() returns a function that gives the values of \\(F_n(x):\\)\n\n## Note: ecdf() returns a function that can be evaluated! \nmyecdf_fun(5.0)\n\n[1] 0.25\n\n\n\n\n\n\nStatistical Properties of \\(F_n\\)\n\\(F_n(x)\\) depends on the i.i.d. random sample \\(X_1,\\dots,X_n\\) and thus is itself a random function.\nWe obtain \\[\nnF_n(x)\\sim B(n, p=F(x))\\quad\\text{for each}\\quad x\\in\\mathbb{R}\n\\tag{3.3}\\]\nI.e., \\(nF_n(x)\\) has a binomial distribution with parameters:\n\n\\(n\\) (‚Äúnumber of trials‚Äù)\n\\(p=F(x)\\) (‚Äúprobability of success on a single trial‚Äù).\n\n\nNote: The result in Equation¬†3.3 holds for any \\(F.\\) Therefore, \\(nF_n\\) and thus also \\(F_n\\) is called distribution free\n\nThus, \\[\n\\begin{align*}\n\\mathbb{E}(nF_n(x))& = np = nF(x)\\\\[2ex]\n\\Rightarrow \\quad \\mathbb{E}(F_n(x))& = p = F(x)\\\\[2ex]\n\\Rightarrow \\quad \\operatorname{Bias}(F_n(x))& = \\mathbb{E}(F_n(x)) - F(x) =0\\\\\n\\end{align*}\n\\] and \\[\n\\begin{align*}\nVar(nF_n(x))& = np(1-p) = nF(x)(1-F(x))\\\\[2ex]\n\\Rightarrow \\quad Var(F_n(x))& = \\frac{nF(x)(1-F(x))}{n^2}=\\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\] Therefore, \\[\n\\begin{align*}\n\\operatorname{MSE}(F_n(x))\n& = (\\operatorname{Bias}(F_n(x)))^2 + Var(F_n(x))\\\\[2ex]\n& =\\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\]\nThis allows us to conclude that \\[\n\\begin{align*}\nF_n(x) & \\to_{m.s.} F(x)\\quad\\text{as}\\quad n\\to\\infty\\\\[2ex]\n\\Rightarrow \\quad F_n(x) & \\to_{p} F(x)\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\] for each \\(x\\in\\mathbb{R}.\\)\nThat is, \\(F_n(x)\\) is point-wise for each \\(x\\in\\mathbb{R}\\) a weakly consistent estimator of \\(F(x).\\)\nThe Clivenko-Cantelli Theorem¬†3.1 states that \\(F_n\\) is even uniformly over \\(\\mathbb{R}\\) a consistent estimator of \\(F.\\)\n\n\n\n\n\n\n\n\n\n\n\nTheorem 3.1 (Theorem of Glivenko-Cantelli) \nLet \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}\\sim X\\) denote a real-valued random sample with \\(X\\sim F.\\) Then \\[\n\\begin{align*}\n&\\quad P\\left(\\lim_{n\\rightarrow\\infty} \\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|=0\\right)=1\\\\[2ex]\n\\Leftrightarrow &\\quad\n\\sup_{x\\in\\mathbb{R}} |F_n(x)-F(x)|\\to_{a.s.} 0.\n\\end{align*}\n\\]\n\n\n\n\n\nBasic Idea of the Bootstrap\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\nSampling from the population distribution \\(F\\) (infeasible Monte Carlo simulation) The random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\] Let \\(\\theta_0\\) denote a distribution parameter of \\(F\\) which we want to estimate, and let \\(\\hat\\theta_n\\) denote an estimator of \\(\\theta_0.\\) If we would know \\(F,\\) we could generate arbitrarily many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_{n,1}, \\hat{\\theta}_{n,2}, \\dots, \\hat{\\theta}_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these realizations. Unfortunately, we don‚Äôt know \\(F,\\) thus Monte Carlo inference is infeasible.\nThe idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:  Instead of random sampling from \\(F,\\) which is infeasible, the bootstrap uses random sampling from the known empirical distribution function \\(F_n\\) to generate arbitrarily many bootstrap realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}^*_{n,1}, \\hat{\\theta}^*_{n,2}, \\dots, \\hat{\\theta}^*_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these bootstrap realizations. This is justified asymptotically since for large \\(n,\\) the empirical distribution \\(F_n\\) is ‚Äúclose‚Äù to the unknown distribution \\(F\\) (Glivenko-Cantelli Theorem¬†3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\)\n\\[\n  \\begin{align*}\n  \\underbrace{\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}}_{=F_n(b)-F_n(a)}&\\to_p \\underbrace{P(X\\in [a,b])}_{=F(b)-F(a)}\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "href": "Ch3_Bootstrap.html#basic-idea-of-the-bootstrap",
    "title": "3¬† The Bootstrap",
    "section": "3.3 Basic Idea of the Bootstrap",
    "text": "3.3 Basic Idea of the Bootstrap\nThe basic idea of the bootstrap is to replace random sampling from the true (unknown) population \\(F\\) (infeasible Monte Carlo simulation) by random sampling from the empirical distribution \\(F_n\\) (feasible Monte Carlo simulation).\nSampling from the population distribution \\(F\\) (infeasible Monte Carlo simulation) The random sample \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\) is generated by drawing observations independently and with replacement from the unknown population distribution function \\(F\\). That is, for each interval \\([a,b]\\) the probability of drawing an observation in \\([a,b]\\) is given by \\[\nP(X\\in [a,b])=F(b)-F(a).\n\\] Let \\(\\theta_0\\) denote a distribution parameter of \\(F\\) which we want to estimate, and let \\(\\hat\\theta_n\\) denote an estimator of \\(\\theta_0.\\) If we would know \\(F,\\) we could generate arbitrarily many realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}_{n,1}, \\hat{\\theta}_{n,2}, \\dots, \\hat{\\theta}_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these realizations. Unfortunately, we don‚Äôt know \\(F,\\) thus Monte Carlo inference is infeasible.\nThe idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:  Instead of random sampling from \\(F,\\) which is infeasible, the bootstrap uses random sampling from the known empirical distribution function \\(F_n\\) to generate arbitrarily many bootstrap realizations of the estimator \\(\\hat{\\theta}_n\\) \\[\n\\hat{\\theta}^*_{n,1}, \\hat{\\theta}^*_{n,2}, \\dots, \\hat{\\theta}^*_{n,m}\n\\] with \\(m\\to\\infty\\) and do inference about \\(\\theta_0\\) using these bootstrap realizations. This is justified asymptotically since for large \\(n,\\) the empirical distribution \\(F_n\\) is ‚Äúclose‚Äù to the unknown distribution \\(F\\) (Glivenko-Cantelli Theorem¬†3.1). That is, for \\(n\\rightarrow\\infty\\) the relative frequency of observations \\(X_i\\) in \\([a,b]\\) converges to \\(P(X\\in [a,b])\\)\n\\[\n  \\begin{align*}\n  \\underbrace{\\frac{1}{n}\\sum_{i=1}^n1_{(X_i\\in[a,b])}}_{=F_n(b)-F_n(a)}&\\to_p \\underbrace{P(X\\in [a,b])}_{=F(b)-F(a)}\n  \\end{align*}\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-basic-bootstrap-method",
    "href": "Ch3_Bootstrap.html#the-basic-bootstrap-method",
    "title": "3¬† The Bootstrap",
    "section": "3.4 The Basic Bootstrap Method",
    "text": "3.4 The Basic Bootstrap Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e.¬†parametric) assumption. The basic bootstrap method is often also called:\n\n(Standard) Nonparametric Bootstrap Method\n\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with real valued \\(X\\sim F.\\)\nThe distribution \\(F\\) is depends on an unknown parameter \\(\\theta_0.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate \\(\\theta_0\\in\\mathbb{R}.\\)\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta_n\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\nMoreover, for simplicity let us focus on unbiased and \\(\\boldsymbol{\\sqrt{n}}\\)-consistent estimators, i.e.\n\n\\(\\mathbb{E}\\left(\\hat\\theta_n\\right)=\\theta_0\\)\n\\(\\operatorname{SE}\\left(\\hat\\theta_n\\right)=\\sqrt{Var\\left(\\hat\\theta_n\\right)}=\\frac{1}{\\sqrt{n}}\\cdot\\text{constant}\\)\n\n\nInference: In order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis, we need to know the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e.¬†in learning the limit of the distribution function \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWe could do asymptotic statistics. For instance, using the Lindeberg-L√©vy CLT, we may be able to show that the limit of \\(H_{n}(x)\\) is the distribution function of the Normal distribution with mean zero and asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big).\\)\nHowever, deriving a useful, explicit expression of the asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big)\\) can be very hard (see Section¬†3.1). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific version of the Bootstrap can be even more accurate then a standard asymptotic Normality result.\n\n\n\n\n\n\nThe Core Part of the Bootstrap Algorithm\n\n\n\n\nDraw a bootstrap sample: Generate a new random sample \\(X_1^*,\\dots,X_n^*\\) by drawing observations independently and with replacement from the available sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*_n\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (for a large value of \\(m,\\) such as \\(m=5000\\) or \\(m=10000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\]\n\n\n\nBy the Clivenko-Cantelli (Theorem¬†3.1) the bootstrap estimators \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] allow us to approximate the bootstrap distribution\n\\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\right|\\mathcal{S}_n\\right)\n\\] arbitrarily well, i.e., \\[\n\\sup_{x\\in\\mathbb{R}}\\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\\right|\\to_{a.s} 0\\quad\\text{as}\\quad m\\to\\infty,\n\\] where \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\left(\\hat\\theta^*_{n,j}-\\hat\\theta_n\\right)\\leq x\\right)}\n\\] denotes the empirical distribution function based on the \\(\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\\) centered by \\(\\hat{\\theta}_n\\) and scaled by \\(\\sqrt{n}.\\)\nSince we can choose \\(m\\) arbitrarily large, we can effectively ignore the approximation error between \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Boot}_{n}(x).\\) That is, we can (and will do so) treat the bootstrap distribution \\(H^{Boot}_{n}(x)\\) as known.\nThe crucial question is, however, whether the bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] is able to approximate the unknown distribution \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\n\\] as \\(n\\to\\infty.\\) This is an important requirement called bootstrap consistency.\n\nBootstrap Consistency\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)|\\mathcal{S}_n\\) is a good approximation of the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\), i.e. \\[\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)\\ |{\\cal S}_n\\right)}_{H_n^{Boot}}\\approx\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\right)}_{H_n}.\n\\] The following definition states this more precisely.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Bootstrap consistency)  Let the limit (as \\(n\\to\\infty\\)) of \\(H_n\\) be a non-degenerate distribution. Then the bootstrap is consistent if and only if \\[\n\\sup_{x\\in\\mathbb{R}} \\Big|\\;\n\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta^*_n-\\hat\\theta_n\\big)\\le x \\ |{\\cal S}_n\\Big)}_{H_n^{Boot}(x)}\n  -\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta_n -\\theta_0\\big)\\le x\\Big)}_{H_n(x)}\n  \\Big|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\n\n\nLuckily, the standard bootstrap is consistent in a large number of statistical problems. However, there are some requirements:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).\nTypically, the distribution of the estimator \\(\\hat\\theta_n-\\theta_0\\) needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions 1 or 2 is violated. For instance,\n\nThe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) does not properly reflect the way how \\(X_1,\\dots,X_n\\) are generated in a first place. (For instance, when \\(X_1,\\dots,X_n\\) is generated by a time-series process with auto-correlated data.)\nThe distribution of the estimator \\(\\hat\\theta\\) is not asymptotically normal. (For instance, in case of extreme value problems.)\n\nNote: In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g.¬†the block-bootstrap in case of time-series data).\n\n\n3.4.1 Example: Inference About the Population Mean\nSetup:\n\n\\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\)\nContinuous random variable \\(X\\sim F\\)\nNon-zero, finite variance \\(0<Var(X)=\\sigma_0^2<\\infty\\)\nUnknown mean \\(\\mathbb{E}(X)=\\mu_0,\\) where\n\\[\n\\mu_0 = \\int x f(x) dx = \\int x d F(x),\n\\] where \\(f=F'\\) denotes the density function.\nEstimator: Empirical mean \\[\n\\begin{align*}\n\\bar{X}_n\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nInference Problem: What is the (asymptotic) distribution of \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\n\\] as \\(n\\to\\infty\\)?\n\n\n\n\n\n\nRecall: Inference using Classic Asymptotic Statistics\n\n\n\nThis example is so simple that we know (by the Lindeberg-L√©vy CLT) that \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\to_d\\mathcal{N}\\left(0,\\sigma_0^2\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., that \\[\n%\\bar{X}_n\\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{1}{n}\\sigma_0\\right).\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x,\\) where \\[\n\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n\\] with \\(\\Phi\\) denoting the distribution function of the standard normal distribution, i.e. \\[\n\\Phi_{\\sigma_0}(x)\n=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x/\\sigma_0}\\exp\\left(-\\frac{1}{2}z^2\\right)\\,dz.\n\\]\n\n\nYes, the asymptotic result is simple here (boring), but can we alternatively use the Bootstrap to approximate this limit result? I.e., is \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] able to approximate \\(\\Phi_{\\sigma_0}(x)\\) for all \\(x\\in\\mathbb{R}\\)?\n\n3.4.1.1 Practice: Empirical Consideration of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\nLet us consider the following observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) shown in Table¬†3.1. The data was generated by drawing from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\) That is, \\(Var(X)=\\sigma_0^2=2\\cdot \\operatorname{df}=4.\\)\n\n\n\n\n\nTable¬†3.1: Observed realization of the random sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) drawn from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\)\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n0.36\n\n\n2\n3.39\n\n\n3\n3.24\n\n\n4\n4.90\n\n\n5\n1.76\n\n\n6\n5.33\n\n\n7\n7.77\n\n\n8\n1.93\n\n\n\n\n\nobservedSample <- c(0.36, 3.39, 3.24, 4.90, \n                    1.76, 5.33, 7.77, 1.93)\n\nSo the observed sample mean is\n\n\\(\\bar X_{n,obs} =\\) mean(observedSample) \\(=\\) 3.585\n\n\nBootstrap:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical ‚Äúpopulation‚Äù in order to generate the i.i.d. bootstrap sample \\[\nX_1^*,\\dots,X_n^*\n\\]\nThese i.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}.\\)\nEach realization of the bootstrap sample leads to a new realization of the bootstrap estimator \\(\\bar{X}^*_n\\) as demonstrated in the following R code:\n\n## generating one realization of the bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n## computing the corresponding realization of the bootstrap estimator\nmean(bootSample)\n\n[1] 4.21375\n\n\nWe can now approximate the bootstrap distribution \\[\nH^{Boot}_n(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] using the empirical distribution function \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)}\n\\] based on the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\n\\] generated using the bootstrap algorithm\n\nGenerate bootstrap sample\nCompute bootstrap estimator\nRepeat Steps 1 and 2 \\(m\\) times\n\nwith a (very) large \\(m.\\) The following R code demonstrates this:\n\nn                <- length(observedSample)\nXbar             <- mean(observedSample)\n\nm                <- 10000 # number of bootstrap samples \nXbar_boot        <- vector(mode = \"double\", length = m)\n\n## Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          <- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        <- mean(bootSample)\n}\n\nplot(ecdf( sqrt(n) * (Xbar_boot - Xbar) ), \n     xlab = \"\", ylab = \"\", \n     main = \"Bootstrap Distribution vs Normal Limit Distribution\")\ncurve(pnorm(x, mean = 0, sd = sqrt(4)), col = \"red\", add = TRUE)     \nlegend(\"topleft\", \n       legend = c(\"Bootstrap Distribution\", \n                  \"Normal Limit Distribution with\\nMean = 0 and Variance = 4\"), \n      col = c(\"black\", \"red\"), lty = c(1,1))\n\n\n\n\nNote: To plot the Normal limit distribution we need to make use of our knowledge that \\(X_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^2_{(\\operatorname{df}=2)}\\) which implies that we know (the usually unknpown) asymptotic variance of the estimator \\(\\bar{X}_n,\\) \\[\nnVar(\\bar{X}_n)=Var(\\sqrt{n}(\\bar{X}_n-\\mu_0))=\\sigma_0^2=2\\cdot\\operatorname{df}=4,\n\\] for each \\(n=1,2,\\dots,\\) thus also \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=4.\\)\nUsually, however, we do not know the value of the asymptotic variance, but need an estimator for this quantity. (Which can be hard to derive.)\nBy contrast, we get the complete bootstrap distribution directly from the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}.\n\\] That is, to estimate the usually unknown value of the asymptotic variance \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=\\sigma_0^2=4,\\) we can simply use the empirical variance of the bootstrap estimators \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\\) multiplied by \\(n,\\) as done in the following R-code:\n\nround(n * var(Xbar_boot), 2)\n\n[1] 4.82\n\n\n\n\n\n3.4.1.2 Theory (Part 1): Mean and Variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\cdot),\\;Var^*(\\cdot),\\;\\text{and}\\;P^*(\\cdot)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\cdot)=\\mathbb{E}(\\cdot|\\mathcal{S}_n),\n\\] the conditional variance \\[\nVar^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the bootstrap distribution, i.e.¬†on the conditional distribution of \\[\n\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n.\n\\]\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze the bootstrap distribution of \\(\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n,\\) since we know ü§ü the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\\;i=1,\\dots,n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F,\\) \\(i=1,\\dots,n.\\)\n\n\n\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable: \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the discrete conditional random variable \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X_i^*)\n&=\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X_n.\n\\end{align*}\n\\] I.e., the empirical mean \\(\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) of the original sample \\(X_1,\\dots,X_n\\) is the ‚Äúpopulation‚Äù variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\nThe conditional variance of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}\\left((X_i^* - \\mathbb{E}(X_i^*|{\\cal S}_n))^2|{\\cal S}_n\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\\\[2ex]\n&=\\hat\\sigma^2_0.\n\\end{align*}\n\\] I.e., the empirical variance \\(\\hat\\sigma^2_{0}=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\) of the original sample \\(X_1,\\dots,X_n\\) is the ‚Äúpopulation‚Äù variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X_i^*))=\\mathbb{E}(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq \\delta)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important.\nThe unconditional distribution of \\(X_i^*\\) is equal to the unknown distribution \\(F.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X_i^*\\leq x)\n&= P(1_{(X_i^*\\leq x)}=1) \\\\[2ex]\n&= P(1_{(X_i^*\\leq x)}=1) \\cdot 1 + P(1_{(X_i^*\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= \\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}|\\mathcal{S}_n\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq x\\right)}}\\right)\\quad[\\text{{\\color{blue}from our derivations above}}]\\\\[2ex]\n&= \\frac{n}{n}\\mathbb{E}\\left(1_{\\left(X_i\\leq x\\right)}\\right)\\\\[2ex]\n&= P(1_{(X_i\\leq x)}=1) \\cdot 1 + P(1_{(X_i\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= P\\left(X_i\\leq x\\right)=F(x)\n\\end{align*}\n\\]\n\n\nNow we can consider the mean and the variance of \\(\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\)\n\nThe conditional mean of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=\\mathbb{E}\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\,\\mathbb{E}\\left(\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right)- \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right)\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\color{red}\\mathbb{E}\\left(X^*_i|{\\cal S}_n\\right)}- \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}\\mathbb{E}\\left(X_i|{\\cal S}_n\\right)}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{n}{n}{\\color{red}\\bar{X}_n} - \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}X_i}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\bar{X}_n - \\bar{X}_n\\right)\\\\[2ex]\n&= 0.\n\\end{align*}\n\\]\nThe conditional variance of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\nVar^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=Var\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\left(\\big(\\bar X^*_n-\\bar{X}_n\\big)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\big(\\bar X^*_n|{\\cal S}_n\\big)\\quad[\\text{cond.~on $\\mathcal{S}_n,$ $\\bar{X}_n$ is a constant}]\\\\[2ex]\n&=n\\,Var\\Big(\\frac{1}{n}\\sum_{i=1}^n X_i^*\\Big|{\\cal S}_n\\Big)\\\\\n&=n\\,\\frac{1}{n^2}\\sum_{i=1}^n Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=n\\,\\frac{n}{n^2} Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=Var\\big(X_i^*|{\\cal S}_n\\big)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\quad[\\text{derived above}]\\\\[2ex]\n&=\\hat\\sigma^2_0,\n\\end{align*}\n\\] where \\[\n\\hat\\sigma^2_0\\to_p \\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThus, we know now that for large \\(n\\) (\\(n\\to\\infty\\)) the mean and the variance of the bootstrap distribution of \\[\n\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n\n\\] matches the mean (zero) and the variance (\\(\\sigma_0^2\\)) of the limit distribution \\(\\Phi_{\\sigma_0}.\\)\nBootstrap consistency, however, addresses the total distribution‚Äînot only the first two moments.\n\n\n\n\n3.4.1.3 Theory (Part 2): Bootstrap Consistency\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.4 (Characteristic Function) Let \\(X\\in\\mathbb{R}\\) be a random variable and let \\(\\mathcal{i}=\\sqrt{-1}\\) be the imaginary unit. Then the function \\(\\psi_X:\\mathbb{R}\\to\\mathbb{C}\\) defined by \\[\n\\psi_X(t) = \\mathbb{E}(\\exp(\\mathcal{i}tX))\n\\] is called the characteristic function of \\(X.\\)\n\n\n\n\n\n\n\n\n\nCharacteristic Function: Some useful facts\n\n\n\nThe characteristic function ‚Ä¶\n\n‚Ä¶ uniquely determines its associated probability distribution.\n‚Ä¶ can be used to easily derive (all) the moments of a random variable.\n‚Ä¶ is often used to prove that two distributions are equal.\nThe characteristic function of \\(\\Phi_{\\sigma_0}\\) is \\[\n\\psi_{\\Phi_{\\sigma_0}}(t)=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)=\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\n\\tag{3.3}\\]\nThe characteristic function of \\(\\sum_{i=1}^nW_i,\\) where \\(W_1,\\dots,W_n\\) are i.i.d., is \\[\n\\psi_{\\sum_{i=1}^nW_i}(t)=\\left(\\psi_{W_1}(t)\\right)^n.\n\\tag{3.4}\\]\nLet \\(W\\) be a random variable with \\(\\mathbb{E}(W)=0\\) and \\(Var(W)=\\sigma_W^2.\\) Then, we have that (see Equation (26.11) in Billingsley (1995)) \\[\n\\psi_W(t)=1-\\frac{1}{2}\\sigma_W^2 \\, t^2 + \\lambda(t),\n\\tag{3.5}\\] where \\(|\\lambda(t)|\\leq |t^2|\\,\\mathbb{E}\\left(\\min(|t|\\,|W|^3, W^2)\\right).\\)\n\n\n\n\nThe following can be found in Example 3.1 in Shao and Tu (1996)\n\nIt follows from the Lindeberg-L√©vy CLT that \\[\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all \\(x\\in\\mathbb{R}.\\) This result can be proven by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) To see this, rewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n& = \\sum_{i=1}^n\\frac{X_i-\\mu_0}{\\sqrt{n}}\\\\[2ex]\n& = \\sum_{i=1}^n W_{i,n}\n\\end{align*}\n\\] where\n\n\\(W_{1,n},\\dots,W_{n,n}\\) are i.i.d. with\n\\(\\mathbb{E}(W_{i,n})=0\\) and\n\\(Var(W_{i,n})=\\frac{1}{n}\\sigma_0^2.\\)\n\nTherefore, by Equation¬†3.4 together with Equation¬†3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n(t)|\n&\\leq |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,\\left|W_{1,n}\\right|^3, \\left|W_{1,n}\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_1-\\mu_0\\right|^3, n^{-1}\\left|X_1-\\mu_0\\right|^2\\big)\\right).\n\\end{align*}\n\\] That is, \\[\nn|\\lambda_n(t)|\\to 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] which means that \\(|\\lambda_n(t)|\\to 0\\) faster than \\(n^{-1}.\\)\nThus, by Equation¬†3.3 \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&= \\lim_{n\\to\\infty}\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t)\n\\end{align*}\n\\]\nOK, we have shown that \\(H_n\\) tends to \\(\\Phi_{\\sigma_0}\\) by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\) (I.e. we have shown the Lindeberg-L√©vy CLT.)\nTo show bootstrap consistency we need to show that \\(H_n^{Boot}\\) tends to \\(\\Phi_{\\sigma_0}.\\) To do so, we can mimic the above prove, by showing that the characteristic function of \\(H_n^{Boot}\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nRewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}^*_n- \\bar{X}_n\\right)|\\mathcal{S}_n\n& = \\sum_{i=1}^n\\frac{X^*_i- \\bar{X}_n}{\\sqrt{n}}|\\mathcal{S}_n\\\\[2ex]\n& = \\sum_{i=1}^n W^*_{i,n}|\\mathcal{S}_n\n\\end{align*}\n\\] where\n\n\\(W^*_{1,n}|\\mathcal{S}_n,\\dots,W^*_{n,n}|\\mathcal{S}_n\\) is i.i.d. with\n\\(\\mathbb{E}^*(W^*_{n})=\\mathbb{E}(W^*_{n}|\\mathcal{S}_n)=0\\) and\n\\(Var^*(W^*_{n})=Var(W^*_{n}|\\mathcal{S}_n)=\\frac{1}{n}\\hat{\\sigma}_0^2=\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right)\\)\n\nTherefore, by Equation¬†3.4 together with Equation¬†3.5 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}|\\mathcal{S}_n}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}|\\mathcal{S}_n}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}{\\color{darkgreen}\\hat{\\sigma}_0^2} \\, t^2 + {\\color{red}\\lambda_n^*(t)}\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n^*(t)|\n&\\leq |t^2|\\,{\\color{blue}\\mathbb{E}^*}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_1-\\bar{X}_n\\right|^3, n^{-1}\\left|X_1^* - \\bar{X}_n\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,{\\color{blue}\\frac{1}{n}\\sum_{i=1}^n}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_i-\\bar{X}_n\\right|^3, n^{-1}\\left|X_i^* - \\bar{X}_n\\right|^2\\big)\\right).\n\\end{align*}\n\\] By the Marcinkiewicz strong law of large numbers, we obtain that \\[\nn{\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., \\({\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\) faster than \\(n^{-1}.\\) Moreover, since \\[\n{\\color{darkgreen}\\hat\\sigma_0^2} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\to_{a.s.}\\sigma_0^2\n\\] we have that (using Equation¬†3.3) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n\\to_{a.s.}&\n\\lim_{n\\to\\infty}\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t).\n\\end{align*}\n\\] This implies that the limit (\\(n\\to\\infty\\)) of \\(H_n^{Boot}\\) is \\(\\Phi_{\\sigma_0}\\) almost surely.\nHence we have shown that the basic bootstrap is consistent for doing inference about \\(\\mu_0\\) using \\(\\bar{X}_n.\\)\n\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\nH_n^{Boot}(x)=P\\left(\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x|\\mathcal{S}_n)\\right) \\approx\n\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar X^*_{n,j}-\\bar X_n\\right)\\leq x\\right)}=H_{n,m}^{Boot}(x),  \n\\] as done in Section¬†3.4.1.1.\n\n\n\n\n3.4.2 The Basic Bootstrap Confidence Interval\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\theta_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\n\\(\\theta_0\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2)\\) as \\(n\\to\\infty,\\)\n\\(\\hat{v}_n\\to_{p} v_0\\) as \\(n\\to\\infty\\)\n\nAn approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}_n - z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}},\n\\hat{\\theta}_n + z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}}\n\\right],\n\\] where \\(z_{1-\\frac{\\alpha}{2}}\\) denotes the \\((1-\\alpha)/2\\) quantile of the standard Normal distribution. This confidence interval is approximate, since it is only asymptotically justified; i.e.¬†it is not exact in finite samples.\n\n\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v_n\\) of \\(v_0\\) (see ?sec-Illustraction). Statistical inference is then usually based on the bootstrap confidence intervals.\nIn many situations it can be shown that bootstrap confidence intervals (or tests) are even more precise than asymptotic normality based confidence intervals. (This particularly applies to the bootstrap t-method discussed in the next section.)\n\nAlgorithm of the Basic Bootstrap Confidence Interval for \\(\\theta_0\\):\nSetup:\n\nData: i.i.d. random sample \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}\n\\] with \\(X_i\\sim F\\) for all \\(i=1,\\dots,n\\), where the distribution \\(F\\) depends on the unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nProblem: Construct a confidence interval for \\(\\theta_0\\in\\mathbb{R}.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is Consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e.¬†that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^*_n -\\hat{\\theta}_n)|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}_n-\\theta))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}_n -\\theta)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\n\n\nAlgorithm (3 Steps):\n\nGenerate \\(m\\) bootstrap estimates\n\\[\n\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] by repeatedly (\\(m\\) times) drawing bootstrap samples \\(X_{1}^*,\\dots,X_{n}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nUse the \\(m\\) bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantiles \\[\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\quad\\text{and}\\quad \\hat q^*_{n,1-\\frac{\\alpha}{2}}\n\\] of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat q^*_{n,p}=\\left\\{\n  \\begin{array}{ll}\n  \\hat\\theta^*_{n,(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n  (\\hat\\theta^*_{n,(mp)}+\\hat\\theta^*_{n,(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.6}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{(j)}^*\\) denotes the \\(j\\)th order statistic \\[\n\\hat\\theta_{n,(1)}^* \\leq \\hat\\theta_{n,(2)}^*\\leq \\dots\\leq \\hat\\theta_{n,(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g.¬†\\(\\lfloor 4.9\\rfloor = 4\\)).\nThe approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval is then given by \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\tag{3.7}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe quantiles \\(\\hat q^*_{n,p}\\) are those of the distribution \\[\nG_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\hat{\\theta}^*_{n,j}\\leq x\\right)}.\n\\] However, we‚Äôll treat the quantiles \\(\\hat q^*_{n,p}\\) as quantiles of the distribution \\[\nG_{n}^{Boot}(x)=P\\left(\\hat{\\theta}^*_{n}\\leq x\\,\\big|\\,\\mathcal{S}_n\\right),\n\\] since for large \\(m\\) (\\(m\\to\\infty\\)) the difference between \\(G_{n,m}^{Boot}\\) and \\(G_{n}^{Boot}\\) is negligible (Glivenko-Cantelli Theorem¬†3.1) and we can choose \\(m\\) to be large.\n\n\nJustifying the Basic Bootstrap CI (Equation¬†3.7) for \\(\\theta_0\\): \\[\n\\begin{align*}\n&P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\leq \\hat{\\theta}^*_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n \\leq\\hat{\\theta}^*_n -\\hat{\\theta}_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\]\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\leq\\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat{\\theta}_n-(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\le \\theta_0\\le \\hat{\\theta}_n-\n(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\le \\theta_0\\le 2\\hat{\\theta}_n-\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\] This demonstrates that the basic bootstrap confidence interval in Equation¬†3.7 \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] is indeed an asymptotically valid (i.e.¬†approximate) \\((1-\\alpha)\\times 100\\%\\) (symmetric) confidence interval.\n\n\nExample: Basic Bootstrap Confidence Interval for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0.\\)\nEstimator: \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu_0.\\)\nInference Problem: Construct a confidence interval for \\(\\mu_0.\\)\n\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\mu_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\nBy the CLT: \\(\\sqrt{n}(\\bar X_n - \\mu_0)\\to_d\\mathcal{N}(0,\\sigma^2_0)\\) as \\(n\\to\\infty\\)\nEstimation of \\(\\sigma^2_0\\): \\(s^2_n=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\)\nThis implies: \\(\\sqrt{n}((\\bar X_n -\\mu_0)/s_n)\\to_d\\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\)\n\nLet \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1).\\) Since \\(z_{\\alpha/2} = -z_{1-\\alpha/2},\\) we have that \\[\n\\begin{align*}\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\le \\frac{\\sqrt{n}(\\bar X_n -\\mu_0)}{s_n}\\le z_{1-\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\bar X_n -\\mu_0\\le z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\le \\mu_0\\le\n        \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\n  \\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\nApproximate \\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}},\n    \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{s_n}{\\sqrt{n}}\\right]\n\\]\n\n\n\nAlgorithm of the basic bootstrap confidence interval for \\(\\mu_0\\):\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation (see Section¬†3.4.1.3).\n\nDraw \\(m\\) bootstrap samples (e.g.¬†\\(m=10,000\\)) and calculate the corresponding estimates \\[\n\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\n\\]\nCompute the empirical quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) basic bootstrap confidence interval according to Equation¬†3.7: \\[\n\\left[2\\bar X_n -\\hat q^*_{n,1-\\frac{\\alpha}{2}},\n   2\\bar X_n -\\hat q^*_{n,\\frac{\\alpha}{2}}\\right]\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#the-bootstrap-t-method",
    "href": "Ch3_Bootstrap.html#the-bootstrap-t-method",
    "title": "3¬† The Bootstrap",
    "section": "3.5 The Bootstrap-\\(t\\) Method",
    "text": "3.5 The Bootstrap-\\(t\\) Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e.¬†parametric) assumption.\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the (nonparametric) bootstrap-\\(t\\) method (one also speaks of the ‚Äústudentized bootstrap‚Äù). The construction relies on so-called (asymptotically) pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.5 ((Asymptotically) Pivotal Statistics) \nA statistic \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called exact pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter.\nA statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications.\nIt is, however, often possible to construct an asymptotically pivotal statistic. Consider, for instance, an asymptotically normal \\(\\sqrt{n}\\)-consistent estimator \\(\\hat{\\theta}_n\\) of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator of \\(v_0^2\\) \\[\n\\hat v_n^2 \\rightarrow_p v_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\] which implies that also \\[\n\\hat v_n \\rightarrow_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Then, \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] is asymptotically pivotal, since \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\n\nExample: \\(\\bar{X}_n\\) is a Pivotal Statistic\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu_0\\), variance \\(0<Var(X)=\\sigma_0^2<\\infty\\), and \\(\\mathbb{E}(|X|^4)=\\beta<\\infty\\).\n\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\sim t_{n-1}\\quad\\text{for any}\\quad n=2,3,\\dots\n\\] with \\(s_n^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is exact pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\rightarrow_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistic.\n\n\n\nBootstrap-\\(t\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*\\big|\\mathcal{S}_n =\\sqrt{n}\\frac{(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}{\\hat v_n^*}\\Big|\\mathcal{S}_n,\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the variance estimate \\(\\hat{v}_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*=\\hat{v}(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(t\\) consistency follows if the basic bootstrap is consistent\n\n\n\nIf the basic bootstrap is consistent, i.e.¬†if the conditional distribution of \\(\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), yields a consistent estimate of \\(\\mathcal{N}(0,v^2)\\), then also the bootstrap-\\(t\\) method is consistent. That is, then the conditional distribution of \\(T_n^*|\\mathcal{S}_n\\), given \\(\\mathcal{S}_n\\), provides a consistent estimate of the asymptotic distribution of \\(T_n\\rightarrow_d \\mathcal{N}(0,1)\\) such that \\[\n\\sup_{x\\in\\mathbb{R}} \\left|P\\left(\\left.\\sqrt{n} \\frac{(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{\\hat v_n^*}\\le x \\;\\right|\\;{\\cal S}_n\\right)-\\Phi(x)\\right|\\rightarrow_p 0,\\quad\\text{as}\\quad n\\to\\infty,\n\\] where \\(\\Phi\\) denotes the distribution function of the standard normal distribution.\n\n\n\n\n3.5.1 The Bootstrap-\\(t\\) Confidence Interval\nSetup:\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2\\) of \\(\\hat{\\theta}_n,\\) i.e.¬† \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\hat v^2_n\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty,\n\\]\nand that \\[\n\\hat v_n\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\nAlgorithm of the Bootstrap-\\(t\\) Confidence Interval for \\(\\theta_0\\):\nAlgorithm (3 Steps):\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*_n\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\n\\hat v^*_n\\equiv \\hat v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT_n^*&=\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g.¬†\\(m=100,000\\)) many bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\n\\]\nUse the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) empirical quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) (see Equation¬†3.6).\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval\n\\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n   \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\tag{3.8}\\] where \\(\\hat\\theta_n\\) and \\(\\hat v_n\\) are the estimates of \\(\\theta_0\\) and \\(v_0\\) based on the original sample \\(X_1,\\dots,X_n.\\)\n\nJustifying the Bootstrap-\\(t\\) CI (Equation¬†3.8) for \\(\\theta_0\\):\nThe bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\n\\] yield the empirical bootstrap distribution \\[\nH_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right)}\n\\] which approximates the bootstrap distribution \\[\nH_{n}^{Boot}(x)=P\\left(\\left.\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] arbitrarily precise as \\(m\\to\\infty\\) (Glivenko-Cantelli Theorem¬†3.1).\nThus, the empirical bootstrap quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) of \\(H_{n,m}^{Boot}\\) are indeed consistent (\\(m\\to\\infty\\)) for the quantiles \\(\\hat \\tau_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution \\(H_{n}^{Boot}.\\) This implies, for large \\(m,\\) \\[\nP^*\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha.\n\\]\nMoreover, due to the assumed consistencies of the bootstrap and of the estimator \\(\\hat v_n,\\) we have that for large \\(n\\) that \\[\n\\left.{\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v_n^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}}.\n\\] Therefore, for large \\(n,\\) \\[\n\\begin{align*}\n& P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}} \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq  \\hat{\\theta}_n-\\theta_0 \\leq \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(- \\hat{\\theta}_n + \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\leq -\\theta_0 \\leq - \\hat{\\theta}_n + \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq \\theta_0 \\leq \\hat{\\theta}_n - \\hat \\tau^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) (symmetric) bootstrap-\\(t\\) confidence interval (Equation¬†3.8) \\[\n\\left[\\hat{\\theta}_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\] is indeed an asymptotic (i.e.¬†approximate) \\((1-\\alpha)\\times 100\\%\\) CI.\n\n\nExample: Bootstrap-\\(t\\) Confidence Interval for the Mean\nHere \\(\\hat\\theta_n = \\bar{X}_n\\) and the estimator of the asymptotic variance of \\(\\bar{X}_n\\) is \\(s^2\\approx \\lim_{n\\to\\infty}n Var(\\bar{X}_n)=\\sigma_0^2\\), where \\(s^2\\) denotes the sample variance \\[\ns_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2.\n\\]\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(s_n^*=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*_n)^2}\\) to generate \\(m\\) (e.g.¬†\\(m=100,000\\)) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat \\tau^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat \\tau^*_{n,1-\\frac{\\alpha}{2}}\\) from \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\] using Equation¬†3.6.\nThis yields the \\((1-\\alpha)\\times 100 \\%\\) confidence interval (using Equation¬†3.8): \\[\n\\left[\\bar X_n - \\hat \\tau^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right),\n    \\bar X_n - \\hat \\tau^*_{n,  \\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right)\\right],\n\\] where \\(s_n\\) is computed from the original sample, i.e., \\[\ns_n=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2}.\n\\]\n\n\n\n\n3.5.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the basic bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{v^*_n}\\;\\right|\\;\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(v^*_n\\) depends on the bootstrap sample ‚Äî not the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)\\;\\right|\\;\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\([L_n,U_n]\\) of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g.¬†basic bootstrap vs.¬†bootstrap-\\(t\\)).\n\n(Symmetric) confidence intervals are said to be first-order accurate if there exist some constants \\(c_1,c_2<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_1}{\\sqrt{n}}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_2}{\\sqrt{n}}\n\\end{align*}\n\\]\n(Symmetric) confidence intervals are said to be second-order accurate if there exist some constants \\(c_3,c_4<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta<L_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_3}{n}\\\\\n\\left|P(\\theta>U_n)-\\frac{\\alpha}{2}\\right|\\le \\frac{c_4}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic approximations are first-order accurate.\nBasic bootstrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field."
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "href": "Ch3_Bootstrap.html#regression-analysis-bootstrapping-pairs",
    "title": "3¬† The Bootstrap",
    "section": "3.6 Regression Analysis: Bootstrapping Pairs",
    "text": "3.6 Regression Analysis: Bootstrapping Pairs\nConsider the linear regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or ‚Äúdependent‚Äù) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Random and fixed design) \nRandom Design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0,\\) \\(M=\\mathbb{E}(X_iX_i^T)\\) non-singular, and with either\n\nhomoskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0\\), \\(i=1,\\dots,n\\), for a constant \\(\\sigma^2<\\infty\\) or\nheteroskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed Design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean, \\(\\mathbb{E}(\\varepsilon_i)=0,\\) and homoskedastic errors, \\(\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0,\\) for all \\(i=1,\\dots,n.\\)\n\n\n\nThe least squares estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i.\n\\end{align*}\n\\]\nUsing that \\(Y_i=X_i^\\top\\beta_0+\\varepsilon_i,\\) one can derive that \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\beta_0+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\n\n3.6.1 Bootstrapping Pairs: Bootstrap under Random Design\nUnder a random design (Definition¬†3.6), we assume that there exists a non-singular (thus invertible) matrix \\(M\\) \\[\nM=\\mathbb{E}(X_iX_i^T).\n\\] This implies that the following matrix \\(Q\\) is also non-singular: \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\varepsilon_i^2X_iX_i^T)\\\\[2ex]\n&=\\mathbb{E}(\\mathbb{E}(\\varepsilon_i^2X_iX_i^T|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T\\mathbb{E}(1|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIn case of homoskedastic errors, we have that \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\\\\\n&=\\sigma^2_0\\;\\mathbb{E}(X_iX_i^T)\\\\[2ex]\n&=\\sigma^2_0\\;M.\n\\end{align*}\n\\]\n\n\nThe law of large numbers, the continuous mapping theorem, Slutsky‚Äôs theorem, and the central limit theorem (see econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta_0)\\rightarrow_d\\mathcal{N}_p(0,M^{-1}QM^{-1}),\\quad n\\to\\infty,\n\\] where \\(\\mathcal{N}_p(0,M^{-1}QM^{-1})\\) denotes the \\(p\\)-dimensional normal distribution with \\((p\\times 1)\\)-dimensional mean \\(0\\) and \\((p\\times p)\\)-dimensional variance-covariance matrix \\(M^{-1}QM^{-1}.\\)\nThe idea of bootstraping pairs is very simple: The procedure builds upon the assumption that \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. which suggests a bootstrap based on resampling the pairs \\((Y_i,X_i),\\) \\(i=1,\\dots,n.\\)\nBootstraping Pairs Algorithm:\n\nGenerate bootstrap samples \\[\n  (Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n  \\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*):\\) \\[\n\\hat\\beta^*_n=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nRepeating Steps 1-2 \\(m\\)-many times yields \\(m\\) (e.g.¬†\\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\nIt can be shown that bootstrapping pairs is consistent; i.e.¬†that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\\approx\\mathcal{N}_p(0,M^{-1}QM^{-1})\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "href": "Ch3_Bootstrap.html#regression-analysis-residual-bootstrap",
    "title": "3¬† The Bootstrap",
    "section": "3.7 Regression Analysis: Residual Bootstrap",
    "text": "3.7 Regression Analysis: Residual Bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure (Section¬†3.6.1) proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoskedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] under fixed design (Definition¬†3.6), where \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d. with zero mean \\[\n\\mathbb{E}(\\varepsilon_i)=0\n\\] and homoskedastic errors \\[\n\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0.\n\\]\n\n\n\n\n\n\nApplicability of the Residual Bootstrap under Random Designs\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs‚Äîeven when the \\(X\\)-variables are correlated (e.g.¬†time-series).\nIn such cases, the following arguments are meant conditionally on the given \\(X_1,\\dots,X_n\\).\nThe above assumptions on the error terms then, of course, also have to be satisfied conditionally on \\(X_1,\\dots,X_n.\\)\n\n\nThe idea of the residual bootstrap is very simple: The procedure builds upon the assumption that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i=Y_i-X_i^T\\hat\\beta_n, \\quad i=1,\\dots,n,\n\\] where \\[\n\\hat\\beta_n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator based on the original sample \\(\\mathcal{S}_n\\).\nIt is well known that \\[\n\\hat\\sigma^2_n= \\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides a consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\n\\hat\\sigma^2_n\\rightarrow_p \\sigma_0^2\n\\] as \\(n\\to\\infty.\\)\nResidual Bootstrap Algorithm:\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta_n\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta_n + \\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*_n = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) (e.g.¬†\\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\hat\\beta^*_{n,2},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\n\nMotivating the Residual Bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoskedastic (!) errors. We have \\[\n\\hat\\beta_n-\\beta_0=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) we have that \\[\n\\sqrt{n}(\\hat\\beta_n - \\beta_0)\\to_d\\mathcal{N}_p(0,\\sigma^2_0 M^{-1}),\n\\] where \\(\\mathcal{N}_p(0,\\sigma^2 M^{-1})\\) denotes the \\(p\\) dimensional normal distribution with \\((p\\times 1)\\) mean \\(0\\) and \\((p\\times p)\\) variance-covariance matrix \\(\\sigma^2_0 M^{-1}.\\)\nOn the other hand (the bootstrap world), we have the construction \\[\n\\hat\\beta^*_n - \\hat\\beta_n\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*.\n\\] Conditionally on \\({\\cal S}_n,\\) the bootstrap error terms \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) are i.i.d with \\[\n\\mathbb{E}(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 = \\hat\\sigma^2_n,\n\\] where \\(\\hat\\sigma^2_n\\to\\sigma^2_0\\) as \\(n\\to\\infty.\\)\nAn appropriate central limit theorem argument implies that \\[\n\\left.\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n)\\right|\\mathcal{S}_n\\to_d\\mathcal{N}\\left(0,\\sigma^2_0\\, M\\right),\n\\] as \\(n\\to\\infty.\\)\nThat is, for large \\(n\\), we have that \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta_n-\\beta_0))}_{\\mathcal{N}\\left(0,\\sigma^2_0\\, M\\right)}\n\\]\n\n\n3.7.1 Bootstrap Confidence Intervals for the Regression Coefficients\n\nBasic Bootstrap Confidence Intervals for \\(\\beta_{0,j}\\)\nThis allows to construct basic bootstrap confidence intervals for the \\(j\\)th regression coefficient \\(\\beta_{0,j}\\), \\(j=1,\\dots,p\\), of \\(\\beta_0\\in\\mathbb{R}^p.\\)\n\nGenerate \\(m\\) (e.g.¬†\\(m=100,000\\)) bootstrap realizations \\[\n\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast\n\\]\nDetermine the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) from the bootstrap realizations \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast\\) using Equation¬†3.7.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval as in Equation¬†3.8: \\[\n\\left[2\\hat\\beta_{nj}-\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j},\n    2\\hat\\beta_{nj}-\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\right],\n\\] where \\(\\hat\\beta_{nj}\\) denotes the \\(j\\)th component of \\(\\hat\\beta_{n}\\) computed from the original sample \\(\\mathcal{S}_n,\\) and where the empirical quantiles \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) are computed from the bootstrap estimates \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast.\\)\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are heteroskedastic.\nThis is not true for the standard confidence intervals usually provided by standard software packages. For instance, the standard confint(object) function in R for an object returned by the lm() function uses the standard error formula for homoskedastic errors.\n\n\n\n\n\nBootstrap-\\(t\\) Confidence Intervals\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_j\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nLet \\(\\gamma_{jj}\\) denote the \\(j\\)-th diagonal element of the matrix \\((\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\), i.e., \\[\n\\gamma_{jj}:=\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\right]_{jj}.\n\\] Then \\[\n\\frac{\\sqrt{n}(\\hat\\beta_{n,j}-\\beta_{0,j})}{\\hat\\sigma_n\\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\n\\] is an asymptotically pivotal statistic, \\[\n\\frac{\\sqrt{n}(\\hat\\beta_{n,j}-\\beta_{0,j})}{\\hat\\sigma_n\\sqrt{\\gamma_{jj}}}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_{0,j}\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\n\nGenerate bootstrap estimates \\[\nT^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*,\n\\] where \\[\nT^*_{n}=\\frac{\\hat\\beta_{n,j}^*-\\hat\\beta_{0,j}}{\\hat\\sigma_n^* \\sqrt{\\gamma_{jj}}}\n\\] with \\[\n\\hat\\sigma^{*2}_n:=\\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^{*2}.\n\\]\nCompute the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q_{n,1-\\frac{\\alpha}{2},j}\\) (see Equation¬†3.7) from the bootstrap estimates \\(T^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*.\\)\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation¬†3.9: \\[\n\\left[\n  \\hat\\beta_{n,j}-\\hat q_{1-\\frac{\\alpha}{2},n,j}\\hat\\sigma_n \\sqrt{\\gamma_{jj}},\\;\n  \\hat\\beta_{n,j}-\\hat q_{\\frac{\\alpha}{2},n,j}\\hat\\sigma_n \\sqrt{\\gamma_{jj}}\n\\right],\n\\] where \\(\\hat\\beta_{n,j}\\) and \\(\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\\) are computed from the original sample \\(\\mathcal{S}_n.\\)\n\n\n\n\n\n\n\nTip\n\n\n\nThere are furhter bootstrap procedures. In case of heteroskedastic errors, for instance, there‚Äôs also the Wild Bootstrap or the Multiplier Bootstrap (see Section 6 in Horowitz (2001)). These two methods are similar to the residual bootstrap since it generates new outcome variables \\(Y^\\ast_i\\) by generating new residuals, conditionally on the predictors \\(X_i\\). However these Boostrap methods generate new residuals by multiplying a real random variable \\(W_i\\in\\mathbb{R}\\) to the observed (not resampled) residuals \\[\nY_i^\\ast = X_i^\\top \\hat\\beta_n + \\hat\\varepsilon_n \\cdot W_i,\n\\] \\(W_i\\) has a specific distribution (Rademacher distribution, Mammen‚Äôs distribution, Normal distribution, etc). This approach is very successful and also works in high-dimensional (\\(p\\) as large as \\(n\\) or larger) problems."
  },
  {
    "objectID": "Ch3_Bootstrap.html#exercises",
    "href": "Ch3_Bootstrap.html#exercises",
    "title": "3¬† The Bootstrap",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nConsider the empirical distribution function \\[\nF_n(x) = \\frac{1}{n}\\sum_{i=1}^n 1_{(X_i\\leq x)}\n\\] for a random sample \\[\nX_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim} F.\n\\]\n\nDerive the exact distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nDerive the asymptotic distribution of \\(F_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\nShow that \\(F_n(x)\\) is a point-wise (weakly) consistent estimator of \\(F(x)\\) for each given \\(x\\in\\mathbb{R}\\).\n\n\n\nExercise 2.\n\n\n\n\n\n\nTip\n\n\n\nExercise 1 shows that the empirical distribution function is a point-wise consistent estimator for each given \\(x\\in\\mathbb{R}.\\) However, point-wise consistency generally does not imply uniformly consistency for all \\(x\\in\\mathbb{R},\\) and therefore the Clivenko-Cantelli (Theorem¬†3.1) is so famous.\nThis exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.\n\n\nPoint-wise convergence of a function \\(g_n(x),\\) i.e., \\[\n|g_n(x) - g(x)|\\to 0\n\\] for each \\(x\\in\\mathcal{X}\\subset\\mathbb{R}\\) as \\(n\\to\\infty\\) generally does not imply uniform convergence, i.e., \\[\n\\sup_{x\\in\\mathcal{X}}|g_n(x) - g(x)|\\to 0\n\\] as \\(n\\to\\infty.\\)\nShow this by providing an example for \\(g_n\\) which converges point-wise, but not uniformly for \\(x\\in\\mathcal{X}\\).\n\n\n\nExercise 3.\nConsider the following setup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(Var(X_i)=\\sigma^2<\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\n\nDerive the classic confidence interval for \\(\\mu\\) using the asymptotic normality of the estimator \\(\\bar{X}.\\) Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of \\(n=20\\) and,\n\n\nPart 1: For \\(F\\) being the normal distribution with \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\), and\nPart 2: For \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom.\n\n\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.\nReconsider the case of \\(n=20\\) and \\(F\\) being the \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-\\(t\\) confidence interval.\n\n\n\nExercise 4.\n\nLet \\(\\mathcal{S}_n = \\{Y_1 , \\dots, Y_n\\}\\) be a random sample from a population with mean \\(\\mu\\), variance \\(\\sigma^2,\\) and distribution function \\(F.\\) Let \\(F_n\\) be the empirical distribution function. Let \\(\\bar{Y}\\) be the sample mean for \\(\\mathcal{S}_n.\\) Let \\(\\mathcal{S}^*_n = \\{Y_1^‚àó,\\dots, Y_n^‚àó\\}\\) be a random sample taken independently and with replacement from \\(\\mathcal{S}_n.\\) Let \\(\\bar{Y}^*\\) be the sample mean for \\(\\mathcal{S}^*_n.\\)\n\nShow that \\[\n\\mathbb{E}^*(\\bar{Y}^*) = \\bar{Y}\n\\]\nShow that \\[\n\\mathbb{E}(\\bar{Y}^*) = \\mu\n\\]"
  },
  {
    "objectID": "Ch3_Bootstrap.html#references",
    "href": "Ch3_Bootstrap.html#references",
    "title": "3¬† The Bootstrap",
    "section": "References",
    "text": "References\n\n\n\n\nBillingsley, Patrick. 1995. Probability and Measure. 3rd ed. Wiley.\n\n\nDavidson, Russell, and Emmanuel Flachaire. 2008. ‚ÄúThe Wild Bootstrap, Tamed at Last.‚Äù Journal of Econometrics 146 (1): 162‚Äì69.\n\n\nDavison, Anthony Christopher, and David Victor Hinkley. 2013. Bootstrap Methods and Their Application. Cambridge University Press.\n\n\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\n\n\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer Science.\n\n\nHorowitz, Joel L. 2001. ‚ÄúThe Bootstrap.‚Äù In Handbook of Econometrics, 5:3159‚Äì3228.\n\n\nKoike, Yuta. 2024. ‚ÄúHigh-Dimensional Bootstrap and Asymptotic Expansion.‚Äù arXiv Preprint arXiv:2404.05006.\n\n\nMammen, Enno. 1992. ‚ÄúWhen Does Bootstrap Work: Asymptotic Results and Simulations.‚Äù Lecture Notes in Statistics 77.\n\n\n‚Äî‚Äî‚Äî. 1993. ‚ÄúBootstrap and Wild Bootstrap for High Dimensional Linear Models.‚Äù The Annals of Statistics 21 (1): 255‚Äì85.\n\n\nShao, Jun, and Dongsheng Tu. 1996. The Jackknife and Bootstrap. Springer Science."
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-BasicBootstrap",
    "href": "Ch3_Bootstrap.html#sec-BasicBootstrap",
    "title": "3¬† The Bootstrap",
    "section": "3.3 The Basic Bootstrap Method",
    "text": "3.3 The Basic Bootstrap Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e.¬†parametric) assumption. The basic bootstrap method is often also called:\n\n(Standard) Nonparametric Bootstrap Method or\nNonparametric Percentile Bootstrap Method\n\nSetup:\n\ni.i.d. sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where \\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with real valued \\(X\\sim F.\\)\nThe distribution \\(F\\) is depends on an unknown parameter \\(\\theta_0.\\)\nThe data \\(X_1,\\dots,X_n\\) is used to estimate \\(\\theta_0\\in\\mathbb{R}.\\)\nThus, the estimator is a function of the random sample \\[\n\\hat\\theta_n\\equiv \\hat\\theta(X_1,\\dots,X_n).\n\\]\nMoreover, for simplicity let us focus on unbiased and \\(\\boldsymbol{\\sqrt{n}}\\)-consistent estimators, i.e.\n\n\\(\\mathbb{E}\\left(\\hat\\theta_n\\right)=\\theta_0\\)\n\\(\\operatorname{SE}\\left(\\hat\\theta_n\\right)=\\sqrt{Var\\left(\\hat\\theta_n\\right)}=\\frac{1}{\\sqrt{n}}\\cdot\\text{constant}\\)\n\n\nInference: In order to provide (approximate for \\(n\\to\\infty\\)) standard errors, construct confidence intervals, and to perform tests of hypothesis, we need to know the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\quad\\text{as}\\quad n\\to\\infty;\n\\] i.e.¬†we need to know the limit of the distribution function \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nWe could use asymptotic statistics to derive this limit. For instance, using the Lindeberg-L√©vy CLT, we may be able to show that the limit of \\(H_{n}(x)\\) is the distribution function of the Normal distribution with mean zero and asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big).\\)\nHowever, deriving a useful, explicit expression of the asymptotic variance \\(\\lim_{n\\to\\infty}n\\cdot Var\\big(\\hat\\theta_n\\big)\\) can be very hard (see Section¬†3.1). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific version of the Bootstrap can be even more accurate then a standard asymptotic Normality result.\n\n\n\n\n\n\nThe Core Part of the Bootstrap Algorithm\n\n\n\n\nDraw a bootstrap sample: Generate a new random sample \\[\nX_1^*,\\dots,X_n^*\n\\] by drawing observations independently and with replacement from the available sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}.\\)\nCompute bootstrap estimate: Compute the estimate \\[\n\\hat\\theta^*_n\\equiv \\hat\\theta(X_1^*,\\dots,X_n^*)\n\\]\nBootstrap replications: Repeat Steps 1 and 2 \\(m\\) times (for a large value of \\(m,\\) such as \\(m=5000\\) or \\(m=10000\\)) leading to \\(m\\) bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\]\n\n\n\nBy the Clivenko-Cantelli (Theorem¬†3.1) the bootstrap estimates \\[\n\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] allow us to approximate the bootstrap distribution\n\\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\right|\\mathcal{S}_n\\right)\n\\] arbitrarily well, i.e., \\[\n\\sup_{x\\in\\mathbb{R}}\\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\\right|\\to_{a.s} 0\\quad\\text{as}\\quad m\\to\\infty,\n\\] where \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\sqrt{n}\\left(\\hat\\theta^*_{n,j}-\\hat\\theta_n\\right)\\leq x\\right)}\n\\] denotes the empirical distribution function based on the \\(\\hat\\theta_{n,1}^*,\\hat\\theta_{n,2}^*,\\dots,\\hat\\theta_{n,m}^*\\) centered by \\(\\hat{\\theta}_n\\) and scaled by \\(\\sqrt{n}.\\)\nSince we can choose \\(m\\) arbitrarily large, we can effectively ignore the approximation error between \\(H^{Boot}_{n,m}(x)\\) and \\(H^{Boot}_{n}(x).\\) That is, we can (and will do so) treat the bootstrap distribution \\(H^{Boot}_{n}(x)\\) as known. ü§ì\nThe crucial question is, however, whether the (effectively known) bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\hat\\theta^*_n-\\hat\\theta_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] is able to approximate the unknown distribution \\[\nH_{n}(x)=P\\left(\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\\leq x\\right)\n\\] as \\(n\\to\\infty.\\) This is a basic requirement called bootstrap consistency. If a bootstrap method is inconsistent, you shall not use it in practice.\n\nBootstrap Consistency\nThe bootstrap does not always work. A necessary condition for the use of the bootstrap is the consistency of the bootstrap approximation.\nThe bootstrap is called consistent if, for large \\(n\\), the bootstrap distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)|\\mathcal{S}_n\\) is a good approximation of the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big);\\) i.e., if \\[\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}^*_n -\\hat{\\theta}_n\\big)\\ |{\\cal S}_n\\right)}_{H_n^{Boot}}\\approx\n\\underbrace{\\text{distribution}\\left(\\sqrt{n}\\big(\\hat{\\theta}_n-\\theta_0\\big)\\right)}_{H_n}.\n\\] for large \\(n.\\)\nThe following definition states this more precisely.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.3 (Bootstrap Consistency)  Let the limit (as \\(n\\to\\infty\\)) of \\(H_n\\) be a non-degenerate distribution. Then the bootstrap is consistent if and only if \\[\n\\sup_{x\\in\\mathbb{R}} \\Big|\\;\n\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta^*_n-\\hat\\theta_n\\big)\\le x \\ |{\\cal S}_n\\Big)}_{H_n^{Boot}(x)}\n  -\\underbrace{P\\Big(\\sqrt{n}\\big(\\hat\\theta_n -\\theta_0\\big)\\le x\\Big)}_{H_n(x)}\n  \\Big|\\rightarrow_p 0\n\\] as \\(n\\to\\infty.\\)\n\n\n\nLuckily, the standard bootstrap is consistent in a large number of statistical problems. Typically, the bootstrap is consistent if the following two requirements hold:\n\nGeneration of the bootstrap sample must reflect appropriately the way in which the original sample has been generated. That is,\n\nif the original sample was generated by i.i.d. sampling, then also the bootstrap samples need to be generated by i.i.d. sampling.\nif the original sample was generated by cluster sampling, then also the bootstrap samples need to be generated by cluster sampling.\n\nTypically, the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\n\\] needs to be asymptotically normal.\n\nThe standard bootstrap will usually fail if one of the above conditions is violated. For instance, ‚Ä¶\n\nthe bootstrap will not work if the i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) does not properly reflect the way how \\(X_1,\\dots,X_n\\) are generated in the first place. (For instance, when \\(X_1,\\dots,X_n\\) is generated by a time-series process with auto-correlated data, but the bootstrap samples are generated by i.i.d. sampling from \\(\\mathcal{S}_n\\))\nthe bootstrap will not work if the distribution of \\[\n\\sqrt{n}\\left(\\hat\\theta_n-\\theta_0\\right)\n\\] is not asymptotically normal. (For instance, in case of extreme value problems.)\n\nNote: In order to deal with more complex sampling schemes alternative bootstrap procedures have been proposed in the literature (e.g.¬†the block-bootstrap in case of time-series data).\n\n\n3.3.1 Example: Inference About the Population Mean\nSetup:\n\n\\(X_1,\\dots,X_n\\overset{\\text{i.i.d.}}{\\sim}X\\) with \\(X\\sim F\\)\nContinuous random variable \\(X\\sim F\\)\nNon-zero, finite variance \\(0<Var(X)=\\sigma_0^2<\\infty\\)\nUnknown mean \\(\\mathbb{E}(X)=\\mu_0,\\) where\n\\[\n\\mu_0 = \\int x f(x) dx = \\int x d F(x),\n\\] where \\(f=F'\\) denotes the density function.\nEstimator: Empirical mean \\[\n\\begin{align*}\n\\bar{X}_n\n&\\equiv \\bar{X}(X_1,\\dots,X_n) \\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n X_i \\\\[2ex]\n&=\\int x d F_n(x)\n\\end{align*}\n\\]\n\nInference Problem: What is the (asymptotic) distribution of \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\n\\] as \\(n\\to\\infty\\)?\n\n\n\n\n\n\nRecall: Inference using Classic Asymptotic Statistics\n\n\n\nThis example is so simple that we know (by the Lindeberg-L√©vy CLT) that \\[\n\\sqrt{n}\\left(\\bar{X}_n -\\mu_0\\right)\\to_d\\mathcal{N}\\left(0,\\sigma_0^2\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., that \\[\n%\\bar{X}_n\\overset{a}{\\sim}\\mathcal{N}\\left(\\mu_0,\\frac{1}{n}\\sigma_0\\right).\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all continuity points \\(x,\\) where \\[\n\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n\\] with \\(\\Phi\\) denoting the distribution function of the standard normal distribution, i.e. \\[\n\\Phi_{\\sigma_0}(x)\n=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\n=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{x/\\sigma_0}\\exp\\left(-\\frac{1}{2}z^2\\right)\\,dz.\n\\]\n\n\nYes, the asymptotic result is simple here (boring ü•±), but can we alternatively use the Bootstrap to approximate this limit result? I.e., is \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] able to approximate \\(\\Phi_{\\sigma_0}(x)\\) for all \\(x\\in\\mathbb{R}\\)?\nBefore we answer this question theoretically (see Section¬†3.3.1.2 and Section¬†3.3.1.3), we check it empirically.\n\n3.3.1.1 Practice: Empirical Consideration of the Bootstrap distribution\nIn this chapter we investigate the distribution of \\[\n\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\n\\] i.e., the bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] empirically using artificial data.\n\nQuestion to be checked: Is \\(H^{Boot}_{n}(x)\\) able to approximate the asymptotic distribution \\(\\Phi_{\\sigma_0}(x)\\)?\n\nLet us consider the following observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with a rather smallish sample size of \\(n=8\\) shown in Table¬†3.1. The data was generated by drawing from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\) That is, \\(Var(X)=\\sigma_0^2=2\\cdot \\operatorname{df}=4.\\)\n\nThe bootstrap is justified asymptotically \\((n\\to\\infty).\\) Choosing a smallish data size of \\(n=8\\) is done out of curiosity. The approximation of \\(\\Phi_{\\sigma_0}(x)\\) by \\(H^{Boot}_{n}(x)\\) will become better for larger sample sizes.\n\n\n\n\n\n\nTable¬†3.1: Observed realization of the random sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) with sample size \\(n=8\\) drawn from a \\(\\chi^2\\)-distribution with \\(\\operatorname{df}=2.\\)\n\n\n\\(i\\)\n\\(X_i\\)\n\n\n\n\n1\n0.36\n\n\n2\n3.39\n\n\n3\n3.24\n\n\n4\n4.90\n\n\n5\n1.76\n\n\n6\n5.33\n\n\n7\n7.77\n\n\n8\n1.93\n\n\n\n\n\nobservedSample <- c(0.36, 3.39, 3.24, 4.90, \n                    1.76, 5.33, 7.77, 1.93)\n\nSo the observed sample mean is\n\n\\(\\bar X_{n,obs} =\\) mean(observedSample) \\(=\\) 3.585\n\n\nBootstrap:\nThe observed sample \\[\n{\\cal S}_n=\\{X_1,\\dots,X_n\\}\n\\] is taken as underlying empirical ‚Äúpopulation‚Äù in order to generate the i.i.d. bootstrap sample \\[\nX_1^*,\\dots,X_n^*\n\\]\nThese i.i.d. samples \\(X_1^*,\\dots,X_n^*\\) are generated by drawing observations independently and with replacement from \\({\\cal S}_n=\\{X_1,\\dots,X_n\\}.\\)\nEach realization of the bootstrap sample leads to a new realization of the bootstrap estimator \\(\\bar{X}^*_n\\) as demonstrated in the following R code:\n\n## generating one realization of the bootstrap sample\nbootSample <- sample(x       = observedSample, \n                     size    = length(observedSample), \n                     replace = TRUE)\n\n## computing the realization of the bootstrap estimator\nmean(bootSample)\n\n[1] 4.21375\n\n\nWe can now approximate the bootstrap distribution \\[\nH^{Boot}_n(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] using the empirical distribution function \\[\nH^{Boot}_{n,m}(x)=\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar{X}^*_{n,j}-\\bar{X}_n\\right)\\leq x\\right)}\n\\] based on the bootstrap estimators \\[\n\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\n\\] generated using the bootstrap algorithm\n\nGenerate bootstrap sample\nCompute bootstrap estimator\nRepeat Steps 1 and 2 \\(m\\) times\n\nwith a (very) large \\(m.\\) The following R code demonstrates this:\n\nn                <- length(observedSample)\nXbar             <- mean(observedSample)\n\nm                <- 10000 # number of bootstrap samples \nXbar_boot        <- vector(mode = \"double\", length = m)\n\n## Bootstrap algorithm\nfor(k in seq_len(m)){\n bootSample          <- sample(x       = observedSample, \n                               size    = n, \n                               replace = TRUE)\n Xbar_boot[k]        <- mean(bootSample)\n}\n\nplot(ecdf( sqrt(n) * (Xbar_boot - Xbar) ), \n     xlab = \"\", ylab = \"\", \n     main = \"Bootstrap Distribution vs Normal Limit Distribution\")\ncurve(pnorm(x, mean = 0, sd = sqrt(4)), col = \"red\", add = TRUE)     \nlegend(\"topleft\", \n       legend = c(\"Bootstrap Distribution\", \n                  \"Normal Limit Distribution with\\nMean = 0 and Variance = 4\"), \n      col = c(\"black\", \"red\"), lty = c(1,1))\n\n\n\n\nNote: To plot the Normal limit distribution we need to make use of our knowledge that \\(X_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^2_{(\\operatorname{df}=2)}\\) which implies that we know the usually unknpown asymptotic variance of the estimator \\(\\bar{X}_n:\\) \\[\nnVar(\\bar{X}_n)=Var(\\sqrt{n}(\\bar{X}_n-\\mu_0))=\\sigma_0^2=2\\cdot\\operatorname{df}=4,\n\\] for each \\(n=1,2,\\dots,\\) thus also \\(\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=4.\\)\nUsually, however, we do not know the value of the asymptotic variance, but need an estimator for this quantity. (Which can be hard to derive.)\nBy contrast to the asymptotic normality result from applying the CLT, the bootstrap distribution gives us the complete distribution without having to know the asymptotic variance.\nThat is, to estimate the usually unknown value of the asymptotic variance \\[\n\\lim_{n\\to\\infty}nVar(\\bar{X}_n)=\\sigma_0^2\n\\] (here \\(\\sigma_0^2=4\\)), we can simply use the empirical variance of the bootstrap estimators \\(\\bar{X}^*_{n,1},\\dots,\\bar{X}^*_{n,m}\\) multiplied by \\(n,\\) i.e. \\[\nn\\;\\cdot\\;\\underbrace{\\frac{1}{m}\\sum_{j=1}^m \\left(\\bar{X}^*_{n,j} - \\left(\\frac{1}{m}\\sum_{j=1}^m\\bar{X}^*_{n,j}\\right)\\right)^2}_{\\text{estimator of the usually unknown }Var(\\bar{X}_n)}\n\\]\nas done in the following R-code:\n\nround(n * var(Xbar_boot), 2)\n\n[1] 4.82\n\n\n\n\n\n3.3.1.2 Theory (Part 1): Mean and Variance of the Bootstrap distribution\nIn this chapter we begin with the theoretical consideration of the Bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right).\n\\] We begin with focusing on the mean and the variance of \\(H^{Boot}_{n}.\\)\n\n\n\n\n\n\nNotation \\(\\mathbb{E}^*(\\cdot),\\) \\(Var^*(\\cdot),\\) and \\(P^*(\\cdot)\\)\n\n\n\nIn the bootstrap literature one frequently finds the notation \\[\n\\mathbb{E}^*(\\cdot),\\;Var^*(\\cdot),\\;\\text{and}\\;P^*(\\cdot)\n\\] to denote the conditional expectation \\[\n\\mathbb{E}^*(\\cdot)=\\mathbb{E}(\\cdot|\\mathcal{S}_n),\n\\] the conditional variance \\[\nVar^*(\\cdot)=Var(\\cdot|\\mathcal{S}_n),\n\\] and the conditional probability \\[\nP^*(\\cdot)=P(\\cdot|\\mathcal{S}_n),\n\\] given the sample \\({\\cal S}_n.\\)\n\n\nThe bootstrap focuses on the bootstrap distribution, i.e.¬†on the conditional distribution of \\[\n\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n.\n\\]\n\n\n\n\n\n\nWe know the distribution of \\(X_i^*|\\mathcal{S}_n\\)\n\n\n\nWe can analyze the bootstrap distribution of \\(\\sqrt{n}(\\bar X^*_n -\\bar X_n)|\\mathcal{S}_n,\\) since we know ü§ü the discrete distribution of the conditional random variables \\[\nX_i^*|\\mathcal{S}_n,\\;i=1,\\dots,n,\n\\] even though, we do not know the distribution of \\(X_i\\sim F,\\) \\(i=1,\\dots,n.\\)\n\n\n\nFor each \\(i=1,\\dots,n\\), the possible values of the discrete random variable \\(X_i^*|\\mathcal{S}_n\\) are \\[\nX_i^*|\\mathcal{S}_n\\in\\{X_1,\\dots,X_n\\},\n\\] and each of these values is equally probable: \\[\n\\begin{align*}\nP^*(X_i^*=X_1)&= P(X_i^*=X_1|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\nP^*(X_i^*=X_2)&= P(X_i^*=X_2|{\\cal S}_n) = \\frac{1}{n} \\\\[2ex]\n&\\vdots\\\\[2ex]\nP^*(X_i^*=X_n)&= P(X_i^*=X_n|{\\cal S}_n) = \\frac{1}{n}.\n\\end{align*}\n\\]\nThus, we know the whole distribution of the discrete conditional random variable \\(X_i^*|\\mathcal{S}_n\\) and, therefore, can compute, for instance, easily its conditional mean and its variance.\n\nThe conditional mean of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*(X_i^*)\n&=\\mathbb{E}(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\frac{1}{n}X_1+\\frac{1}{n}X_2+\\dots+\\frac{1}{n}X_n\\\\[2ex]\n&=\\bar X_n.\n\\end{align*}\n\\] I.e., the empirical mean \\(\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) of the original sample \\(X_1,\\dots,X_n\\) is the ‚Äúpopulation‚Äù variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\nThe conditional variance of \\(X_i^*\\) given \\({\\cal S}_n\\) is \\[\n\\begin{align*}\nVar^*(X_i^*)\n&=Var(X_i^*|{\\cal S}_n)\\\\[2ex]\n&=\\mathbb{E}\\left((X_i^* - \\mathbb{E}(X_i^*|{\\cal S}_n))^2|{\\cal S}_n\\right)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2\\\\[2ex]\n&=\\hat\\sigma^2_0.\n\\end{align*}\n\\] I.e., the empirical variance \\(\\hat\\sigma^2_{0}=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2\\) of the original sample \\(X_1,\\dots,X_n\\) is the ‚Äúpopulation‚Äù variance of the bootstrap sample \\(X^*_1,\\dots,X^*_n.\\)\n\n\n\n\n\n\n\nGeneral case: Conditional moments of transformed \\(g(X_i^*)\\)\n\n\n\nFor any (measurable) function \\(g\\) we have \\[\n\\mathbb{E}^*(g(X_i^*))=\\mathbb{E}(g(X_i^*)|\\mathcal{S}_n)=\\frac{1}{n}\\sum_{i=1}^n g(X_i).\n\\] For instance, \\(g(X_i)=1_{(X_i\\leq x)}.\\)\n\n\n\n\n\n\n\n\nCaution: Conditioning on \\(\\mathcal{S}_n\\) in important!\n\n\n\nConditioning on the observed sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) is very important.\nThe unconditional distribution of \\(X_i^*\\) is equal to the unknown distribution \\(F.\\) This can be seen from the following derivation: \\[\n\\begin{align*}\nP(X_i^*\\leq x)\n&= P(1_{(X_i^*\\leq x)}=1) \\\\[2ex]\n&= P(1_{(X_i^*\\leq x)}=1) \\cdot 1 + P(1_{(X_i^*\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= \\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\mathbb{E}\\left(1_{\\left(X_i^*\\leq x\\right)}|\\mathcal{S}_n\\right)}\\right)\\\\[2ex]\n&= \\mathbb{E}\\left({\\color{blue}\\frac{1}{n}\\sum_{i=1}^n 1_{\\left(X_i\\leq x\\right)}}\\right)\\quad[\\text{{\\color{blue}from our derivations above}}]\\\\[2ex]\n&= \\frac{n}{n}\\mathbb{E}\\left(1_{\\left(X_i\\leq x\\right)}\\right)\\\\[2ex]\n&= P(1_{(X_i\\leq x)}=1) \\cdot 1 + P(1_{(X_i\\leq x)}=0) \\cdot 0\\\\[2ex]\n&= P\\left(X_i\\leq x\\right)=F(x)\n\\end{align*}\n\\]\n\n\nNow we can consider the mean and the variance of: \\[\n\\sqrt{n}\\left.\\left(\\bar X^*_n-\\bar{X}_n\\right)\\;\\right|\\;\\mathcal{S}_n.\n\\]\n\nThe conditional mean of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\n\\mathbb{E}^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=\\mathbb{E}\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\,\\mathbb{E}\\left(\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\mathbb{E}\\left(\\bar X^*_n|{\\cal S}_n\\right)- \\mathbb{E}\\left(\\bar{X}_n|{\\cal S}_n\\right)\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{1}{n}\\sum_{i=1}^n{\\color{red}\\mathbb{E}\\left(X^*_i|{\\cal S}_n\\right)}- \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}\\mathbb{E}\\left(X_i|{\\cal S}_n\\right)}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\frac{n}{n}{\\color{red}\\bar{X}_n} - \\frac{1}{n}\\sum_{i=1}^n{\\color{blue}X_i}\\right)\\\\[2ex]\n&=\\sqrt{n}\\left(\\bar{X}_n - \\bar{X}_n\\right)\\\\[2ex]\n&= 0.\n\\end{align*}\n\\]\nThe conditional variance of \\(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\) given \\(\\mathcal{S}_n\\) is \\[\n\\begin{align*}\nVar^*\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)\\right)\n&=Var\\left(\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\left(\\big(\\bar X^*_n-\\bar{X}_n\\big)|{\\cal S}_n\\right)\\\\[2ex]\n&=n\\,Var\\big(\\bar X^*_n|{\\cal S}_n\\big)\\quad[\\text{cond.~on $\\mathcal{S}_n,$ $\\bar{X}_n$ is a constant}]\\\\[2ex]\n&=n\\,Var\\Big(\\frac{1}{n}\\sum_{i=1}^n X_i^*\\Big|{\\cal S}_n\\Big)\\\\\n&=n\\,\\frac{1}{n^2}\\sum_{i=1}^n Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=n\\,\\frac{n}{n^2} Var\\big(X_i^*|{\\cal S}_n\\big)\\\\\n&=Var\\big(X_i^*|{\\cal S}_n\\big)\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\quad[\\text{derived above}]\\\\[2ex]\n&=\\hat\\sigma^2_0,\n\\end{align*}\n\\] where \\[\n\\hat\\sigma^2_0\\to_p \\sigma_0^2\\quad\\text{as}\\quad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThus, we know now that for large \\(n\\) (\\(n\\to\\infty\\)) mean (zero) and the variance (\\(\\sigma_0^2\\)) of the bootstrap distribution of \\[\n\\sqrt{n}\\left(\\bar X^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n\n\\] matches the mean (zero) and the variance (\\(\\sigma_0^2\\)) of the limit distribution \\(\\Phi_{\\sigma_0}.\\)\nBootstrap consistency, however, addresses the total distribution‚Äînot only the first two moments.\n\n\n\n\n3.3.1.3 Theory (Part 2): Bootstrap Consistency\nIn this chapter we continue our theoretical consideration of the Bootstrap distribution \\[\nH^{Boot}_{n}(x)=P\\left(\\left.\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x\\;\\right|\\;\\mathcal{S}_n\\right),\n\\] but consider now the total distribution‚Äînot only mean and variance.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.4 (Characteristic Function) Let \\(X\\in\\mathbb{R}\\) be a random variable and let \\(\\mathcal{i}=\\sqrt{-1}\\) be the imaginary unit. Then the function \\(\\psi_X:\\mathbb{R}\\to\\mathbb{C}\\) defined by \\[\n\\psi_X(t) = \\mathbb{E}(\\exp(\\mathcal{i}tX))\n\\] is called the characteristic function of \\(X.\\)\n\n\n\n\n\n\n\n\n\nCharacteristic Function: Some useful facts\n\n\n\nThe characteristic function ‚Ä¶\n\n‚Ä¶ uniquely determines its associated probability distribution.\n‚Ä¶ can be used to easily derive (all) the moments of a random variable by \\[\n\\mathbb{E}(X^n) = \\mathcal{i}^n \\left[\\frac{d^n}{d t^n}\\psi_X(t)\\right]_{t=0}\n\\]\n‚Ä¶ is often used to prove that two distributions are equal.\nThe characteristic function of \\(\\Phi_{\\sigma_0}\\) is \\[\n\\begin{align*}\n\\psi_{\\Phi_{\\sigma_0}}(t)\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\n\\end{align*}\n\\tag{3.4}\\]\nThe characteristic function of \\(\\sum_{i=1}^nW_i,\\) where \\(W_1,\\dots,W_n\\) are i.i.d., is \\[\n\\psi_{\\sum_{i=1}^nW_i}(t)=\\left(\\psi_{W_1}(t)\\right)^n\n\\tag{3.5}\\]\nLet \\(W\\) be a random variable with \\(\\mathbb{E}(W)=0\\) and \\(Var(W)=\\sigma_W^2.\\) Then, we have that (see Equation (26.11) in Billingsley (1995)) \\[\n\\psi_W(t)=1-\\frac{1}{2}\\sigma_W^2 \\, t^2 + \\lambda(t),\n\\tag{3.6}\\] where \\(|\\lambda(t)|\\leq |t^2|\\,\\mathbb{E}\\left(\\min(|t|\\,|W|^3, W^2)\\right).\\)\n\n\n\n\nThe following can be found in Example 3.1 in Shao and Tu (1996)\n\nIt follows from the Lindeberg-L√©vy CLT that \\[\nH_n(x)=P\\left(\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\\leq x\\right)\\to\\Phi_{\\sigma_0}(x)=\\Phi\\left(\\frac{x}{\\sigma_0}\\right)\\quad\\text{as}\\quad n\\to\\infty,\n\\] for all \\(x\\in\\mathbb{R}.\\) This result can be proven by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nTo see this, rewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)\n& = \\sum_{i=1}^n\\frac{X_i-\\mu_0}{\\sqrt{n}}\\\\[2ex]\n& = \\sum_{i=1}^n W_{i,n}\n\\end{align*}\n\\] where\n\n\\(W_{1,n},\\dots,W_{n,n}\\) are i.i.d. with\n\\(\\mathbb{E}(W_{i,n})=0\\) and\n\\(Var(W_{i,n})=\\frac{1}{n}\\sigma_0^2.\\)\n\nTherefore, by Equation¬†3.5 together with Equation¬†3.6 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n(t)|\n&\\leq |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,\\left|W_{1,n}\\right|^3, \\left|W_{1,n}\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,\\mathbb{E}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_1-\\mu_0\\right|^3, n^{-1}\\left|X_1-\\mu_0\\right|^2\\big)\\right).\n\\end{align*}\n\\] That is, \\[\nn|\\lambda_n(t)|\\to 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] which means that \\(|\\lambda_n(t)|\\to 0\\) faster than \\(n^{-1}\\) for any fixed \\(t\\) (and thus also for any \\(t\\) in the neighborhood around zero, which is all we need).\nThat is, for any fixed \\(t,\\) \\[\n\\begin{align*}\n\\lambda_n(t) & = o(n^{-1})\\quad (\\Leftrightarrow n|\\lambda_n(t)|\\to 0\\quad\\text{as}\\quad n\\to\\infty)\\\\[2ex]\n\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 & = O(n^{-1})\n\\end{align*}\n\\]\nThus, by Equation¬†3.4 \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}\\psi_{\\sqrt{n}\\left(\\bar{X}_n-\\mu_0\\right)}(t)\n&= \\lim_{n\\to\\infty}\\psi_{\\sum_{i=1}^n W_{i,n}}(t)\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2 + \\lambda_n(t)\\right)^n\\\\[2ex]\n&= \\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\,\\frac{1}{2}\\,\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t)\n\\end{align*}\n\\]\nOK, we have shown that \\(H_n\\) tends to \\(\\Phi_{\\sigma_0}\\) by showing that the characteristic function of \\(H_n\\) tends to that of \\(\\Phi_{\\sigma_0};\\) i.e.¬†we have shown the Lindeberg-L√©vy CLT.\nTo show bootstrap consistency we need to show that \\(H_n^{Boot}\\) also tends to \\(\\Phi_{\\sigma_0}.\\) To do so, we can mimic the above proof, by showing that the characteristic function of \\(H_n^{Boot}\\) tends to that of \\(\\Phi_{\\sigma_0}.\\)\nRewrite \\[\n\\begin{align*}\n\\sqrt{n}\\left(\\bar{X}^*_n- \\bar{X}_n\\right)|\\mathcal{S}_n\n& = \\sum_{i=1}^n\\frac{X^*_i- \\bar{X}_n}{\\sqrt{n}}|\\mathcal{S}_n\\\\[2ex]\n& = \\sum_{i=1}^n W^*_{i,n}|\\mathcal{S}_n\n\\end{align*}\n\\] where\n\n\\(W^*_{1,n}|\\mathcal{S}_n,\\dots,W^*_{n,n}|\\mathcal{S}_n\\) is i.i.d. with\n\\(\\mathbb{E}(W^*_{1,n}|\\mathcal{S}_n)=0\\) and\n\\(Var(W^*_{1,n}|\\mathcal{S}_n)=\\frac{1}{n}\\hat{\\sigma}_n^2=\\frac{1}{n}\\left(\\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right)\\)\n\nTherefore, by Equation¬†3.5 together with Equation¬†3.6 \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n&=\\psi_{\\sum_{i=1}^n W_{i,n}|\\mathcal{S}_n}(t)\\\\[2ex]\n&=\\left(\\psi_{W_{1,n}|\\mathcal{S}_n}(t)\\right)^n\\\\[2ex]\n&=\\left(1-\\frac{1}{2}\\,\\frac{1}{n}{\\color{darkgreen}\\hat{\\sigma}_n^2} \\, t^2 + {\\color{red}\\lambda_n^*(t)}\\right)^n,\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n|\\lambda_n^*(t)|\n&\\leq |t^2|\\,{\\color{blue}\\mathbb{E}^*}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X^*_1-\\bar{X}_n\\right|^3, n^{-1}\\left|X_1^* - \\bar{X}_n\\right|^2\\big)\\right)\\\\[2ex]\n&= |t^2|\\,{\\color{blue}\\frac{1}{n}\\sum_{i=1}^n}\\left(\\min\\big(|t|\\,n^{-3/2}\\left|X_i-\\bar{X}_n\\right|^3, n^{-1}\\left|X_i - \\bar{X}_n\\right|^2\\big)\\right).\n\\end{align*}\n\\] By the Marcinkiewicz strong law of large numbers, we obtain that \\[\nn{\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\quad\\text{as}\\quad n\\to\\infty,\n\\] i.e., \\({\\color{red}|\\lambda^*_n(t)|}\\to_{a.s.} 0\\) faster than \\(n^{-1}.\\)\nMoreover, \\[\n{\\color{darkgreen}\\hat\\sigma_n^2} = \\frac{1}{n}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\to_{a.s.}\\sigma_0^2\n\\]\nThus, we have that (using Equation¬†3.4) \\[\n\\begin{align*}\n\\psi_{\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)|\\mathcal{S}_n}(t)\n\\to_{a.s.}&\n\\lim_{n\\to\\infty}\\left(1-\\frac{1}{2}\\,\\frac{1}{n}\\sigma_0^2 \\, t^2\\right)^n\\\\[2ex]\n&=\\exp\\left(-\\frac{1}{2}\\sigma_0^2\\,t^2\\right)\\\\[2ex]\n&=\\psi_{\\Phi_{\\sigma_0}}(t).\n\\end{align*}\n\\] This implies that the limit (\\(n\\to\\infty\\)) of \\(H_n^{Boot}\\) is \\(\\Phi_{\\sigma_0}\\) almost surely.\nHence we have shown that the basic bootstrap is consistent for doing inference about \\(\\mu_0\\) using \\(\\bar{X}_n.\\)\n\nThis bootstrap consistency result justifies using the bootstrap distribution \\[\n\\begin{align*}\nH_n^{Boot}(x)\n&=P\\left(\\sqrt{n}\\left(\\bar{X}^*_n-\\bar{X}_n\\right)\\leq x|\\mathcal{S}_n\\right)\\\\[2ex]\n&\\approx\n\\frac{1}{m}\\sum_{j=1}^m 1_{\\left(\\sqrt{n}\\left(\\bar X^*_{n,j}-\\bar X_n\\right)\\leq x\\right)}=H_{n,m}^{Boot}(x),  \n\\end{align*}\n\\] of \\(\\bar{X}_n^\\ast\\) for doing inference about \\(\\mu_0.\\)\nIn the following section, we show how to build a confidence interval using the bootstrap distribution of an estimator \\(\\hat\\theta_n.\\)\n\n\n\n\n3.3.2 The Basic Bootstrap Confidence Interval\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\theta_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\n\\(\\theta_0\\in\\mathbb{R}\\) and\n\\(\\sqrt{n}(\\hat\\theta_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2)\\) as \\(n\\to\\infty,\\)\n\\(\\hat{v}_n\\to_{p} v_0\\) as \\(n\\to\\infty\\)\n\nAn approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval is then given by \\[\n\\left[\n\\hat{\\theta}_n - z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}},\n\\hat{\\theta}_n + z_{1-\\frac{\\alpha}{2}}\\frac{\\hat v_n}{\\sqrt{n}}\n\\right],\n\\] where \\(z_{1-\\frac{\\alpha}{2}}\\) denotes the \\((1-\\alpha)/2\\) quantile of the standard Normal distribution \\((z_{0.975}=1.96).\\) This confidence interval is approximate, since it is only asymptotically justified, and, thus, is generally not exact in finite samples.\n\n\nIn some cases it is, however, very difficult to obtain approximations \\(\\hat v_n\\) of \\(v_0\\) (see Section¬†3.1). Statistical inference is then usually based on the bootstrap confidence intervals.\nIn many situations it can be shown that bootstrap confidence intervals (or tests) are even more precise than asymptotic normality based confidence intervals. (This applies to the bootstrap-\\(t\\) method discussed in Section¬†3.4.)\n\nAlgorithm of the Basic Bootstrap Confidence Interval for \\(\\theta_0:\\)\nSetup:\n\nData: i.i.d. random sample \\[\n{\\cal S}_n:=\\{X_1,\\dots,X_n\\}\n\\] with \\(X_i\\overset{\\text{i.i.d.}}{\\sim} F\\) for all \\(i=1,\\dots,n.\\)\nThe parameter of interest \\(\\theta_0\\in\\mathbb{R}\\) is an parameter of \\(F\\).\n\\(\\hat{\\theta}_n\\) denotes the estimator of \\(\\theta_0\\in\\mathbb{R}.\\)\nProblem: Construct a confidence interval for \\(\\theta_0\\in\\mathbb{R}.\\)\n\n\n\n\n\n\n\nAssumption: Bootstrap is Consistent\n\n\n\nIn the following, we will assume that the bootstrap is consistent; i.e.¬†that \\[\n\\begin{align*}\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}^*_n -\\hat{\\theta}_n)|{\\cal S}_n)\n&\\approx\n\\text{distribution}(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0))\\\\\n\\text{short:}\\quad\\quad\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)|{\\cal S}_n\n&\\overset{d}{\\approx} \\sqrt{n}(\\hat{\\theta}_n -\\theta_0)\n\\end{align*}\n\\] if \\(n\\) is sufficiently large.\nCaution: This is not always the case and in cases of doubt one needs to show this property.\nGood to know: Theorem 1 in Mammen (1992) shows that the basic bootstrap is consistent if \\(\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\to_d\\mathcal{N}(0,v_0^2).\\)\n\n\nAlgorithm (3 Steps):\n\nGenerate \\(m\\) bootstrap estimates\n\\[\n\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\n\\] by repeatedly (\\(m\\) times) drawing bootstrap samples \\(X_{1}^*,\\dots,X_{n}^*\\) independently and with replacement from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) and computing \\(\\hat{\\theta}^\\ast_{n,j}\\equiv \\hat{\\theta}^\\ast_{j}(X_{1}^*,\\dots,X_{n}^*),\\) \\(j=1,\\dots,m.\\)\nUse the \\(m\\) bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*\\) to approximate the \\(\\frac{\\alpha}{2}\\) and the \\(1-\\frac{\\alpha}{2}\\) quantiles of the conditional distribution of \\(\\hat{\\theta}^*\\) given \\({\\cal S}_n.\\) This can be done with negligible approximation error (for \\(m\\) large) using the empirical quantiles \\[\n\\hat q^*_{n,p}=\\left\\{\n  \\begin{array}{ll}\n  \\hat\\theta^*_{n,(\\lfloor mp\\rfloor+1)},         & mp \\text{ not a whole number}\\\\\n  (\\hat\\theta^*_{n,(mp)}+\\hat\\theta^*_{n,(mp+1)})/2,& mp \\text{ a whole number}\n\\end{array}\\right.\n\\tag{3.7}\\] for \\(p=\\frac{\\alpha}{2}\\) or \\(p=1-\\frac{\\alpha}{2},\\) where \\(\\hat\\theta_{n,(j)}^*\\) denotes the \\(j\\)th order statistic \\[\n\\hat\\theta_{n,(1)}^* \\leq \\hat\\theta_{n,(2)}^*\\leq \\dots\\leq \\hat\\theta_{n,(m)}^*,\n\\] and \\(\\lfloor mp\\rfloor\\) denotes the greatest whole number less than or equal to \\(mp\\) (e.g.¬†\\(\\lfloor 4.9\\rfloor = 4\\)).\nThe approximate \\((1-\\alpha)\\times 100\\%\\) basic bootstrap confidence interval is then given by \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\tag{3.8}\\] where \\(\\hat{\\theta}_n\\) is computed from the original sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) and the empirical quantiles \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimates \\(\\hat\\theta_{n,1}^*,\\dots,\\hat\\theta_{n,m}^*.\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe quantiles \\(\\hat q^*_{n,p}\\) are those of the distribution \\[\nG_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(\\hat{\\theta}^*_{n,j}\\leq x\\right)}.\n\\] However, we‚Äôll treat the quantiles \\(\\hat q^*_{n,p}\\) as quantiles of the distribution \\[\nG_{n}^{Boot}(x)=P\\left(\\hat{\\theta}^*_{n}\\leq x\\,\\big|\\,\\mathcal{S}_n\\right),\n\\] since for large \\(m\\) (\\(m\\to\\infty\\)) the difference between \\(G_{n,m}^{Boot}\\) and \\(G_{n}^{Boot}\\) is negligible (Glivenko-Cantelli Theorem¬†3.1) and we can choose \\(m\\) to be large.\n\n\nJustifying the Basic Bootstrap CI (Equation¬†3.8) for \\(\\theta_0\\):\nThe following three approximate statements \\((\\approx (1-\\alpha))\\) are exact for \\(m\\to\\infty:\\) \\[\n\\begin{align*}\n&P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\leq \\hat{\\theta}^*_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n \\leq\\hat{\\theta}^*_n -\\hat{\\theta}_n \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P^*\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{red}\\sqrt{n}(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\]\nDue to the assumed consistency of the bootstrap, we have that for large \\(n\\) \\[\n{\\color{red}\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}|{\\cal S}_n\\overset{d}{\\approx} {\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}.\n\\] Therefore, for large \\(n\\) and large \\(m,\\) \\[\n\\begin{align*}\n&P\\left(\n\\sqrt{n}(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\leq{\\color{blue}\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)}\\leq \\sqrt{n}(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\leq\\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}-2\\hat{\\theta}_n\\leq-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}-2\\hat{\\theta}_n\\right)\n\\approx 1-\\alpha\\\\[2ex]\n%\\Rightarrow &P\\left(\\hat{\\theta}_n-(\\hat q^*_{n,1-\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\le \\theta_0\\le \\hat{\\theta}_n-(\\hat q^*_{n,\\frac{\\alpha}{2}}-\\hat{\\theta}_n)\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow &P\\left(2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\le \\theta_0\\le 2\\hat{\\theta}_n-\n\\hat q^*_{n,\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha.\n\\end{align*}\n\\] This demonstrates that the basic bootstrap confidence interval in Equation¬†3.8 \\[\n\\left[2\\hat{\\theta}_n-\\hat q^*_{n,1-\\frac{\\alpha}{2}}, 2\\hat{\\theta}_n-\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] is indeed an asymptotically valid (i.e.¬†approximate) \\((1-\\alpha)\\times 100\\%\\) confidence interval.\n\n\nExample: Basic Bootstrap Confidence Interval for the Population Mean\nSetup:\n\nData: Let \\(X_1,\\dots,X_n\\) denote an i.i.d. random sample from \\(X\\sim F\\) with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0.\\)\nEstimator: \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of \\(\\mu_0.\\)\nInference Problem: Construct a confidence interval for \\(\\mu_0.\\)\n\n\n\n\n\n\n\nRecall: Confidence Interval for \\(\\mu_0\\) using Classic Asymptotic Statistics\n\n\n\nSetup:\n\nBy the CLT: \\(\\sqrt{n}(\\bar X_n - \\mu_0)\\to_d\\mathcal{N}(0,\\sigma^2_0)\\) as \\(n\\to\\infty\\)\nEstimation of \\(\\sigma^2_0\\): \\(\\hat{\\sigma}^2_n=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X_n)^2,\\) where \\(\\hat{\\sigma}^2_n\\to_p\\sigma^2_0\\) as \\(n\\to\\infty.\\)\nThis implies: \\(\\sqrt{n}((\\bar X_n -\\mu_0)/\\hat{\\sigma}_n)\\to_d\\mathcal{N}(0,1)\\) as \\(n\\to\\infty\\)\n\nLet \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) denote the \\(\\alpha/2\\) and the \\((1-\\alpha/2)\\)-quantile of \\(\\mathcal{N}(0,1).\\) Since \\(z_{\\alpha/2} = -z_{1-\\alpha/2},\\) we have that \\[\n\\begin{align*}\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\le \\frac{\\sqrt{n}(\\bar X_n -\\mu_0)}{\\hat{\\sigma}_n}\\le z_{1-\\frac{\\alpha}{2}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(-z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\le \\bar X_n -\\mu_0\\le z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\right)\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow\\quad\n&P\\left(\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\le \\mu_0\\le\n        \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\n  \\right)\\approx 1-\\alpha\n\\end{align*}\n\\]\n\nApproximate \\((1-\\alpha)\\times 100\\%\\) confidence interval: \\[\n\\left[\\bar X_n -z_{1-\\frac{\\alpha}{2}}\\left(\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\right),\n    \\bar X_n +z_{1-\\frac{\\alpha}{2}}\\left(\\frac{\\hat{\\sigma}_n}{\\sqrt{n}}\\right)\\right]\n\\]\n\n\n\nAlgorithm of the basic bootstrap confidence interval for \\(\\mu_0\\):\nThe bootstrap offers an alternative method for constructing approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals. We already know that the bootstrap is consistent in this situation (see Section¬†3.3.1.3).\n\nDraw \\(m\\) bootstrap samples (e.g.¬†\\(m=10,000\\)) and calculate the corresponding estimates \\[\n\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\n\\]\nCompute the empirical quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\(\\bar X^*_{n,1},\\bar X^*_{n,2},\\dots,\\bar X^*_{n,m}.\\)\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) basic bootstrap confidence interval according to Equation¬†3.8: \\[\n\\left[2\\bar X_n -\\hat q^*_{n,1-\\frac{\\alpha}{2}},\n   2\\bar X_n -\\hat q^*_{n,\\frac{\\alpha}{2}}\\right],\n\\] where \\(\\bar{X}_n\\) is computed from the original sample \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) and the empirical quantiles \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimates \\(\\bar{X}_{n,1}^*,\\dots,\\bar{X}_{n,m}^*.\\)"
  },
  {
    "objectID": "Ch3_Bootstrap.html#sec-BootT",
    "href": "Ch3_Bootstrap.html#sec-BootT",
    "title": "3¬† The Bootstrap",
    "section": "3.4 The Bootstrap-\\(t\\) Method",
    "text": "3.4 The Bootstrap-\\(t\\) Method\nThe basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e.¬†parametric) assumption.\nIn many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-\\(t\\) method (one also speaks of the ‚Äústudentized bootstrap‚Äù), which is also a nonparametric bootstrap method. The construction relies on so-called (asymptotically) pivotal statistics.\nLet \\(X_1,\\dots,X_n\\) be an i.i.d. random sample and assume that the distribution of \\(X\\) depends on an unknown parameter (or parameter vector) \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.5 ((Asymptotically) Pivotal Statistics) \nA statistic (i.e.¬†a function of the random sample) \\[\nT_n\\equiv T(X_1,\\dots,X_n)\n\\] is called exact pivotal, if the distribution of \\(T_n\\) does not depend on any unknown population parameter.\nA statistic \\(T_n\\) is called asymptotically pivotal, if the asymptotic distribution of \\(T_n\\) does not depend on any unknown population parameter.\n\n\n\nExact pivotal statistics are rare and not available in most statistical or econometric applications.\nIt is, however, often possible to construct an asymptotically pivotal statistic. Consider, for instance, an asymptotically normal \\(\\sqrt{n}\\)-consistent estimator \\(\\hat{\\theta}_n\\) of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}(\\hat{\\theta}_n-\\theta_0)\\rightarrow_d\\mathcal{N}(0,v_0^2),\n\\] where \\(v^2\\) denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator of \\(v_0^2\\) \\[\n\\hat v_n^2 \\rightarrow_p v_0^2\\quad\\text{as}\\quad n\\to\\infty,\n\\] which implies (Continuous Mapping Theorem) that also \\[\n\\hat v_n \\rightarrow_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\] Then, \\[\nT_n= \\sqrt{n}\\frac{(\\hat{\\theta}-\\theta)}{\\hat v_n}\n\\] is asymptotically pivotal, since \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\\rightarrow_d \\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\n\nExample: \\(\\bar{X}_n\\)\nLet \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\}\\) be a i.i.d. random sample with \\(X_i\\sim X\\) for all \\(i=1,\\dots,n,\\) with mean \\(\\mathbb{E}(X)=\\mu_0\\), variance \\(0<Var(X)=\\sigma_0^2<\\infty\\), and \\(\\mathbb{E}(|X|^4)=\\beta<\\infty\\).\n\nIf \\(X\\) is normally distributed, we obtain \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\sim t_{n-1}\\quad\\text{for any}\\quad n=2,3,\\dots\n\\] with \\(s_n^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X_n)^2\\), where \\(t_{n-1}\\) denotes the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. We can conclude that \\(T_n\\) is exact pivotal.\nIf \\(X\\) is not normally distributed, the central limit theorem implies that \\[\nT_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu_0)}{s_n}\\rightarrow_d\\mathcal{N}(0,1),\\quad\\text{as}\\quad n\\to\\infty.\n\\] In this case \\(T_n\\) is an asymptotically pivotal statistic.\n\n\n\nBootstrap-\\(t\\) Consistency\nThe general idea of the bootstrap-\\(t\\) method relies on approximating the unknown distribution of \\[\nT_n = \\sqrt{n}\\frac{(\\hat{\\theta}_n-\\theta_0)}{\\hat v_n}\n\\] by the approximable (via bootstrap resampling) conditional distribution of \\[\nT_n^*\\big|\\mathcal{S}_n =\\sqrt{n}\\frac{(\\hat{\\theta}_n^*-\\hat{\\theta}_n)}{\\hat v_n^*}\\Big|\\mathcal{S}_n,\n\\] given \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) where the standard deviation estimate \\(\\hat{v}_n^*\\) is computed from the bootstrap sample \\(X_1^*,\\dots,X_n^*,\\) i.e. \\[\n\\hat v_n^*\\equiv \\hat{v}(X_1^*,\\dots,X_n^*).\n\\]\n\n\n\n\n\n\nGood news: Bootstrap-\\(t\\) consistency follows if the basic bootstrap is consistent\n\n\n\nIf the basic bootstrap is consistent and if the variance estimator \\(\\hat{v}_n^2\\) is consistent, then also the bootstrap-\\(t\\) method is consistent.\n\n\n\n\n\n3.4.1 The Bootstrap-\\(t\\) Confidence Interval\nSetup:\n\nLet \\({\\cal S}_n:=\\{X_1,\\dots,X_n\\}\\) be an i.i.d. random sample from \\(X\\sim F\\) with unknown parameter of interest \\(\\theta_0\\in\\mathbb{R}.\\)\nLet \\(\\hat{\\theta}_n\\) be a \\(\\sqrt{n}\\)-consistent, asymptotically normal estimator of \\(\\theta_0,\\) i.e. \\[\n\\sqrt{n}\\left(\\hat{\\theta}_n - \\theta_0\\right)\\to_d\\mathcal{N}(0,v_0^2)\\quad\\text{as}\\quad n\\to\\infty\n\\]\nAssume that the bootstrap is consistent.\nLet \\(\\hat{v}_n^2\\) denote a consistent estimator of the asymptotic variance \\(v_0^2\\) of \\(\\hat{\\theta}_n,\\) i.e.¬† \\[\n\\hat v^2_n\\equiv \\hat v^2(X_1,\\dots,X_n)\n\\] such that \\[\n\\begin{align*}\n\\hat v^2_n &\\to_p v_0^2\\quad\\text{as}\\quad n\\to\\infty\n\\end{align*}\n\\] and \\[\n\\begin{align*}\n\\hat v_n   &\\to_p v_0\\quad\\text{as}\\quad n\\to\\infty.\n\\end{align*}\n\\]\n\n\nAlgorithm of the Bootstrap-\\(t\\) Confidence Interval for \\(\\theta_0\\):\nAlgorithm (3 Steps):\n\nBased on an i.i.d. re-sample \\(X_1^*,\\dots,X_n^*\\) from \\(\\mathcal{S}_n=\\{X_1,\\dots,X_n\\},\\) calculate the bootstrap estimates \\[\n\\hat{\\theta}^*_n\\equiv \\hat{\\theta}^*(X_1^*,\\dots,X_n^*)\n\\] and \\[\n\\hat v^*_n\\equiv \\hat v^*(X_1^*,\\dots,X_n^*)\n\\] and the bootstrap statistic \\[\n\\begin{align*}\nT_n^*&=\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}.\n\\end{align*}\n\\] Repeating this yields \\(m\\) (e.g.¬†\\(m=100,000\\)) many bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\n\\]\nCompute the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\\) (see Equation¬†3.7).\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval\n\\[\n\\left[\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n   \\hat{\\theta}_n - \\hat q^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\tag{3.9}\\] where \\(\\hat\\theta_n\\) and \\(\\hat v_n\\) are the estimates of \\(\\theta_0\\) and \\(v_0\\) based on the original sample \\(\\mathcal{S}_n=\\left\\{X_1,\\dots,X_n\\right\\},\\) and where the empirical quantiles \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimates \\(T_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*.\\)\n\nJustifying the Bootstrap-\\(t\\) CI (Equation¬†3.9) for \\(\\theta_0\\):\nThe bootstrap estimates \\[\nT_{n,1}^*,T_{n,2}^*, \\dots, T_{n,m}^*\n\\] yield the empirical bootstrap distribution \\[\nH_{n,m}^{Boot}(x)=\\frac{1}{m}\\sum_{j=1}^m1_{\\left(T_{n,j}^*\\;\\leq\\; x\\right)}\n\\] which approximates the bootstrap distribution \\[\nH_{n}^{Boot}(x)=P\\left(\\left.T_{n}^*\\leq x\\;\\right|\\;\\mathcal{S}_n\\right)\n\\] arbitrarily precise as \\(m\\to\\infty\\) (Glivenko-Cantelli Theorem¬†3.1).\nThus, the empirical bootstrap quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) of \\(H_{n,m}^{Boot}\\) are indeed consistent (\\(m\\to\\infty\\)) for the quantiles \\(\\hat q_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q_{n,1-\\frac{\\alpha}{2}}\\) of the bootstrap distribution \\(H_{n}^{Boot}.\\) This implies, for large \\(m,\\) \\[\nP^*\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*_n-\\hat{\\theta}_n}{\\hat v^*_n}} \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha.\n\\]\nMoreover, due to the assumed consistencies of the bootstrap and of the estimator \\(\\hat v_n,\\) we have that for large \\(n\\) that \\[\n\\left.{\\color{red}\\sqrt{n}\\frac{\\hat{\\theta}^*-\\hat{\\theta}}{\\hat v_n^*}}\\right|\\mathcal{S}_n\\overset{d}{\\approx}\n{\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}}.\n\\] Therefore, for large \\(n\\) and large \\(m,\\) \\[\n\\begin{align*}\n& P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}}\\leq {\\color{blue}\\sqrt{n}\\frac{\\hat{\\theta}_n-\\theta_0}{v_0}} \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat q^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq  \\hat{\\theta}_n-\\theta_0 \\leq \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(- \\hat{\\theta}_n + \\hat q^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\leq -\\theta_0 \\leq - \\hat{\\theta}_n + \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right)\\right)\n\\approx 1-\\alpha\\\\[2ex]\n\\Rightarrow & P\\left(\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}}\\left(\\frac{v_0}{\\sqrt{n}}\\right)\\leq \\theta_0 \\leq \\hat{\\theta}_n - \\hat q^*_{n,\\frac{\\alpha}{2}} \\left(\\frac{v_0}{\\sqrt{n}}\\right) \\right)\n\\approx 1-\\alpha\n\\end{align*}\n\\] Thus, the approximate \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval (Equation¬†3.9) \\[\n\\left[\\hat{\\theta}_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right),\\;\n      \\hat{\\theta}_n - \\hat q^*_{n,  \\frac{\\alpha}{2}}   \\left(\\frac{\\hat v_n}{\\sqrt{n}}\\right)\\right],\n\\] is indeed an asymptotic (i.e.¬†approximate) \\((1-\\alpha)\\times 100\\%\\) CI.\n\n\nExample: Bootstrap-\\(t\\) Confidence Interval for the Mean\nHere \\(\\hat\\theta_n = \\bar{X}_n\\) and the estimator of the asymptotic variance of \\(\\bar{X}_n\\) is \\(s^2\\approx \\lim_{n\\to\\infty}n Var(\\bar{X}_n)=\\sigma_0^2\\), where \\(s^2\\) denotes the sample variance \\[\ns_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2.\n\\]\nAlgorithm:\n\nDraw i.i.d. random samples \\(X_1^*,\\dots,X_n^*\\) from \\({\\cal S}_n\\) and calculate \\(\\bar X^*\\) as well as \\(s_n^*=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i^*-\\bar X^*_n)^2}\\) to generate \\(m\\) (e.g.¬†\\(m=100,000\\)) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\]\nDetermine \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) from \\[\n\\sqrt{n}\\frac{\\bar X^*_{n,1}-\\bar X_n}{s^*_{n,1}},\\dots,\\sqrt{n}\\frac{\\bar X^*_{n,m}-\\bar X_n}{s^*_{n,m}}\n\\] using Equation¬†3.7.\nThis yields the \\((1-\\alpha)\\times 100 \\%\\) confidence interval (using Equation¬†3.9): \\[\n\\left[\\bar X_n - \\hat q^*_{n,1-\\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right),\n    \\bar X_n - \\hat q^*_{n,  \\frac{\\alpha}{2}} \\left(\\frac{s_n}{\\sqrt{n}}\\right)\\right],\n\\] where \\(\\bar X_n\\) and \\(s_n\\) are computed from the original sample, i.e., \\[\ns_n=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}\\right)^2},  \n\\] and where the empirical quantiles \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap estimates \\(\\sqrt{n}\\frac{\\bar X^*_{n,j}-\\bar X_n}{s^*_{n,j}},\\) \\(j=1,\\dots,m.\\)\n\n\n\n\n3.4.2 Accuracy of the Bootstrap-\\(t\\) method\nUsually, the bootstrap-\\(t\\) provides a gain in accuracy over the basic bootstrap. The reason is that the approximation of the law of \\(T_n\\) by the bootstrap law of \\[\n\\left.\\frac{\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)}{\\hat{v}^*_n}\\;\\right|\\;\\mathcal{S}_n\n\\] is more direct and hence more accurate (\\(\\hat{v}^*_n\\) depends also on the bootstrap sample‚Äînot on the original sample) than by the bootstrap law of \\[\n\\left.\\sqrt{n}(\\hat{\\theta}^*_n-\\hat{\\theta}_n)\\;\\right|\\;\\mathcal{S}_n.\n\\]\nThe use of pivotal statistics and the corresponding construction of bootstrap-\\(t\\) confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-\\(t\\) methods are second order accurate.\nConsider generally \\((1-\\alpha)\\times 100\\%\\) confidence intervals of the form \\[\n[L_n,U_n]\n\\] of \\(\\theta\\). The lower, \\(L_n\\), and upper bounds, \\(U_n\\), of such intervals are determined from the data and are thus random, \\[\nL_n\\equiv L(X_1,\\dots,X_n)\n\\] \\[\nU_n\\equiv U(X_1,\\dots,X_n)\n\\] and their accuracy depends on the particular procedure applied (e.g.¬†basic bootstrap vs.¬†bootstrap-\\(t\\)).\n\nTwo-sided \\((1-\\alpha)\\cdot 100\\%\\) confidence intervals \\([L_n,U_n]\\) are said to be first-order accurate if there exist some constant \\(c<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta_0\\not\\in [L_n,U_n])-\\alpha\\right|\\le \\frac{c}{\\sqrt{n}}\n\\end{align*}\n\\]\nTwo-sided \\((1-\\alpha)\\cdot 100\\%\\) confidence intervals \\([L_n,U_n]\\) are said to be second-order accurate if there exist some constant \\(c<\\infty\\) such that for sufficiently large \\(n\\) \\[\n\\begin{align*}\n\\left|P(\\theta_0\\not\\in[L_n,U_n])-\\alpha\\right|\\le \\frac{c}{n}\n\\end{align*}\n\\]\n\nIf the distribution of \\(\\hat\\theta_n\\) is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that\n\nStandard confidence intervals based on asymptotic normality approximations are first-order accurate.\nBasic bootstrap confidence intervals are first-order accurate.\nBootstrap-\\(t\\) confidence intervals are second-order accurate.\n\nThe difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to much better approximations.\n\n\n\n\n\n\nNote\n\n\n\nProofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field (see, for instance, Koike (2024))."
  },
  {
    "objectID": "Ch2_EMAlgorithmus.html#solutions",
    "href": "Ch2_EMAlgorithmus.html#solutions",
    "title": "2¬† EM Algorithm & Cluster Analysis",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\n\n(a) Likelihood function\nThe probability mass function of \\(X\\sim\\text{Bernoulli}(p)\\) is \\[\nf(x)=p^{x}(1-p)^{1-x},\\quad\\text{with}\\quad x\\in\\{0,1\\}.\n\\] Thus the likelihood and the log-likelihood functions are \\[\n\\begin{align*}\n\\mathcal{L}(p;\\mathbf{x})    & = \\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}\\\\\n\\mathcal{\\ell}(p;\\mathbf{x}) & = \\sum_{i=1}^n \\ln\\left(p^{x_i}(1-p)^{1-x_i}\\right),\n\\end{align*}\n\\] where \\(\\mathbf{x}=(x_1,\\dots,x_n)\\) denotes the vector of observed data points.\n\n\n(b) Likelihood function for a Bernoulli mixture distribution\nThe probability mass function of a Bernoulli mixture distribution is \\[\n\\begin{align*}\nf_G(x)\n& =\\sum_{g=1}^G \\pi_g\\; f_g(x)\\\\[2ex]\n& =\\sum_{g=1}^G \\pi_g\\; p_g^{x}(1-p_g)^{1-x}\\quad\\text{with}\\quad x\\in\\{0,1\\}.\n\\end{align*}\n\\] Thus the likelihood function is \\[\n\\begin{align*}\n\\mathcal{L}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x})    & = \\prod_{i=1}^n\\left(\\sum_{g=1}^G \\pi_g\\; p_g^{x_i}(1-p_g)^{1-x_i}\\right)\\\\[2ex]\n\\ell(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x}) & = \\sum_{i=1}^n\\ln\\left(\\sum_{g=1}^G\\pi_g\\; p_g^{x_i}(1-p_g)^{1-x_i}\\right)\n\\end{align*}\n\\] where \\(\\mathbf{p}=(p_1,\\dots,p_G)\\) and \\(\\boldsymbol{\\pi}=(\\pi_1,\\dots,\\pi_G)\\).\n\n\n(c) Likelihood function with group indicator random variables\n\\[\n\\begin{align*}\n\\tilde{\\mathcal{L}}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x}, \\mathbf{Z})\n& = \\prod_{i=1}^n \\prod_{g=1}^G\\left(\\pi_g p_g^{x_i}(1-p_g)^{1-x_i}\\right)^{Z_{ig}}\\\\[2ex]\n\\tilde{\\ell}(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x}, \\mathbf{Z})    \n& = \\sum_{i=1}^n \\sum_{g=1}^G Z_{ig}\\left(\\ln(\\pi_g) + \\ln\\left(p_g^{x_i}(1-p_g)^{1-x_i}\\right)\\right)\n\\end{align*}\n\\]\n\n\n(d) Posterior probability\n\\[\n\\begin{align*}\n\\mathfrak{p}_{ig} = P(Z_{ig}=1 | X_i = x_i)\n&=\\frac{ P(Z_{ig}=1) f_{X|Z}(x_i|Z_{ig}=1)}{f_G(x_i)} \\\\[2ex]\n&=\\frac{ P(Z_{ig}=1) f_g(x_i)}{f_G(x_i)} \\\\[2ex]\n&=\\frac{\\pi_g\\; p_g^{x_i} (1-p_g)^{1-x_i}}{\\sum_{g=1}^G \\pi_g\\; p_g^{x_i} (1-p_g)^{1-x_i}}\n\\end{align*}\n\\]\n\n\n(e) Conditional Expectation of \\(\\tilde\\ell\\)\n\\[\n\\begin{align*}\n&\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi};\\mathbf{x},\\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\sum_{g=1}^G \\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(Z_{ig}\\big|X_i=x_i\\right)\\left(\\ln(\\pi_g) + \\ln\\left(p_g^{x_i}(1-p_g)^{1-x_i}\\right)\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\left(\\ln(\\pi_g) + \\ln\\left(p_g^{x_i}(1-p_g)^{1-x_i}\\right)\\right),\n\\end{align*}\n\\] since \\[\n\\begin{align*}\n&\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(Z_{ig}\\big|\\mathbf{X}=\\mathbf{x}\\right)\\\\[2ex]\n&= \\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(Z_{ig}\\big|X_i=x_i\\right)\\\\[2ex]\n&= P(Z_{ig}=1 | X_i = x_i)\n= \\mathfrak{p}_{ig}.\n\\end{align*}\n\\]\n\n\n(f) Maximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x}, \\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\) with respect to \\(p_g:\\)\n\\[\n\\begin{align*}\n&\\frac{\\partial}{\\partial p_g}\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x}, \\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\\\[2ex]\n& = \\frac{\\partial}{\\partial p_g}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\left(\\ln(\\pi_g) + \\ln\\left(p_g^{x_i}(1-p_g)^{1-x_i}\\right)\\right)\\\\[2ex]\n& = \\frac{\\partial}{\\partial p_g}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\left(\\ln(\\pi_g) + \\left(x_i\\ln(p_g) + (1-x_i)\\ln(1-p_g)\\right)\\right)\\\\[2ex]\n%& = \\sum_{i=1}^n \\mathfrak{p}_{ig} \\frac{\\partial}{\\partial p_g} \\left(x_i\\ln(p_g) + (1-x_i)\\ln(1-p_g)\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\mathfrak{p}_{ig} \\left(\\frac{x_i}{p_g} - \\frac{(1-x_i)}{(1-p_g)}\\right)\\\\[2ex]\n\\end{align*}\n\\] First order condition \\[\n\\begin{align*}\n\\sum_{i=1}^n \\mathfrak{p}_{ig} \\left(\\frac{x_i}{\\hat{p}_g} - \\frac{(1-x_i)}{(1-\\hat{p}_g)}\\right)&\\overset{!}{=}0\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig} \\left(\\frac{x_i(1-\\hat{p}_g)}{\\hat{p}_g(1-\\hat{p}_g)} - \\frac{(1-x_i)\\hat{p}_g}{\\hat{p}_g(1-\\hat{p}_g)}\\right)&=0\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig} \\left(x_i(1-\\hat{p}_g) - (1-x_i)\\hat{p}_g\\right)&=0\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig} \\left(x_i - \\hat{p}_g \\right)&=0\\\\[2ex]\n\\hat{p}_g\\sum_{i=1}^n \\mathfrak{p}_{ig} &= \\sum_{i=1}^n \\mathfrak{p}_{ig}x_i \\\\[2ex]\n\\hat{p}_g & = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}x_i}{\\sum_{i=1}^n \\mathfrak{p}_{ig}}\n\\end{align*}\n\\]\n\n\n(g) Maximize \\(\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x}, \\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\) with respect to \\(\\pi_g:\\)\nNote: We only need to focus on the \\(\\pi_g\\) terms since the other terms in\n\\[\n\\begin{align*}\n&\\mathbb{E}_{\\mathbf{p},\\boldsymbol{\\pi}}\\left(\\tilde\\ell(\\mathbf{p},\\boldsymbol{\\pi}|\\mathbf{x}, \\mathbf{Z})\\big|\\mathbf{X}=\\mathbf{x}\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\left(\\ln(\\pi_g) + \\ln\\left(p_g^{x_i}(1-p_g)^{1-x_i}\\right)\\right)\n\\end{align*}\n\\] are no functions of \\(\\pi_g\\).\nHowever, we need to do maximization under the side constraint that \\(\\sum_{g=1}^G\\pi_g=1.\\) So, we use the method of Lagrange multipliers.\nThe Lagrange function is given by \\[\n\\begin{align*}\nL(\\boldsymbol{\\pi},\\lambda)=\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\ln(\\pi_g) - \\lambda \\left(\\sum_{g=1}^G\\pi_g-1\\right).\n\\end{align*}\n\\] First order condition with respect to \\(\\pi_g\\): \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\pi_g}L(\\boldsymbol{\\pi},\\lambda)=\n\\sum_{i=1}^n \\mathfrak{p}_{ig}\\frac{1}{\\pi_g}  - \\lambda &\\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig}\\frac{1}{\\hat{\\pi}_g}  - \\lambda  &\\overset{!}{=}0 \\\\[2ex]\n\\sum_{i=1}^n \\mathfrak{p}_{ig}   &=\\lambda \\hat{\\pi}_g\\\\[2ex]\n\\hat{\\pi}_g & = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\lambda}  \\\\[2ex]\n\\end{align*}\n\\] Plugging \\(\\hat{\\pi}_g = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\lambda}\\) into \\(L(\\boldsymbol{\\pi},\\lambda)\\): \\[\n\\begin{align*}\nL(\\lambda)&=\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\ln\\left(\\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\lambda}\\right) - \\lambda \\left(\\sum_{g=1}^G\\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\lambda}-1\\right) \\\\[2ex]\n&=\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\left(\\ln\\left(\\sum_{i=1}^n \\mathfrak{p}_{ig}\\right) - \\ln\\left(\\lambda\\right)\\right) -  \\left(\\sum_{g=1}^G\\sum_{i=1}^n \\mathfrak{p}_{ig}-\\lambda \\right) \\\\[2ex]\n\\end{align*}\n\\] First order condition with respect to \\(\\lambda\\): \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} L(\\lambda)=\n- \\frac{1}{\\lambda}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} +1 &\\\\[2ex]\n- \\frac{1}{\\hat\\lambda}\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} +1 & \\overset{!}{=}0\\\\[2ex]\n\\hat\\lambda & = \\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig} \\\\[2ex]\n\\end{align*}\n\\] So, we have that \\[\n\\begin{align*}\n\\hat{\\pi}_g\n& = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\hat{\\lambda}}\\\\[2ex]\n& = \\frac{\\sum_{i=1}^n \\mathfrak{p}_{ig}}{\\sum_{i=1}^n \\sum_{g=1}^G \\mathfrak{p}_{ig}}\n\\end{align*}\n\\]\n\n\n(h) Sketch of the EM-Algorithm\n\nInitialization \\[\n\\hat{p}_1^{(0)},\\dots,\\hat{p}_G^{(0)}\n\\] \\[\n\\hat{\\pi}_1^{(0)},\\dots,\\hat{\\pi}_G^{(0)}\n\\] For instance: \\(\\hat{p}_g^{(0)}=0.5\\) and \\(\\hat{\\pi}_g^{(0)}=\\frac{1}{G}\\) for all \\(g=1,\\dots,G.\\)\nFor \\(r=1,2,\\dots\\)\n\nExpectation-Step: \\[\n\\begin{align*}\n\\hat{\\mathfrak{p}}_{ig}^{(r-1)}\n&=\\frac{\\hat{\\pi}_g^{(r-1)}\\; \\left(\\hat{p}_g^{(r-1)}\\right)^{x_i} \\left(1-\\hat{p}_g^{(r-1)}\\right)^{1-x_i}}{\\sum_{g=1}^G \\hat{\\pi}_g^{(r-1)}\\; \\left(\\hat{p}_g^{(r-1)}\\right)^{x_i} \\left(1-\\hat{p}_g^{(r-1)}\\right)^{1-x_i}}\n\\end{align*}\n\\]\nMaximization-Step: \\[\n\\begin{align*}\n\\hat{p}_g^{(r)} & = \\frac{\\sum_{i=1}^n \\hat{\\mathfrak{p}}_{ig}^{(r-1)}x_i}{\\sum_{i=1}^n \\hat{\\mathfrak{p}}_{ig}^{(r-1)}}\\\\[2ex]\n\\hat{\\pi}_g^{(r)} & = \\frac{\\sum_{i=1}^n \\hat{\\mathfrak{p}}_{ig}^{(r-1)}}{\\sum_{i=1}^n \\sum_{g=1}^G \\hat{\\mathfrak{p}}_{ig}^{(r-1)}}  \\\\[2ex]\n\\end{align*}\n\\]\n\nCheck convergence (no relevant change of the maximized log-likelihood function)."
  },
  {
    "objectID": "Ch4_NPRegression.html",
    "href": "Ch4_NPRegression.html",
    "title": "4¬† Nonparametric Regression",
    "section": "",
    "text": "Let us consider the case of univariate nonparametric regression, i.e., with one single explanatory variable \\(X\\in\\mathbb{R}\\).\nData: \\[\n(Y_{1},X_{1}),\\dots,(Y_{n},X_{n})\\overset{\\text{i.i.d}}{\\sim}(Y,X)\n\\]\n\n\\(Y_{i}\\in \\mathbb{R}\\) real response variable\n\\(X_{i}\\in [a,b]\\subset \\mathbb{R}\\) real explanatory variable\n\\(n\\) sufficiently large sample size (e.g., \\(n\\geq 40\\))\n\n\n\n\\[\nY_i=m(X_i)+\\varepsilon_i\n\\]\n\n\\(m(X)=\\mathbb{E}(Y_i|X=X_i)\\) regression function\n\\(\\mathbb{E}(\\varepsilon_i)=0\\)\n\\(Var(\\varepsilon_i|X_i) = Var(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) and \\(X_i\\) are independent or at least mean-independent, i.e., \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\)\n\nSpecial cases of parametric regression models:\n\nLinear regression: \\(m\\) is a straight line \\[\nm(X)=\\beta_0+\\beta_1 X\n\\]\nPolynomial generalizations: \\(m\\) is a quadratic or cubic polynomial \\[\n\\begin{align*}\n              m(X)&=\\beta_0 +\\beta_1 X+\\beta_2 X^2\\\\\n\\text{or} \\quad m(X)&=\\beta_0+\\beta_1 X+\\beta_2 X^2+\\beta_3 X^3\n\\end{align*}\n\\]\n\nMany important applications lead to regression functions possessing a complicated structure. Standard models then are ‚Äútoo simple‚Äù and do not provide useful approximations of \\(m(x)\\).\n\n\n\n\n\n\nAs George Box is saying it:\n\n\n\n‚ÄúAll models are false, but some are useful‚Äù (G. Box)\n\n\nAn important point in theoretical analysis is the way how the observations \\(X_1,\\dots,X_n\\) have been generated. One distinguishes between fixed and random design.\n\nFixed design: The observation points \\(X_1,\\dots,X_n\\) are fixed (non stochastic) values.\n\nExample: Crop yield (\\(Y\\)) in dependence of the amount of fertilizer (\\(X\\)) used, when the amount is determined deterministically by the experimenter.\nEquidistant Design: (Most important special case of fixed design)\n\\[\nX_{i+1}-X_i=\\frac{b-a}{n}.\n\\]\n\nRandom design: The observation points \\(X_1,\\dots,X_n\\) are (realizations of) i.i.d. random variables with density \\(f\\). The density \\(f\\) is called ‚Äúdesign density‚Äù. Throughout this chapter it will be assumed that \\(f(x)>0\\) for all \\(x\\in [a,b]\\).\n\nExample: Sample \\((Y_1,X_1),\\dots,(Y_n,X_n)\\) of log-wages (\\(Y\\)) and age (\\(X\\)) of randomly selected individuals.\n\n\nIn the case of random design, \\(m(x)\\) is the conditional expectation of \\(Y\\) given \\(X=x\\), \\[\nm(x)=\\mathbb{E}(Y|\\ X=x)\n\\]\nand \\(Var(\\varepsilon_i|X_i)=\\sigma^2\\).\n\n\n\n\n\n\nNote\n\n\n\nFor random design all expectations (as well as variances) have to be interpreted as conditional expectations (variances) given \\(X_1,\\dots,X_n\\).\n\n\nExample: Canadian cross-section wage data consisting of a random sample taken from the 1971 Canadian Census Public Use Tapes for male individuals having common education (grade 13); see Figure¬†4.1.\n\nsuppressPackageStartupMessages(library(\"np\"))\ndata(\"cps71\")\nplot(cps71$age, cps71$logwage, xlab=\"Age\", ylab=\"log(wage)\")\n\n\n\n\nFigure¬†4.1: Canadian cross-section wage data.\n\n\n\n\nWhat would be a good/reasonable model assumption for \\(m(x)\\) to estimate the conditional mean function for the data shown in Figure¬†4.1?\n\n\n\n\n\n\nNo specific model assumption\n\n\n\nIn nonparametric regression analysis, we do not make assumptions about the specific structure of the regression function \\(m(x).\\) We only make the qualitative assumption that \\(m\\) is a sufficiently smooth function, i.e.¬†that \\(m(t)\\) is sufficiently often differentiable at every \\(t\\in(a,b)\\)."
  },
  {
    "objectID": "Ch4_NPRegression.html#basis-function-expansions",
    "href": "Ch4_NPRegression.html#basis-function-expansions",
    "title": "4¬† Nonparametric Regression",
    "section": "4.2 Basis Function Expansions",
    "text": "4.2 Basis Function Expansions\nSome frequently used approaches to nonparametric regression rely on expansions of the form \\[\nm(x)\\approx \\sum_{j=1}^p \\beta_j b_j(x),\n\\] where \\(b_1(x),b_2(x),\\dots\\) are suitable basis functions \\[\nb_j:\\mathbb{R}\\to\\mathbb{R},\\quad j=1,\\dots,p.\n\\]\nThe basis functions \\(b_1,b_2,\\dots\\) have to be chosen in such a way that for any possible smooth function \\(m\\) the squared approximation error tends to zero as \\(p\\rightarrow\\infty,\\)\n\\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\left(m(x)-\\sum_{j=1}^p \\vartheta_j b_j(x)\\right)^2\\to 0,\\quad p\\to\\infty.\n\\] However, for every fixed value \\(p,\\) we typically have that \\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\left(m(x)-\\sum_{j=1}^p \\vartheta_j b_j(x)\\right)^2 \\neq 0\n\\] which leads to a biased estimation procedure.\nFor a fixed value \\(p,\\) an estimator \\(\\hat m_p\\) of \\(m\\) is determined by \\[\n\\hat m_p(x)=\\sum_{j=1}^p \\hat\\beta_j b_j(x),\n\\] where the coefficients \\(\\hat\\beta_j\\) are obtained by ordinary least squares \\[\n\\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\hat\\beta_j \\underbrace{b_j(X_i)}_{X_{ij}}\\right)^2\n=\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\vartheta_j \\underbrace{b_j(X_i)}_{X_{ij}}\\right)^2\n\\] with \\[\n\\hat\\beta = \\left(\\begin{matrix}\\hat\\beta_1\\\\ \\vdots\\\\\\hat\\beta_p\\end{matrix}\\right)=\\left(\\mathbf{X}^\\top\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\top Y,\n\\] where \\(\\mathbf{X}\\) denotes the \\((n\\times p)\\)-dimensional matrix with elements \\[\nX_{ij}=b_j(X_i),\n\\] \\(i=1,\\dots,n\\) and \\(j=1,\\dots,p,\\) and \\(Y=(Y_1,\\dots,Y_n)^\\top.\\)\nExamples of basis functions \\(b_1(x),\\,b_2(x)\\,\\dots\\):\n\npolynomials (monomial basis)\nspline functions\nwavelets\nFourier expansions (for periodic functions)\n\n\n4.2.1 Polynomial Regression\nTheoretical Justification: Every smooth function can be well approximated by a polynomial of sufficiently high degree.\nApproach:\n\nMonomial basis functions \\(b_j(x) = x^{j-1}\\), \\(\\;\\;j=1,\\dots,p-1.\\)\nChoose \\(p\\) (polynomial degree) and fit a polynomial of degree \\(p-1\\): \\[\n\\min_{\\vartheta_1,\\dots,\\vartheta_p}\\sum_{i=1}^n \\left(Y_i-\\sum_{j=1}^p \\vartheta_{j} X^{j-1}\\right)^2\n\\] \\[\n\\Rightarrow\\quad {\\hat m}_p(X)={\\hat \\beta}_{1}+\\sum_{j=2}^{p-1}\n{\\hat \\beta}_{j} X_i^{j-1}\n\\]\nThis corresponds to an approximation with basis functions \\[\n\\begin{align*}\nb_1(x)&=1\\\\[2ex]\nb_2(x)&=x\\\\[2ex]\nb_3(x)&=x^2\\\\[2ex]\n\\vdots\\\\[2ex]\nb_{p}(x)&=x^{p-1}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIt is only assumed that \\(m(x)\\) can be approximated by a polynomial \\(\\hat{m}_p(x)\\) of degree \\(p\\) as \\(p\\to\\infty.\\) However, for a given choice of \\(p,\\) there will usually still exist an approximation error.\nTherefore, \\(\\hat{m}_p(x)\\) is typically a biased estimator for given values of \\(p,\\) and a given value of \\(x\\in[a,b],\\) \\[\n\\begin{align*}\n\\operatorname{Bias}(\\hat{m}_p(x))\\quad & \\neq 0\\\\\n\\mathbb{E}(\\hat{m}_p(x)) - m(x)        & \\neq 0.\n\\end{align*}\n\\]\n\n\nR-Code to Compute Polynomial Regressions:\nGenerate some artificial data, where the usually unknown \\[\nm(x)=\\sin(5 x)\n\\] with \\(x\\in[0,1]\\):\n\nset.seed(1)\n# Generate some data: \nn      <- 100     # Sample Size\nx_vec  <- (1:n)/n # Equidistant X \n\n# Gaussian iid error term \ne_vec  <- rnorm(n = n, mean = 0, sd = .5)\n\n# Dependent variable Y\ny_vec  <-  sin(x_vec * 5) + e_vec\n\n# Save all in a dataframe\ndb     <-  data.frame(x=x_vec,y=y_vec)\n\nCompute the ordinary least squares regressions of different polynomial regression models:\n\n# Fitting of polynomials to the data (parametric models):\n# Constant line fit: (Basis function x^0)\nreg_p1 <- lm(y ~ 1, data=db)\n\n# Basis functions: x^0 + ... + x^3\nreg_p4 <- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)\n\n# Basis functions: x^0 + ... + x^6\nreg_p7 <- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)\n\nTake a look at the fits:\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(db, main=\"Truth\")\n# True (usually unknown) regression function\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n\n## Fit by degree 0 polynomial\nplot(db, main=\"Degree 0\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p1, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n## Fit by degree 3 polynomial\nplot(db, main=\"Degree 3\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p4, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n## Fit by degree 6 polynomial\nplot(db, main=\"Degree 6\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\nlines(y = predict(reg_p7, newdata = db), \n      x = x_vec, col=\"red\", lwd=1.5)\n\n\n\n\nThe quality of the approximation obviously depends on the choice of the model selection parameter \\(p\\) which serves as a smoothing parameter.\nLet‚Äôs look at the fits across 200 Monte Carlo replications:\n\nm_true    <- sin(x_vec * 5)\n\nn_MCrepl  <- 200 # MC-replications\n\nm_hat_p1  <- matrix(NA, n, n_MCrepl)\nm_hat_p4  <- matrix(NA, n, n_MCrepl)\nm_hat_p7  <- matrix(NA, n, n_MCrepl)\n\nfor(r in 1:n_MCrepl){\n    # Generate some data: \n    e_vec  <- rnorm(n = n, mean = 0, sd = .5)\n    y_vec  <-  sin(x_vec * 5) + e_vec\n    db     <-  data.frame(x = x_vec,y = y_vec)\n    # Estimations\n    reg_p1 <- lm(y ~ 1, data=db)\n    reg_p4 <- lm(y ~ poly(x, degree = 3, raw = TRUE), data=db)\n    reg_p7 <- lm(y ~ poly(x, degree = 6, raw = TRUE), data=db)\n    # Save predictions (y hat)\n    m_hat_p1[,r] <- predict(reg_p1, newdata = db)\n    m_hat_p4[,r] <- predict(reg_p4, newdata = db)\n    m_hat_p7[,r] <- predict(reg_p7, newdata = db)\n}\n\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,2.1))\nplot(db, main=\"Truth\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nsubSelect <- 25\nmatplot(y = m_hat_p1[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",  \n        ylim = range(m_hat_p1[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=0.5, main = \"Degree p=0\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nmatplot(y = m_hat_p4[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",\n        ylim = range(m_hat_p4[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=.5, main = \"Degree p=3\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n##\nmatplot(y = m_hat_p7[,1:subSelect], \n        x = x_vec, type = \"l\", lty = 1, ylab = \"\", xlab = \"x\",\n        ylim = range(m_hat_p7[,1:subSelect], sin(x_vec * 5)), \n        col=rep(\"red\",n), lwd=.5, main = \"Degree p=6\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"blue\", lwd=1.5)\n\n\n\n\nUsing the 200 fits from above, we can approximate the\n\nsquared bias \\(\\left(\\operatorname{Bias}(\\hat{m}_p(x))\\right)^2\\) and the\nvariance \\(Var(\\hat{m}_p(x))\\)\n\npoint-wise at each \\(x\\)-value in x_vec.\n\n## Pointwise (for each x) biases of \\hat{m}(x): \nPt_Bias_p1 <- rowMeans(m_hat_p1) - m_true\nPt_Bias_p4 <- rowMeans(m_hat_p4) - m_true\nPt_Bias_p7 <- rowMeans(m_hat_p7) - m_true\n\n## Pointwise squared biases\nPt_BiasSq_p1 <- Pt_Bias_p1^2\nPt_BiasSq_p4 <- Pt_Bias_p4^2\nPt_BiasSq_p7 <- Pt_Bias_p7^2\n\n## Pointwise (for each x) variances \\hat{m}(x):\nPt_Var_p1  <- apply(m_hat_p1, 1, var)\nPt_Var_p4  <- apply(m_hat_p4, 1, var)\nPt_Var_p7  <- apply(m_hat_p7, 1, var)\n\npar(mfrow=c(1,2))\nmatplot(y = cbind(Pt_BiasSq_p1, Pt_BiasSq_p4, Pt_BiasSq_p7), \n        x = x_vec, type = \"l\", col = c(1,2,\"darkgreen\"), \n        main = \"Pointwise Squared Bias\", ylab=\"\", xlab=\"x\")\nlegend(\"topleft\", col = c(1,2,\"darkgreen\"), lty = c(1,2,3), \nlegend = c(\"Degree p=0\", \"Degree p=3\", \"Degree p=6\"))\nmatplot(y = cbind(Pt_Var_p1, Pt_Var_p4, Pt_Var_p7), \n        x = x_vec, type = \"l\", col = c(1,2,\"darkgreen\"), \n        main = \"Pointwise Variance\", ylab=\"\", xlab=\"x\")\nlegend(\"top\", col = c(1,2,\"darkgreen\"), lty = c(1,2,3), \nlegend = c(\"Degree p=0\", \"Degree p=3\", \"Degree p=6\"))\n\n\n\n\nThe Bias-Variance Trade-Off:\n\n\\(p\\) small: Variance of the estimator is small, but (squared) bias is large.\n\\(p\\) large: Variance of the estimator is large, but (squared) bias is small.\n\n\n\n\n\n\n\nRemark\n\n\n\nPolynomial regression is not very popular in practice. Reasons are numerical problems in fitting high dimensional polynomials. Furthermore, high order polynomials often posses an erratic, difficult to interpret behavior at the boundaries.\n\n\n\n\n4.2.2 Regression Splines\nThe practical disadvantages of global basis functions (like polynomials), explain the success of local basis functions. A frequently used system of basis functions are local polynomials, i.e., so-called spline functions.\nA spline function is a piece wise polynomial function. They are defined with respect to a pre-specified sequence of \\(q\\) knots \\[\na=\\tau_1<\\tau_2\\leq\\dots\\leq \\tau_{q-1}<\\tau_q=b.\n\\] Different specifications of the knot sequence lead to different splines.\nMore precisely, for a given knot sequence, a spline function \\(s(x)\\) of degree \\(k\\) is defined by the following properties:\n\n\\(s(x)\\) is a polynomial of degree \\(k\\) (i.e.¬†of order1 \\(k+1\\)) in every interval \\([\\tau_j,\\tau_{j+1}]\\), i.e. \\[\ns(x)=s_0+s_1x+s_2x^2+\\dots+s_kx^{k},\\quad x\\in[\\tau_j,\\tau_{j+1}]\n\\] with \\(s_0,\\dots,s_k\\in\\mathbb{R}.\\)\n\n\\(s(x)\\) is called a linear spline if \\(k=1\\)\n\\(s(x)\\) is a quadratic spline if \\(k=2\\)\n\\(s(x)\\) is a cubic spline if \\(k=3\\)\n\n\\(s(x)\\) is \\(k-1\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\nIn practice, the most frequently used splines are cubic spline functions based on an equidistant sequence of \\(q\\) knots, i.e., \\[\n\\tau_{j+1}-\\tau_j=\\tau_j-\\tau_{j-1}\n\\] for all \\(j=2,\\dots,q-1.\\)\nThe space of all spline functions of degree \\(k\\) defined with respect to a given knot sequence \\[\na=\\tau_1<\\tau_2\\leq\\dots\\leq \\tau_{q-1}<\\tau_q=b\n\\] is a \\[\n\\begin{array}{lccccc}\np & = & \\text{number of knots} &+& \\text{polynomial degree} & -\\;\\;1\\\\\n& = & q & + & k & -\\;\\;1\n\\end{array}\n\\] dimensional linear function space \\[\n{\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}=\\operatorname{span}(b_{1,k},\\dots,b_{p,k}),\n\\] where \\(b_{1,k},\\dots,b_{p,k}\\) denote the basis-functions.\n\n\n\nConstruction of B-Spline Basis Functions\nThe so-called B-spline basis functions are almost always used in practice, since they possess a number of advantages from a numerical point of view.\nThe B-spline basis functions for splines of degree \\(k\\) are defined with respect to a given knot sequence \\[\n\\underbrace{a=\\tau_1}_{\\text{lower boundary knot}}\\quad {\\color{red}<}\\quad\\overbrace{\\tau_2\\leq\\dots\\leq \\tau_{q-1}}^{\\text{interior knots}}\\quad{\\color{red}<}\\quad\\underbrace{\\tau_q=b}_{\\text{upper boundary knot}}.\n\\]\nTo construct the B-spline basis functions, one augments the knot sequence by repeating each of the boundary knots \\(k+1\\) times: \\[\n\\underbrace{\\tau_{-(k-1)}=\\dots=\\tau_0=\\tau_1}_{\\text{$k+1$ lower boundary knots}}\\quad {\\color{red}<}\\quad\\overbrace{\\tau_2\\leq\\dots\\leq \\tau_{q-1}}^{\\text{interior knots}}\\quad{\\color{red}<}\\quad\\underbrace{\\tau_q=\\tau_{q+1}=\\dots=\\tau_{q+k}}_{\\text{$k+1$ upper boundary knots}}.\n\\] Let \\[\n\\tau^\\ast_{1}, \\dots,\\tau^\\ast_{q+2k}\n\\] denote the augmented knot sequence after resetting the index to start at \\(1.\\)\nThe spline basis functions are calculated by a recursive (over \\(l=0,1,\\dots,k\\)) procedure.\nThe initial level (\\(l=0\\)) are piece-wise constant functions \\[\nb_{j,0}(x)=\\left\\{\n\\begin{matrix}  \n1 & \\text{ if } \\tau_{j}^*\\leq x <\\tau_{j+1}^*\\\\\n0 & \\text{ else}\n\\end{matrix}\n\\right.,\n\\tag{4.1}\\] for \\(j=1,\\dots,q+2k,\\) and \\(x\\in [a,b].\\)\nFor \\(l=1,\\dots,k\\) the recursion is defined by \\[\n\\begin{align*}\nb_{j,l}(x)\n& =\\frac{x-\\tau_j^*}{\\tau_{l+j}^*-\\tau_j^*}b_{j,l-1}(x)\\\\\n& +\\frac{\\tau_{l+j+1}^*-x}{\\tau_{l+j+1}^*-\\tau_{j+1}^*}b_{j+1,l-1}(x),\n\\end{align*}\n\\tag{4.2}\\] \\(j=1,\\dots,q+2k\\), and \\(x\\in [a,b].\\)\nNote: The definitions in Equation¬†4.1 and Equation¬†4.2 are understood that if the denominator is 0, then the function is defined to be 0. The remaining non-degenerated basis functions are then the \\[\nb_{j,k},\\quad j=1,\\dots,p=q+k-1,\n\\] B-spline basis functions.\n\n\nR-Code to Compute B-Spline Basis Functions:\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=1} - 1=7\n\\] linear (\\(k=1\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         <- 1\ninternal_knots <- seq(from = 0.1, to = 0.9, len = 5)\nboundary_knots <- c(0, 1)\n\n## evaluation points (for plotting)\nx_vec <- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree1 <- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree1, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 1\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\nDegree 1\n\n\n\nHere, the basis functions are piecewise linear (\\(k=1\\)) functions with local support over at most \\(k+1=1+1=2\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns &\\in\\mathcal{S}_{k=1,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=5)\\\\[2ex]\ns(x)&= \\sum_{j=1}^{7}\\vartheta_j b_{j,1}(0)\n\\end{align*}\n\\] is \\(k-1=1-1=0\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=2} - 1=8\n\\] quadratic (\\(k=2\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         <- 2\ninternal_knots <- seq(from = 0.1, to = 0.9, len = 5)\nboundary_knots <- c(0, 1)\n\n## evaluation points (for plotting)\nx_vec <- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree2 <- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree2, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 2\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\nDegree 2\n\n\n\nHere, the basis functions are piecewise quadratic (\\(k=2\\)) functions with local support over at most \\(k+1=2+1=3\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns&\\in\\mathcal{S}_{k=2,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=5)\\\\[2ex]\ns(x)&= \\sum_{j=1}^{8}\\vartheta_j b_{j,1}(x)\n\\end{align*}\n\\] is \\(k-1=2-1=1\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\nThe following R code generates\n\\[\np=\\underbrace{\\texttt{Numbr.of Knots}}_{q=7} + \\underbrace{\\texttt{degree}}_{k=3} - 1=9\n\\] cubic (\\(k=3\\)) B-spline basis functions:\n\nlibrary(\"splines2\")\n\ndegree         <- 3\ninternal_knots <- seq(from = 0.1, to = 0.9, len = 5)\nboundary_knots <- c(0, 1)\n\n## evaluation points (for plotting)\nx_vec <- seq(from = boundary_knots[1], \n             to   = boundary_knots[2], len = 100)\n\nX_mat_degree3 <- splines2::bSpline(\n        x              = x_vec, \n        knots          = internal_knots, \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = boundary_knots)\n\nmatplot(x = x_vec, \n        y = X_mat_degree3, \n        type = \"l\", main = \"B-Spline Basis Functions \\nDegree 3\",\n        ylab = \"\", xlab = \"x\", axes = FALSE)\naxis(1)\naxis(2)\nabline(v   = c(boundary_knots[1], internal_knots, boundary_knots[2]), \n       col = \"gray\")\n\n\n\n\n\n\n\n\n\n\nDegree 3 (usual case)\n\n\n\nHere, the basis functions are piecewise cubic (\\(k=3\\)) functions with local support over at most \\(k+1=3+1=4\\) knot-intervals. Any spline function \\[\n\\begin{align*}\ns &\\in\\mathcal{S}_{k=3,\\tau_1,\\dots,\\tau_q}\\quad(\\text{with}\\;\\;q=5)\\\\[2ex]\ns(x) &= \\sum_{j=1}^{9} \\vartheta_j b_{j,1}(x)\n\\end{align*}\n\\] is \\(k-1=3-1=2\\) times continuously differentiable at each knot point \\(x=\\tau_j\\), \\(j=1,\\dots,q\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nNormalized: The B-spline basis system has a property that is often useful: the sum of the B-spline basis function values at any point \\(x\\) is equal to one. Note, for example, that the first and last basis functions are exactly one at the boundaries. This is because all the other basis functions go to zero at these end points.\nCompact Support: Basis functions are positive only over at most \\(k+1\\) intervals and zero over the remaining intervals. This compact support property is important for computational efficiency. \nMultiple knots: A multiple interior knot (\\(\\tau_j=\\tau_{j+1}\\)) reduces the degree of continuity at that knot value. At a normal interior knot, a spline function is \\(k-1\\) times continuously differentiable. Each extra knot with the same value reduces continuity at that knot by one. This is the only way to reduce the continuity of the curve at the knot values. If there are \\(k\\) (or more) equal knots, then you get a discontinuity in the curve at this knot-location.\n\n\n\n\n4.2.2.1 Regression Splines with Equidistant Knots\nRemember the nonparametric regression model setup:\n\\[\nY_i=m(X_i)+\\varepsilon_i\n\\]\n\n\\(m(X)=\\mathbb{E}(Y_i|X=X_i)\\) regression function\n\\(\\mathbb{E}(\\varepsilon_i)=0\\)\n\\(Var(\\varepsilon_i|X_i) = Var(\\varepsilon_i)=\\sigma^2\\)\n\\(\\varepsilon_i\\) and \\(X_i\\) are independent or at least mean-independent, i.e., \\(\\mathbb{E}(\\varepsilon_i|X_i)=0\\)\n\nThe so-called regression spline (or B-spline) approach to estimating a regression function \\(m(x)\\) is based on fitting a set of spline basis functions to the data.\nTypically, cubic splines (\\(k=3\\)) with equidistant knots are applied:\n\n\\(k=3\\) (cubic splines)\n\\(\\tau_1=a\\)\n\\(\\tau_{j+1}=\\tau_j + (b-a)/(q-1),\\quad j=1,\\dots,q-1\\)\n\nsuch that \\(\\tau_q=b\\)\n\nIn this case the number of knots \\(q,\\) or more precisely the total number of basis functions \\(p\\) \\[\n\\begin{align*}\np\n&=q+k-1\\\\[2ex]\n&=q+2\\qquad (\\text{using that}\\quad k=3)\n\\end{align*}\n\\] serves as the smoothing parameter which has to be selected by the statistician.\nFor a given choice of \\(p,\\) let \\[\n\\underset{(n\\times p)}{\\mathbf{X}}\n\\] denote the \\(n\\times p\\) matrix with elements \\[\nX_{ij}=b_{j,k}(X_i),\\quad i=1,\\dots,n,\\quad j=1,\\dots,p,\n\\] and let \\[\nY=(Y_1,\\dots,Y_n)^\\top\n\\] denote the vector of response variables.\nAn estimator \\(\\hat{m}_p(x)\\) of \\(m(x)\\) is then given by \\[\n\\hat m_p(x)=\\sum_{j=1}^p \\hat\\beta_j b_{j,k}(x),\n\\] where the coefficients \\(\\hat\\beta_j\\) are determined by ordinary least squares \\[\n\\hat\\beta_1,\\dots,\\hat\\beta_p=\\arg\\min_{\\vartheta_1,\\dots,\\vartheta_p} \\sum_{i=1}^n \\left( Y_i-\\sum_{j=1}^p \\vartheta_j \\underbrace{b_{j,k}(X_i)}_{X_{ij}}\\right)^2.\n\\] That is, the vector of coefficients \\[\n\\hat \\beta=(\\hat\\beta_1,\\dots,\\hat\\beta_p)^\\top\n\\] can be written as \\[\n\\hat\\beta=(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top Y.\n\\] The fitted values are given by \\[\n\\left(\\begin{array}{c}\n{\\hat m}_p(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ {\\hat m}_p(X_n)\n\\end{array}\\right)=\\mathbf{X}\\hat\\beta=\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top}_{=:S_p}Y = S_p Y\n\\]\nThe matrix \\[\nS_p = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\n\\] is referred to as the smoothing matrix.\n\n\n\n\n\n\nRemark\n\n\n\nQuite generally, the most important nonparametric regression procedures are linear smoothing methods. This means that in dependence of some smoothing parameter (here \\(p\\)), estimates of the vector \\[\n(m(X_1),\\dots,m(X_n))^\\top\n\\] are obtained by multiplying a smoother matrix \\(S_p\\) with \\(Y\\).\nThat is, \\[\n\\left(\\begin{array}{c}\nm(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ m(X_n)\n\\end{array}\\right)\\approx\n\\left(\\begin{array}{c}\n{\\hat m}_p(X_1)\\\\\n\\vdots%\\\\ \\cdot\\\\ \\cdot\n\\\\ {\\hat m}_p(X_n)\n\\end{array}\\right)=S_p Y\n\\]\n\n\nR Code to Compute Regression Splines:\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      <- 100     # Sample Size\nx_vec  <- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  <- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  <-  sin(x_vec * 5) + e_vec\n\nThen, we generate cubic B-spline basis functions with equidistant knot sequence (different to x_vec) and evaluate them at x_vec:\n\ndegree      <- 3 # piecewise cubic splines\n\nknot_seq_5  <- seq(from = 0, to = 1, len = 5)# knots\n\nX_mat_p7    <- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq_5[-c(1, length(knot_seq_5))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq_5[ c(1, length(knot_seq_5))]\n    )\n\nknot_seq_15  <- seq(from = 0, to = 1, len = 15)# knots\n\nX_mat_p17    <- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq_15[-c(1, length(knot_seq_15))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq_15[ c(1, length(knot_seq_15))]\n    )    \n\nComputing the smoothing matrices \\(S_p\\) for \\(p=7\\) and \\(p=17\\):\n\nS_p7  <- X_mat_p7  %*% solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) \nS_p17 <- X_mat_p17 %*% solve(t(X_mat_p17) %*% X_mat_p17) %*% t(X_mat_p17) \n\nComputing the estimates \\(\\hat{m}_p(X_1),\\dots,\\hat{m}_p(X_n)\\) for \\(p=7\\) and \\(p=17\\):\n\nm_hat_p7  <- S_p7  %*% y_vec\nm_hat_p17 <- S_p17 %*% y_vec\n\nPlotting the estimation results:\n\nplot(y=y_vec, x=x_vec, xlab=\"X\", ylab=\"Y\", \n     main=\"Regression Splines\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"red\", lty=2, lwd=1.5)\nlines(y=m_hat_p7, x=x_vec, col=\"blue\", lwd=1.5)\nlines(y=m_hat_p17, x=x_vec, col=\"darkorange\", lwd=1.5)\nlegend(\"bottomleft\", \n       c(\"(Unknown) Regression Function m\", \n         \"Regr.-Spline Fit with p=7\", \n         \"Regr.-Spline Fit with p=17\"), \n       col=c(\"red\",\"blue\", \"darkorange\"), \n       lty=c(2,1,1), lwd=c(2,2,2))\n\n\n\n\nThe following plot shows the regression spline fit (with \\(p=7\\)) \\[\n\\hat{m}_{7}(x)=\\sum_{j=1}^{7}\\hat\\beta_j b_{j,3}(x)\n\\] along with the \\(p=7\\) basis functions each multiplied by the fitted linear coefficient \\[\n\\hat\\beta_j b_{j,3}(x),\\quad j=1,\\dots,7.\n\\]\n\nbeta_hat_p7 <- solve(t(X_mat_p7)  %*% X_mat_p7)  %*% t(X_mat_p7) %*% y_vec\nplot(y=m_hat_p7, x=x_vec, ylim = c(-3.5,1.5),\n     col=\"blue\", lwd=1.5, type = \"l\", xlab=\"x\", ylab=\"\")\nabline(v = knot_seq_5, col = gray(0.5))     \nmatlines(X_mat_p7 * matrix(rep(beta_hat_p7, each = n), ncol=7), \n         x = x_vec, type=\"l\", lty = 2, col = c(1,2,3,4,5,6,7))\nlegend(\"topright\", \n       c(\"Regr.-Spline Fit with p=7\"), \n       col = \"blue\", lty= 1, lwd= 2)\nlegend(\"bottomleft\", \n       c(expression(paste(\"1. basis function times \",hat(beta)[1])),\n         expression(paste(\"2. basis function times \",hat(beta)[2])),\n         expression(paste(\"3. basis function times \",hat(beta)[3])),\n         expression(paste(\"4. basis function times \",hat(beta)[4])),\n         expression(paste(\"5. basis function times \",hat(beta)[5])),\n         expression(paste(\"6. basis function times \",hat(beta)[6])),\n         expression(paste(\"7. basis function times \",hat(beta)[7]))\n       ), \n       col = c(1,2,3,4,5,6,7), lty = 2, lwd = 1)       \n\n\n\n\n\n\n\n4.2.3 Mean Average Squared Error of Regression Splines\nIn a nonparametric regression context we do not assume that the unknown true regression function \\(m(x)\\) exactly corresponds to a spline function. Thus, \\[\n\\hat m_p=(\\hat{m}_p(X_1),\\dots,\\hat{m}_p(X_n))^\\top\n\\] typically possesses a systematic estimation error (bias). That is, \\[\n\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\neq m(X_i).\n\\]\nTo simplify notation, we will in the following write \\[\n\\mathbb{E}_\\varepsilon(\\cdot)\\quad\\text{and}\\quad Var_\\varepsilon(\\cdot)\n\\] to denote expectation and variance ‚Äúwith respect to the random variable \\(\\varepsilon\\), only‚Äù.\nIn the case of random design, \\[\n\\mathbb{E}_\\varepsilon(\\cdot)\\quad\\text{and}\\quad Var_\\varepsilon(\\cdot)\n\\] thus denote the conditional expectation \\[\n\\mathbb{E}(\\cdot|X_1,\\dots,X_n)\n\\] and the conditional variance \\[\nVar(\\cdot|X_1,\\dots,X_n)\n\\] given the observed \\(X\\)-values.\nFor random design, these conditional expectations depend on the observed sample, and thus are random. For fixed design, such expectations are of course fixed values.\nIt will always be assumed that the matrix \\[\n\\mathbf{X}^\\top \\mathbf{X},\n\\] with \\(\\mathbf{X}=(b_{j,k}(X_i))_{i,j}\\), is invertible (under our conditions on the design density this holds with probability 1 for the random design).\nThe behavior of nonparametric function estimates is usually evaluated with respect to quadratic risk (i.e.¬†mean squared error).\nA commonly used global measure of accuracy of a spline estimator \\(\\hat m_p\\) is the Mean Average Squared Error (MASE), which averages the local (at each \\(X_i\\)) mean squared errors over all \\(X_i,\\) \\(i=1,\\dots,n:\\) \\[\n\\begin{align*}\n&\\operatorname{MASE}(\\hat m_p):=\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left(m(X_i)-\\hat{m}_p(X_i)\\right)^2\\\\\n= &\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\left(\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))-m(X_i)\\right)^2}_{(\\operatorname{Bias}_\\varepsilon(\\hat{m}_p(X_i)))^2} + \\\\[2ex]\n&\\frac{1}{n}\\sum_{i=1}^n \\underbrace{\\mathbb{E}_\\varepsilon\\left((\\hat{m}_p(X_i)-\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))\\right)^2}_{Var_\\varepsilon(\\hat{m}_p(X_i))}\n\\end{align*}\n\\]\nAnother frequently used measure is the Mean Integrated Squared Error (MISE): \\[\n\\begin{align*}\n\\operatorname{MISE}(\\hat m_p):=\\int_a^b \\mathbb{E}_\\varepsilon\\left(m(x)-\\hat m_p(x)\\right)^2dx\n\\end{align*}\n\\]\n\nMASE versus MISE:\n\nEquidistant design: \\[\n\\operatorname{MISE}(\\hat m_p)=\\operatorname{MASE}(\\hat m_p) + O(n^{-1})\n\\]\nMISE and MASE are generally not asymptotically equivalent in the case of random design \\[\n\\operatorname{MASE}(\\hat m_p)=\\int_a^b \\mathbb{E}_\\varepsilon\\left(m(x)-\\hat m_p(x)\\right)^2 f(x)dx + O_P(n^{-1}).\n\\]\n\n\n\n\n\n\n\nLandau symbol ‚ÄúBig Oh‚Äù \\(O(r_n)\\)\n\n\n\n\\[\nO(r_n)\\quad\\text{with}\\quad r_n>0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all sequences \\(x_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\dfrac{|x_n|}{r_n}\\to c\\) as \\(n\\to\\infty,\\) where \\(c\\) is a constant with \\(0\\leq c < \\infty.\\)\n\nNote: This includes the case \\(\\dfrac{|x_n|}{r_n}\\to 0.\\)\nExamples:\n\n\\(-\\dfrac{1}{n}=O(n^{-1})\\)\n\\(\\dfrac{1}{n^2}=O(n^{-1})\\)\n\\(\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m-1} g\\left(x_j\\right)(x_{j+1}-x_j) = \\int_a^b g(x)dx + O(m^{-1})\\),  where \\(x_j=a+\\dfrac{j-1}{m-1}(b-a)\\) for sufficiently smooth (continuously differentiable over \\((a,b)\\)) \\(g\\).\n\n\n\n\n\n\n\n\n\nLandau symbol ‚ÄúSmall oh‚Äù \\(o(r_n)\\)\n\n\n\n\\[\no(r_n)\\quad\\text{with}\\quad r_n>0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all sequences \\(x_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\dfrac{|x_n|}{r_n}\\to 0\\) as \\(n\\to\\infty.\\)\n\nExamples:\n\n\\(\\dfrac{1}{n^2}=o(n^{-1})\\)\n\\(n^{-a}=o(n^{-b})\\) for all \\(a>b>0.\\)\n\nNote: For every sequence \\(x_n=o(r_n)\\) it holds that \\(x_n=O(r_n),\\) but not the other way round.\n\n\n\n\n\n\n\n\nSpecial Cases \\(O(1)\\) and \\(o(1)\\)\n\n\n\n\\[\nO(1)\\quad \\text{and}\\quad o(1)\n\\]\nExamples:\n\n\\(1 + \\dfrac{1}{n} = O(1)\\)\n\\(\\dfrac{1}{n^2}=o(1)\\)\n\n\n\n\n\n\n\n\n\nStochastic Landau symbol ‚ÄúBig Oh P‚Äù \\(O_P(r_n)\\)\n\n\n\n\\[\nO_P(r_n)\\quad\\text{with}\\quad r_n>0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all stochastic sequences \\(X_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\displaystyle P\\left(\\frac{|X_n|}{r_n}>\\delta\\right)<\\epsilon\\) for all sufficiently large \\(n\\) and all \\(\\delta>0\\) and \\(\\epsilon>0\\).  Plain English: ‚Äúsuch that \\(\\frac{|X_n|}{r_n}\\) is bounded in probability for all large enough \\(n\\)‚Äù\n\nExample:\n\nIf \\(\\displaystyle \\sqrt{n}(\\bar{X}_n-\\mu)\\to_d\\mathcal{N}(0,\\sigma^2),\\) then \\[\n\\begin{align*}\n\\sqrt{n}(\\bar{X}_n-\\mu) &= O_P(1)\\\\[2ex]\n(\\bar{X}_n-\\mu) &= O_P(n^{-1/2})\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\nStochastic Landau symbol ‚ÄúSmall oh P‚Äù \\(o_P(r_n)\\)\n\n\n\n\\[\no_P(r_n)\\quad\\text{with}\\quad r_n>0,\\;n=1,2,\\dots\n\\] is a placeholder symbol describing the family of all stochastic sequences \\(X_n\\), \\(n=1,2,\\dots,\\) such that\n\n\\(\\displaystyle \\frac{|X_n|}{r_n}\\to_P 0\\quad\\) as \\(\\quad n\\to\\infty\\)\n\nExample:\n\nIf \\(\\displaystyle \\sqrt{n}(\\bar{X}_n-\\mu)\\to_d\\mathcal{N}(0,\\sigma^2),\\) then \\[\n\\begin{align*}\n(\\bar{X}_n-\\mu)         &= o_P(1)\n\\end{align*}\n\\]\n\n\n\nIn the following we focus on the MASE which has the advantage that we can use matrix algebra.\nFirst, we look at the local bias of \\(\\hat{m}_p(X_i)\\) at a single evaluation point \\(X_i:\\) \\[\n\\operatorname{Bias}_\\varepsilon(\\hat{m}_p(X_i))=\\mathbb{E}_\\varepsilon(\\hat m_p(X_i))-m(X_i),\n\\] where \\[\n\\begin{align*}\n  \\mathbb{E}_\\varepsilon(\\hat m_p(X_i))&=\\mathbb{E}_\\varepsilon\\Big(\\sum_{j=1}^p \\hat{\\beta}_j b_{j,k}(X_i)\\Big)\\\\\n&=\\sum_{j=1}^p\\mathbb{E}_\\varepsilon(\\hat{\\beta}_j) b_{j,k}(X_i),\n\\end{align*}\n\\] with \\[\n\\hat{\\beta}=\\left(\\begin{matrix}\\hat{\\beta}_1\\\\\\vdots\\\\\\hat{\\beta}_p\\end{matrix}\\right)=(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top Y\n\\] and with \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\hat\\beta)\n&=\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  (\\overbrace{m+\\varepsilon}^{=Y})\\Big)\\\\[2ex]\n&=\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m\\Big)\\;+\\;\\mathbb{E}_\\varepsilon\\Big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\varepsilon\\Big)\\\\[2ex]\n&=\\overbrace{(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m}^{=(\\beta_1,\\dots,\\beta_p)^\\top=\\beta}\\;+\\;\\underbrace{(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\overbrace{\\mathbb{E}_\\varepsilon(\\varepsilon)}^{=0}}_{=0}\\\\[2ex]\n&=(\\beta_1,\\dots,\\beta_p)^\\top=\\beta\n\\end{align*}\n\\] where \\[\nm=\\left(\\begin{matrix}m(X_1)\\\\\\vdots\\\\m(X_n)\\end{matrix}\\right)\n\\] denotes the vector of true function values, and \\[\n\\varepsilon=\\left(\\begin{matrix}\\varepsilon_1\\\\\\vdots\\\\\\varepsilon_n\\end{matrix}\\right)\n\\] denotes the vector of error terms.\nThat is, the mean of the spline regression estimator evaluated at \\(X_i\\) is given by \\[\n\\mathbb{E}_\\varepsilon(\\hat m_p(X_i)) = \\sum_{j=1}^p\\beta_j b_{j,k}(X_i),\n\\] where \\[\n\\beta=\\left(\\begin{matrix}\\beta_1\\\\\\vdots\\\\\\beta_p\\end{matrix}\\right)=(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  m\n\\] is a least squares solution; namely of the following least squares problem that tries to approximate the unknown vector \\(m=(m(X_1),\\dots,m(X_n))^\\top\\) using a spline function \\(s\\in {\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}:\\) \\[\n\\begin{align*}\n&\\sum_{i=1}^n \\left(m(X_i)-\\sum_{j=1}^p \\beta_j b_{j,k}(X_i)\\right)^2\\\\[2ex]\n&=\\min_{\\vartheta_1,\\dots,\\vartheta_p}\\sum_{i=1}^n \\left(m(X_i)-\\sum_{j=1}^p \\vartheta_j  b_{j,k}(X_i)\\right)^2\\\\[2ex]\n&=\\min_{s\\in {\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}} \\sum_{i=1}^n \\left(m(X_i)-s(X_i)\\right)^2.\n\\end{align*}\n\\]\nThat is, the mean of the spline regression estimator evaluated at \\(X_i\\) \\[\n\\mathbb{E}_\\varepsilon(\\hat m_p(X_i))=\\sum_{j=1}^p \\beta_j b_j(X_i)=:\\tilde m_p(X_i)\n\\] is the best least squares (\\(L_2\\)) approximation of the true, but unknown, regression function vector \\(m=(m(X_1),\\dots,m(X_n))^\\top\\) by means of a spline function vector \\(s=(s(X_1),\\dots,s(X_n))^\\top\\) with \\(s\\in{\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q}.\\)\n\n\n\n\n\n\nImportant\n\n\n\nIf the true, but unknown, regression function \\(m\\) happens to be an element of the space of spline functions \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q},\\) then \\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) = 0,\\quad i=1,\\dots,n.\n\\] However, generally we do not expect that \\(m\\) is actually an element of \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_q},\\) such that\n\\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) \\neq  0,\\quad i=1,\\dots,n.\n\\] For consistency, however, we need that \\[\n\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)) = \\tilde m_p(X_i) - m(X_i) \\to 0,\\quad i=1,\\dots,n.\n\\] as \\(n\\to\\infty\\) and \\(p\\equiv p_n\\to\\infty;\\) i.e.¬†that \\(m\\) becomes eventually an element of the then very large space \\({\\cal{S}}_{k,\\tau_1,\\dots,\\tau_{q_n}}\\) as \\(q_n\\to\\infty\\) with \\(n\\to\\infty.\\)\n\n\nFrom the general approximation properties of cubic splines (\\(k=3\\)) with \\(q=p-2\\) equidistant knots (De Boor and De Boor (1978) or Eubank (1999)), we know that:\n\nIf \\(m\\) is twice continuously differentiable over \\((a,b)\\), then \\[\n\\begin{align*}\n(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\n&=\\frac{1}{n}\\sum_{i=1}^n (\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n \\left(\\tilde m_p(X_i) - m(X_i)\\right)^2=O_p(p^{-4})\n\\end{align*}\n\\]\nIf \\(m\\) is four times continuously differentiable, then \\[\n\\begin{align*}\n(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\n&=\\frac{1}{n}\\sum_{i=1}^n (\\operatorname{Bias}_\\varepsilon(\\hat m_p(X_i)))^2\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\left(\\tilde m_p(X_i) - m(X_i)\\right)^2=O_p(p^{-8})\n\\end{align*}\n\\]\n\nThe next step is to compute the (average) variance of the estimator, which can be obtained by the usual type of arguments applied in parametric regression:  \\[\n\\begin{align*}\nVar_\\varepsilon(\\hat m_p)\n&=\\frac{1}{n}\\sum_{i=1}^nVar_\\varepsilon(\\hat{m}_p(X_i))\\\\[2ex]\n&=\\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}_\\varepsilon\\big((\\hat{m}_p(X_i)-\\overbrace{\\tilde{m}(X_i)}^{=\\mathbb{E}_\\varepsilon(\\hat{m}_p(X_i))})^2\\big)\\\\[2ex]\n%\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y-\n% \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\tilde m_p\\Vert_2^2\\right)\\\\\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\left\\Vert \\left(\\begin{matrix}\\hat{m}_p(X_1)\\\\\\vdots\\\\\\hat{m}_p(X_n)\\end{matrix}\\right)-\\left(\\begin{matrix}\\tilde{m}_p(X_1)\\\\\\vdots\\\\\\tilde{m}_p(X_n)\\end{matrix}\\right)\\right\\Vert_{2}^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y-\n\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top m\\Vert_2^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top (Y-m)\\Vert_2^2\\right)\\\\[2ex]\n&=\\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\Vert \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\Vert_2^2\\right)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  (\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top )^\\top}_{(1\\times n)}\\;\\;\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(n\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  (\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top )}_{(1\\times n)}\\;\\;\\underbrace{\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(n\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\Big(\\underbrace{\\varepsilon^\\top  \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon}_{(1\\times 1)}\\Big)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\operatorname{trace}\\left(\\varepsilon^\\top  \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\right)\\right)\\\\[2ex]\n&= \\frac{1}{n}\\mathbb{E}_\\varepsilon\\left(\\operatorname{trace}\\left(  (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\varepsilon\\varepsilon^\\top\\mathbf{X}\\right)\\right)\\\\[2ex]\n&=\\frac{1}{n}\\operatorname{trace}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\mathbb{E}_\\varepsilon(\\varepsilon\\varepsilon^\\top ) \\mathbf{X}\\right)\\quad(\\text{with }\\mathbb{E}_\\varepsilon(\\varepsilon\\varepsilon^\\top )=I_n\\sigma_\\varepsilon)\\\\[2ex]\n&=\\frac{1}{n} \\sigma^2 \\text{trace}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top  \\mathbf{X}\\right)\\\\[2ex]\n&=\\frac{1}{n} \\sigma^2 \\text{trace}\\left(I_p\\right)\\\\[2ex]\n&=\\sigma^2  \\frac{p}{n}\n\\end{align*}\n\\]\n\n\n\n\n\n\nTrace-Trick\n\n\n\nFor any \\((m\\times n)\\) matrix \\(A\\) and any \\((n\\times m)\\) matrix \\(B\\) we have the identity \\[\\text{trace}(AB)=\\text{trace}(BA)\\]\n\n\n\n\nSummary\nFor cubic (\\(k=3\\)) splines with \\(q\\) equidistant knots and a twice differentiable function \\(m\\) we have that:\n\\[\n\\begin{align*}\n\\displaystyle(\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2&=O_p(p^{-4})\\\\[2ex]\n\\displaystyle Var_\\varepsilon(\\hat m_p)&= \\sigma^2\\frac{p}{n},\n\\end{align*}\n\\tag{4.3}\\] where \\(p=q+2\\) is the number of basis functions (i.e.¬†the smoothing parameter).\nThis leads to the classical trade-off between (average) squared bias and (average) variance that is typical for nonparametric statistics:\n\n\\(p\\) small: \\(\\displaystyle Var_\\varepsilon(\\hat m_p)\\) is small, but squared bias \\(\\displaystyle (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\) is large.\n\\(p\\) large: \\(\\displaystyle Var_\\varepsilon(\\hat m_p)\\) is large, but squared bias \\(\\displaystyle (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2\\) is small.\n\n\nFrom Equation¬†4.3 we have that \\[\n\\begin{align*}\n\\operatorname{MASE}(\\hat m_{p})\n&= (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2 + Var_\\varepsilon(\\hat m_p)\\\\[2ex]\n&=O_p(p^{-4}) + \\sigma^2\\frac{p}{n}\n\\end{align*}\n\\]\nThus, if \\[\np\\equiv p_n\\to\\infty\\quad\\text{as}\\quad n\\to\\infty,\n\\] but sufficiently slow, such that \\[\n\\dfrac{p_n}{n}\\to 0,\n\\] then \\[\n\\begin{align*}\n\\operatorname{MASE}(\\hat m_{p})\n&= (\\operatorname{Bias}_\\varepsilon(\\hat m_p))^2 + Var_\\varepsilon(\\hat m_p)\\to 0\n\\end{align*}\n\\] as \\(n\\to 0.\\)\nAn optimal smoothing parameter \\(p_{opt}\\) that minimizes \\(\\operatorname{MASE}(\\hat m_{p})\\) will balance the squared bias and variance: \\[\n\\begin{align*}\n\\frac{d}{d\\,p}\\operatorname{MASE}(\\hat m_{p})\n&=\\frac{d}{d\\,p}\\left(\\texttt{const}\\cdot p^{-4}+\\sigma^2\\frac{p}{n}\\right)\\\\[2ex]\n&=-4\\cdot\\texttt{const}\\cdot p^{-5}+\\sigma^2\\frac{1}{n}.\n\\end{align*}\n\\] Setting zero and solving for \\(p_{opt}\\) yields \\[\np_{opt}=\\texttt{const}\\cdot\\left(\\frac{1}{n}\\right)^{-1/5},\n\\] where \\(\\texttt{const}\\) collects all constant factors; i.e., \\[\np_{opt}=\\texttt{const} \\cdot n^{1/5}\n\\] for some finite and positive constant \\(0 <\\texttt{const}<\\infty.\\)\nThus \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O_p(n^{-4/5}).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nFor an estimator \\(\\hat m\\) based on a valid (!) parametric model we have \\[\n\\operatorname{MASE}(\\hat m_{p_{opt}})=O_p(n^{-1}),\n\\] since parametric models have no bias‚Äîprovided, the model assumption is correct.\n\n\nSimilar results can be obtained for the mean integrated squared error (MISE): If \\(m\\) is twice continuously differentiable, and \\(p_{opt} \\sim n^{1/5}\\), then \\[\n\\operatorname{MISE}(\\hat m_{p_{opt}})=\\mathbb{E}_\\varepsilon\\left(\\int_a^b(m(x)-\\hat m_{p_{opt}}(x))^2dx\\right)=O_p(n^{-4/5}).\n\\]"
  },
  {
    "objectID": "Ch4_NPRegression.html#selecting-the-smoothing-parameter-p",
    "href": "Ch4_NPRegression.html#selecting-the-smoothing-parameter-p",
    "title": "4¬† Nonparametric Regression",
    "section": "4.3 Selecting the Smoothing Parameter \\(p\\)",
    "text": "4.3 Selecting the Smoothing Parameter \\(p\\)\nProblem: Since \\(m\\) is unknown, we cannot directly compute \\(\\operatorname{MASE}(\\hat{m}_p)\\) and thus cannot compute the exact value of \\(p_{opt}\\). However, we need to choose the smoothing parameter \\(p\\) in an (somehow) optimal and objective manner.\nApproach: Determine an estimate \\(\\hat p_{opt}\\) of the unknown optimal smoothing parameter \\(p_{opt}\\) by minimizing a suitable error criterion with the following properties:\n\nFor every possible \\(p\\) the error criterion function can be calculated from the data.\nFor every possible \\(p\\) the error criterion provides ‚Äúinformation‚Äù about the respective \\(\\operatorname{MASE}(\\hat{m}_p)\\).\n\nRecall: With \\[\n\\hat m_p=(\\hat m_p(X_1),\\dots,\\hat m_p(X_n))^\\top\n\\] we have \\[\n\\hat m_p=\\mathbf{X}\\hat\\beta=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top Y=S_pY.\n\\] Note that \\[\n\\begin{align*}\n\\operatorname{trace}(S_p)\n&=\\operatorname{trace}\\big(\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\big)\\\\[2ex]\n&=\\operatorname{trace}\\big((\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{X}\\big)\\\\[2ex]\n&=\\operatorname{trace}\\big(I_p\\big)=p\n\\end{align*}\n\\]\nThat is, for given \\(p\\), the number of parameters to estimate by the spline method (one also speaks of the ‚Äúdegrees of freedom‚Äù of the smoothing procedure) is equal to \\(p\\) which corresponds to the trace of the smoother matrix \\(S_p=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top.\\)\nMost frequently used error criteria are Cross-Validation (CV) and Generalized Cross-Validation (GCV):\n\nCross-Validation (CV):  For a given value \\(p,\\) cross-validation tries to approximate the out-of-sample prediction error \\[\n\\operatorname{CV}(p)=\\frac{1}{n} \\sum_{i=1}^n\\biggl( Y_i-\n{\\hat m}_{p,-i}(X_i)\\biggr)^2\n\\] Here, for any \\(i=1,\\dots,n\\), \\({\\hat m}_{p,-i}(\\cdot)\\) is the ‚Äúleave-one-out‚Äù estimator of \\(m(\\cdot)\\) to be obtained when a spline function is fitted using the \\(n-1\\) observations: \\[\n(Y_1,X_1),\\dots,(Y_{i-1},X_{i-1}),(Y_{i+1},X_{i+1}),\\dots,(Y_{n},X_{n}).\n\\] Motivation: We have \\[\n\\begin{align*}\n&\\mathbb{E}_\\varepsilon(\\operatorname{CV}(p))\n= \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\biggl( \\overbrace{m(X_i)+\\varepsilon_i}^{=Y_i}-\n{\\hat m}_{p,-i}(X_i)\\biggr)^2\\right]\\\\[2ex]\n= & \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\left(\\left( m(X_i)-\n{\\hat m}_{p,-i}(X_i) \\right)^2 +2\\left( m(X_i)-\n{\\hat m}_{p,-i}(X_i) \\right)\\varepsilon_i +\\varepsilon_i^2\\right)\\right]\\\\[2ex]\n= &\\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_\\varepsilon\\left[\\left(m(X_i)-\n{\\hat m}_{p,-i}(X_i)\\right)^2\\right]}_{\\operatorname{MASE}(\\hat m_p)} \\\\[2ex]\n&+ 2\\frac{1}{n} \\sum_{i=1}^n\n\\underbrace{\\mathbb{E}_\\varepsilon\\left[( m(X_i)-\n{\\hat m}_{p,-i}(X_i))\\varepsilon_i\\right]}_{=0}+\\sigma^2\n\\end{align*}\n\\] Thus, \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{CV}(p)) = \\operatorname{MASE}(\\hat m_p) + \\sigma^2,\n\\end{align*}\n\\] such that \\[\n\\begin{align*}\np_{opt}\n&= \\arg\\min_p\\mathbb{E}_\\varepsilon(\\operatorname{CV}(p))\\\\[2ex]\n&= \\arg\\min_p\\operatorname{MASE}(\\hat m_p).\n\\end{align*}\n\\] That is, at least on average, minimizing \\(CV(p)\\) is equivalent to minimizing \\(\\operatorname{MASE}(\\hat m_p)\\).\n\\(k\\)-fold Cross-Validation (CV):  In practice, one usually works with \\(k\\)-fold CV (\\(k=5\\) or \\(k=10\\)). For this the index set \\(I=\\{1,\\dots,n\\}\\) is partitioned into \\(k\\) disjoint index sets \\(I_1,\\dots,I_k\\) of (roughly) equal sizes, i.e.¬†\\(|I_1|\\approx|I_2|\\approx\\dots\\approx|I_k|\\), such that \\(I_1\\cup I_1\\cup \\dots \\cup I_k=I\\) and \\(I_j\\cap I_k=\\emptyset\\) for all \\(j\\neq k\\). \\[\n\\operatorname{CV}_k(p)=\\frac{1}{k}\\sum_{k=1}^K\\frac{1}{|I_k|} \\sum_{i\\in I_k}\\left( Y_i-\n{\\hat m}_{p,-I_k}(X_i)\\right)^2.\n\\]\nGeneralized Cross-Validation (GCV): \\[\n\\operatorname{GCV}(p)=\\frac{1}{n\\left(1-\\frac{p}{n}\\right)^2}\\sum_{i=1}^n \\left( Y_i-\n{\\hat m}_p(X_i)\\right)^2\n\\] Motivation: The average residual sum of squares are given by \\[\n\\operatorname{ARSS}(p):=\\frac{1}{n}\\sum_{i=1}^n\\biggl( Y_i-\n\\hat{m}_{p}(X_i)\\biggr)^2\n\\tag{4.4}\\] which allows us to rewrite \\(\\operatorname{GCV}(p)\\) as \\[\n\\operatorname{GCV}(p)=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\operatorname{ARSS}(p)\n\\] Some lengthy derivations show that the expected value of \\(\\operatorname{ARSS}(p)\\) is \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\] Moreover, a Taylor expansion of \\(f(p)=\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\) around \\(f(0)\\) yields that \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation becomes precise as \\(\\frac{p}{n}\\to 0.\\) Thus \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{GCV}(p))\n&\\approx \\left(1 + 2\\frac{p}{n}\\right)\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p)) \\\\[2ex]\n&=\\left(1 + 2\\frac{p}{n}\\right) \\left(\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\\right)\\\\[2ex]\n&= \\operatorname{MASE}(\\hat m_p) +\\sigma^2 +\\\\[2ex]\n&+ \\underbrace{2\\frac{p}{n} \\operatorname{MASE}(\\hat m_p)}_{=O_p\\left(\\frac{p}{n}\\right)} - \\underbrace{4\\frac{p^2}{n^2} \\sigma^2}_{=O\\left(\\frac{p^2}{n^2}\\right)=o\\left(\\frac{p}{n}\\right)}\\\\[2ex]\n&= \\operatorname{MASE}(\\hat m_p) +\\sigma^2 + O_p\\left(\\frac{p}{n}\\right)\n\\end{align*}\n\\]\n\nThus, at least on average, minimizing \\(\\operatorname{GCV}(p)\\) is approximately (for \\(p_n/n\\to 0\\) as \\(n\\to\\infty\\)) equivalent to minimizing \\(\\operatorname{MASE}(\\hat m_p),\\) since \\(\\sigma^2\\) does not depend on \\(p.\\)\n\n\n\n\n\n\n\nOver-fitting\n\n\n\nNote that in econometrics (where we assumed to know the model apart from the model parameters) we used Equation¬†4.4 as an estimator for \\(\\sigma^2.\\) Moreover, Equation¬†4.4 is the main component to compute the \\(R^2\\) coefficient.\nIn non-parametrics, where we do not assume to know the model, but try to learn the model form the data, Equation¬†4.4 cannot be used as an estimator for \\(\\sigma^2\\), firstly, since we do not know \\(p\\), and secondly, since we expect \\(\\hat{m}_{p}\\) to be biased for each \\(p\\).\nIn fact, for a large enough \\(p,\\) the regression function \\(\\hat{m}_{p}\\) becomes so flexible that \\(Y_i\\approx \\hat{m}_{p}(X_i)\\) for all \\(i=1,\\dots,n\\) such that \\(\\operatorname{ARSS}(p)\\approx 0\\) and thus \\(R^2\\approx 1.\\)\nHowever, an estimate \\(\\hat{m}_{p}\\) for which \\(\\operatorname{ARSS}(p)\\approx 0,\\) typically has fitted the noise component \\(\\varepsilon\\)‚Äîadditionally to the signal component \\(m\\). Such an over-fitted estimate \\(\\hat{m}_{p}\\) will typically perform very poorly when used to predict a new outcomes \\(Y_{new}\\) for a given new predictor \\(X_{new}\\) by \\(\\hat{Y}_{new} = \\hat{m}_{p}(X_{new}),\\) i.e. \\[\nY_{new}\\not\\approx \\hat{m}_{p}(X_{new}).\n\\]\nThe term \\(2\\sigma^2\\frac{p}{n}\\) in \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\] is called the optimism of the model and quantifies the amount by which the in-sample average residual sum of squares (ARSS) systematically under-estimates the true mean average squared error (MASE) of \\(\\hat m_p.\\)\n\n\n\nR-Code to Compute an Estimate of the Optimal Smoothing Parameter\nFirst, we generate some data.\n\nset.seed(1)\n# Generate some data: #################\nn      <- 100     # Sample Size\nx_vec  <- (1:n)/n # Equidistant X \n# Gaussian iid error term \ne_vec  <- rnorm(n = n, mean = 0, sd = .5)\n# Dependent variable Y\ny_vec  <-  sin(x_vec * 5) + e_vec\n\nThen, compute the GCV scores for different numbers of basis functions \\(p\\) and plot them to select an estimate for the optimal value of the smoothing parameter \\(p\\).\n\np_vec <- 6:12 \nGCV_p <- numeric(length(p_vec))\n\nfor(j in 1:length(p_vec)){\n\np         <- p_vec[j] # number of basis functions\nq         <- p - 2    # number of equidistant knots \nknot_seq  <- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     <- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nS_p      <- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) \nm_hat_p  <- S_p   %*% y_vec\nARSS     <- mean(c(y_vec - m_hat_p)^2)\nGCV_p[j] <- ARSS/((1-p/n)^2)\n}\nplot(y = GCV_p, x = p_vec, type=\"o\")\n\n\n\n\nCompute the nonparametric regression estimate using the GCV optimal smoothing parameter \\(p=8.\\)\n\np         <- 8\nq         <- p - 2    # number of equidistant knots \nknot_seq  <- seq(from = 0, to = 1, len = q)# knots\n\nX_mat     <- splines2::bSpline(\n        x              = x_vec, # evaluation points\n        knots          = knot_seq[-c(1, length(knot_seq))], \n        degree         = degree,\n        intercept      = TRUE, \n        Boundary.knots = knot_seq[ c(1, length(knot_seq))]\n    )\n\nS_p      <- X_mat %*% solve(t(X_mat)  %*% X_mat)  %*% t(X_mat) \nm_hat_p  <- S_p   %*% y_vec\n\nLet‚Äôs plot the results:\n\nplot(y=y_vec, x=x_vec, xlab=\"X\", ylab=\"Y\", \n     main=\"Regression Splines\")\nlines(y=sin(x_vec * 5), x=x_vec, col=\"red\", lty=2, lwd=1.5)\nlines(y=m_hat_p, x=x_vec, col=\"blue\", lwd=1.5)\nlegend(\"bottomleft\", \n       c(\"(Unknown) Regression Function m\", \n         \"Regr.-Spline Fit with GCV optimal p=8\"), \n       col=c(\"red\",\"blue\"), \n       lty=c(2,1,1), lwd=c(2,2,2))"
  },
  {
    "objectID": "Ch4_NPRegression.html#exercises",
    "href": "Ch4_NPRegression.html#exercises",
    "title": "4¬† Nonparametric Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nShow that \\[\n\\frac{1}{\\left(1-\\frac{p}{n}\\right)^2}\\approx 1 + 2\\frac{p}{n},\n\\] where the approximation becomes precise as \\(\\frac{p}{n}\\to 0.\\)\n\n\nExercise 2.\nShow that \\[\n\\begin{align*}\n\\mathbb{E}_\\varepsilon(\\operatorname{ARSS}(p))=\\operatorname{MASE}(\\hat m_p)-2\\sigma^2\\frac{p}{n}+\\sigma^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "Ch4_NPRegression.html#references",
    "href": "Ch4_NPRegression.html#references",
    "title": "4¬† Nonparametric Regression",
    "section": "References",
    "text": "References\n\n\n\n\nDe Boor, Carl, and Carl De Boor. 1978. A Practical Guide to Splines. Vol. 27. springer.\n\n\nEubank, Randall L. 1999. Nonparametric Regression and Spline Smoothing. CRC press."
  },
  {
    "objectID": "Ch3_Bootstrap.html#bootstrap-and-linear-regression-analysis",
    "href": "Ch3_Bootstrap.html#bootstrap-and-linear-regression-analysis",
    "title": "3¬† The Bootstrap",
    "section": "3.5 Bootstrap and Linear Regression Analysis",
    "text": "3.5 Bootstrap and Linear Regression Analysis\nIn this chapter we consider two different bootstrap resampling procedures that can be applied in linear regression analysis. Section¬†3.5.1 considers the Bootstrapping Pairs algorithm and Section¬†3.5.2 considers the Residual Bootstrap algorithm. We begin with outlining the general setup.\nConsider the linear regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i,\\quad  i=1,\\dots,n,\n\\] where \\(Y_i\\in\\mathbb{R}\\) denotes the response (or ‚Äúdependent‚Äù) variable and \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p\n\\] denotes the vector of predictor variables. In the following, we differentiate between a random design and a fixed design.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.6 (Random and fixed design) \nRandom Design: \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. random variables with \\(\\mathbb{E}(\\varepsilon_i|X_i)=0,\\) \\(M=\\mathbb{E}(X_iX_i^T)\\) non-singular, and with either\n\nhomoskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0\\), \\(i=1,\\dots,n\\), for a constant \\(\\sigma^2<\\infty\\) or\nheteroskedastic errors: \\(\\mathbb{E}(\\varepsilon_i^2|X_i)=\\sigma^2_0(X_i)<\\infty\\), \\(i=1,\\dots,n.\\)\n\nFixed Design: \\[\nX_1, X_2, \\dots, X_n\n\\] are deterministic vectors in \\(\\mathbb{R}^p\\) and \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are i.i.d. random variables with zero mean, \\(\\mathbb{E}(\\varepsilon_i)=0,\\) and homoskedastic errors, \\(\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0,\\) for all \\(i=1,\\dots,n.\\)\n\n\n\nThe least squares estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p\\) is given by \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i.\n\\end{align*}\n\\]\nUsing that \\(Y_i=X_i^\\top\\beta_0+\\varepsilon_i,\\) one can derive that \\[\n\\begin{align*}\n\\hat\\beta_n\n&=\\beta_0+\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i.\n\\end{align*}\n\\]\n\n3.5.1 Bootstrap under Random Design: Bootstrapping Pairs\nUnder a random design (Definition¬†3.6), we assume that there exists a non-singular (thus invertible) matrix \\(M\\) \\[\nM=\\mathbb{E}(X_iX_i^T).\n\\] This implies that the following matrix \\(Q\\) is also non-singular: \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\varepsilon_i^2X_iX_i^T)\\\\[2ex]\n&=\\mathbb{E}(\\mathbb{E}(\\varepsilon_i^2X_iX_i^T|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T\\mathbb{E}(1|X_i))\\\\[2ex]\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIn case of homoskedastic errors, we have that \\[\n\\begin{align*}\nQ\n&=\\mathbb{E}(\\sigma^2_0(X_i)X_iX_i^T)\\\\\n&=\\sigma^2_0\\;\\mathbb{E}(X_iX_i^T)\\\\[2ex]\n&=\\sigma^2_0\\;M.\n\\end{align*}\n\\]\n\n\nThe law of large numbers, the continuous mapping theorem, Slutsky‚Äôs theorem, and the central limit theorem (see econometrics lecture) implies that \\[\n\\sqrt{n}(\\hat\\beta_n-\\beta_0)\\rightarrow_d\\mathcal{N}_p(0,M^{-1}QM^{-1}),\\quad n\\to\\infty,\n\\] where \\(\\mathcal{N}_p(0,M^{-1}QM^{-1})\\) denotes the \\(p\\)-dimensional normal distribution with \\((p\\times 1)\\)-dimensional mean \\(0\\) and \\((p\\times p)\\)-dimensional variance-covariance matrix \\(M^{-1}QM^{-1}.\\)\nThe idea of bootstraping pairs is very simple: The procedure builds upon the assumption that \\[\n(Y_1,X_1), (Y_2,X_2), \\dots, (Y_n,X_n)\n\\] are i.i.d. which suggests a bootstrap based on resampling the pairs \\((Y_i,X_i),\\) \\(i=1,\\dots,n.\\)\nBootstraping Pairs Algorithm:\n\nGenerate bootstrap samples \\[\n  (Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*)\n  \\] by drawing observations independently and with replacement from \\({\\cal S}_n.\\)\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1^*),\\dots,(Y_n^*,X_n^*):\\) \\[\n\\hat\\beta^*_n=\\left(\\sum_{i=1}^n X_i^*X_i^{*T}\\right)^{-1}\\sum_{i=1}^n X_i^*Y_i^*\n\\]\n\nRepeating Steps 1-2 \\(m\\)-many times yields \\(m\\) (e.g.¬†\\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\nIt can be shown that bootstrapping pairs is consistent; i.e.¬†that for large \\(n\\) \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\\approx\\mathcal{N}_p(0,M^{-1}QM^{-1})\n\\]\n\n\n3.5.2 Bootstrap under Fixed Design: The Residual Bootstrap\nIf the sample \\[\n(Y_1,X_1),\\dots,(Y_n,X_n)\n\\] is not an i.i.d. sample, the bootstrapping pairs procedure (Section¬†3.5.1) proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for fixed designs and also generally not in time-series regression contexts. However, if error terms are homoskedastic, then it is possible to rely on the residual bootstrap.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] under fixed design (Definition¬†3.6), where \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d. with zero mean \\[\n\\mathbb{E}(\\varepsilon_i)=0\n\\] and homoskedastic errors \\[\n\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_0.\n\\]\n\n\n\n\n\n\nApplicability of the Residual Bootstrap under Random Designs\n\n\n\nThough we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs‚Äîeven when the \\(X\\)-variables are correlated (e.g.¬†time-series).\nIn such cases, the following arguments are meant conditionally on the observed predictors \\(X_1,\\dots,X_n\\).\nThe above assumptions on the error terms then, of course, also have to be satisfied conditionally on \\(X_1,\\dots,X_n.\\)\n\n\nThe idea of the residual bootstrap is very simple: The procedure builds upon the assumption that the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are i.i.d which suggests a bootstrap based on resampling the error terms.\nThese errors are, of course, unobserved, but they can be approximated by the corresponding residuals \\[\n\\hat \\varepsilon_i=Y_i-X_i^T\\hat\\beta_n, \\quad i=1,\\dots,n,\n\\] where \\[\n\\hat\\beta_n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i\n\\] denotes the least squares estimator based on the original sample \\(\\mathcal{S}_n\\).\nIt is well known that \\[\n\\hat\\sigma^2_n= \\frac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2\n\\] provides a consistent estimator of the error variance \\(\\sigma^2\\). That is, \\[\n\\hat\\sigma^2_n\\rightarrow_p \\sigma_0^2\n\\] as \\(n\\to\\infty.\\)\nResidual Bootstrap Algorithm:\nBased on the original data \\((Y_i,X_i)\\), \\(i=1,\\dots,n\\), and the least squares estimate \\(\\hat\\beta_n\\), calculate the residuals \\(\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\).\n\nGenerate random bootstrap samples \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) of residuals by drawing observations independently and with replacement from \\[\n{\\cal S}_n:=\\{\\hat\\varepsilon_1,\\dots,\\hat \\varepsilon_n\\}.\n\\]\nCalculate new depend variables \\[\nY_i^*=X_i^T\\hat\\beta_n + \\hat\\varepsilon_i^*,\\quad i=1,\\dots,n\n\\]\nBootstrap estimators \\(\\hat\\beta^*_n\\) are determined by least squares estimation from the data \\((Y_1^*,X_1),\\dots,(Y_n^*,X_n)\\): \\[\n\\hat\\beta^*_n = \\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_iY_i^*\n\\]\n\nRepeating Steps 1-3 \\(m\\) many times yields \\(m\\) (e.g.¬†\\(m=10,000\\)) bootstrap estimates \\[\n\\hat\\beta^*_{n,1},\\hat\\beta^*_{n,2},\\dots,\\hat\\beta^*_{n,m}\n\\] which allow us to approximate the bootstrap distribution \\(\\hat\\beta^*_n-\\hat\\beta_n|\\mathcal{S}_n\\) arbitrarily well as \\(m\\to\\infty.\\)\n\nMotivating the Residual Bootstrap\nIt is not difficult to understand why the residual bootstrap generally works for homoskedastic (!) errors. We have \\[\n\\hat\\beta_n-\\beta_0=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\varepsilon_i\n\\] and for large \\(n\\) we have that \\[\n\\sqrt{n}(\\hat\\beta_n - \\beta_0)\\to_d\\mathcal{N}_p(0,\\sigma^2_0 M^{-1}),\n\\] where \\(\\mathcal{N}_p(0,\\sigma^2 M^{-1})\\) denotes the \\(p\\) dimensional normal distribution with \\((p\\times 1)\\) mean \\(0\\) and \\((p\\times p)\\) variance-covariance matrix \\(\\sigma^2_0 M^{-1}.\\)\nOn the other hand (the bootstrap world), we have the construction \\[\n\\hat\\beta^*_n - \\hat\\beta_n\n=\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^n X_i\\hat\\varepsilon_i^*.\n\\] Conditionally on \\({\\cal S}_n,\\) the bootstrap error terms \\(\\hat\\varepsilon_1^*,\\dots,\\hat\\varepsilon_n^*\\) are i.i.d with \\[\n\\mathbb{E}(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i =0\n\\] and \\[\nVar(\\hat\\varepsilon_i^*| {\\cal S}_n)=\\frac{1}{n} \\sum_{i=1}^n \\hat \\varepsilon_i^2 = \\hat\\sigma^2_n,\n\\] where \\(\\hat\\sigma^2_n\\to\\sigma^2_0\\) as \\(n\\to\\infty.\\)\nAn appropriate central limit theorem argument implies that \\[\n\\left.\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n)\\right|\\mathcal{S}_n\\to_d\\mathcal{N}_p\\left(0,\\sigma^2_0\\, M\\right),\n\\] as \\(n\\to\\infty,\\) assuming that \\(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T\\to M\\) as \\(n\\to\\infty.\\)\nThat is, for large \\(n\\), we have that \\[\n\\text{distribution}(\\sqrt{n}(\\hat\\beta^*_n-\\hat\\beta_n) |{\\cal S}_n)\n\\approx\\underbrace{\\text{distribution}(\\sqrt{n}(\\hat\\beta_n-\\beta_0))}_{\\mathcal{N}_p\\left(0,\\sigma^2_0\\, M\\right)}\n\\]\n\n\n\n3.5.3 Bootstrap under Fixed Design: The Wild Bootstrap\nThe wild bootstrap is a method for generating bootstrap samples that do not consist of resampling the original data (bootstrapping pairs in Section¬†3.5.1) or residuals (bootstrapping residuals in Section¬†3.5.2). Rather, the wild bootstrap combines the data with random variables drawn from a known distribution to form a bootstrap sample.\nThe wild bootstrap provides a way to deal with issues such as heteroskedasticity of unknown form in fixed-design regression models or random-design models in which one conditions on the predictors.\nIn the following we will formally assume a regression model \\[\nY_i=X_i^T\\beta_0 + \\varepsilon_i, \\quad i=1,\\dots,n,\n\\] with \\[\nX_i:=(\\underbrace{X_{i1}}_{=1},X_{i2},\\ldots,X_{ip})^T\\in\\mathbb{R}^p,\n\\] where the \\(X_i\\)‚Äôs are fixed in repeated samples (fixed design), and where the error terms \\[\n\\varepsilon_1,\\dots,\\varepsilon_n\n\\] are independent across \\(i=1,\\dots,n,\\) with\n\\[\n\\mathbb{E}(\\varepsilon_i)=0\\quad\\text{for all}\\quad i=1,\\dots,n,\n\\] and possibly heteroskedastic variances (\\(\\sigma^2_{0,i}\\neq \\sigma^2_{0,j}\\) for \\(i\\neq j=1,\\dots,n\\)) \\[\n0<\\mathbb{E}(\\varepsilon_i^2)=\\sigma^2_{0,i}<\\infty\\quad\\text{for all}\\quad i=1,\\dots,n.\n\\]\nThat is, the data generating process is here independent across \\(i=1,\\dots,n,\\) but not necessarily identically distributed across \\(i=1,\\dots,n.\\)\n\n\n\n\n\n\nApplicability of the Wild Bootstrap under Random Designs\n\n\n\nThough we will formally rely on a fixed design assumption, the wild bootstrap (as the residual bootstrap) is also applicable for random designs.\nIn random designs, the following arguments are meant conditionally on the observed predictors \\(X_1,\\dots,X_n\\).\nThe above assumptions on the error terms then, of course, also have to be satisfied conditionally on \\(X_1,\\dots,X_n.\\)\n\n\nAs the residual bootstrap (Section¬†3.5.2), the wild bootstrap uses the \\(X_i\\)‚Äôs from the original data. I.e., the \\(X_i\\)‚Äôs are not resampled. The wild bootstrap generates bootstrap samples \\[\n\\{(Y_1^\\ast,X_1),\\dots,(Y_n^\\ast,X_n)\\}\n\\] from \\[\nY_i^\\ast=X_i^\\top\\hat\\beta_n + \\varepsilon^\\ast_i,\\quad i=1,\\dots,n,\n\\] where the \\(\\varepsilon_i^\\ast\\)‚Äôs are generated by either of the following two methods:\n\nLet \\[\n\\hat\\varepsilon_i=Y_i - X_i^\\top\\hat\\beta_n,\\quad i=1,\\dots,n,\n\\]\nbe the OLS residuals. For each \\(i=1,\\dots,n,\\) let \\(F_i\\) be the \\(i\\)-specific distribution of a discrete two-point random variable \\[\nW_i\\in\\left\\{\\left(1-\\sqrt{5}\\right)\\hat\\varepsilon_i,\\;\\left(1+\\sqrt{5}\\right)\\frac{\\hat\\varepsilon_i}{2}\\right\\}\n\\] with \\[\n\\begin{align*}\nP\\left(W_i = \\left(1-\\sqrt{5}\\right)\\hat\\varepsilon_i\\right) &= \\frac{1+\\sqrt{5}}{2\\sqrt{5}}\\\\[2ex]\nP\\left(W_i = \\left(1+\\sqrt{5}\\right)\\frac{\\hat\\varepsilon_i}{2}\\right)&=1 - \\frac{1+\\sqrt{5}}{2\\sqrt{5}}.\n\\end{align*}\n\\] Under this construction, we have that \\[\n\\begin{align*}\n\\mathbb{E}(W_i)  &=0\\\\[2ex]\n\\mathbb{E}(W_i^2)&=\\hat\\varepsilon_i^2\\\\[2ex]\n\\mathbb{E}(W_i^3)&=\\hat\\varepsilon_i^3.\n\\end{align*}\n\\] The wild bootstrap uses \\[\n\\varepsilon_i^\\ast = W_i\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] to generate the bootstrap samples. Mammen (1993) provides a detailed discussion of the properties of this method.\nThe second method is an example of the multiplier bootstrap, meaning that the \\(\\varepsilon_i^\\ast\\)‚Äôs are multiples of transformations of the residuals \\(\\hat{\\varepsilon}_i\\) and independent random variables. Specifically, let \\(U_i,\\) \\(i=1,\\dots,n,\\) be real valued random variables that are independent from each other and independent of the residuals \\(\\hat{\\varepsilon}_i.\\) Let \\(\\mathbb{E}(U_i)=0\\) and \\(\\mathbb{E}(U_i^2)=1\\) for all \\(i=1,\\dots,n.\\) One possibility would be \\(U_i\\sim\\mathcal{N}(0,1).\\) Let \\(f(\\hat{\\varepsilon}_i)\\) be a transformation of the residuals, where \\(f(\\hat{\\varepsilon}_i)=\\hat{\\varepsilon}_i\\) is one possibility. The multiplier bootstrap uses \\[\n\\varepsilon_i^\\ast = U_i\\,f(\\hat{\\varepsilon}_i)\\quad\\text{for each}\\quad i=1,\\dots,n\n\\] to generate the bootstrap samples. Davidson and Flachaire (2008) discuss properties of this method.\n\nRepeatedly generating new bootstrap errors \\[\n\\varepsilon^\\ast_1,\\dots,\\varepsilon^\\ast_n,\n\\] allows us to repeatedly generate new bootstrap samples \\[\n\\{(\\underbrace{X_1^\\top\\hat\\beta_n + \\varepsilon^\\ast_1}_{=Y_1^\\ast},X_1),\\dots,(\\underbrace{X_n^\\top\\hat\\beta_n + \\varepsilon^\\ast_n}_{=Y_n^\\ast},X_n)\\},\n\\] which allows us to repeatedly generate new bootstrap realizations \\[\n\\hat\\beta_n^\\ast = \\left(X^\\top X\\right)^{-1}X^\\top Y^\\ast.\n\\]\n\n\n3.5.4 Bootstrap Confidence Intervals for the \\(j\\)th Component of the Regression Coefficient \\(\\beta_{0,j}\\)\nThis chapter introduces two confidence intervals. The first uses the basic bootstrap method (Section¬†3.3); the second uses the bootstrap-\\(t\\) method (Section¬†3.4).\nBoth confidence intervals can be constructed either via bootstrapping pairs (Section¬†3.5.1) or via bootstrapping residuals (Section¬†3.5.2).\n\n\n\n\n\n\nTip\n\n\n\nWhile the bootstrap confidence intervals based on bootstrapping pairs (Section¬†3.5.1) are heteroskedasticity robust, the bootstrap confidence intervals based on bootstrapping residuals are only valid for homoskedastic errors.\n\n\n\n3.5.4.1 Basic Bootstrap Confidence Intervals for \\(\\beta_{0,j}\\)\nLet \\(\\beta_{0,j}\\in\\mathbb{R}\\), \\(j=1,\\dots,p\\), denote the \\(j\\)th component of \\(\\beta_0\\in\\mathbb{R}^p,\\) and let \\(\\hat{\\beta}_{j,n}\\in\\mathbb{R}\\) denote the \\(j\\)th component of the estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p.\\)\nThe basic bootstrap confidence interval for \\(\\beta_{0,j}\\in\\mathbb{R}\\) can be constructed as following:\n\nUse either bootstrapping pairs (Section¬†3.5.1) or bootstrapping residuals (Section¬†3.5.2) or the wild bootstrap (Section¬†3.5.3) to generate \\(m\\) (e.g.¬†\\(m=10,000\\)) bootstrap realizations \\[\n\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast.\n\\]\nDetermine the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) from the bootstrap realizations \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast\\) using Equation¬†3.7.\nCompute the approximate \\((1-\\alpha)\\times 100\\%\\) confidence interval as in Equation¬†3.8: \\[\n\\left[2\\hat\\beta_{nj}-\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j},\n   2\\hat\\beta_{nj}-\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\right],\n\\] where \\(\\hat\\beta_{nj}\\) denotes the \\(j\\)th component of \\(\\hat\\beta_{n}\\) computed from the original sample \\(\\mathcal{S}_n,\\) and where the empirical quantiles \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) are computed from the bootstrap estimates \\(\\hat{\\beta}_{j,n,1}^\\ast,\\dots,\\hat\\beta_{j,n,m}^\\ast.\\)\n\n\n\n\n\n\n\nRemark\n\n\n\nThis basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval for homoskedastic errors, but also for heteroskedastic errors. In case of heteroskedastic errors, one needs to use an appropriate boostrap such as the pairs boostrap (random design) or the wild bootstrap (fixed design).\nNote that standard confidence intervals usually provided by statistical software packages are for homoskedastic errors. For instance, the confint(object) function in R for an object returned by the lm() function uses the standard error formula for homoskedastic errors.\n\n\n\n\n\n3.5.4.2 Bootstrap-\\(t\\) Confidence Intervals for \\(\\beta_{0,j}\\)\nBootstrap-\\(t\\) confidence intervals for the regression coefficients \\(\\beta_{0,j}\\), \\(j=1,\\dots,p,\\) can be constructed as following:\nConsider the statistic \\[\nT_n = \\frac{\\hat\\beta_{j,n} -\\beta_{0,j}}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})},\n\\] where\n\n\\(\\beta_{0,j}\\) denotes the \\(j\\)th element of \\(\\beta_0\\in\\mathbb{R}^p,\\)\n\\(\\hat\\beta_{j,n}\\) denotes the \\(j\\)th element of the OLS estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p.\\)\n\nIn the case of homoskedastic error terms \\[\n\\begin{align*}\n\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\n&=\\frac{\\hat{\\sigma}_n\\sqrt{\\hat{\\gamma}_{jj,n}}}{\\sqrt{n}},\n\\end{align*}\n\\] where \\[\n\\hat{\\sigma}_n=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\hat{\\varepsilon}_i^2}\n\\] and where \\[\n\\hat{\\gamma}_{jj,n}\n=\\left[\\widehat{M}_n^{-1}\\right]_{jj}\n=\\left[\\left(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^\\top\\right)^{-1}\\right]_{jj}\n\\] denotes the \\(j\\)-th diagonal element of the \\((p\\times p)\\)-dimensional matrix \\(\\widehat{M}_n^{-1}=(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}.\\)\nIn case of heteroskedastic errors, one can use \\[\n\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\n=\\sqrt{\\left[\\widehat{M}_n^{-1}\\widehat{Q}_n\\widehat{M}_n^{-1}\\right]_{jj}}\n\\] where\n\n\\(\\widehat{M}_n^{-1}=(\\frac{1}{n}\\sum_{i=1}^n X_iX_i^T)^{-1}\\)\n\\(\\widehat{Q}_n=\\widehat{\\mathbb{E}}(\\varepsilon_i^2X_iX_i^\\top)\\) denotes a Heteroskedasticity Consistent (HC) estimators of \\(\\mathbb{E}(\\varepsilon_i^2X_iX_i^\\top);\\) e.g.¬†the HC2-estimator \\(\\widehat{\\mathbb{E}}(\\varepsilon_i^2X_iX_i^\\top)=\\frac{1}{n-p}\\sum_{i=1}^n\\hat\\varepsilon_i^2X_iX_i^\\top.\\)\n\nNote that \\(T_n\\) is an asymptotically pivotal statistic; i.e., \\[\nT_n=\\sqrt{n}\\frac{(\\hat\\beta_{n,j}-\\beta_{0,j})}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})}\\rightarrow_d\\mathcal{N}(0,1),\\quad n\\to\\infty.\n\\]\nA bootstrap-\\(t\\) interval for \\(\\beta_{0,j}\\), \\(j=1,\\dots,p\\), can thus be constructed as follows:\n\nUse\n\n\nbootstrapping pairs (Section¬†3.5.1) for random designs,\nbootstrapping residuals (Section¬†3.5.2) for fixed designs and homoskedastic errors, or\nwild bootstrap (Section¬†3.5.3) for fixed desgins and heteroskedastic errors, to generate \\(m\\) (e.g.¬†\\(m=10,000\\)) bootstrap realizations \\[\nT^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*,\n\\] with \\[\nT^*_{n,k}=\\frac{\\hat\\beta_{n,j}^*-\\hat\\beta_{0,j}}{\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})},\\quad k=1,\\dots,m,\n\\] where\n\n\\(\\hat\\beta_{0,j}\\) is computed from the original sample\n\\(\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})\\) is an appropriate estimate of the standard error (see above)\nthe residual components in \\(\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})\\) are resampled\nthe \\(X\\)-components in \\(\\widehat{\\operatorname{SE}}(\\hat\\beta^*_{j,n})\\) are resampled only in random designs, but kept fix in fixed desgins.\n\n\n\nCompute the empirical \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles \\(\\hat q^\\ast_{n,\\frac{\\alpha}{2},j}\\) and \\(\\hat q^\\ast_{n,1-\\frac{\\alpha}{2},j}\\) (see Equation¬†3.7) from the bootstrap estimates \\(T^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*.\\)\nCompute the \\((1-\\alpha)\\times 100\\%\\) bootstrap-\\(t\\) confidence interval as in Equation¬†3.9: \\[\n\\left[\n  \\hat\\beta_{j,n}-\\hat q^\\ast_{1-\\frac{\\alpha}{2},n,j}\\;\\left(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\right),\\;\n  \\hat\\beta_{j,n}-\\hat q^\\ast_{\\frac{\\alpha}{2},n,j}\\;\\left(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\right)\n\\right],\n\\] where \\(\\hat\\beta_{n,j}\\) and \\(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\) are computed from the original sample \\(\\mathcal{S}_n=((Y_1,X_1),\\dots,(Y_n,X_n)),\\) and where the empirical quantiles \\(\\hat q^*_{n,1-\\frac{\\alpha}{2}}\\) and \\(\\hat q^*_{n,\\frac{\\alpha}{2}}\\) are computed from the bootstrap realizations \\(T^*_{n,1},T_{n,2}^*,\\dots, T_{n,m}^*.\\)\n\n\n\n\n\n\n\nRemark\n\n\n\nIn case of heteroskedastic errors, one needs to use an appropriate bootstrap version (e.g.¬†bootstrapping pairs (random desgin) or wild bootstrap (fixed desgin)) and an appropriate (heteroskedasticity robust) version of \\(\\widehat{\\operatorname{SE}}(\\hat{\\beta}_{j,n}^\\ast).\\)\n\n\n\n\n\n3.5.5 Statistical Hypothesis Testing\nIn the following, we consider a fixed design, where one can use the residual bootstrap (Section¬†3.5.2) or the wild bootstrap (Section¬†3.5.3).\nSuppose we want to test the hypothesis \\[\n\\begin{align*}\nH_0                    &: \\beta_{0,j}    = 0\\\\[2ex]\n\\text{against}\\quad H_1&: \\beta_{0,j} \\neq 0\n\\end{align*}\n\\] using the test statistic \\[\nT_n = \\frac{\\hat\\beta_{j,n} - 0}{\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})},\n\\] where\n\n\\(\\beta_{0,j}\\) denotes the \\(j\\)th element of \\(\\beta_0\\in\\mathbb{R}^p,\\)\n\\(\\hat\\beta_{j,n}\\) denotes the \\(j\\)th element of the OLS estimator \\(\\hat\\beta_n\\in\\mathbb{R}^p.\\)\n\\(\\widehat{\\operatorname{SE}}(\\hat\\beta_{j,n})\\) is an appropriate estimate of the standard error (see above)\n\nThe \\(p\\)-value is given by \\[\n\\begin{align*}\n&p_{obs} = \\\\[2ex]\n&2\\,\\min\\left\\{P(T_n \\geq T_{n,obs}|H_0\\;\\text{is true}),\\;P(T_n \\leq T_{n,obs}|H_0\\;\\text{is true})\\right\\}\n\\end{align*}\n\\] where \\(T_{n,obs}\\) is the value of the test statistic computed from the original sample \\[\n\\mathcal{S}_n=\\left\\{(Y_1,X_1),\\dots,(Y_n,X_n)\\right\\}.\n\\]\nTo conduct the test using the bootstrap, we have to estimate \\(p_{obs}\\) by the bootstrap algorithm.\nCentral question: How to generate bootstrap samples under \\(H_0\\)?\nTo estimate \\(\\beta_0\\in\\mathbb{R}^p\\) under \\(H_0,\\) we need to estimate all elements in \\(\\beta_0\\) that are not specified/fixed by \\(H_0\\) leaving the other elements at their \\(H_0\\)-values.\nLet \\(\\beta^{H_0}_0\\in\\mathbb{R}^{(p-1)}\\) denote the parameter vector that contains all elements of \\(\\beta_0\\in\\mathbb{R}^p\\) that are not specified by \\(H_0.\\)  The estimator of \\(\\beta^{H_0}_0\\in\\mathbb{R}^{(p-1)}\\) is then given by \\[\n\\underset{((p-1)\\times 1)}{\\hat{\\beta}^{H_0}_{n}}=\\left(\\tilde{X}^\\top\\tilde{X}\\right)^{-1}\\tilde{X}^\\top Y\n\\] where the \\((n\\times (p-1))\\)-matrix \\(\\tilde{X}\\) is the matrix \\(X\\) with the \\(j\\)th column removed.\nUsing \\(\\hat{\\beta}^{H_0}_{n},\\) we can compute the \\((n\\times 1)\\)-vector of residuals under \\(H_0\\) as \\[\n\\left(\\begin{matrix}\\hat{\\varepsilon}^{H_0}_1\\\\ \\vdots\\\\\\hat{\\varepsilon}^{H_0}_n\\end{matrix}\\right)=\\hat{\\varepsilon}^{H_0}=Y-\\tilde{X}\\hat{\\beta}^{H_0}_{n}.\n\\]\nBootstrap algorithm:\n1.1 Residual Bootstrap: Draw independently and with replacement \\(n\\) values from \\[\n\\{\\hat{\\varepsilon}^{H_0}_1, \\dots, \\hat{\\varepsilon}^{H_0}_n\\}\n\\] to generate bootstrap realizations under \\(H_0\\) \\[\n\\{\\hat{\\varepsilon}^{H_0\\ast}_1, \\dots, \\hat{\\varepsilon}^{H_0\\ast}_n\\}.\n\\] These allow us to generate the bootstrap samples under \\(H_0,\\) \\[\n\\left\\{(Y_1^{H_0\\ast},X_1),\\dots,(Y_n^{H_0\\ast},X_n)\\right\\},\n\\] where \\[\nY_i^{H_0\\ast} = \\tilde{X}_i^\\top \\hat{\\beta}^{H_0}_{n} + \\hat{\\varepsilon}^{H_0\\ast}_i,\\quad i=1,\\dots,n.\n\\]\n1.2 Wild Bootstrap: Use\n\\[\n\\{\\hat{\\varepsilon}^{H_0}_1, \\dots, \\hat{\\varepsilon}^{H_0}_n\\}\n\\] to generate wild bootstrap errors under \\(H_0\\) \\[\n\\varepsilon^{H_0\\ast}_i=\\left\\{\n  \\begin{array}{ll}\n  (1-\\sqrt{5})\\hat{\\varepsilon}^{H_0}_i&\\text{with propability }(1+\\sqrt{5})/2\\sqrt{5}\\\\\n  (1+\\sqrt{5})\\hat{\\varepsilon}^{H_0}_i/2&\\text{with propability }1-(1+\\sqrt{5})/2\\sqrt{5}\n  \\end{array}\n\\right.\n\\] These allow us to generate the bootstrap samples under \\(H_0,\\) \\[\n\\left\\{(Y_1^{H_0\\ast},X_1),\\dots,(Y_n^{H_0\\ast},X_n)\\right\\},\n\\] where \\[\nY_i^{H_0\\ast} = \\tilde{X}_i^\\top \\hat{\\beta}^{H_0}_{n} + \\varepsilon^{H_0\\ast}_i,\\quad i=1,\\dots,n.\n\\] 2. Based on the bootstrap sample \\[\n\\left\\{(Y_1^{H_0\\ast},X_1),\\dots,(Y_n^{H_0\\ast},X_n)\\right\\}\n\\] we can compute the bootstrap realization of the OLS estimator under \\(H_0,\\) \\[\n\\hat{\\beta}^{\\ast}_{n}=\\left(X^\\top X\\right)^{-1} X^\\top Y^{H_0\\ast},\n\\] which allows us to generate the corresponding realization of the test statistic \\[\nT_n^{\\ast}\n\\] under \\(H_0.\\)\n\nRepeating Steps 1-2 leads to \\(m\\)-many (e.g.¬†\\(m=10,000\\)) bootstrap realizations of the test statistic\n\\[\nT_{n,1}^{\\ast},\\dots,T_{n,m}^{\\ast}\n\\] under \\(H_0.\\)\n\nTo estimate the unknown \\(p_{obs},\\) we can use now the following estimator \\[\n\\begin{align*}\n&\\hat p_{obs} = \\\\[2ex]\n&=2\\,\\min\\left\\{\\hat{P}(T_n \\geq T_{n,obs}|H_0\\;\\text{is true}),\\;\\hat{P}(T_n \\leq T_{n,obs}|H_0\\;\\text{is true})\\right\\}\\\\[2ex]\n&=2\\,\\min\\left\\{\n  \\frac{1}{m}\\sum_{j=1}^m 1_{\\left(T_{n,j}^{\\ast} \\geq T_{n,obs}\\right)},\\;\n  \\frac{1}{m}\\sum_{j=1}^m 1_{\\left(T_{n,j}^{\\ast} \\leq T_{n,obs}\\right)}\n  \\right\\}\n\\end{align*}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nIn case of heteroskedasticity, the wild bootstrap and a corresponding formula for \\(\\widehat{\\operatorname{SE}}(\\hat{\\beta}_j)\\) has to be used."
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html",
    "href": "Ch5_FunctionalDataAnalysis.html",
    "title": "5¬† Functional Data Analysis",
    "section": "",
    "text": "Literature:\nR-packages used in this chapter:"
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html#from-raw-data-to-functional-data",
    "href": "Ch5_FunctionalDataAnalysis.html#from-raw-data-to-functional-data",
    "title": "5¬† Functional Data Analysis",
    "section": "5.1 From Raw Data to Functional Data",
    "text": "5.1 From Raw Data to Functional Data\nThe simplest raw data set encountered in functional data analysis is a sample of the form: \\[\n\\begin{align}\nX_i(t_j),\\quad t_j\\in[a,b],\\quad i=1,\\dots,n \\quad j=1,\\dots,J.\n\\end{align}\n\\] The theoretical objects we wish to study are smooth curves: \\[\n\\begin{align}\nX_i(t),\\quad t\\in[a,b],\\quad i=1,\\dots,n \\quad j=1,\\dots,J.\\\\[4ex]\n\\end{align}\n\\]\n\npar(mfrow=c(1,2), mar=c(5.1, 4.1, 2.1, 2.1))\nmatplot(x=growth$age, y=growth$hgtf[,1:5], type=\"p\", lty=1, pch=21, \n        xlab=\"Age\", ylab=\"Height (cm)\", cex.lab=1.2,col=1:5,bg=1:5,\n        main=expression(X[i](t[j])))\nmatplot(x=growth$age, y=growth$hgtf[,1:5], type=\"l\", lty=1, pch=1, \n        xlab=\"Age\", ylab=\"Height (cm)\", cex.lab=1.2,\n        main=expression(X[i](t)))\npar(mfrow=c(1,1), mar=c(5.1, 4.1, 4.1, 2.1))\n\n\n\n\nFigure¬†5.5: Actually observed data points \\(X_i(t_j)\\) versus the theoretical objects \\(X(t).\\)\n\n\n\n\nPre-Processing the Data using Basis Expansions:\nTypically, the first step in working with functional data is to express the functional data by means of a basis expansion \\[\nX_i(t)\\approx\\sum_{m=1}^M c_{im} B_m(t),\\quad t\\in[a,b],\n\\] where \\(B_1(t),\\dots,B_M(t)\\) are some standard collection of basis functions like:\n\nsplines\nwavelets\nsine and cosine functions\netc.\n\nExample: B-spline basis functions \\(B_1(t),\\dots,B_M(t)\\):\n\nbspl_bf <- fda::create.bspline.basis(rangeval = c(0,1), \n                                     norder   = 3, \n                                     breaks   = seq(0,1,len=7))\nplot(bspl_bf, \n     main = \"B-spline Basis Functions\", \n     xlab = \"[a,b]=[0,1]\")\n\n\n\n\n\n\n\n\nExample: Fourier basis functions \\(B_1(t),\\dots,B_M(t)\\):\n\nfourier_bf <- fda::create.fourier.basis(rangeval = c(0,1), \n                                        nbasis   = 5)\nplot(fourier_bf, \n     main = \"Fourier Basis Functions\", \n     xlab = \"[a,b]=[0,1]\")\n\n\n\n\n\n\n\n\nExample: Pre-processing the data of the Berkeley growth study\n\nSmObj <- fda::smooth.basisPar(growth$age, \n                              y = growth$hgtf[,1:5],lam=0.1)\n\nresult <- plot(SmObj$fd, \n     xlab = \"Age (years)\", \n     ylab = \"Height (cm)\", cex.lab=1.2,\n     main = \"5 Girls in Berkeley Growth Study\", lty=1)\n\n\n\n\n\n\n\n\nExample: 1st derivative of the functional data from the Berkeley growth study\n\nresult <- plot(fda::deriv.fd(SmObj$fd, 1), \n     xlab = \"Age (years)\", \n     ylab = \"Growth Rate (cm / year)\", cex.lab=1.2,\n     main = \"5 Girls in Berkeley Growth Study (1st Derivative)\", lty=1)\n\n\n\n\n\n\n\n\nExample: 2nd derivative of the functional data from the Berkeley growth study\n\nresult <- plot(fda::deriv.fd(SmObj$fd, 2), \n     xlab = \"Age (years)\", \n     ylab = \"Growth Rate (cm / year)\", cex.lab=1.2,\n     main = \"5 Girls in Berkeley Growth Study (1st Derivative)\", lty=1)"
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html#sample-mean-and-covariance",
    "href": "Ch5_FunctionalDataAnalysis.html#sample-mean-and-covariance",
    "title": "5¬† Functional Data Analysis",
    "section": "5.2 Sample Mean and Covariance",
    "text": "5.2 Sample Mean and Covariance\nPointwise mean: \\[\n\\bar{X}_n(t)=\\frac{1}{n}\\sum_{i=1}^n X_i(t)\n\\]\nPointwise standard deviation: \\[\nS_n(t)=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n\\Big(X_i(t)-\\bar{X}_n(t)\\Big)^2}\n\\]\nPointwise covariance function: \\[\n\\hat{c}_n(t,s)=\\frac{1}{n-1}\\sum_{i=1}^n\\Big(X_i(t)-\\bar{X}_n(t)\\Big)\\Big(X_i(s)-\\bar{X}_n(s)\\Big)\n\\]\nThe sample covariance function is extensively used in Functional Data Analysis. The interpretation of the values of \\(\\hat{c}_n(t,s)\\) is the same as for the usual variance-covariance matrix.\nExample Data: Brownian motion trajectories\n\nset.seed(109)\n\nn       <- 50   # sample size\nJ       <- 500  # number of evaluation points\n\nBM.mat  <- matrix(0, ncol=n, nrow=J)\n\nfor(i in 1:n){\n     BM.mat[,i] <- cumsum(rnorm(J, sd = 1/100))\n}\n\nComputing and plotting the pointwise mean and standard deviations in R:\n\nBM.Mean <- rowMeans(BM.mat)\nBM.SD   <- apply(BM.mat,1,sd)\n\n\nxx      <- seq(0,1,len=J) \n\npar(mfrow=c(1,2))\nmatplot(x    = xx, \n        y    = BM.mat, \n        xlab = \"\", ylab = \"\", \n        type =\"l\", \n        col  = gray(.7), lty=1, \n        main = \"Brownian Motions\")\nlines(x = xx, \n      y = BM.Mean)\nlines(x = xx, \n      y = BM.SD, lty=2)\nlegend(\"topleft\", lty = c(1,2), \n       legend = c(\"Sample Mean\",\"Sample SD\"))\n##\nmatplot(x    = xx, \n        y    = BM.mat, \n        xlab = \"\", ylab = \"\", \n        type = \"l\", \n        col  = gray(.7), lty=1, \n        main = \"Brownian Motion\")\nlines(x = c(0,1), \n      y = c(0,0), lty=1)\nlines(x = c(0,1), \n      y = c(0,sqrt(J*(1/100)^2)), lty=2)\nlegend(\"topleft\", lty = c(1,2), \n       legend = c(\"True Mean\",\"True SD\"))\n\n\n\n\n\n\n\n\nComputing and plotting the pointwise covariance function in R:\n\nBM.cov <- var(t(BM.mat))\n\nslct   <- c(seq.int(1,500,by=20),500)\n\npar(mfrow=c(1,2), mar=c(1, 1.1, 1.2, 1.1))\npersp(x = xx[slct], \n      y = xx[slct], \n      z = BM.cov[slct,slct], \n      xlab = \"s\", ylab = \"t\", zlab = \"\", \n      main = \"Sample Covariance Function\",\n      theta = -40, phi = 20, expand = .75, col = \"lightblue\", shade = 1.05)\n\nx <- seq(0, 1, length= 30)\ny <- x\nf <- function(x, y){min(x,y)}\nf <- Vectorize(f)\nz <- outer(x, y, f)\n\npersp(x = x, y = y, z = z, xlab=\"s\", ylab=\"t\", zlab=\"\",\n      main  = \"True Covariance Function\",\n      theta = -40, phi = 20, expand = .75, col = \"lightblue\", shade = 1.05)"
  },
  {
    "objectID": "Ch5_FunctionalDataAnalysis.html#functional-principal-component-analysis",
    "href": "Ch5_FunctionalDataAnalysis.html#functional-principal-component-analysis",
    "title": "5¬† Functional Data Analysis",
    "section": "5.3 Functional Principal Component Analysis",
    "text": "5.3 Functional Principal Component Analysis\nPrincipal Component Analysis (PCA) is effectively an eigendecomposition of the empirical covariance function \\(\\hat{c}_n(t,s)\\) \\[\n\\begin{align*}\n\\hat{c}_n(t,s)\n&=\\sum_{j=1}^n\\hat{\\lambda}_j\\hat{v}_j(t)\\hat{v}_j(s),\\\\[2ex]\n&\\approx\\sum_{j=1}^{p<n}\\hat{\\lambda}_j\\hat{v}_j(t)\\hat{v}_j(s),\n\\end{align*}\n\\] where \\[\n\\hat{\\lambda}_1>\\hat{\\lambda}_2>\\dots \\geq 0\n\\] denote the estimated eigenvalues and where \\[\n\\hat{v}_1(t),\\dots,\\hat{v}_p(t)\n\\] denote the estimated principal component (or eigen) functions.\nThe estimated eigenfunctions are orthonormal, i.e.¬† \\[\n\\int_a^b\\hat{v}_j(t)\\hat{v}_k(t)dt=\n\\begin{cases}\n1, & j=k \\\\\n0, &j\\neq k.\n\\end{cases}\n\\]\nIn FDA, functional PCA is very prominent since the Estimated Functional Principal Components (EFPC‚Äôs) \\(\\hat{v}_j\\) are very well suited as basis functions for \\(X_i\\): \\[\n\\begin{align*}\nX_i(t)\n&=\\bar{X}_n(t) + \\sum_{j=1}^n\\hat{\\xi}_{ij}\\hat{v}_j(t)\\\\[2ex]\n&\\approx\\bar{X}_n(t) + \\sum_{j=1}^{p<n}\\hat{\\xi}_{ij}\\hat{v}_j(t),\n\\end{align*}\n\\] where \\(\\hat{\\xi}_{ij}\\) denote the estimated scores \\[\n\\hat{\\xi}_{ij}=\\int_a^b (X_i(t)-\\bar{X}_n(t))\\hat{v}_j(t)dt.\n\\]\nNote that \\[\n\\frac{1}{n}\\sum_{i=1}^n\\hat{\\xi}_{ij} = 0,\\quad\\text{for all }j=1,2,\\dots\n\\] and that \\[\n\\frac{1}{n}\\sum_{i=1}^n\\hat{\\xi}_{ij}^2 = \\hat{\\lambda}_j,\\quad\\text{for all }j=1,2,\\dots\n\\]\nExample: Computing the Functional PCA in R:\n\nBSPL.basis <- create.bspline.basis(rangeval=c(0,1), nbasis=200)\nBM.fd      <- smooth.basis(argvals=xx, y=BM.mat, fdParobj=BSPL.basis)\nBM.pca     <- pca.fd(BM.fd$fd, nharm=3)\npar(mfrow=c(1,3), mar=c(2.1, 1.1, 4.1, 1.1))\npersp(xx[slct], xx[slct], BM.cov[slct,slct], xlab=\"s\", ylab=\"t\", zlab=\"\",\n      main=\"Sample Covariance Function\", theta = -40, phi = 20, expand = .75, col = \"blue\", shade = 1.05)\ninvisible(plot(BM.pca$values[1:3], type=\"o\", ylab=\"\", main=\"Estimated Eigenvalues (p=3)\"))\ninvisible(plot(BM.pca$harmonics, lwd=3, ylab=\"\", main=\"Estimated FPC's (p=3)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Quantities\n\n\n\n\\[\n\\begin{align*}\nCov(X(t),X(s)) = c(t,s)\n&=\\sum_{j=1}^n \\lambda_j v_j(t) v_j(s),\\\\[2ex]\n&\\approx\\sum_{j=1}^{p<n} \\lambda_j v_j(t) v_j(s),\n\\end{align*}\n\\] where \\[\n\\lambda_1 > \\lambda_2>\\dots \\geq 0\n\\] denote the true eigenvalues and where \\[\nv_1(t),\\dots,v_p(t)\n\\] denote the true principal component (or eigen) functions.\nThe eigenfunctions are orthonormal, i.e.¬† \\[\n\\int_a^b v_j(t)v_k(t)dt=\n\\begin{cases}\n1, & j=k \\\\\n0, &j\\neq k.\n\\end{cases}\n\\]\nEigenfunctions as basis functions \\[\n\\begin{align*}\nX_i(t)\n&=\\mathbb{E}(X(t)) + \\sum_{j=1}^n \\xi_{ij}v_j(t)\\\\[2ex]\n&\\approx\\mathbb{E}(X(t)) + \\sum_{j=1}^{p<n}\\xi_{ij}v_j(t),\n\\end{align*}\n\\] where \\(\\xi_{ij}\\) denote the true scores \\[\n\\xi_{ij}=\\int_a^b (X_i(t)-\\mathbb{E}(X(t)) )v_j(t)dt.\n\\]\nNote that \\[\n\\mathbb{E}(\\xi_{ij}) = 0,\\quad\\text{for all }j=1,2,\\dots\n\\] and that \\[\nVar(\\xi_{ij}) = \\lambda_j,\\quad\\text{for all }j=1,2,\\dots.\n\\]\n\n\nBest basis property: The EFPC‚Äôs are the best basis; i.e.¬†they have the smallest approximation error (in the mean square sense).\n\\[\n\\begin{align}\nX_i(t)&\\approx\\bar{X}_n(t) + \\sum_{j=1}^p\\hat{\\xi}_{ij}\\hat{v}_j(t)\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFerraty, Fr√©d√©ric, and Philippe Vieu. 2006. Nonparametric Functional Data Analysis: Theory and Practice. Springer.\n\n\nHsing, Tailen, and Randall Eubank. 2015. Theoretical Foundations of Functional Data Analysis, with an Introduction to Linear Operators. John Wiley & Sons.\n\n\nKokoszka, Piotr, and Matthew Reimherr. 2017. Introduction to Functional Data Analysis. Chapman; Hall/CRC.\n\n\nRamsay, J. O., and B. W. Silverman. 2005. Functional Data Analysis. 2. ed. Springer."
  },
  {
    "objectID": "Ch3_Bootstrap.html#solutions",
    "href": "Ch3_Bootstrap.html#solutions",
    "title": "3¬† The Bootstrap",
    "section": "Solutions",
    "text": "Solutions\n\nSolutions of Exercise 1.\n\n(a)\nThe exact point-wise distribution of \\(nF_n(x)\\) for a given \\(x\\in\\mathbb{R}.\\)\n\\[\n\\begin{align*}\nF_n(x)\n& = \\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\\\\n\\Rightarrow nF_n(x)\n& = \\sum_{i=1}^n 1_{(X_i\\leq x)} \\sim \\mathcal{Binom}\\left(n,p=F(x)\\right),\n\\end{align*}\n\\] since \\(1_{(X_i\\leq x)}\\) is a Bernoulli random variable with parameter \\[\n\\begin{align*}\np\n& = P(1_{(X_i\\leq x)} = 1)\\\\[2ex]\n& = P(X_i \\leq x)\\\\[2ex]\n& = F(x).\n\\end{align*}\n\\] Note that this holds for any distribution of \\(X_i.\\) Therefore, one says that \\(nF_n(x)\\) is distribution free.\n\n\n(b)\nFrom (a), we have that (using the standard mean and variance expressions for Binomial distributed random variables): \\[\n\\begin{align*}\n\\mathbb{E}(nF_n(x)) &= nF(x)\\\\[2ex]\n\\Leftrightarrow\\quad  \\mathbb{E}(F_n(x)) &= F(x)\n\\end{align*}\n\\] and that \\[\n\\begin{align*}\nVar(nF_n(x)) &= nF(x)(1-F(x))\\\\[2ex]\n\\Leftrightarrow \\quad Var(F_n(x)) &= \\frac{F(x)(1-F(x))}{n}.\n\\end{align*}\n\\]\nMoreover, since \\(F_n(x) = \\frac{1}{n} \\sum_{i=1}^n 1_{(X_i\\leq x)}\\) is an average over i.i.d. random variables \\(1_{(X_1\\leq x)},\\dots,1_{(X_n\\leq x)},\\) the standard CLT (Lindeberg-L√©vy) implies that \\[\n\\frac{F_n(x)-F(x)}{\\sqrt{\\frac{F(x)(1-F(x))}{n}}}\\to_d\\mathcal{N}(0,1)\n\\] as \\(n\\to\\infty.\\) Or with a slight abuse of notation: \\[\nF_n(x)\\overset{a}{\\sim}\\mathcal{N}\\left(F(x),\\frac{F(x)(1-F(x))}{n}\\right).\n\\]\n\n\n(c)\nThe mean squared error between \\(F_n(x)\\) and \\(F(x)\\) is given by \\[\n\\begin{align*}\n\\operatorname{MSE}(F_n(x))\n&= \\mathbb{E}\\left((F_n(x)-F(x))^2\\right)\\\\[2ex]\n&= Var(F_n(x)) + \\left(\\mathbb{E}(F_n(x))-F(x)\\right)^2.\n\\end{align*}\n\\] It follows from our previous results that for each \\(x\\in\\mathbb{R}\\) \\[\nVar(F_n(x)) = \\frac{F(x)(1-F(x))}{n} \\to 0\n\\] as \\(n\\to\\infty,\\) and that \\[\n\\mathbb{E}(F_n(x)) -F(x) = 0\n\\] for all \\(n.\\) Therefore, \\[\n\\operatorname{MSE}(F_n(x)) = Var(F_n(x)) \\to 0\n\\] as \\(n\\to\\infty.\\)\nThus we can conclude that \\(F_n(x)\\) converges in the mean-square sense to \\(F(x)\\) for each \\(x\\in\\mathbb{R},\\) \\[\nF_n(x)\\to_{ms} F(x)\n\\] as \\(n\\to\\infty.\\)\nSince convergence in the mean square sense implies convergence in probability, we also have that for each \\(x\\in\\mathbb{R}\\) \\[\nF_n(x)\\to_{p} F(x)\n\\] as \\(n\\to\\infty\\) which shows that \\(F_n(x)\\) is weakly consistent for \\(F(x)\\) for each \\(x\\in\\mathbb{R}.\\)\n\n\n\nSolutions of Exercise 2.\n\n\n\n\n\n\nTip\n\n\n\nAnother, equivalent way to define uniform convergence:\n\\(g_n(\\cdot)\\) converges uniformly to \\(g(\\cdot)\\) if for every \\(\\varepsilon>0,\\) there exists an \\(N\\) such that \\[\n|g_n(x) - g(x)| < \\varepsilon\n\\] for all \\(n\\geq N\\) and for all \\(x\\in\\mathcal{X},\\) where \\(\\mathcal{X}\\) denotes the domain of the functions \\(g_n\\) and \\(g.\\)\nI.e., \\(g_n(\\cdot)\\) converges uniformly to \\(g(\\cdot)\\) if it is possible to draw an \\(\\varepsilon\\)-band around the graph of \\(g(x)\\) that contains all of the graphs of \\(g_n(x)\\) for large enough \\(n\\geq N.\\)\n\n\nExample 1: \\(\\mathcal{X}=\\mathbb{R}\\) The function \\[\ng_n(x) = x\\left(1+\\frac{1}{n}\\right)\n\\] converges point-wise (for each given \\(x\\in\\mathbb{R}\\)) to \\[\ng(x)=x,\n\\] since \\[\n|g_n(x)-g(x)|=\\frac{|x|}{n}\\to 0\\quad \\text{as}\\quad n\\to\\infty.\n\\]  for each given \\(x\\in\\mathcal{X}.\\)\nHowever, \\(g_n\\) does not converge uniformly to \\(g\\) since \\[\n\\sup_{x\\in\\mathbb{R}}|g_n(x)-g(x)|=\\sup_{x\\in\\mathbb{R}}\\frac{|x|}{n}=\\infty\\neq 0\n\\] for each \\(n.\\)\nNote that for a small \\(\\varepsilon> 0,\\) an \\(\\varepsilon\\)-band around \\(g(x) = x\\) fails to capture the graphs of \\(g_n(x)=x(1+1/n)\\) with \\(n\\geq N,\\) since for any \\(N\\) and \\(n^\\ast\\geq N\\) we have that \\(g_{n^\\ast}(x)=x(1+1/n^\\ast)\\to\\infty\\) as \\(x\\to\\infty.\\)\nExample 2: \\(\\mathcal{X}=(0,1)\\) The function \\[\ng_n(x) = x^n\n\\] converges point-wise (for each given \\(x\\in(0,1)\\)) to \\[\ng(x)=0,\n\\] since \\[\n|g_n(x)-g(x)|=x^n\\to 0\\quad\\text{as}\\quad n\\to\\infty\n\\]  for each given \\(x\\in(0,1).\\)\nHowever, \\(g_n\\) does not converge uniformly to \\(g\\) since \\[\n\\sup_{x\\in(0,1)}|g_n(x)-g(x)|=\\sup_{x\\in(0,1)}x^n=1\\neq 0\n\\] for each \\(n.\\)\nNote that for a small \\(0<\\varepsilon<1,\\) an \\(\\varepsilon\\)-band around \\(g(x) = 0\\) fails to capture the graphs of \\(g_n(x)=x^n,\\) with \\(n\\geq N,\\) since for any \\(N\\) and \\(n^\\ast\\geq N\\) we have that \\(g_{n^\\ast}(x)=x^{n^\\ast}\\to 1\\) as \\(x\\to 1.\\)\n\n\nSolutions of Exercise 3.\n\n(a) Part 1:\nSetup:\n\niid data \\(X_1,\\dots,X_n\\) with \\(X_i\\sim F\\)\n\\(\\mathbb{E}(X_i)=\\mu\\)\n\\(Var(X_i)=\\sigma^2<\\infty\\)\nEstimator: \\(\\bar{X}_n=n^{-1}\\sum_{i=1}^nX_i\\)\n\nIf \\(F\\) is a normal distribution:\n\\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\sim \\mathcal{N}(0,1)\\quad\\text{exactly for all}\\;n.\n\\end{array}\n\\]\nFor non-normal distributions \\(F\\) we have by the classic CLT: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nUsually, we do not know \\(\\sigma\\) and have to estimate this parameter using a consistent estimator such as \\(s^2=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), where \\(s\\to_p\\sigma\\) as \\(n\\to\\infty\\).\nThen by Slusky‚Äôs Theorem (allows to combine \\(\\to_d\\) and \\(\\to_p\\)-statements) we have that: \\[\n\\begin{array}{rlc}\n\\sqrt{n}\\left(\\frac{\\bar{X}_n-\\mu}{s}\\right)\\to_d \\mathcal{N}(0,1)\\quad\\text{as}\\;n\\to\\infty.\n\\end{array}\n\\]\nThe classic confidence interval is then based on the above (asymptotic) normality result: \\[\n\\operatorname{CI}_{\\operatorname{classic},n}=\\left[\\bar{X}_n\\,-\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}},\\bar{X}_n\\,+\\,z_{1-\\alpha/2}\\frac{s}{\\sqrt{n}}\\right],\n\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)\\)-quantile of the standard normal distribution. Alternatively, one can apply a ‚Äúsmall-sample correction‚Äù by using the \\((1-\\alpha/2)\\)-quantile \\(t_{n-1, 1-\\alpha/2}\\) of the \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nFrom the above arguments it follows that: \\[\nP\\left(\\mu\\in \\operatorname{CI}_{\\operatorname{classic},n}\\right)\\to 1-\\alpha\\quad\\text{as}\\quad n\\to\\infty.\n\\]\nLet us consider the finite-\\(n\\) (with \\(n=20\\)) performance of the classic confidence interval for the case where \\(F\\) is a normal distribution with mean \\(\\mu=1\\) and standard deviation \\(\\sigma=2\\):\n\n##  Setup:\nn     <-   20 # Sample Size\nmean  <-    1 # Mean\nsdev  <-    2 # Standard Deviation\nalpha <- 0.05 # Level\n\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.lo.vec  <- rep(NA, B)\nCI.up.vec  <- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     <- rnorm(n=n, mean = mean, sd = sdev) \n  ## Estimates:\n  X.bar.MC     <- mean(X.sample)\n  sd.hat.MC    <- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n\n  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(a) Part 2: Classic Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nNow, we consider the finite-\\(n\\) performance of the classic confidence interval under the same setup as above, but for the case where \\(F\\) is a non-normal distribution, namely, a \\(\\chi^2_1\\)-distribution with \\(1\\) degree of freedom:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nalpha <- 0.05 # Level\n\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.lo.vec  <- rep(NA, B)\nCI.up.vec  <- rep(NA, B)\n  \n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  X.sample     <- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC     <- mean(X.sample)\n  sd.hat.MC    <- sd(X.sample)\n  ## Classic CIs:\n  \n  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))\n  \n  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Classic 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==TRUE], \n       x1=CI.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.lo.vec[CI.checks==FALSE], \n       x1=CI.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(b) Basic Bootstrap Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nLet‚Äôs generate an iid random sample \\(S_n\\) with \\(X_i\\sim\\chi^2_1\\) and the corresponding estimate \\(\\bar X_n\\):\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  <- rchisq(n, df=df)\n\n## Empirical mean:\n(X.bar <- mean(S_n))\n\n[1] 0.6737282\n\n\nThe standard bootstrap confidence interval is given by (see lecture script): \\[\n\\left[2\\bar{X}_n - \\hat{q}^\\ast_{n,1-\\alpha/2}, 2\\bar{X}_n - \\hat{q}^\\ast_{n,\\alpha/2}\\right],\n\\] where \\(\\bar{X}_n\\) denotes the estimate computed from the original sample, and \\(\\hat{q}^\\ast_{\\alpha/2}\\) and \\(\\hat{q}^\\ast_{1-\\alpha/2}\\) denote the \\((\\alpha/2)\\) and \\((1-\\alpha/2)\\)-quantiles of the conditional distribution of \\(\\bar{X}_n^\\ast\\) given \\(\\mathcal{S}_n=\\left\\{X_1,\\dots,X_n\\right\\}.\\)\nIn the following we first generate the \\(m\\) bootstrap realizations \\[\n\\bar{X}_{n,1}^\\ast,\\dots,\\bar{X}_{n,m}^\\ast,\n\\] compute their quantiles \\(\\hat{q}^\\ast_{n,\\alpha/2}\\) and \\(\\hat{q}^\\ast_{n,1-\\alpha/2},\\) and plot all of this:\n\n## Bootstr-Setup:\nalpha            <- 0.05\nn.Bootsrap.draws <- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)\n}\n## Boostrap draws of \\bar{X}_n^*:\nX.bar.bootstr.vec <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\n\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\nq.1 <- quantile(X.bar.bootstr.vec, probs = 1-alpha/2)\nq.2 <- quantile(X.bar.bootstr.vec, probs = alpha/2)\n## plot\nplot(ecdf(X.bar.bootstr.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-Distr. of \",bar(X)[n]^{\" *\"})))\nabline(v=c(q.1,q.2),col=\"red\")\n\n\n\n\nUsing our preparatory work above, the basic bootstrap confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Basic.Bootstr.lo <- 2*X.bar - q.1\nCI.Basic.Bootstr.up <- 2*X.bar - q.2\n\n## Re-labeling of otherwise false names:\nattr(CI.Basic.Bootstr.lo, \"names\") <- c(\"2.5%\")\nattr(CI.Basic.Bootstr.up, \"names\") <- c(\"97.5%\")\n##\nc(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)\n\n     2.5%     97.5% \n0.1545224 1.0425228 \n\n\nNow, we can investigate the finite-\\(n\\) performance of the standard bootstrap confidence interval:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nmean  <-   df\nalpha <- 0.05 # Level\nn.Bootsrap.draws <- 1500\n\n## MC-Setup:\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.Basic.Bstr.lo.vec <- rep(NA, B)\nCI.Basic.Bstr.up.vec <- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        <- rchisq(n, df=df)\n  ## Estimate:\n  X.bar.MC      <- mean(S_n.MC)\n  ## \n  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  ## (1-alpha/2)-quantile:\n  q.1.MC <- quantile(X.bar.bootstr.MC.vec, probs = 1-alpha/2)\n  q.2.MC <- quantile(X.bar.bootstr.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Basic.Bstr.lo.vec[b] <- 2*X.bar.MC - q.1.MC\n  CI.Basic.Bstr.up.vec[b] <- 2*X.bar.MC - q.2.MC\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.Basic.Bstr.lo.vec<=mean & mean<=CI.Basic.Bstr.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Basic Bootrap 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==TRUE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==FALSE], \n       x1=CI.Basic.Bstr.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n(c) Bootstrap-\\(t\\) Confidence Interval (\\(n=20\\) and \\(X_i\\sim \\chi^2_1\\))\nThe bootstrap-\\(t\\) confidence interval is given by (see lecture script): \\[\n\\left[\n  \\bar{X}_n-\\hat{q}^\\ast_{n,1-\\alpha/2}\\left(\\frac{s_n}{\\sqrt{n}}\\right),  \n  \\bar{X}_n-\\hat{q}^\\ast_{n,\\alpha/2}  \\left(\\frac{s_n}{\\sqrt{n}}\\right)\n\\right],\n\\] where \\(\\bar{X}_n\\) and \\(s_n=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) are computed from the original sample, and where \\(\\hat{q}^\\ast_{n,\\alpha/2}\\) and \\(\\hat{q}^\\ast_{n,1-\\alpha/2}\\) denote the empirical \\((\\alpha/2)\\) and the \\((1-\\alpha/2)\\)-quantiles compute from the bootstrap estimates: \\[\n\\sqrt{n}\\frac{\\bar{X}_{n,j}^\\ast-\\bar{X}_n}{s_{n,j}^\\ast}\\quad j=1,\\dots,m.\n\\]\nIn the following we first generate the \\(m\\) bootstrap realizations \\[\n\\sqrt{n}\\frac{\\bar{X}_{n,j}^\\ast-\\bar{X}_n}{s_{n,j}^\\ast}\\quad j=1,\\dots,m,\n\\] compute their quantiles \\(\\hat{q}^\\ast_{n,\\alpha/2}\\) and \\(\\hat{q}^\\ast_{n,1-\\alpha/2}\\), and plot all of this:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\n\n## IID random sample:\nset.seed(123)\nS_n  <- rchisq(n, df=df)\n\n## Empirical mean and sd:\nX.bar   <- mean(S_n)\nsd.hat  <- sd(S_n)\n\n## Bootstr-Setup:\nalpha            <- 0.05\nn.Bootsrap.draws <- 1500\n\n## Generate bootstap samples:\nBootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n\nfor(j in 1:n.Bootsrap.draws){\n  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)\n}\n## Compute boostrap draws of (\\bar{X}_n^*-\\bar{X}_n)/\\hat{\\sigma}^\\ast:\nX.bar.bootstr.vec    <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)\nsd.bootstr.vec       <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = sd)\n##\nBootstr.t.sample.vec <- sqrt(n)*(X.bar.bootstr.vec - X.bar)/sd.bootstr.vec\n## Quantile of the bootstr.-distribution of \\bar{X}_n^*:\nq.1 <- quantile(Bootstr.t.sample.vec, probs = 1-alpha/2)\nq.2 <- quantile(Bootstr.t.sample.vec, probs = alpha/2)\n## plot\nplot(ecdf(Bootstr.t.sample.vec), xlab=\"\", ylab=\"\",\n     main=expression(paste(\"Bootstr.-t-Distr. of \",\n          sqrt(n)(bar(X)[n]^{\" *\"}-bar(X)[n])/s[n]^{\"*\"})))\nabline(v=c(q.1,q.2),col=\"red\")\n\n\n\n\nUsing our preparatory work above, the bootstrap-\\(t\\) confidence interval can be computed as following:\n\n## Basic Bootstrap Confidence Interval:\nCI.Bstr.t.lo <- X.bar - q.1 * sd.hat/sqrt(n)\nCI.Bstr.t.up <- X.bar - q.2 * sd.hat/sqrt(n)\n\n## Re-labeling of otherwise false names:\nattr(CI.Bstr.t.lo, \"names\") <- c(\"2.5%\")\nattr(CI.Bstr.t.up, \"names\") <- c(\"97.5%\")\n##\nc(CI.Bstr.t.lo, CI.Bstr.t.up)\n\n     2.5%     97.5% \n0.3052027 2.0241321 \n\n\nLet us investigate the finite-\\(n\\) performance of the bootstrap-t confidence interval:\n\n## Setup:\nn     <-   20  # Sample Size\ndf    <-    1  # (=> mean==1)\nmean  <-   df\nalpha <- 0.05 # Level\nn.Bootsrap.draws <- 1500\n\n## MC-Setup:\nset.seed(123)\nB          <- 1500 # MC repetitions\nCI.Bstr.t.lo.vec <- rep(NA, B)\nCI.Bstr.t.up.vec <- rep(NA, B)\n\n## MC-Simulation:\nfor(b in 1:B){\n  ## Data Generating Process:\n  S_n.MC        <- rchisq(n, df=df)\n  ## Estimates:\n  X.bar.MC      <- mean(S_n.MC)\n  sd.MC         <- sd(S_n.MC)\n  ## \n  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)\n  for(j in 1:n.Bootsrap.draws){\n    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)\n  }\n  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)\n  sd.bootstr.MC.vec    <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = sd)\n  ## Make it a \"Bootstrap-t\" sample:\n  Bootstr.t.MC.vec     <- sqrt(n)*(X.bar.bootstr.MC.vec - X.bar.MC)/sd.bootstr.MC.vec\n  ## (1-alpha/2)-quantile:\n  q.1.MC <- quantile(Bootstr.t.MC.vec, probs = 1-alpha/2)\n  q.2.MC <- quantile(Bootstr.t.MC.vec, probs = alpha/2)\n  ## Basic Bootstrap CIs:\n  CI.Bstr.t.lo.vec[b] <- X.bar.MC - q.1.MC * sd.MC/sqrt(n)\n  CI.Bstr.t.up.vec[b] <- X.bar.MC - q.2.MC * sd.MC/sqrt(n)\n}\n\n## How often does the classic CI cover the true mean?\nCI.checks      <- CI.Bstr.t.lo.vec<=mean & mean<=CI.Bstr.t.up.vec\nfreq.non.cover <- length(CI.checks[CI.checks==FALSE])/B\n\n## Plot\nplot(x=0,y=0,xlim=range(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), \n     ylim=c(1,B), type=\"n\", \n     ylab=\"MC Repetitions\", xlab=\"\", axes = FALSE, \n     main=\"Bootrap-t 95% Confidence Intervals\\n(Non-Normal DGP)\")\naxis(1, at=c(1), labels =\"True Mean = 1\")\naxis(2); box()\nmtext(side = 1, text=paste0(\"(Freq. of Non-Covering CIs: \",\n      round(freq.non.cover,digits = 2),\")\"), line = 2.5)\n## Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==TRUE], \n       x1=CI.Bstr.t.up.vec[CI.checks==TRUE], \n       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], \n       angle=90, code = 3, length = .1, col=\"black\")\n## Non-Covering CIs:\narrows(x0=CI.Bstr.t.lo.vec[CI.checks==FALSE], \n       x1=CI.Bstr.t.up.vec[CI.checks==FALSE], \n       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], \n       angle=90, code = 3, length = .05, col=\"red\")\nabline(v=mean,col=\"blue\", lwd=1.5)\n\n\n\n\n\n\n\nSolutions of Exercise 4.\n\n(a)\n\\[\n\\begin{align*}\n\\mathbb{E}^*(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\left.\\bar{Y}^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.\\frac{1}{n}\\sum_{i=1}^n Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(\\left.Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(\\left.Y_i^*\\right|\\mathcal{S}_n\\right)\\\\[2ex]\n& = \\sum_{i=1}^n \\frac{1}{n} Y_i\n= \\bar{Y}\n\\end{align*}\n\\] since \\((Y_i^*|\\mathcal{S}_n)\\in\\{Y_1,\\dots,Y_n\\}\\) and \\(P(Y_j^*=Y_i|\\mathcal{S}_n)=\\frac{1}{n}\\) for each \\(i,j\\in 1,\\dots,n.\\)\n\n\n(b)\n\\[\n\\begin{align*}\n\\mathbb{E}(\\bar{Y}^*)\n& = \\mathbb{E}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^*\\right)\\\\[2ex]\n& = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}\\left(Y_i^*\\right)\\\\[2ex]\n& = \\mathbb{E}\\left(Y_i^*\\right)\\\\[2ex]\n& = \\mu\n\\end{align*}\n\\] since \\(Y_i^*\\sim Y_i\\sim F.\\)"
  }
]