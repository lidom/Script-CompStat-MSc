<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; The Bootstrap – Computational Statistics (M.Sc.)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ch4_NP_Density_Estimation.html" rel="next">
<link href="./Ch2_EMAlgorithmus.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Ch3_Bootstrap.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/Uni_Bonn_Logo.jpeg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Computational Statistics (M.Sc.)</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Organization of the Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch1_MaximumLikelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Maximum Likelihood</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch2_EMAlgorithmus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">EM Algorithm &amp; Cluster Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch3_Bootstrap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch4_NP_Density_Estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Density Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ch5_NPRegression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-Illustration" id="toc-sec-Illustration" class="nav-link active" data-scroll-target="#sec-Illustration"><span class="header-section-number">3.1</span> Illustration: When are you happy about the Bootstrap?</a></li>
  <li><a href="#recap-the-empirical-distribution-function" id="toc-recap-the-empirical-distribution-function" class="nav-link" data-scroll-target="#recap-the-empirical-distribution-function"><span class="header-section-number">3.2</span> Recap: The Empirical Distribution Function</a>
  <ul class="collapse">
  <li><a href="#idea-of-the-bootstrap" id="toc-idea-of-the-bootstrap" class="nav-link" data-scroll-target="#idea-of-the-bootstrap"><span class="header-section-number">3.2.1</span> Idea of the Bootstrap</a></li>
  </ul></li>
  <li><a href="#sec-BasicBootstrap" id="toc-sec-BasicBootstrap" class="nav-link" data-scroll-target="#sec-BasicBootstrap"><span class="header-section-number">3.3</span> The Basic Bootstrap Method</a>
  <ul class="collapse">
  <li><a href="#bootstrap-consistency" id="toc-bootstrap-consistency" class="nav-link" data-scroll-target="#bootstrap-consistency">Bootstrap Consistency</a></li>
  <li><a href="#example-inference-about-the-population-mean" id="toc-example-inference-about-the-population-mean" class="nav-link" data-scroll-target="#example-inference-about-the-population-mean"><span class="header-section-number">3.3.1</span> Example: Inference About the Population Mean</a></li>
  <li><a href="#the-basic-bootstrap-confidence-interval" id="toc-the-basic-bootstrap-confidence-interval" class="nav-link" data-scroll-target="#the-basic-bootstrap-confidence-interval"><span class="header-section-number">3.3.2</span> The Basic Bootstrap Confidence Interval</a></li>
  </ul></li>
  <li><a href="#sec-BootT" id="toc-sec-BootT" class="nav-link" data-scroll-target="#sec-BootT"><span class="header-section-number">3.4</span> The Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Method</a>
  <ul class="collapse">
  <li><a href="#the-bootstrap-boldsymbolt-confidence-interval" id="toc-the-bootstrap-boldsymbolt-confidence-interval" class="nav-link" data-scroll-target="#the-bootstrap-boldsymbolt-confidence-interval"><span class="header-section-number">3.4.1</span> The Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Confidence Interval</a></li>
  <li><a href="#accuracy-of-the-bootstrap-boldsymbolt-method" id="toc-accuracy-of-the-bootstrap-boldsymbolt-method" class="nav-link" data-scroll-target="#accuracy-of-the-bootstrap-boldsymbolt-method"><span class="header-section-number">3.4.2</span> Accuracy of the Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> method</a></li>
  </ul></li>
  <li><a href="#bootstrap-and-linear-regression-analysis" id="toc-bootstrap-and-linear-regression-analysis" class="nav-link" data-scroll-target="#bootstrap-and-linear-regression-analysis"><span class="header-section-number">3.5</span> Bootstrap and Linear Regression Analysis</a>
  <ul class="collapse">
  <li><a href="#sec-bootPairs" id="toc-sec-bootPairs" class="nav-link" data-scroll-target="#sec-bootPairs"><span class="header-section-number">3.5.1</span> Bootstrap under Random Design: Bootstrapping Pairs</a></li>
  <li><a href="#sec-bootResid" id="toc-sec-bootResid" class="nav-link" data-scroll-target="#sec-bootResid"><span class="header-section-number">3.5.2</span> Bootstrap under Fixed Design: The Residual Bootstrap</a></li>
  <li><a href="#sec-bootWild" id="toc-sec-bootWild" class="nav-link" data-scroll-target="#sec-bootWild"><span class="header-section-number">3.5.3</span> Bootstrap under Fixed Design: The Wild Bootstrap</a></li>
  <li><a href="#bootstrap-confidence-intervals-for-the-boldsymboljth-component-of-the-regression-coefficient-boldsymbolbeta_0" id="toc-bootstrap-confidence-intervals-for-the-boldsymboljth-component-of-the-regression-coefficient-boldsymbolbeta_0" class="nav-link" data-scroll-target="#bootstrap-confidence-intervals-for-the-boldsymboljth-component-of-the-regression-coefficient-boldsymbolbeta_0"><span class="header-section-number">3.5.4</span> Bootstrap Confidence Intervals for the <span class="math inline">\(\boldsymbol{j}\)</span>th Component of the Regression Coefficient <span class="math inline">\(\boldsymbol{\beta_{0}}\)</span></a></li>
  <li><a href="#statistical-hypothesis-testing" id="toc-statistical-hypothesis-testing" class="nav-link" data-scroll-target="#statistical-hypothesis-testing"><span class="header-section-number">3.5.5</span> Statistical Hypothesis Testing</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#solutions" id="toc-solutions" class="nav-link" data-scroll-target="#solutions">Solutions</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Bootstrap</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- LTeX: language=en-US -->
<!-- TO-DO: 
1. Rework this chapter using the overview article of Horowitz
BOOTSTRAP METHODS IN ECONOMETRICS 
2. Remove the fraction estimator parts 
-->
<p>The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.</p>
<p>Some literature:</p>
<ul>
<li><span class="citation" data-cites="Shao_Tu_1996">Shao and Tu (<a href="#ref-Shao_Tu_1996" role="doc-biblioref">1996</a>)</span>: The Jackknife and Bootstrap</li>
<li><span class="citation" data-cites="Davison_Hinkley_2013">Davison and Hinkley (<a href="#ref-Davison_Hinkley_2013" role="doc-biblioref">2013</a>)</span>: Bootstrap Methods and their Applications</li>
<li><span class="citation" data-cites="Efron_Tibshirani_1994">Efron and Tibshirani (<a href="#ref-Efron_Tibshirani_1994" role="doc-biblioref">1994</a>)</span>: An Introduction to the Bootstrap</li>
<li><span class="citation" data-cites="Hall_1992">Hall (<a href="#ref-Hall_1992" role="doc-biblioref">1992</a>)</span>: The Bootstrap and Edgeworth Expansion</li>
<li><span class="citation" data-cites="Horowitz_2001">Horowitz (<a href="#ref-Horowitz_2001" role="doc-biblioref">2001</a>)</span>: The Bootstrap. In: Handbook of Econometrics</li>
<li><span class="citation" data-cites="Mammen_1992_Book">Mammen (<a href="#ref-Mammen_1992_Book" role="doc-biblioref">1992</a>)</span>: When Does Bootstrap Work: Asymptotic Results and Simulations</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bradley Efron
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap method is attributed to <a href="https://statistics.stanford.edu/people/bradley-efron">Bradley Efron</a>, who received the <em><a href="https://statsandstories.net/methods/2018/9/28/bootstrapping-an-international-prize">International Prize in Statistics</a></em> (the Nobel price of statistics) for his seminal works on the bootstrap method.</p>
</div>
</div>
<section id="sec-Illustration" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-Illustration"><span class="header-section-number">3.1</span> Illustration: When are you happy about the Bootstrap?</h2>
<p>Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y.\)</span> These returns <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random with</p>
<ul>
<li><span class="math inline">\(Var(X)=\sigma^2_X\)</span></li>
<li><span class="math inline">\(Var(Y)=\sigma^2_Y\)</span></li>
<li><span class="math inline">\(Cov(X,Y)=\sigma_{XY}\)</span></li>
</ul>
<p>We want to invest a fraction <span class="math inline">\(\alpha\in(0,1)\)</span> in <span class="math inline">\(X\)</span> and invest the remaining <span class="math inline">\(1-\alpha\)</span> in <span class="math inline">\(Y.\)</span></p>
<p>Our aim is to minimize the variance (risk) of our investment, i.e., we want to minimize <span class="math display">\[
Var\left(\alpha X + (1-\alpha)Y\right).
\]</span> One can show that the value <span class="math inline">\(\alpha\)</span> that minimizes this variance is <span id="eq-alpha"><span class="math display">\[
\alpha = \frac{\sigma^2_Y - \sigma_{XY}}{\sigma^2_X + \sigma^2_Y - 2\sigma_{XY}}.
\tag{3.1}\]</span></span> Using a data set that contains past measurements <span class="math display">\[
((X_1,Y_1),\dots,(X_n,Y_n))
\]</span> for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y,\)</span> we can estimate the unknown <span class="math inline">\(\alpha\)</span> by plugging in estimates of the variances and covariances <span id="eq-alphahat"><span class="math display">\[
\hat\alpha_n = \frac{\hat\sigma^2_{Y,n} - \hat\sigma_{XY,n}}{\hat\sigma^2_{X,n} + \hat\sigma^2_{Y,n} - 2\hat\sigma_{XY,n}}
\tag{3.2}\]</span></span> with <span class="math display">\[
\begin{align*}
\hat{\sigma}^2_{X,n}&amp;=\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2\\
\hat{\sigma}^2_{Y,n}&amp;=\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2\\
\hat{\sigma}_{XY,n}&amp;=\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)\left(Y_i-\bar{Y}\right),
\end{align*}
\]</span> where <span class="math inline">\(\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i\)</span> and <span class="math inline">\(\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i.\)</span></p>
<p>It is natural to wish to quantify the accuracy of our estimator <span class="math display">\[
\hat\alpha_n\approx \alpha.
\]</span></p>
<p>For instance, to construct a confidence interval we need to know the standard error of the estimator <span class="math inline">\(\hat\alpha\)</span>, <span class="math display">\[
\sqrt{Var(\hat\alpha_n)} = \operatorname{SE}(\hat\alpha_n)=?
\]</span> However, deriving an explicit expression for <span class="math inline">\(\operatorname{SE}(\hat\alpha)\)</span> is difficult here due to the definition of <span class="math inline">\(\hat\alpha_n\)</span> in <a href="#eq-alphahat" class="quarto-xref">Equation&nbsp;<span>3.2</span></a> which contains variance estimators also in the denominator.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Conclusion: Why Bootstrap?
</div>
</div>
<div class="callout-body-container callout-body">
<p>In cases as described above, we are happy to use the <strong>Basic Bootstrap Method</strong> (<a href="#sec-BasicBootstrap" class="quarto-xref"><span>Section 3.3</span></a>) which allows estimating <span class="math inline">\(\operatorname{SE}(\hat\alpha)\)</span> by resampling from the data observed; i.e.&nbsp;without the need of an explicit formula of a consistent estimator of <span class="math inline">\(\operatorname{SE}(\hat\alpha).\)</span> The <strong>Basic Bootstrap Method</strong> is found to be as accurate as the standard asymptotic normality results which, however, require an explicit formula of an estimator of <span class="math inline">\(\operatorname{SE}(\hat\alpha)\)</span> to become useful.</p>
<p>If we have a consistent estimator for the <span class="math inline">\(\operatorname{SE}(\hat\alpha),\)</span> then we can make use of this estimator by applying the <strong>Bootstrap-<span class="math inline">\(\mathbf{t}\)</span> Method</strong> (<a href="#sec-BootT" class="quarto-xref"><span>Section 3.4</span></a>). The Bootstrap-<span class="math inline">\(t\)</span> Method is found to be <strong>more accurate</strong> than the standard asymptotic normality results.</p>
</div>
</div>
</section>
<section id="recap-the-empirical-distribution-function" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="recap-the-empirical-distribution-function"><span class="header-section-number">3.2</span> Recap: The Empirical Distribution Function</h2>
<p>The distribution of a real-valued random variable <span class="math inline">\(X\)</span> can be completely described by its (cumulative) distribution function</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-cdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 ((Cumulative) Distribution Function (CDF))</strong></span> <span class="math display">\[
F(x)=P(X \leq x)\quad\text{for all}\quad x\in\mathbb{R}.
\]</span></p>
</div>
</div>
</div>
</div>
<p>The sample analogue of <span class="math inline">\(F\)</span> is the so-called <strong>empirical distribution function</strong>, which is an important tool of statistical inference.</p>
<p>Let<br>
<span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}\sim X
\]</span> denote a real-valued random sample with <span class="math inline">\(X\sim F,\)</span> and let <span class="math inline">\(1_{(\cdot)}\)</span> denote the indicator function, i.e., <span class="math display">\[
\begin{align*}
1_{(\text{TRUE})} &amp;=1\quad\text{and}\quad 1_{(\text{FALSE})}=0.
\end{align*}
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-ecdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Empirical (Cumulative) Distribution Function (ECDF))</strong></span> <span class="math display">\[
F_n(x)=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\quad\text{for all}\quad x\in\mathbb{R}.
\]</span> I.e <span class="math inline">\(F_n(x)\)</span> is the proportion of observations with <span class="math inline">\(X_i\le x,\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
</div>
</div>
</div>
</div>
<p><strong>Properties of the ECDF:</strong></p>
<p><span class="math inline">\(F_n\)</span> is a <strong>monotonically increasing right-continuous step function</strong> that is bounded between zero and one, <span class="math display">\[
0\le F_n(x)\le 1,
\]</span> where <span class="math display">\[
F_n(x)=\left\{
  \begin{array}{ll}
  0&amp;\text{ if }x  &lt; X_{(1)}\\
  1&amp;\text{ if }x\ge X_{(n)}\\
  \end{array}
\right.
\]</span> where <span class="math display">\[
X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}
\]</span> denotes the <strong>order-statistic</strong>.</p>
<p><span class="math inline">\(F_n\)</span> is itself a <strong>distribution function according to <a href="#def-cdf" class="quarto-xref">Definition&nbsp;<span>3.1</span></a></strong>; namely, the distribution function of the <strong>discrete random variable</strong> <span class="math inline">\(X^*,\)</span> where</p>
<p><span class="math display">\[
X^*\in\{X_1,\dots,X_n\}
\]</span> and <span class="math display">\[
P(X^*=X_i)=\frac{1}{n}\quad\text{for each}\quad i=1,\dots,n.
\]</span> Thus <span class="math display">\[
\begin{align*}
F_n(x)
&amp;=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\\[2ex]
&amp;= P\left(X^*\leq x\right).
\end{align*}
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-ecdfexample" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Computing the empirical distribution function <span class="math inline">\(F_n\)</span> in <code>R</code>)</strong></span> <br></p>
<p>Some data, i.e.&nbsp;an observed realization of a random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d}}{\sim}F:\)</span></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.20</td>
</tr>
<tr class="even">
<td>2</td>
<td>4.80</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.30</td>
</tr>
<tr class="even">
<td>4</td>
<td>4.60</td>
</tr>
<tr class="odd">
<td>5</td>
<td>6.10</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.40</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.80</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
</tr>
</tbody>
</table>
<p>Corresponding empirical distribution function using <code>R</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">5.20</span>, <span class="fl">4.80</span>, <span class="fl">5.30</span>, <span class="fl">4.60</span>, </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">6.10</span>, <span class="fl">5.40</span>, <span class="fl">5.80</span>, <span class="fl">5.50</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>myecdf_fun     <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(observedSample)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myecdf_fun, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/ecdfPlot-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The <code>R</code> function <code>ecdf()</code> returns a function that gives the values of <span class="math inline">\(F_n(x):\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: ecdf() returns a function that can be evaluated! </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">myecdf_fun</span>(<span class="fl">5.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.25</code></pre>
</div>
</div>
<p>Sampling (iid) from the empirical distribution function <span class="math inline">\(F_n\)</span> is equivalent to resampling (with replacement and with equal probabilities) data points from the observed data <code>observedSample</code>. The following code generates three “bootstrap” samples from <span class="math inline">\(F_n\colon\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n         <span class="ot">&lt;-</span> <span class="fu">length</span>(observedSample)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>resample1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(observedSample, </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">size    =</span> n, </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>resample1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.3 4.8 5.3 5.4 5.8 6.1 5.4 5.4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>resample2 <span class="ot">&lt;-</span> <span class="fu">sample</span>(observedSample, </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">size    =</span> n, </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>resample2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.4 4.6 5.3 5.3 4.6 6.1 5.2 5.2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>resample3 <span class="ot">&lt;-</span> <span class="fu">sample</span>(observedSample, </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">size    =</span> n, </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>resample3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.8 5.2 6.1 5.2 5.2 4.6 6.1 5.8</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="statistical-properties-of-f_n" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="statistical-properties-of-f_n"><strong>Statistical Properties of <span class="math inline">\(F_n\)</span></strong></h4>
<p><span class="math inline">\(F_n(x)\)</span> depends on the i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span> and thus is itself a <strong>random function</strong>.</p>
<p>We obtain <span id="eq-ecdfDistr"><span class="math display">\[
nF_n(x)\sim B(n, p=F(x))\quad\text{for each}\quad x\in\mathbb{R}
\tag{3.3}\]</span></span></p>
<p>I.e., <span class="math inline">\(nF_n(x)\)</span> has a binomial distribution with parameters:</p>
<ul>
<li><span class="math inline">\(n\)</span> (“number of trials”)</li>
<li><span class="math inline">\(p=F(x)\)</span> (“probability of success on a single trial”).</li>
</ul>
<blockquote class="blockquote">
<p><strong>Note:</strong> The result in <a href="#eq-ecdfDistr" class="quarto-xref">Equation&nbsp;<span>3.3</span></a> holds for any <span class="math inline">\(F.\)</span> Therefore, <span class="math inline">\(nF_n,\)</span> and thus also <span class="math inline">\(F_n,\)</span> is called <strong>distribution free</strong></p>
</blockquote>
<p><a href="#eq-ecdfDistr" class="quarto-xref">Equation&nbsp;<span>3.3</span></a> implies that <span class="math display">\[
\begin{align*}
\mathbb{E}(nF_n(x))&amp; = np = nF(x)\\[2ex]
\Rightarrow \quad \mathbb{E}(F_n(x))&amp; = p = F(x)\\[2ex]
\Rightarrow \quad \operatorname{Bias}(F_n(x))&amp; = \mathbb{E}(F_n(x)) - F(x) =0\\
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
Var(nF_n(x))&amp; = np(1-p) = nF(x)(1-F(x))\\[2ex]
\Rightarrow \quad Var(F_n(x))&amp; = \frac{nF(x)(1-F(x))}{n^2}=\frac{F(x)(1-F(x))}{n}.
\end{align*}
\]</span> Therefore, <span class="math display">\[
\begin{align*}
\operatorname{MSE}(F_n(x))
&amp; = (\operatorname{Bias}(F_n(x)))^2 + Var(F_n(x))\\[2ex]
&amp; =\frac{F(x)(1-F(x))}{n}\\[2ex]
\Rightarrow\quad\operatorname{MSE}(F_n(x))&amp;\to 0\quad\text{as}\quad n\to\infty
\end{align*}
\]</span> pointwise for each <span class="math inline">\(x\in\mathbb{R}.\)</span></p>
<p>This allows us to conclude that <span class="math display">\[
\begin{align*}
F_n(x) &amp; \to_{m.s.} F(x)\quad\text{as}\quad n\to\infty\\[2ex]
\Rightarrow \quad F_n(x) &amp; \to_{p} F(x)\quad\text{as}\quad n\to\infty
\end{align*}
\]</span> pointwise for each <span class="math inline">\(x\in\mathbb{R}.\)</span></p>
<p>That is, <span class="math inline">\(F_n(x)\)</span> is a <strong>pointwise consistent</strong> estimator of <span class="math inline">\(F(x)\)</span> for each <span class="math inline">\(x\in\mathbb{R}.\)</span></p>
<p>The Clivenko-Cantelli <a href="#thm-Clivenko-Cantelli" class="quarto-xref">Theorem&nbsp;<span>3.1</span></a> states that <span class="math inline">\(F_n\)</span> is even an <strong>uniformly consistent</strong> estimator of <span class="math inline">\(F.\)</span></p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-Clivenko-Cantelli" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Theorem of Glivenko-Cantelli)</strong></span> <br></p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}\sim X\)</span> denote a real-valued random sample with <span class="math inline">\(X\sim F.\)</span> Then <span class="math display">\[
\begin{align*}
&amp;\quad P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1\\[2ex]
\Leftrightarrow &amp;\quad
\sup_{x\in\mathbb{R}} |F_n(x)-F(x)|\to_{a.s.} 0\quad\text{as}\quad n\to\infty.
\end{align*}
\]</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="idea-of-the-bootstrap" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="idea-of-the-bootstrap"><span class="header-section-number">3.2.1</span> Idea of the Bootstrap</h3>
<p>The basic idea of the bootstrap is to replace random sampling from the true (unknown) population <span class="math inline">\(F\)</span> (infeasible Monte Carlo simulation) by random sampling from the empirical distribution <span class="math inline">\(F_n\)</span> (feasible “Monte Carlo simulation”).</p>
<p><strong>Sampling from the population distribution <span class="math inline">\(F\)</span> (infeasible Monte Carlo simulation)</strong> <br>The random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\)</span> with <span class="math inline">\(X\sim F\)</span> is generated by drawing observations independently and with replacement from the unknown population distribution function <span class="math inline">\(F\)</span>. That is, for each interval <span class="math inline">\([a,b]\)</span> the probability of drawing an observation in <span class="math inline">\([a,b]\)</span> is given by <span class="math display">\[
P(X\in [a,b])=F(b)-F(a).
\]</span> Let <span class="math inline">\(\theta_0\)</span> denote a distribution parameter of <span class="math inline">\(F\)</span> which we want to estimate, and let <span class="math inline">\(\hat\theta_n\)</span> denote an estimator of <span class="math inline">\(\theta_0.\)</span><br> If we would know <span class="math inline">\(F,\)</span> we could generate arbitrarily many realizations of the estimator <span class="math inline">\(\hat{\theta}_n\)</span> <span class="math display">\[
\hat{\theta}_{n,1}, \hat{\theta}_{n,2}, \dots, \hat{\theta}_{n,m}
\]</span> with <span class="math inline">\(m\to\infty\)</span> and do inference about <span class="math inline">\(\theta_0\)</span> using these realizations.</p>
<!-- 
For instance we could construct confidence intervals using the following consistent estimator for the standard error of $\hat\theta_n\colon$
$$
\left(\frac{1}{m}\sum_{j=1}^m\left(\hat{\theta}_{n,j} - \frac{1}{m}\sum_{k=1}^m\hat{\theta}_{n,k}\right)^2\right)^{1/2}\to_P \operatorname{SE}(\hat{\theta}_n),\quad\text{as}\quad m\to\infty.
$$ 
-->
<p>Unfortunately, we don’t know <span class="math inline">\(F,\)</span> thus Monte Carlo inference is infeasible.</p>
<p><strong>The idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:</strong> <br> Instead of random sampling from <span class="math inline">\(F,\)</span> which is infeasible (as we don’t know <span class="math inline">\(F\)</span>), the bootstrap uses random sampling from the known empirical distribution function <span class="math inline">\(F_n\)</span> to generate arbitrarily many <strong>bootstrap realizations</strong> of the estimator <span class="math inline">\(\hat{\theta}_n\)</span> <span class="math display">\[
\hat{\theta}^*_{n,1}, \hat{\theta}^*_{n,2}, \dots, \hat{\theta}^*_{n,m}
\]</span> with <span class="math inline">\(m\to\infty\)</span> and do inference about <span class="math inline">\(\theta_0\)</span> using these bootstrap realizations.<br> This is justified asymptotically since for large <span class="math inline">\(n,\)</span> the empirical distribution <span class="math inline">\(F_n\)</span> is “close” to the unknown distribution <span class="math inline">\(F\)</span> (Glivenko-Cantelli <a href="#thm-Clivenko-Cantelli" class="quarto-xref">Theorem&nbsp;<span>3.1</span></a>). That is, for <span class="math inline">\(n\rightarrow\infty\)</span> the relative frequency of observations <span class="math inline">\(X_i\)</span> in <span class="math inline">\([a,b]\)</span> converges to <span class="math inline">\(P(X\in [a,b])\)</span><br>
<span class="math display">\[
  \begin{align*}
  \underbrace{\frac{1}{n}\sum_{i=1}^n1_{(X_i\in[a,b])}}_{=F_n(b)-F_n(a)}&amp;\to_p \underbrace{P(X\in [a,b])}_{=F(b)-F(a)}
  \end{align*}
\]</span></p>
</section>
</section>
<section id="sec-BasicBootstrap" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-BasicBootstrap"><span class="header-section-number">3.3</span> The Basic Bootstrap Method</h2>
<p>The basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e.&nbsp;parametric) assumption on <span class="math inline">\(F.\)</span> The basic bootstrap method is often also called:</p>
<ul>
<li>(Standard) Nonparametric Bootstrap Method or</li>
<li>Nonparametric Percentile Bootstrap Method</li>
</ul>
<p><strong>Setup:</strong></p>
<ul>
<li><p>i.i.d. sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> where <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\)</span> with real valued <span class="math inline">\(X\sim F.\)</span></p></li>
<li><p>The distribution <span class="math inline">\(F\)</span> is depends on an unknown parameter <span class="math inline">\(\theta_0.\)</span></p></li>
<li><p>The data <span class="math inline">\(X_1,\dots,X_n\)</span> is used to estimate <span class="math inline">\(\theta_0\in\mathbb{R}.\)</span></p></li>
<li><p>Thus, the estimator is a function of the random sample <span class="math display">\[
\hat\theta_n\equiv \hat\theta(X_1,\dots,X_n).
\]</span></p></li>
<li><p>Moreover, for simplicity let us focus on <strong>unbiased</strong> and <span class="math inline">\(\boldsymbol{\sqrt{n}}\)</span><strong>-consistent</strong> estimators, i.e.</p>
<ul>
<li><span class="math inline">\(\mathbb{E}\left(\hat\theta_n\right)=\theta_0\)</span></li>
<li><span class="math inline">\(\operatorname{SE}\left(\hat\theta_n\right)=\sqrt{Var\left(\hat\theta_n\right)}=\frac{1}{\sqrt{n}}\cdot\text{constant}\)</span></li>
</ul></li>
</ul>
<p><strong>Inference (approximate for <span class="math inline">\(\boldsymbol{n\to\infty}\)</span>):</strong> In order to provide standard errors, construct confidence intervals, and perform tests of hypothesis, we need to know the <strong>distribution</strong> of <span class="math display">\[
\sqrt{n}\left(\hat\theta_n-\theta_0\right)\quad\text{as}\quad n\to\infty.
\]</span> I.e. we need to know the limit of the distribution function <span class="math display">\[
H_{n}(x)=P\left(\sqrt{n}\left(\hat\theta_n-\theta_0\right)\leq x\right)\quad\text{as}\quad n\to\infty.
\]</span></p>
<p>We could use asymptotic statistics to derive this limit. For instance, using the Lindeberg-Lévy CLT, we may be able to show that the limit of <span class="math inline">\(H_{n}(x)\)</span> is the distribution function of the normal distribution with mean zero and asymptotic variance <span class="math inline">\(\lim_{n\to\infty}n\cdot Var\big(\hat\theta_n\big).\)</span></p>
<p>However, deriving a useful, <em>explicit</em> expression of the asymptotic variance <span class="math inline">\(\lim_{n\to\infty}n\cdot Var\big(\hat\theta_n\big)\)</span> can be <em>very</em> hard (see <a href="#sec-Illustration" class="quarto-xref"><span>Section 3.1</span></a>). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific versions of the Bootstrap (<a href="#sec-BootT" class="quarto-xref"><span>Section 3.4</span></a>) can be even more accurate then an asymptotic normality result.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Core Part of the Bootstrap Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Draw a bootstrap sample:</strong> Generate a new random sample <span class="math display">\[
X_1^*,\dots,X_n^*
\]</span> by drawing observations <strong>independently and with replacement</strong> from the available sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span><br></li>
<li><strong>Compute bootstrap estimate:</strong> Compute the estimate <span class="math display">\[
\hat\theta^*_n\equiv \hat\theta(X_1^*,\dots,X_n^*)
\]</span></li>
<li><strong>Bootstrap replications:</strong> Repeat Steps 1 and 2 <span class="math inline">\(m\)</span> times (for a large value of <span class="math inline">\(m,\)</span> such as <span class="math inline">\(m=10000\)</span>) leading to <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*
\]</span></li>
</ol>
</div>
</div>
<p>By the Clivenko-Cantelli (<a href="#thm-Clivenko-Cantelli" class="quarto-xref">Theorem&nbsp;<span>3.1</span></a>) the bootstrap estimates <span class="math display">\[
\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*
\]</span> allow us to approximate the <strong>bootstrap distribution</strong><br>
<span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\hat\theta^*_n-\hat\theta_n\right)\leq x\right|\mathcal{S}_n\right)
\]</span> arbitrarily well, i.e., <span class="math display">\[
\sup_{x\in\mathbb{R}}\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\right|\to_{a.s} 0\quad\text{as}\quad m\to\infty,
\]</span> where <span class="math display">\[
H^{Boot}_{n,m}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(\sqrt{n}\left(\hat\theta^*_{n,j}-\hat\theta_n\right)\leq x\right)}
\]</span> denotes the <strong>empirical distribution function</strong> based on the <span class="math inline">\(\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*\)</span> <strong>centered</strong> by <span class="math inline">\(\hat{\theta}_n\)</span> and <strong>scaled</strong> by <span class="math inline">\(\sqrt{n}.\)</span></p>
<p>Since we can choose <span class="math inline">\(m\)</span> arbitrarily large, we can effectively ignore the approximation error between <span class="math inline">\(H^{Boot}_{n,m}(x)\)</span> and <span class="math inline">\(H^{Boot}_{n}(x)\)</span> and proceed as if <span class="math display">\[
H^{Boot}_{n,m}(x)\overset{(m\to\infty)}{=}H^{Boot}_{n}(x)\quad\text{for all}\quad x.
\]</span></p>
<p>The crucial question is, however, whether the (effectively known) bootstrap distribution <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\hat\theta^*_n-\hat\theta_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
\]</span> can be used to estimate the limit distribution <span class="math display">\[
H_{n}(x)=P\left(\sqrt{n}\left(\hat\theta_n-\theta_0\right)\leq x\right)\quad\text{as}\quad n\to\infty?
\]</span> This is a basic requirement called <strong>bootstrap consistency</strong>. If a bootstrap method is inconsistent, you shall not use it in practice.</p>
<section id="bootstrap-consistency" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="bootstrap-consistency">Bootstrap Consistency</h3>
<p>The bootstrap <strong>does not always work</strong>.</p>
<p>A necessary condition for the use of the bootstrap is the <strong>consistency of the bootstrap approximation</strong>.</p>
<p>The bootstrap is called <strong>consistent</strong> if, for large <span class="math inline">\(n\)</span>, the bootstrap distribution of <span class="math display">\[
\sqrt{n}\big(\hat{\theta}^*_n -\hat{\theta}_n\big)|\mathcal{S}_n
\]</span> is a good approximation of the distribution of <span class="math display">\[
\sqrt{n}\big(\hat{\theta}_n-\theta_0\big);
\]</span> i.e., if <span class="math display">\[
\underbrace{\text{distribution}\left(\sqrt{n}\big(\hat{\theta}^*_n -\hat{\theta}_n\big)\ |{\cal S}_n\right)}_{H_n^{Boot}}\approx
\underbrace{\text{distribution}\left(\sqrt{n}\big(\hat{\theta}_n-\theta_0\big)\right)}_{H_n}.
\]</span> for large <span class="math inline">\(n.\)</span></p>
<p>The following definition states this more precisely.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-BootstrapConsistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 (Bootstrap Consistency)</strong></span> <br> Let the limit (as <span class="math inline">\(n\to\infty\)</span>) of <span class="math inline">\(H_n\)</span> be a non-degenerate distribution. Then the bootstrap is <strong>consistent</strong> if and only if <span class="math display">\[
\sup_{x\in\mathbb{R}} \Big|\;
\underbrace{P\Big(\sqrt{n}\big(\hat\theta^*_n-\hat\theta_n\big)\le x \ |{\cal S}_n\Big)}_{H_n^{Boot}(x)}
  -\underbrace{P\Big(\sqrt{n}\big(\hat\theta_n -\theta_0\big)\le x\Big)}_{H_n(x)}
  \Big|\rightarrow_p 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
</div>
</div>
</div>
</div>
<p>Luckily, the standard bootstrap is consistent in a large number of statistical problems. Typically, the bootstrap is consistent if the following two requirements hold:</p>
<ol type="1">
<li>The bootstrap sampling process must reflect the original sampling process. For instance:
<ul>
<li>if the original sample was generated by i.i.d. sampling, then also the bootstrap samples need to be generated by i.i.d. sampling.</li>
<li>if the original sample was generated by cluster sampling, then also the bootstrap samples need to be generated by cluster sampling.</li>
</ul></li>
<li>Typically, the distribution of <span class="math display">\[
\sqrt{n}\left(\hat\theta_n-\theta_0\right)
\]</span> needs to be asymptotically (<span class="math inline">\(n\to\infty\)</span>) normal.</li>
</ol>
<blockquote class="blockquote">
<p>Theorem 1 in <span class="citation" data-cites="Mammen_1992_Book">Mammen (<a href="#ref-Mammen_1992_Book" role="doc-biblioref">1992</a>)</span> shows that the basic bootstrap is consistent if <span class="math inline">\(\sqrt{n}(\hat{\theta}_n-\theta_0)\to_d\mathcal{N}(0,v_0^2),\)</span> under the assumption that the bootstrap sampling process (e.g.&nbsp;i.i.d.) equals the original sampling process.</p>
</blockquote>
<p>The standard bootstrap <strong>will usually fail</strong> if one of the above conditions is violated.</p>
<!-- 
For instance:

* the  bootstrap will not work if the i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $\mathcal{S}_n=\{X_1,\dots,X_n\}$ does not properly reflect the way how $X_1,\dots,X_n$ are generated in the first place. (For instance, when $X_1,\dots,X_n$ is generated by a time-series process with auto-correlated data, but the bootstrap samples are generated by i.i.d. sampling from $\mathcal{S}_n$) 
* the  bootstrap will not work if the distribution of 
$$
\sqrt{n}\left(\hat\theta_n-\theta_0\right)
$$ 
is not asymptotically normal. (For instance, in case of extreme value problems.) 
-->
<p><strong>Note:</strong> In order to deal with more complex sampling schemes alternative bootstrap procedures have been proposed in the literature (e.g.&nbsp;the block-bootstrap in case of time-series data).</p>
</section>
<section id="example-inference-about-the-population-mean" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="example-inference-about-the-population-mean"><span class="header-section-number">3.3.1</span> Example: Inference About the Population Mean</h3>
<p><strong>Setup:</strong></p>
<ul>
<li><span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\)</span> with <span class="math inline">\(X\sim F\)</span></li>
<li>Continuous random variable <span class="math inline">\(X\)</span> (not necessary, but allows working with the density function <span class="math inline">\(f\)</span>)</li>
<li>Non-zero, finite variance <span class="math inline">\(0&lt;Var(X)=\sigma_0^2&lt;\infty\)</span></li>
<li>Unknown mean <span class="math inline">\(\mathbb{E}(X)=\mu_0,\)</span> where<br>
<span class="math display">\[
\mu_0 = \int x f(x) dx = \int x d F(x),
\]</span> where <span class="math inline">\(f=F'\)</span> denotes the density function.</li>
<li>Estimator: Empirical mean <span class="math display">\[
\begin{align*}
\bar{X}_n
&amp;\equiv \bar{X}(X_1,\dots,X_n) \\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n X_i \\[2ex]
&amp;=\int x d F_n(x)
\end{align*}
\]</span></li>
</ul>
<p>For doing <strong>inference (approximate for <span class="math inline">\(\boldsymbol{n\to\infty}\)</span>) about <span class="math inline">\(\boldsymbol{\mu_0}\)</span></strong> we need to know the (asymptotic) distribution of <span class="math display">\[
\sqrt{n}\left(\bar{X}_n -\mu_0\right)\quad\text{as}\quad n\to\infty.
\]</span></p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="exm-AsympNorm" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Recap: Inference about <span class="math inline">\(\mu_0\)</span> using Classic Asymptotic Statistics)</strong></span> By the Lindeberg-Lévy CLT we know that <span class="math display">\[
\sqrt{n}\left(\bar{X}_n -\mu_0\right)\to_d\mathcal{N}\left(0,v^2\right)\quad\text{as}\quad n\to\infty,
\]</span> which is equivalent to <span class="math display">\[
%\bar{X}_n\overset{a}{\sim}\mathcal{N}\left(\mu_0,\frac{1}{n}\sigma_0\right).
H_n(x)=P\left(\sqrt{n}\left(\bar{X}_n-\mu_0\right)\leq x\right)\to\Phi_{v}(x)\quad\text{as}\quad n\to\infty,
\]</span> for all continuity points <span class="math inline">\(x,\)</span> where <span class="math inline">\(\Phi_{v}\)</span> denotes the distribution function of the normal distribution with mean zero and standard deviation <span class="math inline">\(v,\)</span> (variance <span class="math inline">\(v^2\)</span>), <span class="math display">\[
\Phi_{v}(x)=\Phi\left(\frac{x}{v}\right)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x/v}\exp\left(-\frac{1}{2}z^2\right)\,dz
\]</span> with <span class="math inline">\(\Phi\)</span> denoting the distribution function of the standard normal distribution.</p>
<p>However, without knowledge of <span class="math inline">\(v^2,\)</span> the above result is of limited use. Luckily, in this example, it is rather simple to derive an useful expression for the asymptotic variance <span class="math inline">\(v^2\colon\)</span> <span id="eq-AsympVarDeriv"><span class="math display">\[
\begin{align*}
v^2
&amp;=\lim_{n\to\infty}Var\left(\sqrt{n}\left(\bar{X}_n -\mu_0\right)\right)%\\[2ex]
=\lim_{n\to\infty}nVar(\bar{X}_n)\\[2ex]
&amp;=\lim_{n\to\infty}nVar\left(\frac{1}{n}\sum_{i=1}^n X_i\right)\\[2ex]
&amp;\left[\text{Under our iid setup: }X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\right]\\[2ex]
&amp;=\lim_{n\to\infty}\frac{n}{n^2} nVar\left(X\right)%\\[2ex]
=Var\left(X\right) = \sigma_0^2.
\end{align*}
\tag{3.4}\]</span></span></p>
<p>That is, <span class="math display">\[
\Phi_{v}(x)=\Phi_{\sigma_0}(x)\quad\text{for all}\quad x.
\]</span></p>
<p>The parameter <span class="math inline">\(\sigma_0^2=Var\left(X\right)\)</span> is usually unknown, but can be estimated consistently by <span class="math display">\[
\hat{\sigma}^2_n=\frac{1}{n}\sum_{i=1}^n\left(X_i - \bar{X}_n\right)^2.
\]</span> Thus, for doing inference (constructing confidence intervals, statistical tests, etc.) about <span class="math inline">\(\mu_0,\)</span> we actually use the estimated distribution <span class="math display">\[
H^{Asymp}_n(x)=\Phi_{\hat\sigma_n}(x)=\Phi\left(\frac{x}{\hat\sigma_n}\right)\quad\text{for all}\quad x,
\]</span> where <span class="math inline">\(H^{Asymp}_n(x)\to\Phi_{\sigma_0}(x)\)</span> as <span class="math inline">\(n\to\infty\)</span> for all continuity points <span class="math inline">\(x.\)</span> That is, we use the approximate distribution <span class="math display">\[
\begin{align*}
\bar{X}_n &amp; \overset{a}{\sim}H^{Asymp}_n
\quad\Leftrightarrow\quad
\bar{X}_n \overset{a}{\sim}\mathcal{N}\left(\mu_0,\frac{\hat{\sigma}_n^2}{n}\right)
\end{align*}
\]</span> for computing, for instance, an estimator for the standard error <span class="math inline">\(\operatorname{SE}\big(\bar{X}_n\big),\)</span> <span id="eq-SEAsymp"><span class="math display">\[
\widehat{\operatorname{SE}}^{Asymp}_{n}\big(\bar{X}_n\big)=\sqrt{\frac{\hat{v}^2}{n}}=\sqrt{\frac{\hat\sigma_n^2}{n}}.
\tag{3.5}\]</span></span> Using <span class="math inline">\(\widehat{\operatorname{SE}}^{Asymp}_{n}\big(\bar{X}_n\big),\)</span> we can compute confidence intervals and statistical tests in the usual way.</p>
<p><strong>Note:</strong> The estimator of the standard error in <a href="#eq-SEAsymp" class="quarto-xref">Equation&nbsp;<span>3.5</span></a> requires a formula for the asymptotic variance <span class="math inline">\(v^2=\sigma_0^2\)</span> as derived in <a href="#eq-AsympVarDeriv" class="quarto-xref">Equation&nbsp;<span>3.4</span></a>. Such derivations can be very challenging (see <a href="#sec-Illustration" class="quarto-xref"><span>Section 3.1</span></a>).</p>
</div>
</div>
</div>
</div>
<p>Let us now check, whether the Bootstrap distribution <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
\]</span> is a legit alternative to <span class="math inline">\(H^{Asymp}_n(x).\)</span> I.e. we check whether we can use <span class="math inline">\(H^{Boot}_{n}(x)\)</span> as an alternative for estimating the limit distribution <span class="math inline">\(\Phi_{v}(x)=\Phi_{\sigma_0}(x).\)</span></p>
<p>Before we answer this question theoretically (see <a href="#sec-Theory1" class="quarto-xref"><span>Section 3.3.1.2</span></a> and <a href="#sec-Theory2BootstrapConsist" class="quarto-xref"><span>Section 3.3.1.3</span></a>), we check it empirically using an <strong>artifical data example.</strong> This is then, of course, not a generally valid mathematical proof.</p>
<section id="sec-PracticeXbar" class="level4" data-number="3.3.1.1">
<h4 data-number="3.3.1.1" class="anchored" data-anchor-id="sec-PracticeXbar"><span class="header-section-number">3.3.1.1</span> <strong>Practice: Empirical Consideration of the Bootstrap distribution <span class="math inline">\(\boldsymbol{H^{Boot}_{n}}\)</span></strong></h4>
<p>Let us consider the following (usually unknown) specific setup: <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X,
\]</span> where</p>
<ul>
<li><span class="math inline">\(X\sim \chi^2_{\operatorname{df}}\)</span> with <span class="math inline">\(\operatorname{df}=2\)</span></li>
<li>sample size <span class="math inline">\(n=100\)</span></li>
</ul>
<p>This setup implies (see <a href="#exm-AsympNorm" class="quarto-xref">Example&nbsp;<span>3.2</span></a>) that</p>
<ul>
<li>True (usually unknown) mean: <span class="math inline">\(\mu_0=\mathbb{E}(X)=\operatorname{df}=2\)</span></li>
<li>True (usually unknown) variance: <span class="math inline">\(\sigma_0^2=Var(X)=2\cdot\operatorname{df}=4\)</span></li>
<li>True (usually unknown) standard deviation: <span class="math inline">\(\sigma_0=\sqrt{Var(X)}=2\)</span></li>
<li>True (usually unknown) asymptotic limit distribution: <span class="math display">\[
\begin{align*}
\sqrt{n}(\bar{X}_n-\mu_0)&amp;\to_d\mathcal{N}(0,4)\quad\text{as}\quad n\to\infty,\\[2ex]
\Leftrightarrow\quad H_n(x)&amp;\to \Phi_{2}(x)\quad\text{as}\quad n\to\infty,
\end{align*}
\]</span> for all continuity points <span class="math inline">\(x.\)</span></li>
</ul>
<p>When doing classic asymptotic inference (see <a href="#exm-AsympNorm" class="quarto-xref">Example&nbsp;<span>3.2</span></a>), we would use the feasible <span class="math display">\[
H^{Asymp}_n(x)=\Phi_{\hat\sigma_n}(x)
\]</span> to estimate the (usually unknown) asymptotic limit distribution <span class="math inline">\(\Phi_{2}(x).\)</span></p>
<p>Let us now check (by simulation of artifical data) whether the bootstrap distribution <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
\]</span> can be a legit alternative for estimating the (usually unknown) <span class="math inline">\(\Phi_{2}(x).\)</span></p>
<p>The following <code>R</code>-code generates artifical data <span class="math display">\[
\mathcal{S}_n=\{X_1,\dots,X_n\}
\]</span> by (i.i.d.) sampling <span class="math inline">\(n=100\)</span> data points from a <span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\(\operatorname{df}=2.\)</span> The first six data points are shown in <a href="#tbl-ChiSqSample" class="quarto-xref">Table&nbsp;<span>3.1</span></a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(<span class="at">n =</span> <span class="dv">100</span>, <span class="at">df =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tbl-ChiSqSample" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ChiSqSample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: First six data points of the observed realization of the random sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> with sample size <span class="math inline">\(n=100\)</span> drawn from a <span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\(\operatorname{df}=2.\)</span>
</figcaption>
<div aria-describedby="tbl-ChiSqSample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.36</td>
</tr>
<tr class="even">
<td>2</td>
<td>3.39</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3.24</td>
</tr>
<tr class="even">
<td>4</td>
<td>4.90</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.76</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.33</td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
So the observed sample mean is
<center>
<span class="math inline">\(\bar X_{n} =\)</span> <code>mean(observedSample)</code> <span class="math inline">\(=\)</span> 1.8
</center>
<p><br></p>
<p><strong>Basic Bootstrap Method:</strong></p>
<p>The observed sample <span class="math display">\[
{\cal S}_n=\{X_1,\dots,X_n\}
\]</span> is taken as underlying empirical “population” from which we generate i.i.d. <strong>bootstrap samples</strong> <span class="math display">\[
X_1^*,\dots,X_n^*.
\]</span> by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}.\)</span></p>
<p>Each realization of the bootstrap sample leads to a new realization of the bootstrap estimator <span class="math display">\[
\bar{X}^*_n=\bar{X}^*(X_1^*,\dots,X_n^*).
\]</span> The following <code>R</code> code generates realizations of <span class="math display">\[
\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m}.
\]</span> for <span class="math inline">\(m=10000\)</span> Bootstrap replications.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of bootstrap replications </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>m                <span class="ot">&lt;-</span> <span class="dv">10000</span> </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Container to save the bootstrap estimates</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>Xbar_boot        <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"double"</span>, <span class="at">length =</span> m)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic Bootstrap algorithm</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="fu">seq_len</span>(m)){</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a> bootSample          <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">size    =</span> n, </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a> Xbar_boot[k]        <span class="ot">&lt;-</span> <span class="fu">mean</span>(bootSample)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Pinting the first three elements of Xbar_boot:</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>Xbar_boot[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6649553 2.2564627 1.8510396</code></pre>
</div>
</div>
<p>Based on the realizations of <span class="math display">\[
\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m},
\]</span> we can compute the empirical Bootstrap distribution function <span class="math display">\[
H^{Boot}_{n,m}(x)=\frac{1}{m}\sum_{j=1}^m 1_{\left(\sqrt{n}\left(\bar{X}^*_{n,j}-\bar{X}_n\right)\leq x\right)},
\]</span> which approximates (arbirary well as <span class="math inline">\(m\to\infty\)</span>) the bootstrap distribution <span class="math display">\[
H^{Boot}_n(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n\right)
\]</span></p>
<blockquote class="blockquote">
<p>Thus, by choosing a large <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10000\)</span>) we proceed as if <span class="math display">\[
H^{Boot}_{n,m}(x)=H^{Boot}_{n}(x)\quad\text{for all}\quad x.
\]</span></p>
</blockquote>
<p>If <span class="math display">\[
H^{Boot}_{n,m}(x)\approx \Phi_{2}(x)\quad\text{for all}\quad x,
\]</span> then we can use <span class="math inline">\(H^{Boot}_{n,m}(x)\)</span> (as an alternative to <span class="math inline">\(H^{Asymp}_{n}(x)\)</span>) to do inference about <span class="math inline">\(\mu_0.\)</span></p>
<p>The following <code>R</code>-code computes and compares graphically the following three distribution functions:</p>
<ul>
<li><span class="math inline">\(H^{Boot}_{n,m}(x) =\)</span> <code>ecdf( sqrt(n) * (Xbar_boot - Xbar) )</code> <br></li>
<li><span class="math inline">\(H^{Asymp}_{n}(x) = \Phi_{\hat{\sigma}_n}(x)=\Phi\left(\frac{x}{\hat{\sigma}_n}\right)\)</span></li>
<li><span class="math inline">\(\Phi_{2}(x)=\Phi\left(\frac{x}{2}\right)\)</span></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>n                <span class="ot">&lt;-</span> <span class="fu">length</span>(observedSample)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>Xbar             <span class="ot">&lt;-</span> <span class="fu">mean</span>(observedSample)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>m                <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co"># number of bootstrap samples </span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>Xbar_boot_vec    <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"double"</span>, <span class="at">length =</span> m)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Bootstrap algorithm</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="fu">seq_len</span>(m)){</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a> bootSample          <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">size    =</span> n, </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                               <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a> Xbar_boot_vec[k]        <span class="ot">&lt;-</span> <span class="fu">mean</span>(bootSample)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>( <span class="fu">sqrt</span>(n) <span class="sc">*</span> (Xbar_boot_vec <span class="sc">-</span> Xbar) ), </span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">""</span>, </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Bootstrap Distribution vs Normal Limit Distribution"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">pnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">4</span>)), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">pnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">var</span>(observedSample))), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)     </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="fu">expression</span>(H[<span class="st">'n,m'</span>]<span class="sc">^</span>{<span class="st">'Boot'</span>}),</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">expression</span>(H[<span class="st">'n'</span>]<span class="sc">^</span>{<span class="st">'Asymp'</span>}), </span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"Normal Distr. (Mean = 0 and SD = 2)"</span>), </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-Compare" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Ch3_Bootstrap_files/figure-html/fig-Compare-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Both <span class="math inline">\(H^{Boot}_{n,m}(x)\)</span> and <span class="math inline">\(H^{Asymp}_{n}(x)\)</span> provide here good approximations to the (usually unknown) asymptotic limit distribution <span class="math inline">\(\Phi_{2}(x).\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-Compare" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows that both <span class="math display">\[
H^{Boot}_{n,m}(x)\quad\text{and}\quad H^{Asymp}_{n}(x)
\]</span> provide good approximations to the (usually unknown) asymptotic limit distribution <span class="math inline">\(\Phi_{2}(x)\)</span>.</p>
<p>The bootstrap distribution <span class="math display">\[
\begin{align*}
H^{Boot}_{n,m}(x)&amp;=\frac{1}{m}\sum_{j=1}^m 1_{\left(\sqrt{n}\left(\bar{X}^*_{n,j}-\bar{X}_n\right)\leq x\right)}\\[2ex]
&amp;\overset{m\to\infty}{=}P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n\right)=H^{Boot}_n(x)
\end{align*}
\]</span> can be computed <strong>directly</strong> from <span class="math inline">\(\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m}.\)</span> There’s no need for deriving a formula <span class="math inline">\(v^2=\sigma_0^2\)</span> for the asymptotic variance as it is necessary for <span class="math inline">\(H^{Asymp}_{n}(x)\)</span> (see <a href="#exm-AsympNorm" class="quarto-xref">Example&nbsp;<span>3.2</span></a>).</p>
<p>In fact, using <span class="math display">\[
H^{Boot}_n(x) = P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n\right),
\]</span> we can derive an estimator for the asymptotic variance <span class="math display">\[
v^2=\lim_{n\to\infty}Var\big(\sqrt{n}\big(\bar{X}_n-\mu_0\big)\big)
\]</span> directly from <span class="math inline">\(\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m}\colon\)</span><br>
<span class="math display">\[
\begin{align*}
\hat{v}^2_{Boot,n}
&amp;=Var\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n\right)\\[2ex]
&amp;=n \; Var\left(\left.\left(\bar{X}^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n\right)\\[2ex]
&amp;\left[\text{Conditionally on $\mathcal{S}_n,$ $\bar{X}_n$ is constant}\right]\\[2ex]
&amp;=n \; Var\left(\left.\bar{X}^*_n\;\right|\;\mathcal{S}_n\right),
\end{align*}
\]</span> where the unknown <span class="math inline">\(Var\big(\left.\bar{X}^*_n\;\right|\;\mathcal{S}_n\big)\)</span> can be consistently (for <span class="math inline">\(m\to\infty\)</span>) estimated by <span class="math display">\[
\begin{align*}
\widehat{Var}_m\left(\left.\bar{X}^*_n\;\right|\;\mathcal{S}_n\right) = \frac{1}{m}\sum_{j=1}^m \left(\bar{X}^*_{n,j} - \left(\frac{1}{m}\sum_{j=1}^m\bar{X}^*_{n,j}\right)\right)^2
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p>By choosing a large <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10000\)</span>) we proceed as if <span class="math display">\[
\widehat{Var}_m\big(\left.\bar{X}^*_n\;\right|\;\mathcal{S}_n\big)=Var\big(\left.\bar{X}^*_n\;\right|\;\mathcal{S}_n\big)
\]</span></p>
</blockquote>
The estimator <span class="math display">\[
\hat{v}^2_{Boot,n}=n \; \widehat{Var}_m\left(\left.\bar{X}^*_n\;\right|\;\mathcal{S}_n\right)
\]</span> can be computed <em>directly</em> from <span class="math inline">\(\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m}.\)</span> There’s no need for deriving a formula <span class="math inline">\(v^2=\sigma_0^2\)</span> for the asymptotic variance as in <a href="#exm-AsympNorm" class="quarto-xref">Example&nbsp;<span>3.2</span></a>. For our artifical data example, we have that
<center>
<span class="math inline">\(\hat{v}^2_{Boot,n}=\)</span> <code>n * var(Xbar_boot_vec)</code> <span class="math inline">\(=\)</span> 3.41
</center>
<p><br></p>
<p>This yields an estimate of the standard error <span class="math inline">\(\operatorname{SE}\left(\bar{X}_n\right)\)</span></p>
<center>
<span class="math inline">\(\widehat{SE}_{Boot,n}\left(\bar{X}_n\right)=\sqrt{\frac{\hat{v}^2_{Boot,n}}{n}}\;=\)</span> <code>sd(Xbar_boot_vec)</code> <span class="math inline">\(=\)</span> 0.18
</center>
<p><br></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Computing the Basic Bootstrap estimate of the standard error of an estimator is typically that simple: Compute the standard deviation of <span class="math inline">\(\hat\theta^*_{n,1},\dots,\hat\theta^*_{n,m}\)</span> for a large <span class="math inline">\(m.\)</span></p>
</div>
</div>
<!-- 
:::{.callout-note}
For the given data with $n=8$ observations, there are 
$$
n^n=8^8=16,777,216
$$ 
possible bootstrap samples which are all equally probable. 
:::
-->
</section>
<section id="sec-Theory1" class="level4" data-number="3.3.1.2">
<h4 data-number="3.3.1.2" class="anchored" data-anchor-id="sec-Theory1"><span class="header-section-number">3.3.1.2</span> Theory (Part 1): Mean and Variance of the Bootstrap distribution</h4>
<p>Let <span class="math display">\[
\mathcal{S}_n=\{X_1,\dots,X_n\}
\]</span> where <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\quad\text{with}\quad X\sim F,
\]</span> and let <span class="math display">\[
X_1^*,\dots,X_n^*\overset{\text{i.i.d.}}{\sim}X^*\quad\text{with}\quad X^*\sim F_n,
\]</span> where <span class="math inline">\(F_n\)</span> denotes the ecdf computed from <span class="math inline">\(X_1,\dots,X_n.\)</span></p>
<p>In this chapter we begin with the theoretical consideration of the Bootstrap distribution <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right).
\]</span></p>
<p>We begin with focusing on the mean and the variance and check whether for large <span class="math inline">\(n\)</span> (<span class="math inline">\(b\to\infty\)</span>) the mean and the variance of <span class="math inline">\(H^{Boot}_{n}\)</span> equals the mean and the variance of <span class="math inline">\(H_n.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation <span class="math inline">\(\mathbb{E}^*(\,\cdot\,),\)</span> <span class="math inline">\(Var^*(\,\cdot\,),\)</span> and <span class="math inline">\(P^*(\,\cdot\,)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the bootstrap literature one frequently finds the notation <span class="math display">\[
\mathbb{E}^*(\,\cdot\,),\;Var^*(\,\cdot\,),\;\text{and}\;P^*(\,\cdot\,)
\]</span> to denote the <strong>conditional</strong> expectation <span class="math display">\[
\mathbb{E}^*(\,\cdot\,)=\mathbb{E}(\,\cdot\,|\mathcal{S}_n),
\]</span> the <strong>conditional</strong> variance <span class="math display">\[
Var^*(\,\cdot\,)=Var(\,\cdot\,|\mathcal{S}_n),
\]</span> and the <strong>conditional</strong> probability <span class="math display">\[
P^*(\,\cdot\,)=P(\,\cdot\,|\mathcal{S}_n),
\]</span> given the sample <span class="math inline">\({\cal S}_n.\)</span></p>
</div>
</div>
<p>The bootstrap focuses on the <strong>bootstrap distribution</strong>, i.e.&nbsp;on the conditional distribution of <span class="math display">\[
\sqrt{n}(\bar X^*_n -\bar X_n)|\mathcal{S}_n.
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
We know the distribution of <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can analyze and simulate the bootstrap distribution of <span class="math inline">\(\sqrt{n}(\bar X^*_n -\bar X_n)|\mathcal{S}_n,\)</span> since <strong>we <em>know</em> 🤟 the discrete distribution</strong> of the conditional random variables <span class="math display">\[
X^*|\mathcal{S}_n
\]</span> even though, we do <strong>not know</strong> the distribution of <span class="math inline">\(X\sim F.\)</span></p>
<!-- The discrete distribution of the conditional random variables 
$X_i^*|\mathcal{S}_n,$ is 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\}
$$
with  -->
</div>
</div>
<p>The possible values of the discrete random variable <span class="math inline">\(X^*|\mathcal{S}_n\)</span> are <span class="math display">\[
X^*|\mathcal{S}_n\in\{X_1,\dots,X_n\},
\]</span> and each of these values are equally probable: <span class="math display">\[
\begin{align*}
P^*(X^*=X_1)&amp;= P(X^*=X_1|{\cal S}_n) = \frac{1}{n} \\[2ex]
P^*(X^*=X_2)&amp;= P(X^*=X_2|{\cal S}_n) = \frac{1}{n} \\[2ex]
&amp;\vdots\\[2ex]
P^*(X^*=X_n)&amp;= P(X^*=X_n|{\cal S}_n) = \frac{1}{n}.
\end{align*}
\]</span></p>
<p>Thus, <strong>we know the whole distribution</strong> of <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> and, therefore, can compute, for instance, easily its conditional mean and its variance:</p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(X^*\)</span> given <span class="math inline">\({\cal S}_n\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{E}^*(X^*)
&amp;=\mathbb{E}(X^*|{\cal S}_n)\\[2ex]
&amp;=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_n\\[2ex]
&amp;=\bar X_n.
\end{align*}
\]</span> I.e., the empirical mean <span class="math inline">\(\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i\)</span> of the original sample <span class="math inline">\(X_1,\dots,X_n\)</span> is the “population” mean of the bootstrap sample <span class="math inline">\(X^*_1,\dots,X^*_n.\)</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(X^*\)</span> given <span class="math inline">\({\cal S}_n\)</span> is <span class="math display">\[
\begin{align*}
Var^*(X^*)
&amp;=Var(X^*|{\cal S}_n)\\[2ex]
&amp;=\mathbb{E}\left((X^* - \mathbb{E}(X^*|{\cal S}_n))^2|{\cal S}_n\right)\\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2\\[2ex]
&amp;=\hat\sigma^2_n.
\end{align*}
\]</span> I.e., the empirical variance <span class="math inline">\(\hat\sigma^2_n=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2\)</span> of the original sample <span class="math inline">\(X_1,\dots,X_n\)</span> is the “population” variance of the bootstrap sample <span class="math inline">\(X^*_1,\dots,X^*_n.\)</span></p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
General case: Conditional moments of transformed <span class="math inline">\(g(X_i^*)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any (measurable) function <span class="math inline">\(g\)</span> we have <span class="math display">\[
\mathbb{E}^*(g(X^*))=\mathbb{E}(g(X^*)|\mathcal{S}_n)=\frac{1}{n}\sum_{i=1}^n g(X_i).
\]</span> For instance, <span class="math inline">\(g(X_i)=1_{(X_i\leq x)}.\)</span></p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution: Conditioning on <span class="math inline">\(\mathcal{S}_n\)</span> in important!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Conditioning on the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> is <strong>crucial</strong>.</p>
<p>The <strong>unconditional distribution</strong> of <span class="math inline">\(X^*\)</span> is equal to the <strong>unknown distribution</strong> <span class="math inline">\(F.\)</span> This can be seen from the following derivation: <span class="math display">\[
\begin{align*}
P(X^*\leq x)
&amp;= P(1_{(X^*\leq x)}=1) \\[2ex]
&amp;= P(1_{(X^*\leq x)}=1) \cdot 1 + P(1_{(X^*\leq x)}=0) \cdot 0\\[2ex]
&amp;= \mathbb{E}\left(1_{\left(X^*\leq x\right)}\right)\\[2ex]
&amp;= \mathbb{E}\left({\color{blue}\mathbb{E}\left(1_{\left(X^*\leq x\right)}|\mathcal{S}_n\right)}\right)\quad[\text{{\color{blue}law of iterated expectations}}]\\[2ex]
&amp;= \mathbb{E}\left({\color{blue}\frac{1}{n}\sum_{i=1}^n 1_{\left(X_i\leq x\right)}}\right)\quad[\text{{\color{blue}from our derivations above}}]\\[2ex]
&amp;\quad[\text{Using that $X_1\dots,X_n\overset{\text{i.i.d.}}{\sim}X$ with $X\sim F\colon$}]\\[2ex]
&amp;= \frac{n}{n}\mathbb{E}\left(1_{\left(X\leq x\right)}\right)\\[2ex]
&amp;= P(1_{(X\leq x)}=1) \cdot 1 + P(1_{(X\leq x)}=0) \cdot 0\\[2ex]
&amp;= P\left(X\leq x\right)=F(x)
\end{align*}
\]</span></p>
</div>
</div>
<p>Using our above results, <span class="math display">\[
\begin{align*}
\mathbb{E}(X^*|\mathcal{S}_n)&amp;=\bar{X}_n\\[2ex]
Var(X^*|\mathcal{S}_n) &amp;=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X}_n)^2,
\end{align*}
\]</span> we can consider the conditional mean and the conditional variance of <span class="math display">\[
\sqrt{n}\left.\left(\bar X^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n,
\]</span> for a given realization of <span class="math inline">\(\mathcal{S}_n.\)</span></p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\)</span> given <span class="math inline">\(\mathcal{S}_n\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{E}^*\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\right)
&amp;=\mathbb{E}\left(\left.\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\right|{\cal S}_n\right)\\[2ex]
&amp;=\sqrt{n}\,\mathbb{E}\left(\left.\bar X^*_n-\bar{X}_n\right|{\cal S}_n\right)\\[2ex]
&amp;=\sqrt{n}\left(\mathbb{E}\left(\bar X^*_n|{\cal S}_n\right)- \mathbb{E}\left(\bar{X}_n|{\cal S}_n\right)\right)\\[2ex]
&amp;=\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n{\color{red}\mathbb{E}\left(X^*_i|{\cal S}_n\right)}- \frac{1}{n}\sum_{i=1}^n{\color{blue}\mathbb{E}\left(X_i|{\cal S}_n\right)}\right)\\[2ex]
&amp;=\sqrt{n}\left(\frac{n}{n}{\color{red}\bar{X}_n} - \frac{1}{n}\sum_{i=1}^n{\color{blue}X_i}\right)\\[2ex]
&amp;=\sqrt{n}\left(\bar{X}_n - \bar{X}_n\right)\\[2ex]
&amp;= 0%\\[2ex]
%\Leftrightarrow\quad \mathbb{E}\left(\bar X^*_n|{\cal S}_n\right) &amp; =  \mathbb{E}\left(\bar{X}_n|{\cal S}_n\right).
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\)</span> given <span class="math inline">\(\mathcal{S}_n\)</span> is <span class="math display">\[
\begin{align*}
Var^*\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\right)
&amp;=Var\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)|{\cal S}_n\right)\\[2ex]
&amp;=n\,Var\left(\big(\bar X^*_n-\bar{X}_n\big)|{\cal S}_n\right)\\[2ex]
&amp;[\text{Conditionally on a given}\\
&amp;\text{ realization of $\mathcal{S}_n,$ $\bar{X}_n$ is a constant:}]\\[2ex]
&amp;=n\,Var\big(\bar X^*_n|{\cal S}_n\big)\\[2ex]
&amp;=n\,Var\Big(\frac{1}{n}\sum_{i=1}^n X_i^*\Big|{\cal S}_n\Big)\\
&amp;=n\,\frac{1}{n^2}\sum_{i=1}^n Var\big(X_i^*|{\cal S}_n\big)\\
&amp;=n\,\frac{n}{n^2} Var\big(X_i^*|{\cal S}_n\big)\\
&amp;=Var\big(X_i^*|{\cal S}_n\big)\\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X}_n)^2\quad[\text{derived above}]\\[2ex]
&amp;=\hat\sigma^2_n,
\end{align*}
\]</span> where <span class="math display">\[
\hat\sigma^2_n\to_p \sigma_0^2\quad\text{as}\quad n\to\infty.
\]</span></p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>For large <span class="math inline">\(n\)</span> (<span class="math inline">\(n\to\infty\)</span>) the bootstrap distribution, i.e.&nbsp;the distribution of <span class="math display">\[
\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)|\mathcal{S}_n
\]</span> and the distrubtion of <span class="math display">\[
\sqrt{n}\left(\bar{X}_n-\mu_0\right)
\]</span> have both mean zero and variance <span class="math inline">\(\sigma^2_0.\)</span></p>
<p>That is, as <span class="math inline">\(n\to\infty,\)</span> both distributions become equal with respect to their means and variances.</p>
</div>
</div>
</section>
<section id="sec-Theory2BootstrapConsist" class="level4" data-number="3.3.1.3">
<h4 data-number="3.3.1.3" class="anchored" data-anchor-id="sec-Theory2BootstrapConsist"><span class="header-section-number">3.3.1.3</span> Theory (Part 2): Bootstrap Consistency</h4>
<p>In this chapter we continue our theoretical consideration of the Bootstrap distribution <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right),
\]</span> but consider now the total distribution—not only mean and variance.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-CharacFun" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4 (Characteristic Function)</strong></span> Let <span class="math inline">\(X\in\mathbb{R}\)</span> be a random variable and let <span class="math inline">\(\mathcal{i}=\sqrt{-1}\)</span> be the imaginary unit. Then the function <span class="math inline">\(\psi_X:\mathbb{R}\to\mathbb{C}\)</span> defined by <span class="math display">\[
\psi_X(t) = \mathbb{E}(\exp(\mathcal{i}tX))
\]</span> is called the <strong>characteristic function</strong> of <span class="math inline">\(X.\)</span></p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Characteristic Function: Some useful facts
</div>
</div>
<div class="callout-body-container callout-body">
<p>The characteristic function …</p>
<ul>
<li><p>… uniquely determines its associated probability distribution <span class="math inline">\(F\)</span> of <span class="math inline">\(X.\)</span></p></li>
<li><p>… can be used to easily derive (all) the moments of a random variable by <span class="math display">\[
\mathbb{E}(X^n) = \mathcal{i}^n \left.\frac{d^n}{d t^n}\psi_X(t)\right|_{t=0}
\]</span></p></li>
<li><p>… is often used to prove that two distributions are equal.</p></li>
<li><p>The characteristic function of <span class="math inline">\(\Phi_{\sigma_0}\)</span> is <span id="eq-CharacNormal"><span class="math display">\[
\begin{align*}
\psi_{\Phi_{\sigma_0}}(t)
&amp;=\exp\left(-\frac{1}{2}\sigma_0^2\,t^2\right)\\[2ex]
&amp;=\lim_{n\to\infty}\left(1-\frac{1}{n}\,\frac{1}{2}\,\sigma_0^2 \, t^2\right)^n
\end{align*}
\tag{3.6}\]</span></span></p></li>
<li><p>The characteristic function of <span class="math inline">\(\sum_{i=1}^nW_i,\)</span> where <span class="math inline">\(W_1,\dots,W_n\overset{\text{i.i.d.}}{\sim}W,\)</span> is <span id="eq-CharacFctSum"><span class="math display">\[
\psi_{\sum_{i=1}^nW_i}(t)=\big(\psi_{W}(t)\big)^n
\tag{3.7}\]</span></span></p></li>
<li><p>Let <span class="math inline">\(W\)</span> be a random variable with <span class="math inline">\(\mathbb{E}(W)=0\)</span> and <span class="math inline">\(Var(W)=\sigma_W^2.\)</span> Then, we have that (see Equation (26.11) in <span class="citation" data-cites="Billingsley_1995">Billingsley (<a href="#ref-Billingsley_1995" role="doc-biblioref">1995</a>)</span>) <span id="eq-CharacFctBillingsley"><span class="math display">\[
\psi_W(t)=1-\frac{1}{2}\sigma_W^2 \, t^2 + \lambda(t),
\tag{3.8}\]</span></span> where <span class="math inline">\(|\lambda(t)|\leq t^2\,\mathbb{E}\left(\min(|t|\,|W|^3, W^2)\right).\)</span></p></li>
</ul>
</div>
</div>
<blockquote class="blockquote">
<p>The following is taken from Example 3.1 in <span class="citation" data-cites="Shao_Tu_1996">Shao and Tu (<a href="#ref-Shao_Tu_1996" role="doc-biblioref">1996</a>)</span>. First, we sketch the proof for the Lindeberg-Lévy CLT. Second, we use this principle to sketch the proof for bootstrap consistency.</p>
</blockquote>
<p>Let <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X
\]</span> where <span class="math inline">\(X\sim F\)</span> has</p>
<ul>
<li>mean <span class="math inline">\(\mathbb{E}(X)=\mu_0\)</span> and</li>
<li>variance <span class="math inline">\(Var(X)=\sigma^2_0.\)</span></li>
</ul>
<p>It follows then from the Lindeberg-Lévy CLT that <span class="math display">\[
H_n(x)=P\left(\sqrt{n}\left(\bar{X}_n-\mu_0\right)\leq x\right)\to\Phi_{\sigma_0}(x)=\Phi\left(\frac{x}{\sigma_0}\right)\quad\text{as}\quad n\to\infty,
\]</span> for all continuity points <span class="math inline">\(x\in\mathbb{R}.\)</span> This CLT-result can be proven by showing that the characteristic function of <span class="math inline">\(H_n\)</span> tends the characteristic function of <span class="math inline">\(\Phi_{\sigma_0}.\)</span></p>
<p>To see this, rewrite <span class="math display">\[
\begin{align*}
\sqrt{n}\left(\bar{X}_n-\mu_0\right)
&amp; = \sum_{i=1}^n\overbrace{\;\left(\frac{X_i-\mu_0}{\sqrt{n}}\right)\;}^{=W_{i,n}}\\[2ex]
&amp; = \sum_{i=1}^n W_{i,n}
\end{align*}
\]</span> where <span class="math display">\[
\begin{align*}
&amp;W_{1,n},\dots,W_{n,n}\overset{\text{i.i.d.}}{\sim}W_n,\\[2ex]
&amp;\mathbb{E}(W_{n})=\mathbb{E}\left(\frac{X_i-\mu_0}{\sqrt{n}}\right)=\frac{\mathbb{E}(X_i)-\mu_0}{\sqrt{n}}=0\quad\text{for all}\quad n,\quad\text{and}\\[2ex]
&amp;Var(W_{n})=Var\left(\frac{X_i-\mu_0}{\sqrt{n}}\right)=\frac{1}{n}Var(X_i-\mu_0)=\frac{1}{n}Var(X_i)=\frac{1}{n}\sigma_0^2.
\end{align*}
\]</span></p>
<p>Therefore, by <a href="#eq-CharacFctSum" class="quarto-xref">Equation&nbsp;<span>3.7</span></a> together with <a href="#eq-CharacFctBillingsley" class="quarto-xref">Equation&nbsp;<span>3.8</span></a> <span class="math display">\[
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}_n-\mu_0\right)}(t)
&amp;=\psi_{\sum_{i=1}^n W_{i,n}}(t)\\[2ex]
&amp;=\left(\psi_{W_{n}}(t)\right)^n\\[2ex]
&amp;=\left(1-\frac{1}{2}\,\frac{1}{n}\sigma_0^2 \, t^2 + \lambda_n(t)\right)^n,
\end{align*}
\]</span> where <span class="math display">\[
\begin{align*}
|\lambda_n(t)|
&amp;\leq t^2\,\mathbb{E}\left(\min\left(|t|\,\left|W_{n}\right|^3, \left|W_{n}\right|^2\right)\right)\\[2ex]
&amp;= t^2\,\mathbb{E}\left(\min\left(|t|\,\left|\frac{X-\mu_0}{\sqrt{n}}\right|^3, \left|\frac{X-\mu_0}{\sqrt{n}}\right|^2\right)\right)\\[2ex]
&amp;= t^2\,\mathbb{E}\left(\min\big(|t|\,n^{-3/2}\left|X-\mu_0\right|^3, n^{-1}\left|X-\mu_0\right|^2\big)\right)\\[2ex]
&amp;[\text{for sufficiently large $n$ and any fixed, finite $t$}\\
&amp;\text{(Note: relevant $t$-values to get all moments are}\\
&amp;\text{values around zero.):}]\\[2ex]
&amp;= t^2\,\mathbb{E}\left(|t|\,n^{-3/2}\left|X-\mu_0\right|^3\right)\\[2ex]
&amp;= n^{-3/2}\underbrace{\;t^2\,|t|\,\mathbb{E}\left(\left|X-\mu_0\right|^3\right)}_{\texttt{constant}}\\[2ex]
%\Rightarrow\quad n |\lambda_n(t)|&amp; \leq n \cdot n^{-3/2}\cdot \texttt{constant}\\[2ex]
%\Rightarrow\quad n |\lambda_n(t)|&amp; \leq n^{-1/2} \cdot \texttt{constant}.
\Rightarrow\quad |\lambda_n(t)|&amp; \leq n^{-3/2}\cdot \texttt{constant}\\[2ex]
\Rightarrow\quad |\lambda_n(t)|&amp; =O(n^{-3/2}) \\[2ex]
&amp;=o(n^{-1})\\[2ex]
\end{align*}
\]</span> The latter step follows since sequences of the order of magnitude <span class="math inline">\(n^{-3/2}\)</span> are of a smaller order of magnitude than sequences of the order of magnitude <span class="math inline">\(n^{-1}.\)</span></p>
<p>That is, for large <span class="math inline">\(n\)</span> (<span class="math inline">\(n\to\infty\)</span>) and fixed finite <span class="math inline">\(t,\)</span> <span class="math display">\[
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}_n-\mu_0\right)}(t)
&amp;=\Big(1-\;\underbrace{\frac{1}{2}\,\frac{1}{n}\sigma_0^2 \, t^2}_{=O(n^{-1})}\; +\; \underbrace{\lambda_n(t)}_{=o(n^{-1})}\Big)^n\\[2ex]
\end{align*}
\]</span> the <span class="math inline">\(\lambda_n(t)=o(n^{-1})\)</span> term becomes negligible in comparison to the <span class="math inline">\(\frac{1}{2}\,\frac{1}{n}\sigma_0^2 \, t^2=O(n^{-1})\)</span> term and the constant term (i.e.&nbsp;the <span class="math inline">\(1\)</span>) such that<br>
<!-- 
$$
n|\lambda_n(t)|\to 0\quad\text{as}\quad n\to\infty,
$$
which means that $|\lambda_n(t)|\to 0$ faster than $n^{-1}$ for any fixed $t$ (and thus also for any $t$ in the neighborhood around zero, which is all we need). 

That is, for any fixed $t,$
$$
\begin{align*}
\lambda_n(t) & = o(n^{-1})\quad (\Leftrightarrow n|\lambda_n(t)|\to 0\quad\text{as}\quad n\to\infty)\\[2ex] 
\frac{1}{2}\,\frac{1}{n}\sigma_0^2 & = O(n^{-1})
\end{align*}
$$ --> <span class="math display">\[
\begin{align*}
\lim_{n\to\infty}\psi_{\sqrt{n}\left(\bar{X}_n-\mu_0\right)}(t)
&amp;= \lim_{n\to\infty}\psi_{\sum_{i=1}^n W_{i,n}}(t)\\[2ex]
&amp;= \lim_{n\to\infty}\left(1-\frac{1}{n}\,\frac{1}{2}\,\sigma_0^2 \, t^2 + \lambda_n(t)\right)^n\\[2ex]
&amp;= \lim_{n\to\infty}\left(1-\frac{1}{n}\,\frac{1}{2}\,\sigma_0^2 \, t^2\right)^n\\[2ex]
&amp;=\exp\left(-\frac{1}{2}\sigma_0^2\,t^2\right)\\[2ex]
&amp;=\psi_{\Phi_{\sigma_0}}(t),
\end{align*}
\]</span> where the latter step follows from <a href="#eq-CharacNormal" class="quarto-xref">Equation&nbsp;<span>3.6</span></a>.</p>
<p>OK, we have shown that <span class="math inline">\(H_n\)</span> tends to <span class="math inline">\(\Phi_{\sigma_0}\)</span> by showing that the characteristic function of <span class="math inline">\(H_n\)</span> tends to that of <span class="math inline">\(\Phi_{\sigma_0}.\)</span> That is, we just sketched the proof for the Lindeberg-Lévy CLT.**</p>
<p>To show <strong>bootstrap consistency</strong> we need to show that <span class="math inline">\(H_n^{Boot}\)</span> also tends to <span class="math inline">\(\Phi_{\sigma_0}.\)</span> To do so, we can mimic the above proof sketch and show that the characteristic function of <span class="math inline">\(H_n^{Boot}\)</span> tends to that of <span class="math inline">\(\Phi_{\sigma_0}.\)</span></p>
<p>Let <span class="math display">\[
X_1^*,\dots,X_n^*\overset{\text{i.i.d.}}{\sim}X^*
\]</span> where <span class="math inline">\(X^*\sim F_n\)</span> has</p>
<ul>
<li>conditional mean <span class="math inline">\(\mathbb{E}^*(X^*)=\mathbb{E}(X_i^*|\mathcal{S}_n)=\bar{X}_n\)</span> and</li>
<li>conditional variance <span class="math inline">\(Var^*(X)=Var^*(X|\mathcal{S}_n)=\hat\sigma^2_n.\)</span></li>
</ul>
<p>Rewrite <span class="math display">\[
\begin{align*}
\sqrt{n}\left.\left(\bar{X}^*_n- \bar{X}_n\right)\right|\mathcal{S}_n
&amp; = \sum_{i=1}^n\left.\;\overbrace{\left(\frac{X^*_i- \bar{X}_n}{\sqrt{n}}\right)}^{W^*_n}\right.|\mathcal{S}_n\\[2ex]
&amp; = \sum_{i=1}^n W^*_{i,n}|\mathcal{S}_n
\end{align*}
\]</span> where</p>
<p><span class="math display">\[
\begin{align*}
&amp;W^*_{1,n}|\mathcal{S}_n,\dots,W^*_{n,n}|\mathcal{S}_n\overset{\text{i.i.d.}}{\sim}W_n^*|\mathcal{S}_n,\\[2ex]
&amp;\mathbb{E}(W^*_{n}|\mathcal{S}_n)=\mathbb{E}\left(\left.\frac{X_i^*-\bar{X}_n}{\sqrt{n}}\right|\mathcal{S}_n\right)\\[2ex]
&amp;\phantom{\mathbb{E}(W^*_{n}|\mathcal{S}_n)}=\frac{\mathbb{E}(X_i^*|\mathcal{S}_n)-\bar{X}_n}{\sqrt{n}}=0\quad\text{for all}\quad n,\quad\text{and}\\[2ex]
&amp;Var(W^*_{n}|\mathcal{S}_n)=Var\left(\left.\frac{X_i^*-\bar{X}_n}{\sqrt{n}}\right|\mathcal{S}_n\right)\\[2ex]
&amp;\phantom{Var(W^*_{n}|\mathcal{S}_n)}=\frac{1}{n}Var(X_i^*-\bar{X}_n|\mathcal{S}_n)=\frac{1}{n}Var(X_i^*|\mathcal{S}_n)=\frac{1}{n}\hat\sigma_n^2
\end{align*}
\]</span></p>
<p>Therefore, by <a href="#eq-CharacFctSum" class="quarto-xref">Equation&nbsp;<span>3.7</span></a> together with <a href="#eq-CharacFctBillingsley" class="quarto-xref">Equation&nbsp;<span>3.8</span></a> <span class="math display">\[
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)|\mathcal{S}_n}(t)
&amp;=\psi_{\sum_{i=1}^n W_{i,n}^*|\mathcal{S}_n}(t)\\[2ex]
&amp;=\left(\psi_{W_{n}^*|\mathcal{S}_n}(t)\right)^n\\[2ex]
&amp;=\left(1-\frac{1}{2}\,\frac{1}{n}{\color{darkgreen}\hat{\sigma}_n^2} \, t^2 + {\color{red}\lambda_n^*(t)}\right)^n,
\end{align*}
\]</span> where <span class="math display">\[
\begin{align*}
|\lambda_n^*(t)|
&amp;\leq |t^2|\,{\color{blue}\mathbb{E}^*}\left(\min\big(|t|\,n^{-3/2}\left|X^*_1-\bar{X}_n\right|^3, n^{-1}\left|X_1^* - \bar{X}_n\right|^2\big)\right)\\[2ex]
&amp;= |t^2|\,{\color{blue}\frac{1}{n}\sum_{i=1}^n}\left(\min\big(|t|\,n^{-3/2}\left|X_i-\bar{X}_n\right|^3, n^{-1}\left|X_i - \bar{X}_n\right|^2\big)\right).
\end{align*}
\]</span> By the Marcinkiewicz strong law of large numbers, we obtain that <span class="math display">\[
\begin{align*}
n{\color{red}|\lambda^*_n(t)|}&amp;\to_{a.s.} 0\quad\text{as}\quad n\to\infty\\[2ex]
\Leftrightarrow\quad {\color{red}|\lambda^*_n(t)|}&amp;=o_{a.s.}(1).
\end{align*}
\]</span></p>
<p>Moreover, <span class="math display">\[
{\color{darkgreen}\hat\sigma_n^2} = \frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}_n\right)^2\to_{a.s.}\sigma_0^2\quad\text{as}\quad n\to\infty.
\]</span></p>
<p>Thus, we have that (using <a href="#eq-CharacNormal" class="quarto-xref">Equation&nbsp;<span>3.6</span></a>) <span class="math display">\[
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)|\mathcal{S}_n}(t)
\to_{a.s.}&amp;
\lim_{n\to\infty}\left(1-\frac{1}{2}\,\frac{1}{n}\sigma_0^2 \, t^2\right)^n\\[2ex]
&amp;=\exp\left(-\frac{1}{2}\sigma_0^2\,t^2\right)\\[2ex]
&amp;=\psi_{\Phi_{\sigma_0}}(t).
\end{align*}
\]</span> This implies that the limit (<span class="math inline">\(n\to\infty\)</span>) of <span class="math inline">\(H_n^{Boot}\)</span> is <span class="math inline">\(\Phi_{\sigma_0}\)</span> almost surely.</p>
<p>Hence, we have shown that the basic bootstrap is consistent for doing inference about <span class="math inline">\(\mu_0\)</span> using <span class="math inline">\(\bar{X}_n.\)</span></p>
<!-- 
An appropriate central limit theorem argument implies that
$$
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
$$
Moreover, $\hat\sigma^2$ is a consistent estimator of $\sigma^2,$ and thus asymptotically $\hat\sigma^2$ may be replaced by $\sigma$. Therefore, 
$$
\begin{align*}
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X^* -\bar X)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
$$
On the other hand, by the CLT, we also have that 
$$
\begin{align*}
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right) 
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X - \mu)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
$$
This means that the bootstrap is **consistent**, since the bootstrap distribution of 
$$
\sqrt{n}(\bar X^* -\bar X)|{\cal S}_n
$$ 
asymptotically $(n\rightarrow\infty)$ coincides with the distribution of 
$$
\sqrt{n}(\bar X-\mu).
$$
In other words, for large $n$,
$$
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
$$ 
-->
<!-- 
This bootstrap consistency result justifies using the bootstrap distribution 
$$
\begin{align*}
H_n^{Boot}(x)
&=P\left(\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x|\mathcal{S}_n\right)\\[2ex] 
&\approx
\frac{1}{m}\sum_{j=1}^m 1_{\left(\sqrt{n}\left(\bar X^*_{n,j}-\bar X_n\right)\leq x\right)}=H_{n,m}^{Boot}(x),  
\end{align*}
$$ 
for doing inference about $\mu_0.$  

-->
<p>In the following section, we show how to build a confidence interval using the bootstrap distribution of a general estimator <span class="math inline">\(\hat\theta_n.\)</span></p>
<!-- 
### Example: Inference about a Population Proportion

**Setup:** 

* **Data:** i.i.d. random sample $X_1,\dots,X_n,$ where $X_i\in\{0,1\}$ is dichotomous and $P(X_i=1)=p$, $P(X_i=0)=1-p$. 
* **Estimator:** Let 
$$
S=\sum_{i=1}^n 1_{(X_i = 1)}
$$ 
denote the number of $X_i$ which are equal to $1.$ Then, the  maximum likelihood estimate of $p$ is 
$$
\hat p=\frac{1}{n}S.
$$
* **Inference Problem:** What is the distribution of 
$$
(\hat{p} - p)?
$$



::: {.callout-note}

## Recall Asymptotics:

* $n\hat p=S\sim \mathcal{Binom}(n,p)$
* As $n\rightarrow\infty,$ the central limit theorem implies that
$$
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
$$
Thus for $n$ large, the distributions of $\sqrt{n}(\hat p -p)$ and $\hat p -p$ can be approximated by $\mathcal{N}(0,p(1-p))$ and $\mathcal{N}(0,p(1-p)/n)$, respectively.

::: 

**Bootstrap Approach:**

* Random sample $X_1^*,\dots,X_n^*$  generated by drawing observations
independently and with replacement from
$$
{\cal S}_n:=\{X_1,\dots,X_n\}.
$$ 
* Let 
$$
S^*=\sum_{i=1}^n 1_{(X_i^* = 1)}
$$  
denote the number of $X_i^*$ which are equal to $1.$
* Bootstrap estimate of $p$: 
$$
\hat p^*=\frac{1}{n}S^*
$$

The bootstrap now tries to approximate the true distribution of $\hat p - p$ by the **conditional** distribution of $(\hat p^*-\hat p)|\mathcal{S}_n$ given the observed sample ${\cal S}_n,$ where the latter can be approximated arbitrarily well $(m\to\infty)$ using the bootstrap estimators 
$$
p^*_1,p^*_2,\dots,p^*_m;
$$
namely by
$$
P\left(\hat{p}^* - \hat{p} \leq \delta|\mathcal{S}_n\right)\approx \frac{1}{m}\sum_{k=1}^m 1_{(\hat{p}^*_k - \hat{p} \leq\delta )}. 
$$

The bootstrap is called **consistent** if asymptotically $(n\rightarrow \infty)$ the conditional distribution of $(\hat p^*-\hat p)|{\cal S}_n$  coincides with the true distribution of $\hat p - p.$ (Note: a proper scaling is required!)

**The distribution of $X_i^*|\mathcal{S}_n$**

The conditional random variable $X_i^*|\mathcal{S}_n$ is a binary random variable 
$$
X_i^*|\mathcal{S}_n\in\{0,1\}.
$$
Since $X_i^*$ is drawn independently and with replacement from $\mathcal{S}_n,$ we obtain for each $i=1,\dots,n,$
$$
\begin{align*}
& P^*(X_i^*=1)=P(X_i^*=1|{\cal S}_n)=\hat p, \\[2ex]  
& P^*(X_i^*=0)=P(X_i^*=0|{\cal S}_n)=1-\hat p.
\end{align*}
$$
Thus, $X_i^*|{\cal S}_n$ is a Bernoulli distributed random variable with parameter $p=\hat{p}$
$$
X_i^*|{\cal S}_n \sim\mathcal{Bern}(p=\hat p), \quad i=1,\dots,n.\\[5ex]
$$


**The distribution of $\hat{p}^*|\mathcal{S}_n$**

The above implies that $n \hat{p}^*|{\cal S}_n$ has a Binomial distribution with parameters $n$ and $p=\hat{p},$  
$$
\underbrace{n \hat{p}_i^*}_{=S^*}|{\cal S}_n \sim\mathcal{Binom}(n, p=\hat p), \quad i=1,\dots,n.
$$

Therefore,
$$
\begin{align*}
\mathbb{E}^*(n \hat p^*)
&=\mathbb{E}(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p}\\[2ex]
\Rightarrow \mathbb{E}^*(\hat p^*) & = \hat{p}
\end{align*}
$$
and 
$$
\begin{align*}
Var^*(n \hat p^*)
&=Var(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p} (1- \hat{p})\\[2ex]
\Rightarrow Var^*(\hat p^*) & = \frac{\hat{p}(1-\hat{p})}{n}
\end{align*}
$$

An appropriate central limit theorem argument implies that 
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$

Moreover, $\hat p$ is a consistent estimator of $p,$ and thus 
$$
\hat p(1-\hat p)\rightarrow_p p(1-p),\quad n\rightarrow\infty.
$$ 
Therefore, $\hat p(1-\hat p)$ can be replaced asymptotically by $p(1-p)$, and
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right)\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$
So, we can conclude that, 
$$
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0
$$
as $n\rightarrow\infty,$ where $\Phi$ denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large $n$
$$
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx 
\text{distribution}(\sqrt{n}(\hat p -p))%\approx N(0,p(1-p))
$$
and therefore also
$$
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx 
\text{distribution}(\hat p -p).%\approx N(0,p(1-p)/n)
$$
-->
</section>
</section>
<section id="the-basic-bootstrap-confidence-interval" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="the-basic-bootstrap-confidence-interval"><span class="header-section-number">3.3.2</span> The Basic Bootstrap Confidence Interval</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recall: Confidence Interval for <span class="math inline">\(\theta_0\)</span> using Classic Asymptotic Statistics
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Setup (Classic Asymptotic Statistics):</strong></p>
<ul>
<li><span class="math inline">\(\theta_0\in\mathbb{R}\quad\)</span> and</li>
<li><span class="math inline">\(\sqrt{n}(\hat\theta_n-\theta_0)\rightarrow_d\mathcal{N}(0,v_0^2)\quad\)</span> as <span class="math inline">\(\quad n\to\infty,\)</span></li>
<li><span class="math inline">\(\hat{v}_n\to_{p} v_0\quad\)</span> as <span class="math inline">\(\quad n\to\infty\)</span></li>
</ul>
<p>An approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval is then given by <span class="math display">\[
\left[
\hat{\theta}_n - z_{1-\frac{\alpha}{2}}\frac{\hat v_n}{\sqrt{n}},
\hat{\theta}_n + z_{1-\frac{\alpha}{2}}\frac{\hat v_n}{\sqrt{n}}
\right],
\]</span> where <span class="math inline">\(z_{1-\frac{\alpha}{2}}\)</span> denotes the <span class="math inline">\((1-\alpha)/2\)</span> quantile of the standard Normal distribution <span class="math inline">\((z_{0.975}=1.96).\)</span> This confidence interval is approximate, since it is only asymptotically justified, and, thus, is generally not exact in finite samples.</p>
<p><strong>Note:</strong> Often, however, very difficult to obtain a consistent estimator <span class="math inline">\(\hat v_n\)</span> of <span class="math inline">\(v_0\)</span> (see <a href="#sec-Illustration" class="quarto-xref"><span>Section 3.1</span></a>). Statistical inference is then usually based on the <strong>bootstrap confidence intervals</strong>.</p>
</div>
</div>
<section id="algorithm-of-the-basic-bootstrap-confidence-interval-for-theta_0" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="algorithm-of-the-basic-bootstrap-confidence-interval-for-theta_0">Algorithm of the Basic Bootstrap Confidence Interval for <span class="math inline">\(\theta_0:\)</span></h4>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. random sample <span class="math display">\[
{\cal S}_n:=\{X_1,\dots,X_n\}
\]</span> with <span class="math inline">\(X_i\overset{\text{i.i.d.}}{\sim} F\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></li>
<li>The parameter of interest <span class="math inline">\(\theta_0\in\mathbb{R}\)</span> is an parameter of <span class="math inline">\(F\)</span>.</li>
<li><span class="math inline">\(\hat{\theta}_n\)</span> denotes the estimator of <span class="math inline">\(\theta_0\in\mathbb{R}.\)</span></li>
<li><strong>Problem:</strong> Construct a confidence interval for <span class="math inline">\(\theta_0\in\mathbb{R}.\)</span></li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Assumption: Bootstrap is Consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the following, we will assume that the bootstrap is consistent; i.e.&nbsp;that <span class="math display">\[
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^*_n -\hat{\theta}_n)|{\cal S}_n)
&amp;\approx
\text{distribution}(\sqrt{n}(\hat{\theta}_n-\theta_0))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)|{\cal S}_n
&amp;\overset{d}{\approx} \sqrt{n}(\hat{\theta}_n -\theta_0)
\end{align*}
\]</span> if <span class="math inline">\(n\)</span> is sufficiently large.</p>
<p>Caution: This is not always the case and in cases of doubt one needs to show this property.</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Good to know:</strong> Theorem 1 in <span class="citation" data-cites="Mammen_1992_Book">Mammen (<a href="#ref-Mammen_1992_Book" role="doc-biblioref">1992</a>)</span> shows that the basic bootstrap is consistent if <span class="math inline">\(\sqrt{n}(\hat{\theta}_n-\theta_0)\to_d\mathcal{N}(0,v_0^2),\)</span> under the assumption that the bootstrap sampling process (e.g.&nbsp;i.i.d.) equals the original sampling process.</p>
</div>
</div>
</div>
</div>
</div>
<p><strong>Algorithm (3 Steps):</strong></p>
<ol type="1">
<li><p>Generate <span class="math inline">\(m\)</span> bootstrap estimates<br>
<span class="math display">\[
\hat\theta_{n,1}^*,\dots,\hat\theta_{n,m}^*
\]</span> by repeatedly (<span class="math inline">\(m\)</span> times) drawing bootstrap samples <span class="math inline">\(X_{1}^*,\dots,X_{n}^*\)</span> independently and with replacement from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> and computing <span class="math display">\[
\hat{\theta}^\ast_{n,j}=\hat{\theta}^\ast_{j}(X_{1}^*,\dots,X_{n}^*),\quad j=1,\dots,m.
\]</span></p></li>
<li><p>Use the <span class="math inline">\(m\)</span> bootstrap estimates <span class="math inline">\(\hat\theta_{n,1}^*,\dots,\hat\theta_{n,m}^*\)</span> to approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and the <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles of the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n.\)</span> This can be done with negligible approximation error (for <span class="math inline">\(m\)</span> large) using the empirical quantiles <span id="eq-empiricalQuantile"><span class="math display">\[
\hat q^*_{n,p}=\left\{
  \begin{array}{ll}
  \hat\theta^*_{n,(\lfloor mp\rfloor+1)},           &amp;\text{if $mp$ is not an integer}\\
  (\hat\theta^*_{n,(mp)}+\hat\theta^*_{n,(mp+1)})/2,&amp;\text{if $mp$ is an integer}
\end{array}\right.
\tag{3.9}\]</span></span> for <span class="math inline">\(p=\frac{\alpha}{2}\)</span> or <span class="math inline">\(p=1-\frac{\alpha}{2},\)</span> where <span class="math inline">\(\hat\theta_{n,(j)}^*\)</span> denotes the <span class="math inline">\(j\)</span>th order statistic <span class="math display">\[
\hat\theta_{n,(1)}^* \leq \hat\theta_{n,(2)}^*\leq \dots\leq \hat\theta_{n,(m)}^*,
\]</span> and <span class="math inline">\(\lfloor mp\rfloor\)</span> denotes the greatest whole number less than or equal to <span class="math inline">\(mp\)</span> (e.g.&nbsp;<span class="math inline">\(\lfloor 4.9\rfloor = 4\)</span>).</p></li>
<li><p>The <span class="math inline">\((1-\alpha)\times 100\%\)</span> <strong>basic bootstrap confidence interval</strong> is then given by <span id="eq-NPBootCI"><span class="math display">\[
\left[2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}, 2\hat{\theta}_n-\hat q^*_{n,\frac{\alpha}{2}}\right],
\tag{3.10}\]</span></span> where</p>
<ul>
<li><span class="math inline">\(\hat{\theta}_n\)</span> is computed from the original sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> and</li>
<li><span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> are computed from the bootstrap estimates <span class="math inline">\(\hat\theta_{n,1}^*,\dots,\hat\theta_{n,m}^*.\)</span></li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The quantiles <span class="math inline">\(\hat q^*_{n,p}\)</span> are those of the distribution <span class="math display">\[
G_{n,m}^{Boot}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(\hat{\theta}^*_{n,j}\leq x\right)}.
\]</span> However, we’ll treat the quantiles <span class="math inline">\(\hat q^*_{n,p}\)</span> as quantiles of the distribution <span class="math display">\[
G_{n}^{Boot}(x)=P\left(\hat{\theta}^*_{n}\leq x\,\big|\,\mathcal{S}_n\right),
\]</span> since for large <span class="math inline">\(m\)</span> (<span class="math inline">\(m\to\infty\)</span>) the difference between <span class="math inline">\(G_{n,m}^{Boot}\)</span> and <span class="math inline">\(G_{n}^{Boot}\)</span> is negligible (Glivenko-Cantelli <a href="#thm-Clivenko-Cantelli" class="quarto-xref">Theorem&nbsp;<span>3.1</span></a>) and we can choose <span class="math inline">\(m\)</span> to be large.</p>
</div>
</div>
<p><strong>Justifying the Basic Bootstrap CI (<a href="#eq-NPBootCI" class="quarto-xref">Equation&nbsp;<span>3.10</span></a>) for <span class="math inline">\(\theta_0\)</span>:</strong></p>
<p>The following three approximate statements <span class="math inline">\((\approx (1-\alpha))\)</span> are exact for <span class="math inline">\(m\to\infty:\)</span> <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat q^*_{n,\frac{\alpha}{2}} \leq \hat{\theta}^*_n \leq \hat q^*_{n,1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow\; &amp; P^*\left(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n \leq\hat{\theta}^*_n -\hat{\theta}_n \leq \hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n\right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow\; &amp; P^*\left(
\sqrt{n}(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n)\leq{\color{red}\sqrt{n}(\hat{\theta}_n^*-\hat{\theta}_n)}\leq \sqrt{n}(\hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n)\right)
\approx 1-\alpha
\end{align*}
\]</span></p>
<p>Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> <span class="math display">\[
{\color{red}\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}_n-\theta_0)}.
\]</span> Therefore, for large <span class="math inline">\(n\)</span> and large <span class="math inline">\(m,\)</span> <span class="math display">\[
\begin{align*}
&amp;P\left(
\sqrt{n}(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n)\leq{\color{blue}\sqrt{n}(\hat{\theta}_n-\theta_0)}\leq \sqrt{n}(\hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n)\right)\approx 1-\alpha\\[2ex]
\Leftrightarrow\; &amp;P\left(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n\leq\hat{\theta}_n-\theta_0 \leq \hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n\right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow\; &amp;P\left(\hat q^*_{n,\frac{\alpha}{2}}-2\hat{\theta}_n\leq-\theta_0 \leq \hat q^*_{n,1-\frac{\alpha}{2}}-2\hat{\theta}_n\right)
\approx 1-\alpha\\[2ex]
%\Rightarrow &amp;P\left(\hat{\theta}_n-(\hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n)\le \theta_0\le \hat{\theta}_n-(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n)\right)\approx 1-\alpha\\[2ex]
\Leftrightarrow\; &amp;P\left(2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}\le \theta_0\le 2\hat{\theta}_n-
\hat q^*_{n,\frac{\alpha}{2}}\right)\approx 1-\alpha\\[2ex]
\Leftrightarrow\; &amp;P\left(\theta_0\in\left[2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}, \; 2\hat{\theta}_n-\hat q^*_{n,\frac{\alpha}{2}}\right]\right)\approx 1-\alpha.
\end{align*}
\]</span> This demonstrates that the <strong>basic bootstrap confidence interval</strong> in <a href="#eq-NPBootCI" class="quarto-xref">Equation&nbsp;<span>3.10</span></a> <span class="math display">\[
\left[2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}, \; 2\hat{\theta}_n-\hat q^*_{n,\frac{\alpha}{2}}\right],
\]</span> is indeed an asymptotically (<span class="math inline">\(n\to\infty\)</span> and <span class="math inline">\(m\to\infty\)</span>) valid <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval.</p>
</section>
<section id="example-basic-bootstrap-confidence-interval-for-the-population-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-basic-bootstrap-confidence-interval-for-the-population-mean">Example: Basic Bootstrap Confidence Interval for the Population Mean</h4>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> Let <span class="math inline">\(X_1,\dots,X_n\)</span> denote an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\sigma^2_0.\)</span></li>
<li><strong>Estimator:</strong> <span class="math inline">\(\bar X_n = \frac{1}{n} \sum_{i=1}^n X_i\)</span> is an unbiased estimator of <span class="math inline">\(\mu_0.\)</span></li>
<li><strong>Inference Problem:</strong> Construct a confidence interval for <span class="math inline">\(\mu_0.\)</span></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recall: Confidence Interval for <span class="math inline">\(\mu_0\)</span> using Classic Asymptotic Statistics
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Setup:</strong></p>
<ul>
<li>By the CLT: <span class="math inline">\(\sqrt{n}(\bar X_n - \mu_0)\to_d\mathcal{N}(0,\sigma^2_0)\)</span> as <span class="math inline">\(n\to\infty\)</span></li>
<li>Estimation of <span class="math inline">\(\sigma^2_0\)</span>: <span class="math inline">\(\hat{\sigma}^2_n=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2,\)</span> where <span class="math inline">\(\hat{\sigma}^2_n\to_p\sigma^2_0\)</span> as <span class="math inline">\(n\to\infty.\)</span></li>
<li>This implies: <span class="math inline">\(\sqrt{n}((\bar X_n -\mu_0)/\hat{\sigma}_n)\to_d\mathcal{N}(0,1)\)</span> as <span class="math inline">\(n\to\infty\)</span></li>
</ul>
<p>Let <span class="math inline">\(z_{\alpha/2}\)</span> and <span class="math inline">\(z_{1-\alpha/2}\)</span> denote the <span class="math inline">\(\alpha/2\)</span> and the <span class="math inline">\((1-\alpha/2)\)</span>-quantile of <span class="math inline">\(\mathcal{N}(0,1).\)</span> Since <span class="math inline">\(z_{\alpha/2} = -z_{1-\alpha/2},\)</span> we have that <span class="math display">\[
\begin{align*}
&amp;P\left(-z_{1-\frac{\alpha}{2}}\le \frac{\sqrt{n}(\bar X_n -\mu_0)}{\hat{\sigma}_n}\le z_{1-\frac{\alpha}{2}}\right)\approx 1-\alpha\\[2ex]
\Rightarrow\quad
&amp;P\left(-z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}\le \bar X_n -\mu_0\le z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}\right)\approx 1-\alpha\\[2ex]
\Rightarrow\quad
&amp;P\left(\bar X_n -z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}\le \mu_0\le
        \bar X_n +z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}
  \right)\approx 1-\alpha
\end{align*}
\]</span></p>
<ul>
<li>Approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval: <span class="math display">\[
\left[\bar X_n -z_{1-\frac{\alpha}{2}}\left(\frac{\hat{\sigma}_n}{\sqrt{n}}\right),
    \bar X_n +z_{1-\frac{\alpha}{2}}\left(\frac{\hat{\sigma}_n}{\sqrt{n}}\right)\right]
\]</span></li>
</ul>
</div>
</div>
</section>
<section id="algorithm-of-the-basic-bootstrap-confidence-interval-for-boldsymbolmu_0" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="algorithm-of-the-basic-bootstrap-confidence-interval-for-boldsymbolmu_0"><strong>Algorithm of the basic bootstrap confidence interval for <span class="math inline">\(\boldsymbol{\mu_0}\)</span>:</strong></h4>
<p>The bootstrap offers an alternative method for constructing approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals. We already know that the bootstrap is consistent in this situation (see <a href="#sec-Theory2BootstrapConsist" class="quarto-xref"><span>Section 3.3.1.3</span></a>).</p>
<ol type="1">
<li><p>Draw <span class="math inline">\(m\)</span> bootstrap samples (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) and calculate the corresponding estimates <span class="math display">\[
\bar X^*_{n,1},\bar X^*_{n,2},\dots,\bar X^*_{n,m}.
\]</span></p></li>
<li><p>Compute the empirical quantiles <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> from <span class="math inline">\(\bar X^*_{n,1},\bar X^*_{n,2},\dots,\bar X^*_{n,m}\)</span> using <a href="#eq-empiricalQuantile" class="quarto-xref">Equation&nbsp;<span>3.9</span></a>.</p></li>
<li><p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> basic bootstrap confidence interval according to <a href="#eq-NPBootCI" class="quarto-xref">Equation&nbsp;<span>3.10</span></a>: <span class="math display">\[
\left[2\bar X_n -\hat q^*_{n,1-\frac{\alpha}{2}},
   2\bar X_n -\hat q^*_{n,\frac{\alpha}{2}}\right],
\]</span> where</p>
<ul>
<li><span class="math inline">\(\bar{X}_n\)</span> is computed from the original sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> and</li>
<li><span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> are computed from the bootstrap estimators <span class="math inline">\(\bar{X}_{n,1}^*,\dots,\bar{X}_{n,m}^*.\)</span></li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="sec-BootT" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-BootT"><span class="header-section-number">3.4</span> The Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Method</h2>
<p>In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-<span class="math inline">\(t\)</span> method (one also speaks of the “studentized bootstrap”). As the basic bootstrap method, the bootstrap-<span class="math inline">\(t\)</span> method is a nonparametric bootstrap method. The construction relies on so-called <strong>(asymptotically) pivotal statistics</strong>.</p>
<p><strong>Setup (as in classic asymptotic statistics):</strong></p>
<ul>
<li>Let <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> be an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with unknown parameter of interest <span class="math inline">\(\theta_0\in\mathbb{R}.\)</span></li>
<li>Let <span class="math inline">\(\hat{\theta}_n\)</span> be a <span class="math inline">\(\sqrt{n}\)</span>-consistent, asymptotically normal estimator of <span class="math inline">\(\theta_0,\)</span> i.e. <span class="math display">\[
\sqrt{n}\left(\hat{\theta}_n - \theta_0\right)\to_d\mathcal{N}(0,v_0^2)\quad\text{as}\quad n\to\infty
\]</span></li>
<li>Assume that the <strong>bootstrap is consistent</strong>.</li>
<li>Let <span class="math inline">\(\hat{v}_n^2\)</span> denote a <strong>consistent</strong> estimator of the asymptotic variance <span class="math inline">\(v_0^2=\lim_{n\to\infty}Var\left(\sqrt{n}\left(\hat{\theta}_n-\theta_0\right)\right)\)</span> i.e.&nbsp; <span class="math display">\[
\hat v^2_n\equiv \hat v^2(X_1,\dots,X_n)
\]</span> such that <span class="math display">\[
\begin{align*}
\hat v^2_n &amp;\to_p v_0^2\quad\text{as}\quad n\to\infty\\[2ex]
\hat v_n   &amp;\to_p v_0\quad\text{as}\quad n\to\infty.
\end{align*}
\]</span></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-pivotal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.5 ((Asymptotically) Pivotal Statistics)</strong></span> <br></p>
<p>A statistic (i.e.&nbsp;a function of the random sample) <span class="math display">\[
T_n\equiv T(X_1,\dots,X_n)
\]</span> is called <strong>exact pivotal</strong>, if the distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter.</p>
<p>A statistic <span class="math inline">\(T_n\)</span> is called <strong>asymptotically pivotal</strong>, if the asymptotic distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter.</p>
</div>
</div>
</div>
</div>
<p><strong>Exact pivotal</strong> statistics are rare and not available in most statistical or econometric applications.</p>
<p>It is, however, often possible to construct an <strong><em>asymptotically pivotal</em></strong> statistic. Consider, for instance, an asymptotically normal <span class="math inline">\(\sqrt{n}\)</span>-consistent estimator <span class="math inline">\(\hat{\theta}_n\)</span> of <span class="math inline">\(\theta_0,\)</span> i.e. <span class="math display">\[
\sqrt{n}(\hat{\theta}_n-\theta_0)\rightarrow_d\mathcal{N}(0,v_0^2),
\]</span> where <span class="math inline">\(v^2=\lim_{n\to\infty}Var(\sqrt{n}(\hat{\theta}_n-\theta_0))\)</span> denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a <strong>consistent</strong> estimator of <span class="math inline">\(v_0^2\)</span> <span class="math display">\[
\hat v_n^2 \rightarrow_p v_0^2\quad\text{as}\quad n\to\infty,
\]</span> which implies (Continuous Mapping Theorem) that also <span class="math display">\[
\hat v_n \rightarrow_p v_0\quad\text{as}\quad n\to\infty.
\]</span> Then, <span class="math display">\[
T_n= \sqrt{n}\frac{(\hat{\theta}_n-\theta_0)}{\hat v_n}
\]</span> is <strong>asymptotically pivotal</strong>, since <span class="math display">\[
T_n = \sqrt{n}\frac{(\hat{\theta}_n-\theta_0)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<section id="example-boldsymbolbarx_n-is-asymptotically-pivotal" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-boldsymbolbarx_n-is-asymptotically-pivotal"><strong>Example: <span class="math inline">\(\boldsymbol{\bar{X}_n}\)</span> is Asymptotically Pivotal</strong></h4>
<p>Let <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> be a i.i.d. random sample with <span class="math inline">\(X_i\sim X\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> with mean <span class="math inline">\(\mathbb{E}(X)=\mu_0\)</span>, variance <span class="math inline">\(0&lt;Var(X)=\sigma_0^2&lt;\infty\)</span>, and finite fourth moment <span class="math inline">\(\mathbb{E}(|X|^4)=\beta&lt;\infty\)</span> for estimating <span class="math inline">\(\sigma_0^2.\)</span></p>
<ul>
<li><p>If <span class="math inline">\(X\)</span> is <strong>normally distributed</strong>, we obtain <span class="math display">\[
T_n=\frac{\sqrt{n}(\bar X_n-\mu_0)}{s_n}\sim t_{n-1}\quad\text{for any}\quad n=2,3,\dots
\]</span> with <span class="math inline">\(s_n^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X_n)^2\)</span>, where <span class="math inline">\(t_{n-1}\)</span> denotes the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. We can conclude that <span class="math inline">\(T_n\)</span> is <strong>exact pivotal</strong>.</p></li>
<li><p>If <span class="math inline">\(X\)</span> is <strong><em>not</em> normally distributed</strong>, the central limit theorem implies that <span class="math display">\[
T_n=\frac{\sqrt{n}(\bar X_n-\mu_0)}{s_n}\rightarrow_d\mathcal{N}(0,1),\quad\text{as}\quad n\to\infty.
\]</span> In this case <span class="math inline">\(T_n\)</span> is an <strong>asymptotically pivotal statistic</strong>.<br><br> <strong>Note:</strong> One can replace <span class="math inline">\(s_n = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X_n)^2}\)</span> by <span class="math inline">\(\hat\sigma_n = \sqrt{\frac{1}{n} \sum_{i=1}^n (X_i-\bar X_n)^2},\)</span> since both are asymptotically equivalent, i.e. <span class="math display">\[
\frac{s_n}{\hat\sigma_n}=\sqrt{\frac{n}{n-1}}\to_p 1\quad\text{as}\quad n\to\infty.
\]</span></p></li>
</ul>
</section>
<section id="bootstrap-boldsymbolt-consistency" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-boldsymbolt-consistency"><strong>Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Consistency</strong></h4>
<p>The general idea of the bootstrap-<span class="math inline">\(t\)</span> method relies on approximating the unknown distribution of <span class="math display">\[
T_n = \sqrt{n}\frac{(\hat{\theta}_n-\theta_0)}{\hat v_n}
\]</span> by the approximable (via bootstrap resampling) conditional distribution of <span class="math display">\[
T_n^*\big|\mathcal{S}_n =\sqrt{n}\frac{(\hat{\theta}_n^*-\hat{\theta}_n)}{\hat v_n^*}\Big|\mathcal{S}_n,
\]</span> given <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> where the standard deviation estimate <span class="math inline">\(\hat{v}_n^*\)</span> is computed from the bootstrap sample <span class="math inline">\(X_1^*,\dots,X_n^*,\)</span> i.e. <span class="math display">\[
\hat v_n^*\equiv \hat{v}(X_1^*,\dots,X_n^*).
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Good news: Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> consistency follows if the basic bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the basic bootstrap is consistent and if the variance estimator <span class="math inline">\(\hat{v}_n^2\)</span> is consistent, then also the bootstrap-<span class="math inline">\(t\)</span> method is consistent.</p>
<!-- , i.e. if the conditional distribution of $\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)|\mathcal{S}_n$, given $\mathcal{S}_n$, yields a consistent estimate of $\mathcal{N}(0,v^2)$, then also the bootstrap-$t$ method is consistent. That is, then the conditional distribution of $T_n^*|\mathcal{S}_n$, given $\mathcal{S}_n$, provides a consistent estimate of the asymptotic distribution of $T_n\rightarrow_d \mathcal{N}(0,1)$ such that
$$
\sup_{x\in\mathbb{R}} \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*_n-\hat{\theta}_n)}{\hat v_n^*}\le x \;\right|\;{\cal S}_n\right)-\Phi(x)\right|\rightarrow_p 0,\quad\text{as}\quad n\to\infty,
$$
where $\Phi$ denotes the distribution function of the standard normal distribution.  -->
</div>
</div>
</section>
<section id="the-bootstrap-boldsymbolt-confidence-interval" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="the-bootstrap-boldsymbolt-confidence-interval"><span class="header-section-number">3.4.1</span> The Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Confidence Interval</h3>
<p><strong>Setup (as in classic asymptotic statistics):</strong></p>
<ul>
<li>Let <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> be an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with unknown parameter of interest <span class="math inline">\(\theta_0\in\mathbb{R}.\)</span></li>
<li>Let <span class="math inline">\(\hat{\theta}_n\)</span> be a <span class="math inline">\(\sqrt{n}\)</span>-consistent, asymptotically normal estimator of <span class="math inline">\(\theta_0,\)</span> i.e. <span class="math display">\[
\sqrt{n}\left(\hat{\theta}_n - \theta_0\right)\to_d\mathcal{N}(0,v_0^2)\quad\text{as}\quad n\to\infty
\]</span></li>
<li>Assume that the <strong>bootstrap is consistent</strong>.</li>
<li>Let <span class="math inline">\(\hat{v}_n^2\)</span> denote a <strong>consistent</strong> estimator of the asymptotic variance <span class="math inline">\(v_0^2=\lim_{n\to\infty}Var\left(\sqrt{n}\left(\hat{\theta}_n-\theta_0\right)\right)\)</span> i.e.&nbsp; <span class="math display">\[
\hat v^2_n\equiv \hat v^2(X_1,\dots,X_n)
\]</span> such that <span class="math display">\[
\begin{align*}
\hat v^2_n &amp;\to_p v_0^2\quad\text{as}\quad n\to\infty\\[2ex]
\hat v_n   &amp;\to_p v_0\quad\text{as}\quad n\to\infty.
\end{align*}
\]</span></li>
</ul>
<section id="algorithm-of-the-bootstrap-boldsymbolt-confidence-interval-for-boldsymboltheta_0" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="algorithm-of-the-bootstrap-boldsymbolt-confidence-interval-for-boldsymboltheta_0"><strong>Algorithm of the Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Confidence Interval for <span class="math inline">\(\boldsymbol{\theta_0}\)</span>:</strong></h4>
<p><strong>Algorithm (3 Steps):</strong></p>
<ol type="1">
<li><p>Based on an i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> calculate the bootstrap estimates <span class="math display">\[
\hat{\theta}^*_n\equiv \hat{\theta}^*(X_1^*,\dots,X_n^*)
\]</span> and <span class="math display">\[
\hat v^*_n\equiv \hat v^*(X_1^*,\dots,X_n^*)
\]</span> and the bootstrap statistic <span class="math display">\[
\begin{align*}
T_n^*&amp;=\sqrt{n}\frac{\hat{\theta}^*_n-\hat{\theta}_n}{\hat v^*_n}.
\end{align*}
\]</span> Repeating this yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) many bootstrap estimators <span class="math display">\[
T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*.
\]</span> conditionally on <span class="math inline">\(\mathcal{S}_n.\)</span></p></li>
<li><p>Compute the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> of the bootstrap estimates <span class="math inline">\(T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*\)</span> (see <a href="#eq-empiricalQuantile" class="quarto-xref">Equation&nbsp;<span>3.9</span></a>).</p></li>
<li><p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> bootstrap-<span class="math inline">\(t\)</span> confidence interval<br>
<span id="eq-Boot_tCI"><span class="math display">\[
\left[\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{\hat v_n}{\sqrt{n}}\right),\;
   \hat{\theta}_n - \hat q^*_{n,  \frac{\alpha}{2}}   \left(\frac{\hat v_n}{\sqrt{n}}\right)\right],
\tag{3.11}\]</span></span> where</p>
<ul>
<li><span class="math inline">\(\hat\theta_n\)</span> and <span class="math inline">\(\hat v_n\)</span> are the estimates of <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(v_0\)</span> computed from the original sample <span class="math inline">\(\mathcal{S}_n=\left\{X_1,\dots,X_n\right\},\)</span> and</li>
<li><span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> are computed from the bootstrap estimators <span class="math inline">\(T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*.\)</span></li>
</ul></li>
</ol>
<p><strong>Justifying the Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> CI (<a href="#eq-Boot_tCI" class="quarto-xref">Equation&nbsp;<span>3.11</span></a>) for <span class="math inline">\(\boldsymbol{\theta_0}\)</span>:</strong></p>
<p>The bootstrap estimates <span class="math display">\[
T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*
\]</span> yield the empirical bootstrap distribution <span class="math display">\[
H_{n,m}^{Boot}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(T_{n,j}^*\;\leq\; x\right)}
\]</span> which approximates the bootstrap distribution <span class="math display">\[
H_{n}^{Boot}(x)=P\left(\left.T_{n}^*\leq x\;\right|\;\mathcal{S}_n\right)
\]</span> arbitrarily precise as <span class="math inline">\(m\to\infty\)</span> (Glivenko-Cantelli <a href="#thm-Clivenko-Cantelli" class="quarto-xref">Theorem&nbsp;<span>3.1</span></a>).</p>
<p>Thus, the empirical bootstrap quantiles <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> of <span class="math inline">\(H_{n,m}^{Boot}\)</span> are indeed consistent (<span class="math inline">\(m\to\infty\)</span>) for the quantiles <span class="math inline">\(\hat q_{n,\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q_{n,1-\frac{\alpha}{2}}\)</span> of the bootstrap distribution <span class="math inline">\(H_{n}^{Boot}.\)</span> This implies, for large <span class="math inline">\(m,\)</span> <span class="math display">\[
P^*\left(\hat q^*_{n,\frac{\alpha}{2}}\leq {\color{red}\sqrt{n}\frac{\hat{\theta}^*_n-\hat{\theta}_n}{\hat v^*_n}} \leq \hat q^*_{n,1-\frac{\alpha}{2}}\right)
\approx 1-\alpha.
\]</span></p>
<p>By the (assumed) bootstrap consistency, we have for large <span class="math inline">\(n\)</span> that <span class="math display">\[
\left.{\color{red}\sqrt{n}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v_n^*}}\right|\mathcal{S}_n\overset{d}{\approx}
{\color{blue}\sqrt{n}\frac{\hat{\theta}_n-\theta_0}{v_0}}.
\]</span> Therefore, for large <span class="math inline">\(n\)</span> and large <span class="math inline">\(m,\)</span> <span class="math display">\[
\begin{align*}
&amp; P\left(\hat q^*_{n,\frac{\alpha}{2}}\leq {\color{blue}\sqrt{n}\frac{\hat{\theta}_n-\theta_0}{v_0}} \leq \hat q^*_{n,1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow &amp; P\left(\hat q^*_{n,\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right)\leq  \hat{\theta}_n-\theta_0 \leq \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right) \right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow &amp; P\left(- \hat{\theta}_n + \hat q^*_{n,\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right) \leq -\theta_0 \leq - \hat{\theta}_n + \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right)\right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow &amp; P\left(\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}}\left(\frac{v_0}{\sqrt{n}}\right)\leq \theta_0 \leq \hat{\theta}_n - \hat q^*_{n,\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right) \right)
\approx 1-\alpha\\[2ex]
\Leftrightarrow &amp; P\left(\theta_0\in\left[\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{\hat v_n}{\sqrt{n}}\right),\;
      \hat{\theta}_n - \hat q^*_{n,  \frac{\alpha}{2}}   \left(\frac{\hat v_n}{\sqrt{n}}\right)\right]\right)
\approx 1-\alpha.
\end{align*}
\]</span> Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> bootstrap-<span class="math inline">\(t\)</span> confidence interval (<a href="#eq-Boot_tCI" class="quarto-xref">Equation&nbsp;<span>3.11</span></a>) <span class="math display">\[
\left[\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{\hat v_n}{\sqrt{n}}\right),\;
      \hat{\theta}_n - \hat q^*_{n,  \frac{\alpha}{2}}   \left(\frac{\hat v_n}{\sqrt{n}}\right)\right],
\]</span> is indeed an asymptotic (i.e.&nbsp;approximate) <span class="math inline">\((1-\alpha)\times 100\%\)</span> CI.</p>
</section>
<section id="example-bootstrap-boldsymbolt-confidence-interval-for-boldsymbolmu_0" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-bootstrap-boldsymbolt-confidence-interval-for-boldsymbolmu_0"><strong>Example: Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Confidence Interval for <span class="math inline">\(\boldsymbol{\mu_0}\)</span></strong></h4>
<p>Here <span class="math inline">\(\hat\theta_n = \bar{X}_n\)</span> and the estimator of the asymptotic variance <span class="math inline">\(\sigma_0^2=\lim_{n\to\infty}n Var(\bar{X}_n)\)</span> is <span class="math display">\[
\hat\sigma_n^2 = \frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}_n\right)^2.
\]</span></p>
<p><strong>Algorithm:</strong></p>
<ul>
<li>Repeatedly draw i.i.d. random samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\({\cal S}_n\)</span> and calculate <span class="math display">\[
\bar X^*_n=\bar X^*(X_1^*,\dots,X_n^*)\quad\text{and}\quad
\hat\sigma^*_n=\hat\sigma^*(X_1^*,\dots,X_n^*)
\]</span> to generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) bootstrap realizations <span class="math display">\[
T^*_{n,1}=\sqrt{n}\frac{\bar X^*_{n,1}-\bar X_n}{\hat\sigma^*_{n,1}},\dots,T^*_{n,m}=\sqrt{n}\frac{\bar X^*_{n,m}-\bar X_n}{\hat\sigma^*_{n,m}}
\]</span></li>
<li>Determine <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> from <span class="math display">\[
T^*_{n,1},\dots,T^*_{n,m}
\]</span> using <a href="#eq-empiricalQuantile" class="quarto-xref">Equation&nbsp;<span>3.9</span></a>.</li>
<li>This yields the <span class="math inline">\((1-\alpha)\times 100 \%\)</span> confidence interval (using <a href="#eq-Boot_tCI" class="quarto-xref">Equation&nbsp;<span>3.11</span></a>): <span class="math display">\[
\left[\bar X_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{s_n}{\sqrt{n}}\right),
    \bar X_n - \hat q^*_{n,  \frac{\alpha}{2}} \left(\frac{s_n}{\sqrt{n}}\right)\right],
\]</span> where
<ul>
<li><span class="math inline">\(\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i\)</span> and <span class="math inline">\(\hat{\sigma}_n=\sqrt{\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2}\)</span> are computed from the original sample <span class="math inline">\(X_1,\dots,X_n\)</span> and<br>
</li>
<li><span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> are computed from the bootstrap estimates <span class="math inline">\(T^*_{n,1},\dots,T^*_{n,m}.\)</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="accuracy-of-the-bootstrap-boldsymbolt-method" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="accuracy-of-the-bootstrap-boldsymbolt-method"><span class="header-section-number">3.4.2</span> Accuracy of the Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> method</h3>
<p>Usually, the bootstrap-<span class="math inline">\(t\)</span> provides a <strong>gain in accuracy</strong> over the basic bootstrap. The reason is that the approximation of the law of <span class="math inline">\(T_n\)</span> by the bootstrap law of <span class="math display">\[
\left.\frac{\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)}{\hat{v}^*_n}\;\right|\;\mathcal{S}_n
\]</span> is more direct and hence more accurate (<span class="math inline">\(\hat{v}^*_n\)</span> depends also on the bootstrap sample—not on the original sample) than by the bootstrap law of <span class="math display">\[
\left.\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)\;\right|\;\mathcal{S}_n.
\]</span></p>
<p>The use of pivotal statistics and the corresponding construction of bootstrap-<span class="math inline">\(t\)</span> confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-<span class="math inline">\(t\)</span> methods are <strong>second order accurate</strong>.</p>
<p>Consider generally <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals of the form <span class="math display">\[
[L_n,U_n]
\]</span> of <span class="math inline">\(\theta\)</span>. The lower, <span class="math inline">\(L_n\)</span>, and upper bounds, <span class="math inline">\(U_n\)</span>, of such intervals are determined from the data and are thus random, <span class="math display">\[
L_n\equiv L(X_1,\dots,X_n)
\]</span> <span class="math display">\[
U_n\equiv U(X_1,\dots,X_n)
\]</span> and their accuracy depends on the particular procedure applied (e.g.&nbsp;basic bootstrap vs.&nbsp;bootstrap-<span class="math inline">\(t\)</span>).</p>
<ul>
<li>Two-sided <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence intervals <span class="math inline">\([L_n,U_n]\)</span> are said to be <strong>first-order accurate</strong> if there exist some constant <span class="math inline">\(c&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta_0\in [L_n,U_n])-(1-\alpha)\right|\le \frac{c}{\sqrt{n}}
\end{align*}
\]</span></li>
<li>Two-sided <span class="math inline">\((1-\alpha)\cdot 100\%\)</span> confidence intervals <span class="math inline">\([L_n,U_n]\)</span> are said to be <strong>second-order accurate</strong> if there exist some constant <span class="math inline">\(c&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta_0\in[L_n,U_n])-(1-\alpha)\right|\le \frac{c}{n}
\end{align*}
\]</span></li>
</ul>
<p>If the distribution of <span class="math inline">\(\hat\theta_n\)</span> is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that</p>
<ul>
<li>Standard confidence intervals based on classic asymptotic normality approximations are <strong>first-order</strong> accurate.</li>
<li>Basic bootstrap confidence intervals are <strong>first-order</strong> accurate.</li>
<li>Bootstrap-<span class="math inline">\(t\)</span> confidence intervals are <strong>second-order</strong> accurate.</li>
</ul>
<p>The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to <em>much</em> better approximations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Proofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field (see, for instance, <span class="citation" data-cites="koike2024high">Koike (<a href="#ref-koike2024high" role="doc-biblioref">2024</a>)</span>).</p>
</div>
</div>
</section>
</section>
<section id="bootstrap-and-linear-regression-analysis" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="bootstrap-and-linear-regression-analysis"><span class="header-section-number">3.5</span> Bootstrap and Linear Regression Analysis</h2>
<p>In this chapter we consider two different bootstrap resampling procedures that can be applied in linear regression analysis:</p>
<ul>
<li><a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a> considers the <strong>Bootstrapping Pairs</strong> algorithm<br>
</li>
<li><a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a> considers the <strong>Residual Bootstrap</strong> algorithm.</li>
</ul>
<p><strong>Setup:</strong> Linear regression model <span class="math display">\[
Y_i=X_i^T\beta_0 + \varepsilon_i,\quad  i=1,\dots,n,
\]</span> where <span class="math inline">\(Y_i\in\mathbb{R}\)</span> denotes the response (or “dependent”) variable and <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
\]</span> denotes the vector of predictor variables. In the following, we differentiate between a <strong>random design</strong> and a <strong>fixed design</strong>.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-RandomFixedDesign" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.6 (Random and Fixed Design)</strong></span> <br></p>
<p><strong>Random Design:</strong> <span class="math display">\[
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
\]</span> are i.i.d. random variables with <span class="math inline">\(\mathbb{E}(\varepsilon_i|X_i)=0,\)</span> <span class="math inline">\(M=\mathbb{E}(X_iX_i^T)\)</span> non-singular, and with either</p>
<ul>
<li><strong>homoskedastic</strong> errors: <span class="math inline">\(\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2_0\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, for a constant <span class="math inline">\(\sigma^2&lt;\infty\)</span> or</li>
<li><strong>heteroskedastic</strong> errors: <span class="math inline">\(\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2_0(X_i)&lt;\infty\)</span>, <span class="math inline">\(i=1,\dots,n.\)</span></li>
</ul>
<p><strong>Fixed Design:</strong> <span class="math display">\[
X_1, X_2, \dots, X_n
\]</span> are deterministic vectors in <span class="math inline">\(\mathbb{R}^p\)</span> and <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. random variables with zero mean, <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0,\)</span> and <strong>homoskedastic errors</strong>, <span class="math inline">\(\mathbb{E}(\varepsilon_i^2)=\sigma^2_0,\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></p>
</div>
</div>
</div>
</div>
<p>The least squares estimator <span class="math inline">\(\hat\beta_n\in\mathbb{R}^p\)</span> is given by <span class="math display">\[
\begin{align*}
\hat\beta_n
&amp;=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i.
\end{align*}
\]</span></p>
<p>Using that <span class="math inline">\(Y_i=X_i^\top\beta_0+\varepsilon_i,\)</span> one can derive that <span class="math display">\[
\begin{align*}
\hat\beta_n
&amp;=\beta_0+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i.
\end{align*}
\]</span></p>
<section id="sec-bootPairs" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="sec-bootPairs"><span class="header-section-number">3.5.1</span> Bootstrap under Random Design: Bootstrapping Pairs</h3>
<p>Under a random design (<a href="#def-RandomFixedDesign" class="quarto-xref">Definition&nbsp;<span>3.6</span></a>), we assume that there exists a non-singular (thus invertible) matrix <span class="math inline">\(M\)</span> <span class="math display">\[
M=\mathbb{E}(X_iX_i^T).
\]</span> This implies that the following matrix <span class="math inline">\(Q\)</span> is also non-singular: <span class="math display">\[
\begin{align*}
Q
&amp;=\mathbb{E}(\varepsilon_i^2X_iX_i^T)\\[2ex]
&amp;=\mathbb{E}(\mathbb{E}(\varepsilon_i^2X_iX_i^T|X_i))\\[2ex]
&amp;=\mathbb{E}(\sigma^2_0(X_i)X_iX_i^T\mathbb{E}(1|X_i))\\[2ex]
&amp;=\mathbb{E}(\sigma^2_0(X_i)X_iX_i^T)
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In case of homoskedastic errors, we have that <span class="math display">\[
\begin{align*}
Q
&amp;=\mathbb{E}(\sigma^2_0(X_i)X_iX_i^T)\\
&amp;=\sigma^2_0\;\mathbb{E}(X_iX_i^T)\\[2ex]
&amp;=\sigma^2_0\;M.
\end{align*}
\]</span></p>
</div>
</div>
<p>The law of large numbers, the continuous mapping theorem, Slutsky’s theorem, and the central limit theorem (see econometrics lecture) implies that <span class="math display">\[
\sqrt{n}(\hat\beta_n-\beta_0)\rightarrow_d\mathcal{N}_p(0,M^{-1}QM^{-1}),\quad n\to\infty,
\]</span> where <span class="math inline">\(\mathcal{N}_p(0,M^{-1}QM^{-1})\)</span> denotes the <span class="math inline">\(p\)</span>-dimensional normal distribution with <span class="math inline">\((p\times 1)\)</span>-dimensional mean <span class="math inline">\(0\)</span> and <span class="math inline">\((p\times p)\)</span>-dimensional variance-covariance matrix <span class="math inline">\(M^{-1}QM^{-1}.\)</span></p>
<p>The idea of bootstraping pairs is very simple: The procedure builds upon the assumption that <span class="math display">\[
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
\]</span> are i.i.d. which suggests a bootstrap based on resampling the pairs <span class="math inline">\((Y_i,X_i),\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
<p><strong>Bootstraping Pairs Algorithm:</strong></p>
<ol type="1">
<li>Generate bootstrap samples <span class="math display">\[
  (Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
  \]</span> by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n.\)</span></li>
<li>Bootstrap estimators <span class="math inline">\(\hat\beta^*_n\)</span> are determined by least squares estimation from the data <span class="math inline">\((Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*):\)</span> <span class="math display">\[
\hat\beta^*_n=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
\]</span></li>
</ol>
<p>Repeating Steps 1-2 <span class="math inline">\(m\)</span>-many times yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) bootstrap estimators <span class="math display">\[
\hat\beta^*_{n,1},\dots,\hat\beta^*_{n,m}
\]</span> which allow us to approximate the bootstrap distribution of <span class="math inline">\(\hat\beta^*_n-\hat\beta_n|\mathcal{S}_n\)</span> arbitrarily well as <span class="math inline">\(m\to\infty.\)</span></p>
<p>It can be shown that bootstrapping pairs is <strong>consistent</strong>; i.e.&nbsp;that for large <span class="math inline">\(n\)</span> <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*_n-\hat\beta_n) |{\cal S}_n)\approx\mathcal{N}_p(0,M^{-1}QM^{-1}).
\]</span></p>
</section>
<section id="sec-bootResid" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="sec-bootResid"><span class="header-section-number">3.5.2</span> Bootstrap under Fixed Design: The Residual Bootstrap</h3>
<p>If the sample <span class="math display">\[
(Y_1,X_1),\dots,(Y_n,X_n)
\]</span> is <strong>not</strong> an i.i.d. sample, the bootstrapping pairs procedure (<a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a>) proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for <strong>fixed designs</strong> and also generally not in time-series regression contexts. However, if error terms are <strong>homoskedastic</strong>, then it is possible to rely on the <strong>residual bootstrap</strong>.</p>
<p>In the following we will formally assume a regression model <span class="math display">\[
Y_i=X_i^T\beta_0 + \varepsilon_i, \quad i=1,\dots,n,
\]</span> with <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
\]</span> under the <strong>fixed design</strong> (<a href="#def-RandomFixedDesign" class="quarto-xref">Definition&nbsp;<span>3.6</span></a>), where <span class="math display">\[
\varepsilon_1,\dots,\varepsilon_n\overset{\text{i.i.d.}}{\sim}\varepsilon
\]</span> are i.i.d. with zero mean <span class="math display">\[
\mathbb{E}(\varepsilon)=0
\]</span> and <strong>homoskedastic</strong> variance <span class="math display">\[
\mathbb{E}(\varepsilon^2)=\sigma^2_0.
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Applicability of the Residual Bootstrap under Random Designs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs—even when the <span class="math inline">\(X\)</span>-variables are correlated (e.g.&nbsp;time-series).</p>
<p>In such cases, the following arguments are meant <em>conditionally</em> on the observed predictors <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
<p>The above assumptions on the error terms then, of course, also have to be satisfied <em>conditionally</em> on <span class="math inline">\(X_1,\dots,X_n.\)</span></p>
</div>
</div>
<p>The idea of the residual bootstrap is very simple: The procedure builds upon the assumption that the error terms <span class="math display">\[
\varepsilon_1,\dots,\varepsilon_n\overset{\text{i.i.d.}}{\sim}\varepsilon
\]</span> are i.i.d which suggests a bootstrap based on <strong>resampling the (homoskedastic) error terms</strong>.</p>
<p>These errors are, of course, unobserved, but they can be approximated by their corresponding residuals <span class="math display">\[
\hat \varepsilon_i=Y_i-X_i^T\hat\beta_n, \quad i=1,\dots,n,
\]</span> where <span class="math display">\[
\hat\beta_n=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
\]</span> denotes the least squares estimator based on the original sample <span class="math inline">\(\mathcal{S}_n\)</span>.</p>
<p>It is well known that <span class="math display">\[
\hat\sigma^2_n= \frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i^2
\]</span> provides a consistent estimator of the error variance <span class="math inline">\(\sigma^2\)</span>. That is, <span class="math display">\[
\hat\sigma^2_n\rightarrow_p \sigma_0^2
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p><strong>Residual Bootstrap Algorithm:</strong></p>
<p>Based on the original data <span class="math inline">\((Y_i,X_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, and the least squares estimate <span class="math inline">\(\hat\beta_n\)</span>, calculate the residuals <span class="math inline">\(\hat\varepsilon_1,\dots,\hat \varepsilon_n\)</span>.</p>
<ol type="1">
<li>Generate random bootstrap samples <span class="math inline">\(\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*\)</span> of residuals by drawing observations independently and with replacement from <span class="math display">\[
{\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}.
\]</span></li>
<li>Calculate new depend variables <span class="math display">\[
Y_i^*=X_i^T\hat\beta_n + \hat\varepsilon_i^*,\quad i=1,\dots,n
\]</span></li>
<li>Bootstrap estimators <span class="math inline">\(\hat\beta^*_n\)</span> are determined by least squares estimation from the data <span class="math inline">\((Y_1^*,X_1),\dots,(Y_n^*,X_n)\)</span>: <span class="math display">\[
\hat\beta^*_n = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
\]</span></li>
</ol>
<p>Repeating Steps 1-3 <span class="math inline">\(m\)</span> many times yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) bootstrap estimates <span class="math display">\[
\hat\beta^*_{n,1},\hat\beta^*_{n,2},\dots,\hat\beta^*_{n,m}
\]</span> which allow us to approximate the bootstrap distribution <span class="math inline">\(\hat\beta^*_n-\hat\beta_n|\mathcal{S}_n\)</span> arbitrarily well as <span class="math inline">\(m\to\infty.\)</span></p>
<!-- 
#### Motivating the Residual Bootstrap {-}

It is not difficult to understand why the residual bootstrap generally works for *homoskedastic* (!) errors. We have
$$
\hat\beta_n-\beta_0=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
$$
and for large $n$ we have that
$$
\sqrt{n}(\hat\beta_n - \beta_0)\to_d\mathcal{N}_p(0,\sigma^2_0 M^{-1}),
$$ 
where $\mathcal{N}_p(0,\sigma^2 M^{-1})$ denotes the $p$ dimensional normal distribution with $(p\times 1)$ mean $0$ and $(p\times p)$ variance-covariance matrix $\sigma^2_0 M^{-1}.$

On the other hand (the bootstrap world), we have the construction
$$
\hat\beta^*_n - \hat\beta_n
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*.
$$
Conditionally on ${\cal S}_n,$ the bootstrap error terms $\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*$ are i.i.d with
$$
\mathbb{E}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
$$
and
$$
Var(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 = \hat\sigma^2_n,
$$
where $\hat\sigma^2_n\to\sigma^2_0$ as $n\to\infty.$  
-->
<p>It can be shown that the residual bootstrap is consistent; i.e.&nbsp;that<br>
<span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*_n-\hat\beta_n) |{\cal S}_n)
\approx\underbrace{\text{distribution}(\sqrt{n}(\hat\beta_n-\beta_0))}_{\mathcal{N}_p\left(0,\sigma^2_0\, M^{-1}\right)}
\]</span> for large <span class="math inline">\(n.\)</span></p>
</section>
<section id="sec-bootWild" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="sec-bootWild"><span class="header-section-number">3.5.3</span> Bootstrap under Fixed Design: The Wild Bootstrap</h3>
<p>The wild bootstrap is a method for generating bootstrap samples that do not consist of resampling the original data (bootstrapping pairs in <a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a>) or residuals (bootstrapping residuals in <a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a>). Rather, the wild bootstrap combines the data with random variables drawn from a known distribution to form a bootstrap sample.</p>
<p>The wild bootstrap provides a way to deal with issues such as <strong>heteroskedasticity of unknown form in fixed-design</strong> regression models or random-design models in which one conditions on the predictors <span class="math inline">\(X_1,\dots,X_n.\)</span></p>
<p>In the following we will formally assume a regression model <span class="math display">\[
Y_i=X_i^T\beta_0 + \varepsilon_i, \quad i=1,\dots,n,
\]</span> with <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
\]</span> where the <span class="math inline">\(X_i\)</span>’s are fixed in repeated samples (<strong>fixed design</strong>), and where the error terms <span class="math display">\[
\varepsilon_1,\dots,\varepsilon_n
\]</span> are independent across <span class="math inline">\(i=1,\dots,n,\)</span> with<br>
<span class="math display">\[
\mathbb{E}(\varepsilon_i)=0\quad\text{for all}\quad i=1,\dots,n,
\]</span> and with possibly <strong>heteroskedastic</strong> variances <span class="math display">\[
0&lt;\mathbb{E}(\varepsilon_i^2)=\sigma^2_{0,i}&lt;\infty\quad\text{for all}\quad i=1,\dots,n.
\]</span></p>
<p>That is, the data generating process is here independent across <span class="math inline">\(i=1,\dots,n,\)</span> but not necessarily identically distributed across <span class="math inline">\(i=1,\dots,n.\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Applicability of the Wild Bootstrap under Random Designs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Though we will formally rely on a fixed design assumption. However, as in the case of the residual bootstrap, the wild bootstrap is also applicable for random designs.</p>
<p>In random designs, the following arguments are meant <em>conditionally</em> on the observed predictors <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
<p>The above assumptions on the error terms then, of course, also have to be satisfied <em>conditionally</em> on <span class="math inline">\(X_1,\dots,X_n.\)</span></p>
</div>
</div>
<p>As the residual bootstrap (<a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a>), the wild bootstrap uses the <span class="math inline">\(X_i\)</span>’s from the original data. I.e., the <span class="math inline">\(X_i\)</span>’s are not resampled. The wild bootstrap generates bootstrap samples <span class="math display">\[
\{(\underbrace{X_1^\top\hat\beta_n + \varepsilon^\ast_1}_{=Y_1^\ast},X_1),\dots,(\underbrace{X_n^\top\hat\beta_n + \varepsilon^\ast_n}_{=Y_n^\ast},X_n)\},
\]</span> where <span class="math inline">\(\hat\beta_n\)</span> is computed from the original sample. Repeatedly (<span class="math inline">\(m\)</span> times) generating such bootstrap samples allows generating <span class="math inline">\(m\)</span> realizations of the bootstrap estimator <span class="math display">\[
\hat\beta^\ast_{n,j} = \left(X^\top X\right)^{-1}X^\top Y^\ast,\quad j=1,\dots,m.
\]</span></p>
<p>The bootstrap errors <span class="math inline">\(\varepsilon_i^\ast\)</span>’s are generated by either of the following two methods:</p>
<ol type="1">
<li><p>The wild bootstrap (<span class="citation" data-cites="Mammen_1993">Mammen (<a href="#ref-Mammen_1993" role="doc-biblioref">1993</a>)</span>) uses <span class="math display">\[
\varepsilon_i^\ast = W_i\quad\text{for each}\quad i=1,\dots,n
\]</span> to generate the bootstrap samples <span class="math display">\[
\{(\underbrace{X_1^\top\hat\beta_n + \varepsilon^\ast_1}_{=Y_1^\ast},X_1),\dots,(\underbrace{X_n^\top\hat\beta_n + \varepsilon^\ast_n}_{=Y_n^\ast},X_n)\},
\]</span> where <span class="math inline">\(W_i\)</span> is a discrete random variable taking values <span class="math display">\[
W_i\in\left\{\left(1-\sqrt{5}\right)\hat\varepsilon_i,\;\left(1+\sqrt{5}\right)\frac{\hat\varepsilon_i}{2}\right\}
\]</span> with <span class="math display">\[
\hat\varepsilon_i=Y_i - X_i^\top\hat\beta_n,\quad i=1,\dots,n,
\]</span><br>
denoting the original OLS residuals computed, and with <span class="math display">\[
\begin{align*}
P\left(W_i = \left(1-\sqrt{5}\right)\hat\varepsilon_i\right) &amp;= \frac{1+\sqrt{5}}{2\sqrt{5}}\\[2ex]
P\left(W_i = \left(1+\sqrt{5}\right)\frac{\hat\varepsilon_i}{2}\right)&amp;=1 - \frac{1+\sqrt{5}}{2\sqrt{5}}.
\end{align*}
\]</span> Under this construction, we have that <span class="math display">\[
\begin{align*}
\mathbb{E}(W_i)  &amp;=0\\[2ex]
\mathbb{E}(W_i^2)&amp;=\hat\varepsilon_i^2\\[2ex]
\mathbb{E}(W_i^3)&amp;=\hat\varepsilon_i^3.
\end{align*}
\]</span> See <span class="citation" data-cites="Mammen_1993">Mammen (<a href="#ref-Mammen_1993" role="doc-biblioref">1993</a>)</span> for a detailed discussion of the properties of this method.</p></li>
<li><p>The second method is an example of the <strong>multiplier bootstrap</strong>, which uses <span class="math display">\[
\varepsilon_i^\ast = U_i\,f(\hat{\varepsilon}_i)\quad\text{for each}\quad i=1,\dots,n
\]</span> to generate the bootstrap samples <span class="math display">\[
\{(\underbrace{X_1^\top\hat\beta_n + \varepsilon^\ast_1}_{=Y_1^\ast},X_1),\dots,(\underbrace{X_n^\top\hat\beta_n + \varepsilon^\ast_n}_{=Y_n^\ast},X_n)\},
\]</span> where</p>
<ul>
<li><span class="math inline">\(f(\hat{\varepsilon}_i)\)</span> is a transformation of the residuals with <span class="math inline">\(f(\hat{\varepsilon}_i)=\hat{\varepsilon}_i\)</span> being one possibility</li>
<li><span class="math inline">\(U_1,\dots,U_n\overset{\text{i.i.d.}}{\sim}U\)</span> are real valued i.i.d. random variables, independent of the residuals <span class="math inline">\(\hat{\varepsilon}_i,\)</span> and with <span class="math inline">\(\mathbb{E}(U)=0\)</span> and <span class="math inline">\(\mathbb{E}(U^2)=1.\)</span> One possibility would be <span class="math inline">\(U\sim\mathcal{N}(0,1).\)</span> Another often used possibility is to use a <a href="https://en.wikipedia.org/wiki/Rademacher_distribution">Rademacher distributed</a> random variable <span class="math inline">\(U.\)</span></li>
</ul></li>
</ol>
<p>See <span class="citation" data-cites="Davidson_and_Flachaire_2008">Davidson and Flachaire (<a href="#ref-Davidson_and_Flachaire_2008" role="doc-biblioref">2008</a>)</span> for a detailed discuss of the properties of this method.</p>
</section>
<section id="bootstrap-confidence-intervals-for-the-boldsymboljth-component-of-the-regression-coefficient-boldsymbolbeta_0" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="bootstrap-confidence-intervals-for-the-boldsymboljth-component-of-the-regression-coefficient-boldsymbolbeta_0"><span class="header-section-number">3.5.4</span> Bootstrap Confidence Intervals for the <span class="math inline">\(\boldsymbol{j}\)</span>th Component of the Regression Coefficient <span class="math inline">\(\boldsymbol{\beta_{0}}\)</span></h3>
<p>This chapter introduces two confidence intervals. The first uses the basic bootstrap method (<a href="#sec-BasicBootstrap" class="quarto-xref"><span>Section 3.3</span></a>); the second uses the bootstrap-<span class="math inline">\(t\)</span> method (<a href="#sec-BootT" class="quarto-xref"><span>Section 3.4</span></a>).</p>
<p>Both confidence intervals can be constructed either via bootstrapping pairs (<a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a>) or via bootstrapping residuals (<a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a>).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>While the bootstrap confidence intervals based on bootstrapping pairs (<a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a>) are heteroskedasticity robust, the bootstrap confidence intervals based on bootstrapping residuals are only valid for homoskedastic errors.</p>
</div>
</div>
<section id="basic-bootstrap-confidence-intervals-for-boldsymbolbeta_0j" class="level4" data-number="3.5.4.1">
<h4 data-number="3.5.4.1" class="anchored" data-anchor-id="basic-bootstrap-confidence-intervals-for-boldsymbolbeta_0j"><span class="header-section-number">3.5.4.1</span> <strong>Basic Bootstrap Confidence Intervals for <span class="math inline">\(\boldsymbol{\beta_{0,j}}\)</span></strong></h4>
<p>Let <span class="math display">\[
\beta_{0,j}\in\mathbb{R},
\]</span> <span class="math inline">\(j=1,\dots,p\)</span>, denote the <span class="math inline">\(j\)</span>th component of <span class="math inline">\(\beta_0\in\mathbb{R}^p,\)</span> and let <span class="math display">\[
\hat{\beta}_{j,n}\in\mathbb{R}
\]</span> denote the <span class="math inline">\(j\)</span>th component of the estimator <span class="math inline">\(\hat\beta_n\in\mathbb{R}^p.\)</span></p>
<p>The basic bootstrap confidence interval for <span class="math inline">\(\beta_{0,j}\in\mathbb{R}\)</span> can be constructed as following:</p>
<ol type="1">
<li><p>Use either bootstrapping pairs (<a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a>) or bootstrapping residuals (<a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a>) or the wild bootstrap (<a href="#sec-bootWild" class="quarto-xref"><span>Section 3.5.3</span></a>) to generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) bootstrap realizations <span class="math display">\[
\hat{\beta}_{j,n,1}^\ast,\dots,\hat\beta_{j,n,m}^\ast.
\]</span></p></li>
<li><p>Determine the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat q^\ast_{n,\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat q^\ast_{n,1-\frac{\alpha}{2},j}\)</span> from the bootstrap realizations <span class="math inline">\(\hat{\beta}_{j,n,1}^\ast,\dots,\hat\beta_{j,n,m}^\ast\)</span> using <a href="#eq-empiricalQuantile" class="quarto-xref">Equation&nbsp;<span>3.9</span></a>.</p></li>
<li><p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval as in <a href="#eq-NPBootCI" class="quarto-xref">Equation&nbsp;<span>3.10</span></a>: <span class="math display">\[
\left[2\hat\beta_{nj}-\hat q^\ast_{n,1-\frac{\alpha}{2},j},
   2\hat\beta_{nj}-\hat q^\ast_{n,\frac{\alpha}{2},j}\right],
\]</span> where</p>
<ul>
<li><p><span class="math inline">\(\hat\beta_{nj}\)</span> denotes the <span class="math inline">\(j\)</span>th component of <span class="math inline">\(\hat\beta_{n}\)</span> computed from the original sample <span class="math inline">\(\mathcal{S}_n,\)</span> and</p></li>
<li><p><span class="math inline">\(\hat q^\ast_{n,1-\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat q^\ast_{n,\frac{\alpha}{2},j}\)</span> are the empirical quantiles computed from the bootstrap estimators <span class="math inline">\(\hat{\beta}_{j,n,1}^\ast,\dots,\hat\beta_{j,n,m}^\ast.\)</span></p></li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval for <strong>homoskedastic errors,</strong> but also for <strong>heteroskedastic</strong> errors. In case of heteroskedastic errors, one needs to use an appropriate boostrap such as the pairs boostrap (random design) or the wild bootstrap (fixed design).</p>
<p>Note that standard confidence intervals usually provided by statistical software packages are for homoskedastic errors. For instance, the <code>confint(object)</code> function in <code>R</code> for an <code>object</code> returned by the <code>lm()</code> function uses the standard error formula for <strong>homoskedastic</strong> errors.</p>
</div>
</div>
<!-- 
#### Basic Bootstrap Confidence Intervals {-}

Basic **nonparametric bootstrap** confidence intervals for the regression coefficients $\beta_j$, $j=1,\dots,p,$ can be constructed as following: 

1. Generate $m$ bootstrap estimates
$$
\hat\beta_{n,1,j}^*,\hat\beta_{n,2,j}^*, \dots, \hat\beta_{n,m,j}^*.
$$

2. Compute the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat q_{n,\frac{\alpha}{2},j}$ and $\hat q_{n,1-\frac{\alpha}{2},j}$ (see @eq-empiricalQuantile) of the $m$ bootstrap estimates $\hat\beta_{n,1,j}^*,\hat\beta_{n,2,j}^*, \dots, \hat\beta_{n,m,j}^*.$

3. Compute the approximate $(1-\alpha)\times 100\%$ basic (nonparametric) bootstrap confidence interval as in @eq-NPBootCI:
$$
\left[2\hat\beta_{n,j}-\hat q_{n,1-\frac{\alpha}{2},j}, 
      2\hat\beta_{n,j}-\hat q_{n, \frac{\alpha}{2},j }\right],
$$
where $\hat\beta_{n,j}$ denotes the $j$th component of the estimate $\hat\beta_n\in\mathbb{R}^p$ computed from the orginal sample $\mathcal{S}_n.$ 
-->
</section>
<section id="bootstrap-boldsymbolt-confidence-intervals-for-boldsymbolbeta_0j" class="level4" data-number="3.5.4.2">
<h4 data-number="3.5.4.2" class="anchored" data-anchor-id="bootstrap-boldsymbolt-confidence-intervals-for-boldsymbolbeta_0j"><span class="header-section-number">3.5.4.2</span> <strong>Bootstrap-<span class="math inline">\(\boldsymbol{t}\)</span> Confidence Intervals for <span class="math inline">\(\boldsymbol{\beta_{0,j}}\)</span></strong></h4>
<p>Bootstrap-<span class="math inline">\(t\)</span> confidence intervals for the regression coefficients <span class="math inline">\(\beta_{0,j}\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Consider the statistic <span class="math display">\[
T_n = \frac{\hat\beta_{j,n} -\beta_{0,j}}{\widehat{\operatorname{SE}}(\hat\beta_{j,n})},
\]</span> where</p>
<ul>
<li><span class="math inline">\(\beta_{0,j}\)</span> denotes the <span class="math inline">\(j\)</span>th element of <span class="math inline">\(\beta_0\in\mathbb{R}^p,\)</span></li>
<li><span class="math inline">\(\hat\beta_{j,n}\)</span> denotes the <span class="math inline">\(j\)</span>th element of the OLS estimator <span class="math inline">\(\hat\beta_n\in\mathbb{R}^p.\)</span></li>
</ul>
<p>In the case of homoskedastic error terms <span id="eq-homoskedSE"><span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_{j,n})
=\frac{\hat{\sigma}_n\sqrt{\hat{\gamma}_{jj,n}}}{\sqrt{n}},
\tag{3.12}\]</span></span> where <span class="math inline">\(\hat{\sigma}_n=\sqrt{\frac{1}{n}\sum_{i=1}^n\hat{\varepsilon}_i^2}\)</span> and where <span class="math display">\[
\hat{\gamma}_{jj,n}
=\left[\widehat{M}_n^{-1}\right]_{jj}
=\left[\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^\top\right)^{-1}\right]_{jj}
\]</span> denotes the <span class="math inline">\(j\)</span>-th diagonal element of the <span class="math inline">\((p\times p)\)</span>-dimensional matrix <span class="math inline">\(\widehat{M}_n^{-1}=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}.\)</span></p>
<p>In the case of heteroskedastic errors, one can use <span id="eq-heterokedSE"><span class="math display">\[
\widehat{\operatorname{SE}}(\hat\beta_{j,n})
=\frac{\sqrt{\left[\widehat{M}_n^{-1}\widehat{Q}_n\widehat{M}_n^{-1}\right]_{jj}}}{\sqrt{n}}
\tag{3.13}\]</span></span> where</p>
<ul>
<li><span class="math inline">\(\widehat{M}_n^{-1}=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\)</span></li>
<li><span class="math inline">\(\widehat{Q}_n=\widehat{\mathbb{E}}(\varepsilon_i^2X_iX_i^\top)\)</span> denotes a Heteroskedasticity Consistent (HC) estimators of <span class="math inline">\(\mathbb{E}(\varepsilon_i^2X_iX_i^\top);\)</span> e.g.&nbsp;the HC2-estimator <span class="math inline">\(\widehat{\mathbb{E}}(\varepsilon_i^2X_iX_i^\top)=\frac{1}{n-p}\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i^\top.\)</span></li>
</ul>
<p>Note that <span class="math inline">\(T_n\)</span> is an asymptotically pivotal statistic; i.e., <span class="math display">\[
T_n= \frac{(\hat\beta_{n,j}-\beta_{0,j})}{\widehat{\operatorname{SE}}(\hat\beta_{j,n})}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>A bootstrap-<span class="math inline">\(t\)</span> interval for <span class="math inline">\(\beta_{0,j}\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>, can thus be constructed as follows:</p>
<ol type="1">
<li><p>Use either <strong>bootstrapping pairs</strong> (<a href="#sec-bootPairs" class="quarto-xref"><span>Section 3.5.1</span></a>) for random designs, or <strong>bootstrapping residuals</strong> (<a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a>) for fixed designs and homoskedastic errors, or the <strong>wild bootstrap</strong> (<a href="#sec-bootWild" class="quarto-xref"><span>Section 3.5.3</span></a>) for fixed desgins and heteroskedastic errors, to generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) bootstrap realizations <span class="math display">\[
T^*_{n,1},T_{n,2}^*,\dots, T_{n,m}^*,
\]</span> with <span class="math display">\[
T^*_{n,k}=\frac{\hat\beta_{n,j}^*-\hat\beta_{0,j}}{\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})},\quad k=1,\dots,m,
\]</span> where</p>
<ul>
<li><span class="math inline">\(\hat\beta_{0,j}\)</span> is computed from the original sample</li>
<li><span class="math inline">\(\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})\)</span> is an appropriate estimate of the standard error (e.g. <a href="#eq-homoskedSE" class="quarto-xref">Equation&nbsp;<span>3.12</span></a> or <a href="#eq-heterokedSE" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>)
<ul>
<li>the residual components in <span class="math inline">\(\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})\)</span> are resampled</li>
<li>the <span class="math inline">\(X\)</span>-components in <span class="math inline">\(\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})\)</span> are resampled only in random designs, but kept fix in fixed desgins.</li>
</ul></li>
</ul></li>
<li><p>Compute the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat q^\ast_{n,\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat q^\ast_{n,1-\frac{\alpha}{2},j}\)</span> (see <a href="#eq-empiricalQuantile" class="quarto-xref">Equation&nbsp;<span>3.9</span></a>) from the bootstrap estimates <span class="math inline">\(T^*_{n,1},T_{n,2}^*,\dots, T_{n,m}^*.\)</span></p></li>
<li><p>Compute the <span class="math inline">\((1-\alpha)\times 100\%\)</span> bootstrap-<span class="math inline">\(t\)</span> confidence interval as in <a href="#eq-Boot_tCI" class="quarto-xref">Equation&nbsp;<span>3.11</span></a>: <span class="math display">\[
\left[
  \hat\beta_{j,n}-\hat q^\ast_{1-\frac{\alpha}{2},n,j}\;\left(\widehat{\operatorname{SE}}(\hat\beta_{j,n})\right),\;
  \hat\beta_{j,n}-\hat q^\ast_{\frac{\alpha}{2},n,j}\;\left(\widehat{\operatorname{SE}}(\hat\beta_{j,n})\right)
\right],
\]</span> where</p>
<ul>
<li><span class="math inline">\(\hat\beta_{n,j}\)</span> and <span class="math inline">\(\widehat{\operatorname{SE}}(\hat\beta_{j,n})\)</span> are computed from the original sample <span class="math inline">\(\mathcal{S}_n=((Y_1,X_1),\dots,(Y_n,X_n)),\)</span> and</li>
<li><span class="math inline">\(\hat q^*_{n,1-\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat q^*_{n,\frac{\alpha}{2}}\)</span> are computed from the bootstrap realizations <span class="math inline">\(T^*_{n,1},T_{n,2}^*,\dots, T_{n,m}^*.\)</span></li>
</ul></li>
</ol>
</section>
</section>
<section id="statistical-hypothesis-testing" class="level3" data-number="3.5.5">
<h3 data-number="3.5.5" class="anchored" data-anchor-id="statistical-hypothesis-testing"><span class="header-section-number">3.5.5</span> Statistical Hypothesis Testing</h3>
<p>In the following, we consider a fixed design, where one can use the <strong>residual bootstrap</strong> (<a href="#sec-bootResid" class="quarto-xref"><span>Section 3.5.2</span></a>) or the <strong>wild bootstrap</strong> (<a href="#sec-bootWild" class="quarto-xref"><span>Section 3.5.3</span></a>).</p>
<p>Suppose we want to test the hypothesis <span class="math display">\[
\begin{align*}
H_0                    &amp;: \beta_{0,j}    = 0\\[2ex]
\text{against}\quad H_1&amp;: \beta_{0,j} \neq 0
\end{align*}
\]</span> using the test statistic <span class="math display">\[
T_n = \frac{\hat\beta_{j,n} - 0}{\widehat{\operatorname{SE}}(\hat\beta_{j,n})},
\]</span> where</p>
<ul>
<li><span class="math inline">\(\beta_{0,j}\)</span> denotes the <span class="math inline">\(j\)</span>th element of <span class="math inline">\(\beta_0\in\mathbb{R}^p,\)</span></li>
<li><span class="math inline">\(\hat\beta_{j,n}\)</span> denotes the <span class="math inline">\(j\)</span>th element of the OLS estimator <span class="math inline">\(\hat\beta_n\in\mathbb{R}^p.\)</span></li>
<li><span class="math inline">\(\widehat{\operatorname{SE}}(\hat\beta_{j,n})\)</span> is an appropriate estimator of the standard error (e.g. <a href="#eq-homoskedSE" class="quarto-xref">Equation&nbsp;<span>3.12</span></a> or <a href="#eq-heterokedSE" class="quarto-xref">Equation&nbsp;<span>3.13</span></a>)</li>
</ul>
<p>The <span class="math inline">\(p\)</span>-value is defined as <span class="math display">\[
\begin{align*}
&amp;p_{obs} = \\[2ex]
&amp;2\,\min\left\{P(T_n \geq T_{n,obs}|H_0\;\text{is true}),\;P(T_n \leq T_{n,obs}|H_0\;\text{is true})\right\}
\end{align*}
\]</span> where <span class="math inline">\(T_{n,obs}\)</span> is the value of the test statistic computed from the original sample <span class="math display">\[
\mathcal{S}_n=\left\{(Y_1,X_1),\dots,(Y_n,X_n)\right\}.
\]</span></p>
<p>To conduct the test using the bootstrap, we have to estimate <span class="math inline">\(p_{obs}\)</span> using the bootstrap.</p>
<p>Central question: <strong>How to generate bootstrap samples under <span class="math inline">\(\boldsymbol{H_0}\)</span>?</strong></p>
<p>To estimate <span class="math inline">\(\beta_0\in\mathbb{R}^p\)</span> under <span class="math inline">\(H_0,\)</span> we need to estimate all elements in <span class="math inline">\(\beta_0\)</span> that are not specified/fixed by <span class="math inline">\(H_0\)</span> leaving the other elements at their <span class="math inline">\(H_0\)</span>-values.</p>
<p>Let <span class="math inline">\(\beta^{H_0}_0\in\mathbb{R}^{(p-1)}\)</span> denote the parameter vector that contains all elements of <span class="math inline">\(\beta_0\in\mathbb{R}^p\)</span> that are not specified by <span class="math inline">\(H_0.\)</span> <!-- 
Under $H_0,$ we can rewrite the regression model as 
$$
\begin{align*}
\overbrace{Y_i - \beta_{0,j}^0 X_j}^{=\tilde{Y}_i} = \tilde{X}_i^\top \beta^{H_0}_0 + \varepsilon_i,
\end{align*}
$$
where $\tilde{X}_i\in\mathbb{R}^{(p-1)}$ denotes the predictor vector with the $j$th element removed.  
--> The estimator of <span class="math inline">\(\beta^{H_0}_0\in\mathbb{R}^{(p-1)}\)</span> is then given by <span class="math display">\[
\underset{((p-1)\times 1)}{\hat{\beta}^{H_0}_{n}}=\left(\tilde{X}^\top\tilde{X}\right)^{-1}\tilde{X}^\top Y
\]</span> where the <span class="math inline">\((n\times (p-1))\)</span>-matrix <span class="math inline">\(\tilde{X}\)</span> is the matrix <span class="math inline">\(X\)</span> with the <span class="math inline">\(j\)</span>th column removed.</p>
<p>Using <span class="math inline">\(\hat{\beta}^{H_0}_{n},\)</span> we can compute the <span class="math inline">\((n\times 1)\)</span>-vector of residuals <strong>under <span class="math inline">\(\boldsymbol{H_0}\)</span></strong> as <span class="math display">\[
\left(\begin{matrix}\hat{\varepsilon}^{H_0}_1\\ \vdots\\\hat{\varepsilon}^{H_0}_n\end{matrix}\right)=\hat{\varepsilon}^{H_0}=Y-\tilde{X}\hat{\beta}^{H_0}_{n}.
\]</span></p>
<p><strong>Bootstrap algorithm:</strong></p>
<p><strong>Step 1 Residual Bootstrap Option:</strong> Draw independently and with replacement <span class="math inline">\(n\)</span> values from <span class="math display">\[
\{\hat{\varepsilon}^{H_0}_1, \dots, \hat{\varepsilon}^{H_0}_n\}
\]</span> to generate bootstrap realizations under <span class="math inline">\(H_0\)</span> <span class="math display">\[
\{\hat{\varepsilon}^{H_0\ast}_1, \dots, \hat{\varepsilon}^{H_0\ast}_n\}.
\]</span> These allow us to generate the bootstrap samples under <span class="math inline">\(H_0,\)</span> <span class="math display">\[
\left\{(Y_1^{H_0\ast},X_1),\dots,(Y_n^{H_0\ast},X_n)\right\},
\]</span> where <span class="math display">\[
Y_i^{H_0\ast} = \tilde{X}_i^\top \hat{\beta}^{H_0}_{n} + \hat{\varepsilon}^{H_0\ast}_i,\quad i=1,\dots,n.
\]</span></p>
<p><strong>Step 1 Wild Bootstrap Option:</strong> Use<br>
<span class="math display">\[
\{\hat{\varepsilon}^{H_0}_1, \dots, \hat{\varepsilon}^{H_0}_n\}
\]</span> to generate wild bootstrap errors under <span class="math inline">\(H_0\)</span> <span class="math display">\[
\varepsilon^{H_0\ast}_i=\left\{
  \begin{array}{ll}
  (1-\sqrt{5})\hat{\varepsilon}^{H_0}_i&amp;\text{with propability }(1+\sqrt{5})/2\sqrt{5}\\
  (1+\sqrt{5})\hat{\varepsilon}^{H_0}_i/2&amp;\text{with propability }1-(1+\sqrt{5})/2\sqrt{5}
  \end{array}
\right.
\]</span> These allow us to generate the bootstrap samples under <span class="math inline">\(H_0,\)</span> <span class="math display">\[
\left\{(Y_1^{H_0\ast},X_1),\dots,(Y_n^{H_0\ast},X_n)\right\},
\]</span> where <span class="math display">\[
Y_i^{H_0\ast} = \tilde{X}_i^\top \hat{\beta}^{H_0}_{n} + \varepsilon^{H_0\ast}_i,\quad i=1,\dots,n.
\]</span></p>
<p><strong>Step 2.</strong> Based on the bootstrap sample <span class="math display">\[
\left\{(Y_1^{H_0\ast},X_1),\dots,(Y_n^{H_0\ast},X_n)\right\}
\]</span> we can compute the bootstrap realization of the OLS estimator under <span class="math inline">\(H_0,\)</span> <span class="math display">\[
\hat{\beta}^{\ast}_{n}=\left(X^\top X\right)^{-1} X^\top Y^{H_0\ast},
\]</span> which allows us to generate the corresponding realization of the test statistic <span class="math display">\[
T_n^{\ast}
\]</span> <strong>under <span class="math inline">\(\boldsymbol{H_0}\)</span>.</strong></p>
<p><strong>Step 3.</strong> Repeating Steps 1-2 leads to <span class="math inline">\(m\)</span>-many (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) bootstrap realizations of the test statistic<br>
<span class="math display">\[
T_{n,1}^{\ast},\dots,T_{n,m}^{\ast}
\]</span> <strong>under <span class="math inline">\(\boldsymbol{H_0}\)</span>.</strong></p>
<p>To estimate the unknown <span class="math inline">\(p_{obs},\)</span> we can use now the following estimator <span class="math display">\[
\begin{align*}
&amp;\hat p_{obs} = \\[2ex]
&amp;=2\,\min\left\{\hat{P}(T_n \geq T_{n,obs}|H_0\;\text{is true}),\;\hat{P}(T_n \leq T_{n,obs}|H_0\;\text{is true})\right\}\\[2ex]
&amp;=2\,\min\left\{
  \frac{1}{m}\sum_{j=1}^m 1_{\left(T_{n,j}^{\ast} \geq T_{n,obs}\right)},\;
  \frac{1}{m}\sum_{j=1}^m 1_{\left(T_{n,j}^{\ast} \leq T_{n,obs}\right)}
  \right\}
\end{align*}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In case of heteroskedasticity, the wild bootstrap and a corresponding formula for <span class="math inline">\(\widehat{\operatorname{SE}}(\hat{\beta}_j)\)</span> has to be used.</p>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<section id="exercise-1." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-1.">Exercise 1.</h4>
<p>Consider the empirical distribution function <span class="math display">\[
F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
\]</span> for a random sample <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} F.
\]</span></p>
<ol type="a">
<li><p>Derive the exact distribution of <span class="math inline">\(nF_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span></p></li>
<li><p>Derive the asymptotic distribution of <span class="math inline">\(F_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span></p></li>
<li><p>Show that <span class="math inline">\(F_n(x)\)</span> is a point-wise (weakly) consistent estimator of <span class="math inline">\(F(x)\)</span> for each given <span class="math inline">\(x\in\mathbb{R}\)</span>.</p></li>
</ol>
</section>
<section id="exercise-2." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-2.">Exercise 2.</h4>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Exercise 1 shows that the empirical distribution function is a <strong>point-wise</strong> consistent estimator for each given <span class="math inline">\(x\in\mathbb{R}.\)</span> However, point-wise consistency generally does not imply <strong>uniformly</strong> consistency for all <span class="math inline">\(x\in\mathbb{R},\)</span> and therefore the Clivenko-Cantelli (<a href="#thm-Clivenko-Cantelli" class="quarto-xref">Theorem&nbsp;<span>3.1</span></a>), which shows <strong>uniform</strong> consistency of the empirical distribution function, is so important.</p>
<p>This exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.</p>
</div>
</div>
<p>Point-wise convergence of a function <span class="math inline">\(g_n(x),\)</span> i.e., <span class="math display">\[
|g_n(x) - g(x)|\to 0
\]</span> for each <span class="math inline">\(x\in\mathcal{X}\subset\mathbb{R}\)</span> as <span class="math inline">\(n\to\infty\)</span> generally does not imply uniform convergence, i.e., <span class="math display">\[
\sup_{x\in\mathcal{X}}|g_n(x) - g(x)|\to 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Show this by providing an example for <span class="math inline">\(g_n\)</span> which converges point-wise, but not uniformly for <span class="math inline">\(x\in\mathcal{X}\)</span>.</p>
<!-- 
http://personal.psu.edu/drh20/asymp/fall2002/lectures/ln03.pdf 
-->
</section>
<section id="exercise-3." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-3.">Exercise 3.</h4>
<p>Consider the following setup:</p>
<ul>
<li>iid data <span class="math inline">\(X_1,\dots,X_n\)</span> with <span class="math inline">\(X_i\sim F\)</span></li>
<li><span class="math inline">\(\mathbb{E}(X_i)=\mu\)</span></li>
<li><span class="math inline">\(Var(X_i)=\sigma^2&lt;\infty\)</span></li>
<li>Estimator: <span class="math inline">\(\bar{X}_n=n^{-1}\sum_{i=1}^nX_i\)</span></li>
</ul>
<ol type="a">
<li>Derive the classic confidence interval for <span class="math inline">\(\mu\)</span> using the asymptotic normality of the estimator <span class="math inline">\(\bar{X}.\)</span> Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of <span class="math inline">\(n=20\)</span> and,</li>
</ol>
<ul>
<li>Part 1: For <span class="math inline">\(F\)</span> being the normal distribution with <span class="math inline">\(\mu=1\)</span> and standard deviation <span class="math inline">\(\sigma=2\)</span>, and</li>
<li>Part 2: For <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom.</li>
</ul>
<ol start="2" type="a">
<li><p>Reconsider the case of <span class="math inline">\(n=20\)</span> and <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.</p></li>
<li><p>Reconsider the case of <span class="math inline">\(n=20\)</span> and <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-<span class="math inline">\(t\)</span> confidence interval.</p></li>
</ol>
</section>
<section id="exercise-4." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-4.">Exercise 4.</h4>
<!-- Computational Statistics, James E. Gentle,  Exercise 13.1. -->
<p>Let <span class="math inline">\(\mathcal{S}_n = \{Y_1 , \dots, Y_n\}\)</span> be a random sample from a population with mean <span class="math inline">\(\mu,\)</span> variance <span class="math inline">\(\sigma^2,\)</span> and distribution function <span class="math inline">\(F.\)</span> Let <span class="math inline">\(F_n\)</span> be the empirical distribution function. Let <span class="math inline">\(\bar{Y}\)</span> be the sample mean of <span class="math inline">\(\mathcal{S}_n.\)</span> Let <span class="math inline">\(\mathcal{S}^*_n = \{Y_1^∗,\dots, Y_n^∗\}\)</span> be a random sample taken independently and with replacement from <span class="math inline">\(\mathcal{S}_n.\)</span> Let <span class="math inline">\(\bar{Y}^*\)</span> be the sample mean of <span class="math inline">\(\mathcal{S}^*_n.\)</span></p>
<ol type="a">
<li><p>Show that <span class="math display">\[
\mathbb{E}^*(\bar{Y}^*) = \bar{Y}
\]</span></p></li>
<li><p>Show that <span class="math display">\[
\mathbb{E}(\bar{Y}^*) = \mu
\]</span></p></li>
</ol>
<!-- 
#### Exercise 5. {-}
Computational Statistics, James E. Gentle,  Exercise 13.6. 
-->
<!-- {{< include Ch3_Solutions.qmd >}} -->
</section>
</section>
<section id="solutions" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="solutions">Solutions</h2>
<section id="solution-of-exercise-1." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-of-exercise-1.">Solution of Exercise 1.</h4>
<section id="a" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a">(a)</h5>
<p>The exact point-wise distribution of <span class="math inline">\(nF_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span><br>
<span class="math display">\[
\begin{align*}
F_n(x)
&amp; = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\\
\Rightarrow nF_n(x)
&amp; = \sum_{i=1}^n 1_{(X_i\leq x)} \sim \mathcal{Binom}\left(n,p=F(x)\right),
\end{align*}
\]</span> since <span class="math inline">\(1_{(X_i\leq x)}\)</span> is a Bernoulli random variable with parameter <span class="math display">\[
\begin{align*}
p
&amp; = P(1_{(X_i\leq x)} = 1)\\[2ex]
&amp; = P(X_i \leq x)\\[2ex]
&amp; = F(x).
\end{align*}
\]</span> Note that this holds <strong>for any distribution</strong> of <span class="math inline">\(X_i.\)</span> Therefore, one says that <span class="math inline">\(nF_n(x)\)</span> is <strong>distribution free.</strong></p>
</section>
<section id="b" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="b">(b)</h5>
<p>From (a), we have that (using the standard mean and variance expressions for Binomial distributed random variables): <span class="math display">\[
\begin{align*}
\mathbb{E}(nF_n(x))
&amp;= n p\\[2ex]
&amp;= nF(x)\\[2ex]
\Leftrightarrow\quad  \mathbb{E}(F_n(x))
&amp;= p \\[2ex]
&amp;= F(x)
\end{align*}
\]</span> and that <span class="math display">\[
\begin{align*}
Var(nF_n(x))
&amp;= n p (1-p)\\[2ex]
&amp;= nF(x)(1-F(x))\\[2ex]
\Leftrightarrow \quad Var(F_n(x))
&amp;= \frac{p (1- p)}{n}\\[2ex]
&amp;= \frac{F(x)(1-F(x))}{n}.
\end{align*}
\]</span></p>
<p>Moreover, since <span class="math inline">\(F_n(x)  = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\)</span> is an average over i.i.d. random variables <span class="math display">\[
1_{(X_1\leq x)},\dots,1_{(X_n\leq x)},
\]</span> the standard CLT (Lindeberg-Lévy) implies that <span class="math display">\[
\frac{F_n(x)-F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}}\to_d\mathcal{N}(0,1)
\]</span> as <span class="math inline">\(n\to\infty.\)</span> Or equivalently, with a slight abuse of notation: <span class="math display">\[
F_n(x)\overset{a}{\sim}\mathcal{N}\left(F(x),\frac{F(x)(1-F(x))}{n}\right).
\]</span></p>
</section>
<section id="c" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="c">(c)</h5>
<p>The mean squared error between <span class="math inline">\(F_n(x)\)</span> and <span class="math inline">\(F(x)\)</span> is given by <span class="math display">\[
\begin{align*}
\operatorname{MSE}(F_n(x))
&amp;= \mathbb{E}\left((F_n(x)-F(x))^2\right)\\[2ex]
&amp;= Var(F_n(x)) + \left(\mathbb{E}(F_n(x))-F(x)\right)^2.
\end{align*}
\]</span> It follows from our previous results that for each <span class="math inline">\(x\in\mathbb{R}\)</span> <span class="math display">\[
Var(F_n(x)) = \frac{F(x)(1-F(x))}{n} \to 0
\]</span> as <span class="math inline">\(n\to\infty,\)</span> and that <span class="math display">\[
\mathbb{E}(F_n(x)) -F(x) = 0
\]</span> for all <span class="math inline">\(n.\)</span> Therefore, <span class="math display">\[
\operatorname{MSE}(F_n(x)) = Var(F_n(x)) \to 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Thus we can conclude that <span class="math inline">\(F_n(x)\)</span> converges in the mean-square sense to <span class="math inline">\(F(x)\)</span> for each <span class="math inline">\(x\in\mathbb{R},\)</span> <span class="math display">\[
F_n(x)\to_{ms} F(x)
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Since convergence in the mean square sense implies convergence in probability, we also have that for each <span class="math inline">\(x\in\mathbb{R}\)</span> <span class="math display">\[
F_n(x)\to_{p} F(x)
\]</span> as <span class="math inline">\(n\to\infty,\)</span> which shows that <span class="math inline">\(F_n(x)\)</span> is weakly consistent for <span class="math inline">\(F(x)\)</span> for each <span class="math inline">\(x\in\mathbb{R}.\)</span></p>
</section>
</section>
<section id="solution-of-exercise-2." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-of-exercise-2.">Solution of Exercise 2.</h4>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Another, equivalent way to define uniform convergence:</p>
<p><span class="math inline">\(g_n(\cdot)\)</span> converges <strong>uniformly</strong> to <span class="math inline">\(g(\cdot)\)</span> if for every <span class="math inline">\(\varepsilon&gt;0,\)</span> there exists an <span class="math inline">\(N_\varepsilon\)</span> such that <span class="math display">\[\begin{align*}
&amp;\sup_{x\in\mathcal{X}}|g_n(x) - g(x)| &lt; \varepsilon\quad \text{for all}\quad n\geq N_\varepsilon\\[2ex]
\Leftrightarrow\qquad &amp; g_n(x)\in[g(x)-\varepsilon,g(x)+\varepsilon]\quad\text{for all}\quad x\in\mathcal{X}\quad\text{and all}\quad n\geq N_\varepsilon.
\end{align*}\]</span></p>
<p>I.e., <span class="math inline">\(g_n(\cdot)\)</span> converges <strong>uniformly</strong> to <span class="math inline">\(g(\cdot)\)</span> if it is possible to draw an <span class="math inline">\(\varepsilon\)</span>-band around the graph of <span class="math inline">\(g(x)\)</span> that contains <strong>all of the graphs</strong> of <span class="math inline">\(g_n(x)\)</span> for large enough <span class="math inline">\(n\geq N_\varepsilon.\)</span></p>
</div>
</div>
<p><strong>Example 1:</strong> <span class="math inline">\(\mathcal{X}=\mathbb{R}\)</span><br> The function <span class="math display">\[
g_n(x) = x\left(1+\frac{1}{n}\right)
\]</span> converges point-wise (for each given <span class="math inline">\(x\in\mathbb{R}\)</span>) to <span class="math display">\[
g(x)=x,
\]</span> since <span class="math display">\[
|g_n(x)-g(x)|=\frac{|x|}{n}\to 0\quad \text{as}\quad n\to\infty.
\]</span> for each given <span class="math inline">\(x\in\mathcal{X}.\)</span></p>
<p>However, <span class="math inline">\(g_n\)</span> does not converge uniformly to <span class="math inline">\(g\)</span> since <span class="math display">\[
\sup_{x\in\mathbb{R}}|g_n(x)-g(x)|=\sup_{x\in\mathbb{R}}\frac{|x|}{n}=\infty\neq 0
\]</span> for each <span class="math inline">\(n.\)</span></p>
<p><strong>Example 2:</strong> <span class="math inline">\(\mathcal{X}=(0,1)\)</span><br> The function <span class="math display">\[
g_n(x) = x^n
\]</span> converges point-wise (for each given <span class="math inline">\(x\in(0,1)\)</span>) to <span class="math display">\[
g(x)=0,
\]</span> since <span class="math display">\[
|g_n(x)-g(x)|=x^n\to 0\quad\text{as}\quad n\to\infty
\]</span> for each given <span class="math inline">\(x\in(0,1).\)</span></p>
<p>However, <span class="math inline">\(g_n\)</span> does not converge uniformly to <span class="math inline">\(g\)</span> since <span class="math display">\[
\sup_{x\in(0,1)}|g_n(x)-g(x)|=\sup_{x\in(0,1)}x^n=1\neq 0
\]</span> for each <span class="math inline">\(n.\)</span></p>
</section>
<section id="solution-of-exercise-3." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-of-exercise-3.">Solution of Exercise 3.</h4>
<section id="a-part-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a-part-1">(a) Part 1:</h5>
<p>Setup:</p>
<ul>
<li>iid data <span class="math inline">\(X_1,\dots,X_n\)</span> with <span class="math inline">\(X_i\sim F\)</span></li>
<li><span class="math inline">\(\mathbb{E}(X_i)=\mu\)</span></li>
<li><span class="math inline">\(Var(X_i)=\sigma^2&lt;\infty\)</span></li>
<li>Estimator: <span class="math inline">\(\bar{X}_n=n^{-1}\sum_{i=1}^nX_i\)</span></li>
</ul>
<p>If <span class="math inline">\(F\)</span> is a normal distribution:</p>
<p><span class="math display">\[
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\sim \mathcal{N}(0,1)\quad\text{exactly for all}\;n.
\end{array}
\]</span></p>
<p>For non-normal distributions <span class="math inline">\(F\)</span> we have by the classic CLT: <span class="math display">\[
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
\]</span></p>
<p>Usually, we do not know <span class="math inline">\(\sigma\)</span> and have to estimate this parameter using a consistent estimator such as <span class="math inline">\(s^2=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2\)</span>, where <span class="math inline">\(s\to_p\sigma\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>Then by Slusky’s Theorem (allows to combine <span class="math inline">\(\to_d\)</span> and <span class="math inline">\(\to_p\)</span>-statements) we have that: <span class="math display">\[
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{s}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
\]</span></p>
<p>The <strong>classic confidence interval</strong> is then based on the above (asymptotic) normality result: <span class="math display">\[
\operatorname{CI}_{\operatorname{classic},n}=\left[\bar{X}_n\,-\,z_{1-\alpha/2}\frac{s}{\sqrt{n}},\bar{X}_n\,+\,z_{1-\alpha/2}\frac{s}{\sqrt{n}}\right],
\]</span> where <span class="math inline">\(z_{1-\alpha/2}\)</span> is the <span class="math inline">\((1-\alpha/2)\)</span>-quantile of the standard normal distribution. Alternatively, one can apply a “small-sample correction” by using the <span class="math inline">\((1-\alpha/2)\)</span>-quantile <span class="math inline">\(t_{n-1, 1-\alpha/2}\)</span> of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p>From the above arguments it follows that: <span class="math display">\[
P\left(\mu\in \operatorname{CI}_{\operatorname{classic},n}\right)\to 1-\alpha\quad\text{as}\quad n\to\infty.
\]</span></p>
<p>Let us consider the finite-<span class="math inline">\(n\)</span> (with <span class="math inline">\(n=20\)</span>) performance of the classic confidence interval for the case where <span class="math inline">\(F\)</span> is a <strong>normal distribution</strong> with mean <span class="math inline">\(\mu=1\)</span> and standard deviation <span class="math inline">\(\sigma=2\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="do">##  Setup:</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span> <span class="co"># Sample Size</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>mean  <span class="ot">&lt;-</span>    <span class="dv">1</span> <span class="co"># Mean</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>sdev  <span class="ot">&lt;-</span>    <span class="dv">2</span> <span class="co"># Standard Deviation</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>CI.lo.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>CI.up.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>  X.sample     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n=</span>n, <span class="at">mean =</span> mean, <span class="at">sd =</span> sdev) </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimates:</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  X.bar.MC     <span class="ot">&lt;-</span> <span class="fu">mean</span>(X.sample)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  sd.hat.MC    <span class="ot">&lt;-</span> <span class="fu">sd</span>(X.sample)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Classic CIs:</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>  CI.lo.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>  CI.up.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Possible alternative version: </span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.lo.vec[b] &lt;- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.up.vec[b] &lt;- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.lo.vec <span class="sc">&lt;=</span> mean  <span class="sc">&amp;</span>  mean <span class="sc">&lt;=</span> CI.up.vec</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.lo.vec, CI.up.vec), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Classic 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Normal DGP)"</span>)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-part-2-classic-confidence-interval-n20-and-x_isim-chi2_1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a-part-2-classic-confidence-interval-n20-and-x_isim-chi2_1">(a) Part 2: Classic Confidence Interval (<span class="math inline">\(n=20\)</span> and <span class="math inline">\(X_i\sim \chi^2_1\)</span>)</h5>
<p>Now, we consider the finite-<span class="math inline">\(n\)</span> performance of the classic confidence interval under the same setup as above, but for the case where <span class="math inline">\(F\)</span> is a <strong>non-normal distribution</strong>, namely, a <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>CI.lo.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>CI.up.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  X.sample     <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimates:</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>  X.bar.MC     <span class="ot">&lt;-</span> <span class="fu">mean</span>(X.sample)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>  sd.hat.MC    <span class="ot">&lt;-</span> <span class="fu">sd</span>(X.sample)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Classic CIs:</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>  CI.lo.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>  CI.up.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Possible alternative version: </span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.lo.vec[b] &lt;- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.up.vec[b] &lt;- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.lo.vec <span class="sc">&lt;=</span> mean  <span class="sc">&amp;</span>  mean <span class="sc">&lt;=</span> CI.up.vec</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.lo.vec, CI.up.vec), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Classic 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Non-Normal DGP)"</span>)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="b-basic-bootstrap-confidence-interval-n20-and-x_isim-chi2_1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="b-basic-bootstrap-confidence-interval-n20-and-x_isim-chi2_1">(b) Basic Bootstrap Confidence Interval (<span class="math inline">\(n=20\)</span> and <span class="math inline">\(X_i\sim \chi^2_1\)</span>)</h5>
<p>Let’s generate an iid random sample <span class="math inline">\(S_n\)</span> with <span class="math inline">\(X_i\sim\chi^2_1\)</span> and the corresponding estimate <span class="math inline">\(\bar X_n\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="do">## IID random sample:</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>S_n  <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Empirical mean:</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>(X.bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6737282</code></pre>
</div>
</div>
<p>The <strong>standard bootstrap confidence interval</strong> is given by (see lecture script): <span class="math display">\[
\left[2\bar{X}_n - \hat{q}^\ast_{n,1-\alpha/2}, 2\bar{X}_n - \hat{q}^\ast_{n,\alpha/2}\right],
\]</span> where <span class="math inline">\(\bar{X}_n\)</span> denotes the estimate computed from the original sample, and <span class="math inline">\(\hat{q}^\ast_{\alpha/2}\)</span> and <span class="math inline">\(\hat{q}^\ast_{1-\alpha/2}\)</span> denote the <span class="math inline">\((\alpha/2)\)</span> and <span class="math inline">\((1-\alpha/2)\)</span>-quantiles of the conditional distribution of <span class="math inline">\(\bar{X}_n^\ast\)</span> given <span class="math inline">\(\mathcal{S}_n=\left\{X_1,\dots,X_n\right\}.\)</span></p>
<p>In the following we first generate the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math display">\[
\bar{X}_{n,1}^\ast,\dots,\bar{X}_{n,m}^\ast,
\]</span> compute their quantiles <span class="math inline">\(\hat{q}^\ast_{n,\alpha/2}\)</span> and <span class="math inline">\(\hat{q}^\ast_{n,1-\alpha/2},\)</span> and plot all of this:</p>
<div class="cell" data-fig.margin="true">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Bootstr-Setup:</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>alpha            <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">1500</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate bootstap samples:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>Bootstr.Samples  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Boostrap draws of \bar{X}_n^*:</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>X.bar.bootstr.vec <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Quantile of the bootstr.-distribution of \bar{X}_n^*:</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>q<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>q<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="do">## plot</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(X.bar.bootstr.vec), <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Bootstr.-Distr. of "</span>,<span class="fu">bar</span>(X)[n]<span class="sc">^</span>{<span class="st">" *"</span>})))</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">c</span>(q<span class="fl">.1</span>,q<span class="fl">.2</span>),<span class="at">col=</span><span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="432"></p>
</figure>
</div>
</div>
</div>
<p>Using our preparatory work above, the basic bootstrap confidence interval can be computed as following:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Basic Bootstrap Confidence Interval:</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bootstr.lo <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar <span class="sc">-</span> q<span class="fl">.1</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bootstr.up <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar <span class="sc">-</span> q<span class="fl">.2</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Re-labeling of otherwise false names:</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Basic.Bootstr.lo, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"2.5%"</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Basic.Bootstr.up, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"97.5%"</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%     97.5% 
0.1545224 1.0425228 </code></pre>
</div>
</div>
<p>Now, we can investigate the finite-<span class="math inline">\(n\)</span> performance of the standard bootstrap confidence interval:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>mean  <span class="ot">&lt;-</span>   df</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">1500</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Setup:</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bstr.lo.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bstr.up.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>  S_n.MC        <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimate:</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>  X.bar.MC      <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n.MC)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>  <span class="do">## </span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples.MC  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    Bootstr.Samples.MC[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n.MC, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>  X.bar.bootstr.MC.vec <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples.MC, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>  <span class="do">## (1-alpha/2)-quantile:</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>  q.<span class="fl">1.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.MC.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>  q.<span class="fl">2.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.MC.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Basic Bootstrap CIs:</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>  CI.Basic.Bstr.lo.vec[b] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar.MC <span class="sc">-</span> q.<span class="fl">1.</span>MC</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>  CI.Basic.Bstr.up.vec[b] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar.MC <span class="sc">-</span> q.<span class="fl">2.</span>MC</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.Basic.Bstr.lo.vec<span class="sc">&lt;=</span>mean <span class="sc">&amp;</span> mean<span class="sc">&lt;=</span>CI.Basic.Bstr.up.vec</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), </span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Basic Bootrap 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Non-Normal DGP)"</span>)</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Basic.Bstr.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Basic.Bstr.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Basic.Bstr.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Basic.Bstr.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="c-bootstrap-t-confidence-interval-n20-and-x_isim-chi2_1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="c-bootstrap-t-confidence-interval-n20-and-x_isim-chi2_1">(c) Bootstrap-<span class="math inline">\(t\)</span> Confidence Interval (<span class="math inline">\(n=20\)</span> and <span class="math inline">\(X_i\sim \chi^2_1\)</span>)</h5>
<p>The bootstrap-<span class="math inline">\(t\)</span> confidence interval is given by (see lecture script): <span class="math display">\[
\left[
  \bar{X}_n-\hat{q}^\ast_{n,1-\alpha/2}\left(\frac{s_n}{\sqrt{n}}\right),  
  \bar{X}_n-\hat{q}^\ast_{n,\alpha/2}  \left(\frac{s_n}{\sqrt{n}}\right)
\right],
\]</span> where <span class="math inline">\(\bar{X}_n\)</span> and <span class="math inline">\(s_n=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2\)</span> are computed from the original sample, and where <span class="math inline">\(\hat{q}^\ast_{n,\alpha/2}\)</span> and <span class="math inline">\(\hat{q}^\ast_{n,1-\alpha/2}\)</span> denote the empirical <span class="math inline">\((\alpha/2)\)</span> and the <span class="math inline">\((1-\alpha/2)\)</span>-quantiles compute from the bootstrap estimates: <span class="math display">\[
\sqrt{n}\frac{\bar{X}_{n,j}^\ast-\bar{X}_n}{s_{n,j}^\ast}\quad j=1,\dots,m.
\]</span></p>
<p>In the following we first generate the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math display">\[
\sqrt{n}\frac{\bar{X}_{n,j}^\ast-\bar{X}_n}{s_{n,j}^\ast}\quad j=1,\dots,m,
\]</span> compute their quantiles <span class="math inline">\(\hat{q}^\ast_{n,\alpha/2}\)</span> and <span class="math inline">\(\hat{q}^\ast_{n,1-\alpha/2}\)</span>, and plot all of this:</p>
<div class="cell" data-fig.margin="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="do">## IID random sample:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>S_n  <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Empirical mean and sd:</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>X.bar   <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>sd.hat  <span class="ot">&lt;-</span> <span class="fu">sd</span>(S_n)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Bootstr-Setup:</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>alpha            <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">1500</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate bootstap samples:</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>Bootstr.Samples  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Compute boostrap draws of (\bar{X}_n^*-\bar{X}_n)/\hat{\sigma}^\ast:</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>X.bar.bootstr.vec    <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>sd.bootstr.vec       <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>Bootstr.t.sample.vec <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(X.bar.bootstr.vec <span class="sc">-</span> X.bar)<span class="sc">/</span>sd.bootstr.vec</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="do">## Quantile of the bootstr.-distribution of \bar{X}_n^*:</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>q<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.sample.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>q<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.sample.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="do">## plot</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(Bootstr.t.sample.vec), <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Bootstr.-t-Distr. of "</span>,</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>          <span class="fu">sqrt</span>(n)(<span class="fu">bar</span>(X)[n]<span class="sc">^</span>{<span class="st">" *"</span>}<span class="sc">-</span><span class="fu">bar</span>(X)[n])<span class="sc">/</span>s[n]<span class="sc">^</span>{<span class="st">"*"</span>})))</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">c</span>(q<span class="fl">.1</span>,q<span class="fl">.2</span>),<span class="at">col=</span><span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="432"></p>
</figure>
</div>
</div>
</div>
<p>Using our preparatory work above, the bootstrap-<span class="math inline">\(t\)</span> confidence interval can be computed as following:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Basic Bootstrap Confidence Interval:</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.lo <span class="ot">&lt;-</span> X.bar <span class="sc">-</span> q<span class="fl">.1</span> <span class="sc">*</span> sd.hat<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.up <span class="ot">&lt;-</span> X.bar <span class="sc">-</span> q<span class="fl">.2</span> <span class="sc">*</span> sd.hat<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Re-labeling of otherwise false names:</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Bstr.t.lo, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"2.5%"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Bstr.t.up, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"97.5%"</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(CI.Bstr.t.lo, CI.Bstr.t.up)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%     97.5% 
0.3052027 2.0241321 </code></pre>
</div>
</div>
<p>Let us investigate the finite-<span class="math inline">\(n\)</span> performance of the bootstrap-t confidence interval:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>mean  <span class="ot">&lt;-</span>   df</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">1500</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Setup:</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.lo.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.up.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>  S_n.MC        <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimates:</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>  X.bar.MC      <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n.MC)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>  sd.MC         <span class="ot">&lt;-</span> <span class="fu">sd</span>(S_n.MC)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>  <span class="do">## </span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples.MC  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    Bootstr.Samples.MC[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n.MC, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>  X.bar.bootstr.MC.vec <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples.MC, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>  sd.bootstr.MC.vec    <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples.MC, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Make it a "Bootstrap-t" sample:</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>  Bootstr.t.MC.vec     <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(n)<span class="sc">*</span>(X.bar.bootstr.MC.vec <span class="sc">-</span> X.bar.MC)<span class="sc">/</span>sd.bootstr.MC.vec</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>  <span class="do">## (1-alpha/2)-quantile:</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>  q.<span class="fl">1.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.MC.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>  q.<span class="fl">2.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.MC.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Basic Bootstrap CIs:</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>  CI.Bstr.t.lo.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> q.<span class="fl">1.</span>MC <span class="sc">*</span> sd.MC<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>  CI.Bstr.t.up.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> q.<span class="fl">2.</span>MC <span class="sc">*</span> sd.MC<span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.Bstr.t.lo.vec<span class="sc">&lt;=</span>mean <span class="sc">&amp;</span> mean<span class="sc">&lt;=</span>CI.Bstr.t.up.vec</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), </span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Bootrap-t 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Non-Normal DGP)"</span>)</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Bstr.t.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Bstr.t.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Bstr.t.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Bstr.t.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="solution-of-exercise-4." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solution-of-exercise-4.">Solution of Exercise 4.</h4>
<section id="a-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a-1">(a)</h5>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}^*(\bar{Y}^*)
&amp; = \mathbb{E}\left(\left.\bar{Y}^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \mathbb{E}\left(\left.\frac{1}{n}\sum_{i=1}^n Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(\left.Y^*_i\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \mathbb{E}\left(\left.Y^*\right|\mathcal{S}_n\right)
\end{align*}
\]</span> where we used that <span class="math display">\[
\left(\left.Y^*_1\right|\mathcal{S}_n\right),\dots,\left(\left.Y^*_n\right|\mathcal{S}_n\right)\overset{\text{i.i.d.}}{\sim}\left(\left.Y^*\right|\mathcal{S}_n\right)
\]</span> with <span class="math display">\[
(Y^*|\mathcal{S}_n)\in\{Y_1,\dots,Y_n\}
\]</span> and <span class="math display">\[\begin{align*}
P(Y^*=Y_1|\mathcal{S}_n)&amp;=\frac{1}{n}\\[2ex]
P(Y^*=Y_2|\mathcal{S}_n)&amp;=\frac{1}{n}\\[2ex]
\qquad \vdots &amp; \\[2ex]
P(Y^*=Y_n|\mathcal{S}_n)&amp;=\frac{1}{n}.
\end{align*}\]</span> Thus, we can proceed by computing the mean of a discrete random variable such that <span class="math display">\[\begin{align*}
\mathbb{E}^*(\bar{Y}^*)
&amp; = \mathbb{E}\left(\left.Y^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \sum_{i=1}^n Y_i P(Y^*=Y_1|\mathcal{S}_n)\\[2ex]
&amp; = \sum_{i=1}^n Y_i \frac{1}{n} \\[2ex]
&amp; = \bar{Y}.
\end{align*}\]</span></p>
</section>
<section id="b-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="b-1">(b)</h5>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(\bar{Y}^*)
&amp; = \mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n Y_i^*\right)\\[2ex]
&amp; = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(Y_i^*\right)\\[2ex]
&amp; = \mathbb{E}\left(Y^*\right),
\end{align*}
\]</span> where we used that <span class="math display">\[
Y_1^*,\dots,Y_n^* \overset{\text{i.i.d.}}{\sim}Y^*.
\]</span> Since we do not condition on the observed sample <span class="math inline">\(\mathcal{S}_n,\)</span> we have that <span class="math display">\[
Y^* \sim F,
\]</span> because the unconditional <span class="math inline">\(Y^*\)</span> has the <strong>same distribution as</strong> <span class="math inline">\(Y\sim F.\)</span> Thus, we can proceed by computing the mean of <span class="math inline">\(Y^*\sim F\)</span> such that <span class="math display">\[
\begin{align*}
\mathbb{E}(\bar{Y}^*)
&amp; = \mathbb{E}\left(Y^*\right)\\[2ex]
&amp; = \mathbb{E}\left(Y  \right)\\[2ex]
&amp; = \mu.
\end{align*}
\]</span></p>
</section>
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Billingsley_1995" class="csl-entry" role="listitem">
Billingsley, Patrick. 1995. <em>Probability and Measure</em>. 3rd ed. Wiley.
</div>
<div id="ref-Davidson_and_Flachaire_2008" class="csl-entry" role="listitem">
Davidson, Russell, and Emmanuel Flachaire. 2008. <span>“The Wild Bootstrap, Tamed at Last.”</span> <em>Journal of Econometrics</em> 146 (1): 162–69.
</div>
<div id="ref-Davison_Hinkley_2013" class="csl-entry" role="listitem">
Davison, Anthony Christopher, and David Victor Hinkley. 2013. <em>Bootstrap Methods and Their Application</em>. Cambridge University Press.
</div>
<div id="ref-Efron_Tibshirani_1994" class="csl-entry" role="listitem">
Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.
</div>
<div id="ref-Hall_1992" class="csl-entry" role="listitem">
Hall, Peter. 1992. <em>The Bootstrap and Edgeworth Expansion</em>. Springer Science.
</div>
<div id="ref-Horowitz_2001" class="csl-entry" role="listitem">
Horowitz, Joel L. 2001. <span>“The Bootstrap.”</span> In <em>Handbook of Econometrics</em>, 5:3159–3228.
</div>
<div id="ref-koike2024high" class="csl-entry" role="listitem">
Koike, Yuta. 2024. <span>“High-Dimensional Bootstrap and Asymptotic Expansion.”</span> <em>arXiv Preprint arXiv:2404.05006</em>.
</div>
<div id="ref-Mammen_1992_Book" class="csl-entry" role="listitem">
Mammen, Enno. 1992. <span>“When Does Bootstrap Work: Asymptotic Results and Simulations.”</span> <em>Lecture Notes in Statistics</em> 77.
</div>
<div id="ref-Mammen_1993" class="csl-entry" role="listitem">
———. 1993. <span>“Bootstrap and Wild Bootstrap for High Dimensional Linear Models.”</span> <em>The Annals of Statistics</em> 21 (1): 255–85.
</div>
<div id="ref-Shao_Tu_1996" class="csl-entry" role="listitem">
Shao, Jun, and Dongsheng Tu. 1996. <em>The Jackknife and Bootstrap</em>. Springer Science.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Ch2_EMAlgorithmus.html" class="pagination-link" aria-label="EM Algorithm &amp; Cluster Analysis">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">EM Algorithm &amp; Cluster Analysis</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ch4_NP_Density_Estimation.html" class="pagination-link" aria-label="Nonparametric Density Estimation">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Nonparametric Density Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>