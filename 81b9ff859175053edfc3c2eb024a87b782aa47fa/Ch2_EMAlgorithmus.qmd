<!-- LTeX: language=en-US -->
# EM Algorithm & Cluster Analysis

The Expectation Maximization (EM) algorithm is often used to simplify or to facilitate complex maximum likelihood estimation problems. In this chapter, we present the EM algorithm for estimating **Gaussian mixture distributions**, as this is probably its most well-known application. Even the original work on the EM algorithm [@Dempster_1977] already dealt with the estimation of Gaussian mixture distributions.


#### Possible Applications of the EM-Algorithm and Gaussian mixture distributions {-}

* Finding grouping structures (two or more) in data (**cluster analysis**). For instance: Automatic video editing (e.g., separation of back- and foreground)  
* @Traiberman_AER2019 uses the EM-Algorithm to compute the maximum likelihood estimators. Since he has no information about the beliefs over aggregate shocks (missing data), the maximum likelihood function is very complex. The EM-Algorithm can help here to simplify the likelihood function. 


#### Some literature {-}

* Chapter 9 of [**Pattern Recognition and Machine Learning**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) [@Bishop_2006]. Free PDF version: [**PDF-Version**](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

* Chapter 8.5 of [**Elements of Statistical Learning: Data Mining, Inference and Prediction**](https://web.stanford.edu/~hastie/ElemStatLearn/) [@Hastie_2009]. Free PDF version: [**PDF-Version**](https://web.stanford.edu/~hastie/ElemStatLearn/)


#### `R`-Packages for this Chapter {-}

The following `R`-packages are used in this chapter:
```{r, eval=FALSE, echo=TRUE}
pkgs <- c("tidyverse",      # Tidyverse packages
          "palmerpenguins", # Penguins data
          "scales",         # Transparent colors: alpha()
          "RColorBrewer",   # Nice colors
          "mclust",         # Gaussian mixture models for clustering
          "MASS")           # Used to generate multivariate random normal variables
install.packages(pkgs)
```

For now, we need to load the following `R`-packages:
```{r, eval=FALSE, echo=TRUE}
library("tidyverse")
library("palmerpenguins") # Penguin data 
library("RColorBrewer")   # nice colors
library("scales")         # transparent colors: alpha()
```


```{r, eval=TRUE, echo=FALSE}
suppressPackageStartupMessages(library("tidyverse", quietly = TRUE))
suppressPackageStartupMessages(library("palmerpenguins")) # Penguin data 
suppressPackageStartupMessages(library("RColorBrewer"))   # nice colors
suppressPackageStartupMessages(library("scales"))         # transparent colors: alpha()
```



## Motivation: Cluster Analysis using Gaussian Mixture Models

As a data example we use the [`palmerpenguins`](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) data (@palmerpenguins). 


These data are from surveys of penguin populations on the Palmer Archipelago (Antarctic Peninsula). Penguins are often difficult to distinguish from one another (@fig-cheekypenguin).  We will try to find groupings in the penguin data (fin length) using a Gaussian mixture distribution. To be able to estimate such mixing distributions, we introduce the EM algorithm.


![Cheeky penguin in action.](images/penguins.gif){width=80%  #fig-cheekypenguin}


The following code chunk prepares the dataset and visualizes it using a histogram.  


```{r, fig.align='center', out.width="100%", echo=TRUE, eval=TRUE}
## Select a color palette
col_v <- RColorBrewer::brewer.pal(n = 3, name = "Set2")

## Preparing the data:
penguins <- palmerpenguins::penguins %>%  # penguin data
  tidyr::as_tibble() %>%                  # 'tibble'-dataframe
  dplyr::filter(species!="Adelie") %>%    # remove penguin species 'Adelie' 
  droplevels() %>%                        # remove the non-used factor level
  tidyr::drop_na() %>%                    # remove NAs
  dplyr::mutate(species = species,        # rename variables 
                flipper = flipper_length_mm) %>% 
  dplyr::select(species, flipper)         # select variables 

##  
n      <- nrow(penguins)                  # sample size (n=187)

## Pulling out the variable 'penguin_species':
penguin_species <- dplyr::pull(penguins, species)

## Pulling out the variable 'penguin_flipper':
penguin_flipper <- dplyr::pull(penguins, flipper)

## Plot
## Histogramm:
hist(x = penguin_flipper, freq = FALSE, 
     xlab="Flipper-Length (mm)", main="Penguins\n(Two Groups)",
     col=gray(.65,.5), border=gray(.35,.5), ylim=c(0.0003, 0.039))
## Stipchart hinzufÃ¼gen:
stripchart(x = penguin_flipper, method = "jitter", 
           jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[3],.5), 
           bg=alpha(col_v[3],.5), cex=1.3, add = TRUE)
```



::: {.callout-caution}

# Caution: We do not use the grouping label (penguin species)

We have the information about the different penguin species (`penguin_species`) but in the following we pretend not to know this information.  

We want to determine the group memberships (species) by cluster analysis on the basis of the fin lengths (`penguin_flipper`) alone. 

Afterward we can use the grouping label in `penguin_species` to check how good our cluster analysis is.
:::



#### Clustering using Gaussian Mixture Distributions {-}

At the end of this chapter (@sec-CA), we'll be able to 

1. Estimate the Gaussian mixture distribution using the **EM algorithm** (see @fig-GMM-plot1).
2. Assign each penguin $i$ to one of the two species groups according to its predictor value $X_i$ (flipper length).


```{r}
#| echo: false
#| eval: true
#| label: fig-GMM-plot1
#| fig-cap: Cluster analysis based on a mixture distribution with two weighted normal distributions.

## mclust R package:
## Cluster analysis using Gaussian mixture distributions
suppressMessages(library("mclust"))

## Number of Groups
G <- 2 

## SchÃ¤tzung des GauÃŸschen Mischmodells (per EM Algorithmus)
## und Clusteranalyse
mclust_obj <- mclust::Mclust(data       = penguin_flipper, 
                             G          = G, 
                             modelNames = "V", 
                             verbose    = FALSE)

# summary(mclust_obj)
# str(mclust_obj)

## estimated group assignment 
class <- mclust_obj$classification

## Fraction of correct group assignments:
# cbind(class, penguin_species)
# round(sum(class == as.numeric(penguin_species))/n, 2)

## estimated means of the two Gaussian distributions
mean_m <- t(mclust_obj$parameters$mean)

## estimated variances (and possibly covariances) 
cov_l  <- list("Cov1" = mclust_obj$parameters$variance$sigmasq[1], 
               "Cov2" = mclust_obj$parameters$variance$sigmasq[2])

## estimated mixture weights (prior-probabilities) 
prop_v <- mclust_obj$parameters$pro

## evaluating the Gaussian mixture density function 
np      <- 100 # number of evaluation points
xxd     <- seq(min(penguin_flipper)-3, 
               max(penguin_flipper)+5, 
               length.out = np)
## mixture density
yyd     <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +
           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]
## single densities (scaled by their weights)
yyd1    <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]
yyd2    <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]

## Plot
hist(x = penguin_flipper, xlab="Flipper length (mm)", main="Penguins\n(Two Groups)",
     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))
lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))
lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)
lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)
abline(v=203.1, lty=3)
stripchart(penguin_flipper[class==1], 
           method = "jitter", jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)
stripchart(penguin_flipper[class==2], 
           method = "jitter", jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)
```




Figure @fig-GMM-plot1 shows the result of a cluster analysis based on a mixture distribution of two weighted normal distributions. **Cluster result (training data):** `r round(sum(class == as.numeric(penguin_species))/n, 2)*100`% of the penguins could be correctly assigned - based only on their flipper lengths.


The following `R` codes can be used to reproduce the above cluster analysis (using the `R` function `Mclust` of the package `mclust`) and @fig-GMM-plot1:

```{r}
#| echo: true
#| eval: false

## mclust R package:
## Cluster analysis using Gaussian mixture distributions
suppressMessages(library("mclust"))

## Number of Groups
G <- 2 

## SchÃ¤tzung des GauÃŸschen Mischmodells (per EM Algorithmus)
## und Clusteranalyse
mclust_obj <- mclust::Mclust(data       = penguin_flipper, 
                             G          = G, 
                             modelNames = "V", 
                             verbose    = FALSE)

# summary(mclust_obj)
# str(mclust_obj)

## estimated group assignment 
class <- mclust_obj$classification

## Fraction of correct group assignments:
# cbind(class, penguin_species)
round(sum(class == as.numeric(penguin_species))/n, 2)

## estimated means of the two Gaussian distributions
mean_m <- t(mclust_obj$parameters$mean)

## estimated variances (and possibly covariances) 
cov_l  <- list("Cov1" = mclust_obj$parameters$variance$sigmasq[1], 
               "Cov2" = mclust_obj$parameters$variance$sigmasq[2])

## estimated mixture weights (prior-probabilities) 
prop_v <- mclust_obj$parameters$pro

## evaluating the Gaussian mixture density function 
np      <- 100 # number of evaluation points
xxd     <- seq(min(penguin_flipper)-3, 
               max(penguin_flipper)+5, 
               length.out = np)
## mixture density
yyd     <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1] +
           dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]
## single densities
yyd1    <- dnorm(xxd, mean_m[1], sqrt(cov_l[[1]]))*prop_v[1]
yyd2    <- dnorm(xxd, mean_m[2], sqrt(cov_l[[2]]))*prop_v[2]

## Plot
hist(x = penguin_flipper, xlab="Flipper length (mm)", main="Penguins\n(Two Groups)",
     col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))
lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))
lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)
lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)
abline(v=203.1, lty=3)
stripchart(penguin_flipper[class==1], 
           method = "jitter", jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)
stripchart(penguin_flipper[class==2], 
           method = "jitter", jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)
```

But coding without understanding is dangerous. We'll learn the statistical method implemented in the `R` function `Mclust` it in this chapter. 


## The EM Algorithm for Maximum Likelihood Estimation of Gaussian Mixture Distributions

### Gaussian Mixture Models (GMM)


We denote a random variable $X$ that follows a Gaussian mixture distribution as
$$
X\sim\mathcal{N}_{\mathcal{mix}}(G,\boldsymbol{\pi}_0,\boldsymbol{\mu}_0,\boldsymbol{\sigma}_0)
$$

The corresponding density function of a Gaussian mixture distribution is defined as follows:
$$
f_{GMM}(x;\boldsymbol{\pi}_0,\boldsymbol{\mu}_0,\boldsymbol{\sigma}_0)=\sum_{g=1}^G\pi_{g,0}\varphi(x;\mu_{g,0},\sigma_{g,0}) 
$${#eq-GMMdens}

* **Weights:** $\boldsymbol{\pi}_0=(\pi_{1,0},\dots,\pi_{G,0})$ with $\pi_{g,0}>0$ and $\sum_{g=1}^G\pi_{g,0}=1$
* **Means:** $\boldsymbol{\mu}_0=(\mu_{1,0},\dots,\mu_{G,0})$ with $\mu_{g,0}\in\mathbb{R}$
* **Standard deviations:** $\boldsymbol{\sigma}_0=(\sigma_{1,0},\dots,\sigma_{G,0})$ with $\sigma_{g,0}>0$ 
* **Normal density of group $g=1,\dots,G$:** 
$$
\varphi(x;\mu_{g,0},\sigma_{g,0})=\frac{1}{\sqrt{2\pi}\sigma_{g,0}}\exp\left(-\frac{1}{2}\left(\frac{x-\mu_{g,0}}{\sigma_{g,0}}\right)^2\right)
$$
* **Unknown parameters:** <span style="color:#FF5733">$\boldsymbol{\pi}_0$</span>, <span style="color:#FF5733">$\boldsymbol{\mu}_0$</span> und <span style="color:#FF5733">$\boldsymbol{\sigma}_0$</span>



### Maximum Likelihood (ML) Estimation 

We could try estimating the unknown parameters 

* $\boldsymbol{\pi}_0=(\pi_{1,0},\dots,\pi_{G,0})$, 
* $\boldsymbol{\mu}_0=(\mu_1,\dots,\mu_G)$ and 
* $\boldsymbol{\sigma}_0=(\sigma_1,\dots,\sigma_G)$ 

using the maximum likelihood method. 

> I'll say it right away: This attempt generally fails.

#### Basic Idea of ML Estimation {-}

* **Assumption:** The data $\mathbf{x}=(x_1,\dots,x_n)$ is a realization of a random sample 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X
$$
with
$$ 
X\sim\mathcal{N}_{\mathcal{mix}}(G,\boldsymbol{\pi}_0,\boldsymbol{\mu}_0,\boldsymbol{\sigma}_0).
$$ 


<!-- 
> That is, in a certain sense, the observed data $\mathbf{x}=(x_1,\dots,x_n)$ "know" the unknown parameters $\boldsymbol{\pi},$ $\boldsymbol{\mu}$ und $\boldsymbol{\sigma}$ and we "only" have to elicit this information from them. 
-->


<!-- * **Estimation Idea:** 
Choose $\boldsymbol{\pi}$, $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ such that $f_{GMM}(\cdot;\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})$ **"optimally"** fits the observed data $\mathbf{x}$. 
-->

* **Estimate** the unknown parameters $\boldsymbol{\pi}_0$, $\boldsymbol{\mu}_0$ and $\boldsymbol{\sigma}_0$ by maximizing the log-likelihood function 
$$
\begin{align*}
\ell(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})
=&\sum_{i=1}^n\ln\left(f_{GMM}(X_i;\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})\right)\\
=&\sum_{i=1}^n\ln\left(\sum_{g=1}^G\pi_g\varphi(X_i;\mu_g,\sigma_g)\right)
\end{align*}
$${#eq-logLikGMM}
taking into account the following **maximization constraints:**

::: {.callout-note}
# Maximization constraints
The maximization must take into account the parameter constraints in @eq-GMMdens; namely, 

* $\sigma_g>0$ and 
* $\pi_g>0$ for all $g=1,\dots,G$ such that 
* $\sum_{g=1}^G\pi_g=1$.
::: 

The maximizing parameter values 
<span style="color:#FF5733">$\hat{\boldsymbol{\pi}}$</span>, 
<span style="color:#FF5733">$\hat{\boldsymbol{\mu}}$</span> and 
<span style="color:#FF5733">$\hat{\boldsymbol{\sigma}}$</span> 
are the <span style="color:#FF5733">**ML-Estimators**</span>:

$$
(\hat{\boldsymbol{\pi}},\hat{\boldsymbol{\mu}},\hat{\boldsymbol{\sigma}})=\arg\max_{\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma}}\ell(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})
$$



ðŸ˜’ **Problems with singularities in numerical solutions:** If one tries to solve the above maximization problem [numerically with the help of the computer](https://jaimemosg.github.io/EstimationTools/index.html), one will quickly notice that the results are highly unstable, implausible and not very trustworthy. The reason for these unstable estimates are problems with singularities. 

For real GMMs (i.e. GMMs with more than one group $G>1$), problems with singularities occur very easily during a numerical maximization. This happens, for instance, when one of the normal distribution components tries to describe only one single data point $X_i.$ This leads to a Gaussian density function centered around the **single data point** $X_i$ such that   
$$
\varphi(X_i;{\color{red}\mu_g=X_i},\sigma_g),
$$ 
where 
$$
\sigma_g\to 0.
$$
This degenerating situation leads to **very large** density function values, 
$$
\varphi(X_i;\mu_g=X_i,\sigma_g)\to\infty\quad\text{for}\quad \sigma_g\to 0,
$$ 
and thus "maximizes" the log-likelihood in an *undesirable* way (see @fig-dirac1).
```{r, include=knitr::is_html_output(), animation.hook="gifski", interval=0.1, fig.align="center"}
#| label: fig-dirac1
#| echo: false
#| fig-cap: Gaussian density with $\mu_g=X_i$ for $\sigma_g\to 0$.
np   <- 1000
rep  <- 30
xxd  <- seq(-3, 3, length.out = np)
sd_v <- seq(.5,0.05, len=rep)
par(mar = c(3.1,4.1,4.1,2.1))
for(i in 1:length(sd_v)){
  plot(x=xxd, y=dnorm(xxd, 0, sd_v[i]), type="l", ylim=c(0,max(dnorm(xxd, 0, sd_v[rep-3]))), xlim = c(-1.5,1.5),
       ylab="",xlab="", lwd=2, col="darkblue", axes = FALSE)
  axis(2); axis(1, at=0, labels = expression(X["i"])); box()
}
```
Such **undesirable, trivial maximization solutions** typically lead to implausible, non-useful estimation results.

ðŸ¤“ **Analytic solution:** It is a bit tedious, but one can maximize the log-likelihood function of the GMM (see @eq-logLikGMM) analytically. If you do this, you will get the following expressions:
$$
\begin{align*}
\hat\pi_g&=\frac{1}{n}\sum_{i=1}^np_{ig},\quad
\hat\mu_g=\sum_{i=1}^n\frac{p_{ig}}{\left(\sum_{j=1}^np_{jg}\right)}X_i\\[2ex]
\hat\sigma_g&=\sqrt{\sum_{i=1}^n\frac{p_{ig}}{\left(\sum_{j=1}^np_{jg}\right)}\left(X_i-\hat\mu_g\right)^2},
\end{align*}
$${#eq-AnalyticSol1}
where 
$$
p_{ig}=\frac{\pi_{g,0}\varphi(X_i;\mu_{g,0},\sigma_{g,0})}{f_{GMM}(X_i;\boldsymbol{\pi}_0,\boldsymbol{\mu}_0,\boldsymbol{\sigma}_0)}
$${#eq-AnalyticSol2}
for $i=1,\dots,n$ and $g=1,\dots,G$. 


::: {.callout-note}
Deriving the expressions for $\hat{\mu}_g$, $\hat{\sigma}_g$ and $\hat{\pi}_g$ in @eq-AnalyticSol1 requires multiple applications of the chain rule, product rule, etc., as well as an application of the Lagrange multiplier method for optimization under side-constraints to take into account the maximization constraints.  
<!-- In one of the exercises, you may derive the expression for $\hat\mu_g$. -->
:::


ðŸ™ˆ <span style="color:#FF5733">**However:**</span> The above expressions for $\hat\pi_g$, $\hat\mu_g$ and $\hat\sigma_g$ depend themselves on the <span style="color: #FF5733">**unknown**</span> parameters 

* $\boldsymbol{\pi}_0=(\pi_{1,0},\dots,\pi_{G,0})$, 
* $\boldsymbol{\mu}_0=(\mu_{1,0},\dots,\mu_{G,0})$ and 
* $\boldsymbol{\sigma}_0=(\sigma_{1,0},\dots,\sigma_{G,0})$, 

because the probabilities $0\leq p_{ig}\leq 1$ (defined in @eq-AnalyticSol2) depend on these unknown parameters. 

Thus, the expressions for $\hat\pi_g$, $\hat\mu_g$, and $\hat\sigma_g$ in @eq-AnalyticSol1 do not allow a direct estimation of the unknown parameters $\pi_{g,0}$, $\mu_{g,0}$, and $\sigma_{g,0}$.

::: {.callout-note}
# Prior and Posterior Probabilities

* **Prior Probability:** The probability 
$$
\pi_g = P\left(\text{Penguine $i$ belongs to group}\;g\right)
$$  
in @eq-AnalyticSol2 is called the **prior probability**. The prior probability $\pi_g$ is the probability that a penguine $i$, from which we know nothing about its flipper length, belongs to group $g$.
* **Posterior Probability:** The conditional probability. 
$$
p_{ig} = P\left(\text{Penguine $i$ belongs to group}\;g|X_i\right)
$$ 
in @eq-AnalyticSol2 is called the **posterior probability**. The posterior probability $p_{ig}$ is the probability that penguin $i$ with flipper length $X_{i}$ belongs to group $g.$ 

We'll discuss the prior and the posterior probability in more detail in @sec-PriorPosterior.
:::



ðŸ¥³ <span style="color:#138D75">**Solution: The EM Algorithm**</span>  


### The EM Algorithm for GMMs {#sec-EM1}


The expressions for $\hat\pi_g$, $\hat\mu_g$, and $\hat\sigma_g$ in @eq-AnalyticSol1 suggest a simple iterative maximum likelihood estimation procedure; namely, an **alternating estimation** of 

* the posterior probabilities
$$
\begin{align*}
p_{ig} 
&= P\left(\text{Penguine $i$ belongs to group}\;g|X_i\right)\\[2ex]
& = \frac{\pi_{g,0}\varphi(X_i;\mu_{g,0},\sigma_{g,0})}{f_{GMM}(X_i;\boldsymbol{\pi}_0,\boldsymbol{\mu}_0,\boldsymbol{\sigma}_0)}
\end{align*}
$$ 
for $i=1,\dots,n,$ and $g=1,\dots,G,$ using estimates 
$\hat{\boldsymbol{\pi}}$, 
$\hat{\boldsymbol{\mu}}$, and 
$\hat{\boldsymbol{\sigma}}$ of the unknown 
$\boldsymbol{\pi}_0$, $\boldsymbol{\mu}_0$, and $\boldsymbol{\sigma}_0.$

and of 

* the model parameters 
$$
(\pi_{g,0},\mu_{g,0},\sigma_{g,0}) \quad\text{for}\quad g=1,\dots,G
$$
using $\hat{\pi}_g$, $\hat{\mu}_g$, $\hat{\sigma}_g$ defined in @eq-AnalyticSol1 using estimates $\hat{p}_{ig}$ for the unknown $p_{ig}.$

::: {.callout-note appearance="minimal"}
Given an estimate $\hat{p}_{ig},$ you can compute $(\hat\pi_g, \hat\mu_g,\hat\sigma_g)$ using @eq-AnalyticSol1. 

Given the estimates $(\hat\pi_g, \hat\mu_g,\hat\sigma_g),$ you can compute $\hat{p}_{ig}$ using @eq-AnalyticSol2. 
:::

**The EM Algorithm:**

1. **Initialization:** <br> 
Set starting values 
$$
\begin{align*}
&\boldsymbol{\hat{\pi}}^{(0)}=(\hat{\pi}_1^{(0)},\dots,\hat{\pi}_G^{(0)})\\[2ex]
&\boldsymbol{\hat{\mu}}^{(0)}=(\hat{\mu}_1^{(0)},\dots,\hat{\mu}_G^{(0)})\\[2ex]
&\boldsymbol{\hat{\sigma}}^{(0)}=(\hat{\sigma}_1^{(0)},\dots,\hat{\sigma}_G^{(0)})
\end{align*}
$$

2. **Loop:** <br> 
For $r=1,2,\dots$

    - <span style="color:#FF5733">**(Expectation)**</span> Compute: 
     $$\hat{p}_{ig}^{(r-1)}=\frac{\hat{\pi}_g^{(r-1)}\varphi(X_i;\hat{\mu}^{(r-1)}_g,\hat{\sigma}_g^{(r-1)})}{f_{GMM}(X_i;\boldsymbol{\hat{\pi}}^{(r-1)},\boldsymbol{\hat{\mu}}^{(r-1)},\boldsymbol{\hat{\sigma}}^{(r-1)})}$$
    
    - <span style="color:#2471A3">**(Maximization)**</span>  Compute: 
    <center>$\hat\pi_g^{(r)}=\frac{1}{n}\sum_{i=1}^n\hat{p}_{ig}^{(r-1)},\quad\quad\hat\mu_g^{(r)}=\sum_{i=1}^n\frac{\hat{p}_{ig}^{(r-1)}}{\left(\sum_{j=1}^n\hat{p}_{jg}^{(r-1)}\right)}X_i$</center> 
    <center>$\hat\sigma_g^{(r)}=\sqrt{\sum_{i=1}^n\frac{\hat{p}_{ig}^{(r-1)}}{\left(\sum_{j=1}^n\hat{p}_{jg}^{(r-1)}\right)}\left(X_i-\hat\mu_g^{(r)}\right)^2}$</center> 

3. **Check Convergence:** <br>
    Stop if the value of the maximized log-likelihood function, $\ell(\boldsymbol{\hat{\pi}}^{(r)},\boldsymbol{\hat{\mu}}^{(r)},\boldsymbol{\hat{\sigma}}^{(r)};\mathbf{x})$, does not change any more substantially. Or, equivalently, if the parameter estimates do not change any more substantially. 

The above pseudocode is implemented in the following code chunk:

```{r}
#| echo: true 
#| eval: true
library("MASS")
library("mclust")

## data:
x <- cbind(penguin_flipper) # data [n x d]-dimensional. 
d <- ncol(x)                # dimension (d=1: univariat)
n <- nrow(x)                # sample size
G <- 2                      # number of groups

## further stuff 
llk       <- matrix(NA, n, G)
p         <- matrix(NA, n, G)  
loglikOld <- 1e07
tol       <- 1e-05
it        <- 0
check     <- TRUE 

## EM Algorithm

## 1. Starting values for pi, mu and sigma:
pi    <- rep(1/G, G)              # naive pi 
sigma <- array(diag(d), c(d,d,G)) # varianz = 1
mu    <- t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )

while(check){
  
  ## 2.a Expectation step
  for(g in 1:G){
    p[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])
  }
  p <- sweep(p, 1, STATS = rowSums(p), FUN = "/")
  
  ## 2.b Maximization step 
  par   <- mclust::covw(x, p, normalize = FALSE)
  mu    <- par$mean
  sigma <- par$S
  pi    <- colMeans(p)
  
  ## 3. Check convergence 
  for(g in 1:G) {
    llk[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])
  }
  loglik <- sum(log(rowSums(llk))) # current max. log-likelihood value
  ##
  diff      <- abs(loglik - loglikOld)/abs(loglik) # rate of change
  loglikOld <- loglik
  it        <- it + 1
  ## Check whether rate of change is still large enough (> tol)?
  check     <- diff > tol
}

## Estimation results:
results <- matrix(c(pi, mu, sqrt(sigma)), 
                  nrow = 3, 
                  ncol = 2, 
                  byrow = TRUE,
                  dimnames = list(c("weights", 
                                    "means", 
                                    "standard-deviations"),
                                  c("group 1", 
                                    "group 2"))) 
##
results %>% round(., 2)
```




## The True View on the EM Algorithm: Adding Unobserved Variables {#sec-TrueViewEM}

The EM algorithm allows maximum likelihood problems to be simplified by **adding unobserved ("latent") variables** to the data. This idea is the actually original contribution of the EM Algorithm (@Dempster_1977). While this idea can be applied for solving various maximum likelihood problems, we keep focusing on estimating GMMs.

::: {.callout-note}

# Remember:

We were not able to properly maximize the log-likelihood function
$$
\ell(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})
  =\sum_{i=1}^n\ln\left(\sum_{g=1}^G\pi_g\varphi(X_i;\mu_g,\sigma_g)\right)
$$
directly. In fact, the $\ln(\sum_{g=1}^G[\dots])$-construction makes life difficult here.
:::


### Data Completion

In our penguin data there are two groups $g\in\{1,2\}.$ 

Thus, in principle (albeit unobserved) there are $G=2$ dimensional dummy variable vectors $(Z_{i1},Z_{i2}),$ $i=1,\dots,n,$ which encode the group-labels,
$$
(Z_{i1},Z_{i2})=
\left\{\begin{array}{ll}
(1,0)&\text{if penguin }i\text{ belongs to group }g=1\\
(0,1)&\text{if penguin }i\text{ belongs to group }g=2\\
\end{array}\right.
$$

::: {.callout-tip}
# Case of more than two $G>2$ groups:  
$$
\begin{align*}
&(Z_{i1},\dots,Z_{ig},\dots,Z_{iG})=\\[2ex]
&=\left\{\begin{array}{ll}
(1,0,\dots,0)&\text{if data point }i\text{ belongs to group }g=1\\
(0,1,\dots,0)&\text{if data point }i\text{ belongs to group }g=2\\
\quad\quad\vdots&\\
(0,0,\dots,1)&\text{if data point }i\text{ belongs to group }g=G\\
\end{array}\right.
\end{align*}
$$
:::

The group labels $Z_{ig}$ can take values $Z_{ig}\in\{0,1\},$ for each $i=1,\dots,n$ and $g=1,\dots,G.$ However, it must hold true that each $i$ belongs to only **one** group, i.e.
$$
\sum_{g=1}^G Z_{ig}=1\quad\text{for each}\quad i=1,\dots,n.
$$

::: {.callout-note}
Requiring that
$$
\sum_{g=1}^G Z_{ig}=1\quad\text{for each}\quad i=1,\dots,n
$$
means an important restriction of GMMs, which is not problematic for the penguin data. However, it may be problematic in applications with hierarchical grouping structures, where one can be a member of multiple groups (e.g. member of a gender group and member of a religious group).
:::

Unfortunately, the true group labels $Z_{ig}$ are missing. However, we nevertheless know at least something about their group-assignments. The weights 
$$
\pi_{1,0},\dots,\pi_{G,0}
$$ 
of the Gaussian mixture distribution 
$$
f_{GMM}(x;\boldsymbol{\pi}_0,\boldsymbol{\mu}_0,\boldsymbol{\sigma}_0)=\sum_{g=1}^G\pi_{g,0}\varphi(x;\mu_{g,0},\sigma_{g,0}),
$$
give us the proportions of the individual distributions $\varphi(\cdot;\mu_{g,0},\sigma_{g,0})$ in the total distribution $f_{GMM}.$ Therefore, we know that, on average, 
$$
\pi_{g,0}\cdot 100\%
$$ 
of the data points $i=1,\dots,n$ come from group $g.$ 

Thus, we can consider the missing group label $Z_{ig}$ as a **unobserved realization of a binary random variable** $Z_{ig}\in\{0,1\}$ with probabilities
$$
\begin{align*}
P(Z_{ig}=1)&=\pi_{g,0}\\[2ex]
P(Z_{ig}=0)&=(1-\pi_{g,0})\\[2ex]
\end{align*}
$$
and with the restriction that 
$$
\sum_{g=1}^GZ_{ig}=1\quad\text{for each}\quad i=1,\dots,n.
$$ 

Note that the condition $\sum_{g=1}^GZ_{ig}=1$ implies that if
<!-- fÃ¼r Realisationen $Z_{ig}=1$, dass alle anderen Zuordnungen $Z_{i-g}=0$ immer gleich null  -->
$$
Z_{ig}=1
$$ 
then
$$
Z_{ij}=0\quad \text{for all }j\neq g.
%\quad \Rightarrow\quad Z_{i1}=0,\dots,Z_{ig-1}=0,Z_{ig+1}=0,\dots,Z_{iG}=0.
$$

### Prior and Posterior Probabilities {#sec-PriorPosterior}

**Prior Probability**
$$
\pi_{g,0} = P(Z_{ig}=1)
$$
If we know nothing about the flipper length of penguin $i$ then we are left with the prior probability: 
<br>
<center>
"With probability $\pi_g=P(Z_{ig}=1)$ penguin $i$ belongs to group $g$."
</center> 
<br>


**Posterior Probability**<br>
$$
p_{ig}=P(Z_{ig}=1|X_i)
$$ 
If we know the flipper length of penguin $i$ then we can update the prior probability using **Bayes' Theorem** (see @eq-posteriorGMD) which leads to the posterior probability: 
<br>
<center>
"With probability $p_{ig}=P(Z_{ig}=1|X_i)$ penguin $i$ with flipper length $X_i$ belongs to group $g.$"
</center>
<br>


::: {.callout-note} 
# Bayes' Theorem applied to the Gaussian mixture distribution
$$
\begin{align*}
p_{ig}
=\overbrace{P(Z_{ig}=1|X_i)}^{\text{Posterior-prob}}
&=\frac{\pi_g\varphi(X_i;\mu_g,\sigma_g)}{f_{GMM}(X_i;\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})}\\[2ex]
&=\frac{\overbrace{P(Z_{ig}=1)}^{\text{prior-prob}}\varphi(X_i;\mu_g,\sigma_g)}{f_{GMM}(X_i;\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})}
\end{align*}
$${#eq-posteriorGMD}
:::


::: {.callout-tip}
# Where's the <span style="color:#FF5733">**Expectation** </span> in the <span style="color:#FF5733">**E**</span>M-Algorithm? 
The posterior probabilities $p_{ig}$ are conditional means:
$$
\begin{align*}
p_{ig}
&= P(Z_{ig}=1|X_i)\\[2ex] 
&= 1\cdot P(Z_{ig}=1|X_i)+0\cdot P(Z_{ig}=0|X_i)\\[2ex]
&= \mathbb{E}(Z_{ig}|X_i)\\
\end{align*}
$${#eq-posteriorMean}
Thus, the computation of $p_{ig}$ is the **Expectation**-step of the EM algorithm (@sec-EM1). 
::: 


### The Abstract Version of the EM-Algorithm


If, in addition to the data points (i.e., the predictors such as the flipper lengths), 
$$
(X_1,\dots,X_n),
$$ 
we could also observe the group assignments, 
$$
(Z_{11},\dots,Z_{nG}),
$$ 
then we could establish the following alternative **likelihood ($\tilde{\mathcal{L}}$)** and **log-likelihood ($\tilde{\ell}$) functions**:
$$
\begin{align*}
\tilde{\mathcal{L}}(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})
&=\prod_{i=1}^n\prod_{g=1}^G\left(\pi_g\varphi(X_i;\mu_g,\sigma_g)\right)^{Z_{ig}}\\[2ex]
\tilde{\ell}(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})
&=\sum_{i=1}^n\sum_{g=1}^G Z_{ig}\left\{\ln\left(\pi_g\right)+\ln\left(\varphi(X_i;\mu_g,\sigma_g)\right)\right\}\\[2ex]
&=\sum_{g=1}^G\left(\sum_{i=1}^n Z_{ig}\left\{\ln\left(\pi_g\right)+\ln\left(\varphi(X_i;\mu_g,\sigma_g)\right)\right\}\right)
\end{align*}
$$

Unlike the original log-likelihood function (@eq-logLikGMM), the new log-likelihood function $\tilde\ell$ is **easy to maximize**: We can effectively **maximize separately for each group $g,$** which then involves only a single normal density function and not a too flexible mixture of normal density functions. This simplifies and stabilizes the maximization problem.  By contrast to the normal mixture distribution, the normal density belongs to the exponential family (see @sec-MLAsymp) for which maximum likelihood is known to work very well. 

Unfortunately, we only observe realizations of $X_{i}=X_{i,obs},$ but we do not observe realizations of the group assignment variables 
$$
(Z_{11},\dots,Z_{nG}).
$$ 

**Idea:** Substitute the unobservable $Z_{ig},$ by their best (in the mean square sense) prediction; namely, the conditional mean (using @eq-posteriorGMD and @eq-posteriorMean)
$$
\begin{align*}
p_{ig}
&=\mathbb{E}_{\boldsymbol{\theta}}\left(Z_{ig}|X_i\right)\\[2ex]
&=\overbrace{P_{\boldsymbol{\theta}}(Z_{ig}=1|X_i)}^{\text{Posterior-prob}}\\[2ex]
&=\frac{\pi_g\varphi(X_i;\mu_g,\sigma_g)}{f_{GMM}(X_i;\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})},
\end{align*}
$$
where $\boldsymbol{\theta}=(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})$ is used to denote the vector of parameter values under which the mean is computed.


Effectively, this means to compute the conditional mean of $\tilde{\ell}$ given $X_1,\dots,X_n,$ which motivates the "Expectation"-Step in the EM-algorithm:
$$
\begin{align*}
&\mathbb{E}_{\boldsymbol{\theta}}\left(\tilde{\ell}(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})|X_1,\dots,X_n\right)\\[2ex]
&\quad =\sum_{i=1}^n\sum_{g=1}^G\mathbb{E}_{\boldsymbol{\theta}}\left(Z_{ig}|X_i\right)\left\{\ln\left(\pi_g\right)+\ln\left(\varphi(X_i;\mu_g,\sigma_g)\right)\right\}\\[2ex]
&\quad =\sum_{i=1}^n\sum_{g=1}^G\;\;\;\;\;\;p_{ig}\;\;\;\;\;\;\left\{\ln\left(\pi_g\right)+\ln\left(\varphi(X_i;\mu_g,\sigma_g)\right)\right\},
\end{align*}
$$
where $\boldsymbol{\theta}=(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})$ is used to denote the parameter vector. 


::: {.callout-tip}
<!-- Given the estimates $\hat{p}_{ig},$ for all $i=1,\dots,n$ and $g=1,\dots,G,$ we can compute ML-estimates $(\hat\pi_g, \hat\mu_g,\hat\sigma_g)$ for all $g=1,\dots,G.$ -->
<!-- by maximizing $\mathbb{E}_{\boldsymbol{\theta}}\left(\tilde{\ell}(\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma};\mathbf{x},\mathbf{Z})|\mathbf{X}=\mathbf{x}\right).$  -->

We do not know the posterior probabilities, $p_{ig},$ but given estimates $\hat{\boldsymbol{\theta}}=(\hat{\boldsymbol{\pi}}, \hat{\boldsymbol{\mu}},\hat{\boldsymbol{\sigma}}),$ we can estimate them using  
$$
\begin{align*}
\hat{p}_{ig}
&=\hat{\mathbb{E}}_{\hat{\boldsymbol{\theta}}}(Z_{ig}|X_{i})\\[2ex]
&=\hat{P}_{\hat{\boldsymbol{\theta}}}(Z_{ig}=1|X_{i})\\[2ex]
&=\frac{\hat{\pi}_g\varphi(X_i;\hat{\mu}_g,\hat{\sigma}_g)}{f_{GMM}(X_i;\boldsymbol{\hat\pi},\boldsymbol{\hat\mu},\boldsymbol{\hat\sigma})}.
\end{align*}
$$ 
:::


The following EM algorithm differs only in notation from the version already discussed in @sec-EM1. The notation chosen here clarifies that the **Expectation**-step *updates the log-likelihood function* to be maximized in the **Maximization**-step. 

The chosen notation is abstract enough to transfer the basic idea of the EM algorithm to other maximum likelihood problems. 
<br>

1. **Initialization:**<br>
Set starting values $\boldsymbol{\hat{\theta}}^{(0)}=(\boldsymbol{\hat{\pi}}^{(0)}, \boldsymbol{\hat{\mu}}^{(0)}, \boldsymbol{\hat{\sigma}}^{(0)}),$ where
$$
\begin{align*}
&\boldsymbol{\hat{\pi}}^{(0)}=(\hat{\pi}_1^{(0)},\dots,\hat{\pi}_G^{(0)})\\[2ex]
&\boldsymbol{\hat{\mu}}^{(0)}=(\hat{\mu}_1^{(0)},\dots,\hat{\mu}_G^{(0)})\\[2ex]
&\boldsymbol{\hat{\sigma}}^{(0)}=(\hat{\sigma}_1^{(0)},\dots,\hat{\sigma}_G^{(0)})
\end{align*}
$$

2. **Loop:**</br>
For $r=1,2,\dots$

    - <span style="color:#FF5733">**(Expectation)** </span> Compute:
    $$
    \begin{align*}
    &\mathcal{Q}(\boldsymbol{\theta},\boldsymbol{\hat{\theta}}^{(r-1)})
    =\mathbb{E}_{\boldsymbol{\hat{\theta}}^{(r-1)}}\Big(\tilde{\ell}(\underbrace{\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma}}_{\boldsymbol{\theta}})\big|X_1,\dots,X_n\Big)\\[2ex]
    &=\sum_{i=1}^n\sum_{g=1}^G\mathbb{E}_{\boldsymbol{\theta}^{(r-1)}}\left(Z_{ig}\big|X_i\right)\left\{\ln\left(\pi_g\right)+\ln\left(\varphi(X_i;\mu_g,\sigma_g)\right)\right\}\\[2ex]
    &=\sum_{i=1}^n\sum_{g=1}^G\;\;\;\;\;\;\hat{p}_{ig}^{(r-1)}\;\;\;\;\;\;\left\{\ln\left(\pi_g\right)+\ln\left(\varphi(X_i;\mu_g,\sigma_g)\right)\right\}
    \end{align*}
    $$
    where
    $$
    \hat{p}_{ig}^{(r-1)} = \frac{\hat{\pi}_g^{(r-1)} \varphi(X_i;\hat{\mu}_g^{(r-1)},\hat{\sigma}_g^{(r-1)})}{f_{GMM}(X_i;\boldsymbol{\hat{\pi}}^{(r-1)},\boldsymbol{\hat{\mu}}^{(r-1)},\boldsymbol{\hat{\sigma}}^{(r-1)})}
    $$

    - <span style="color:#2471A3">**(Maximization)**</span>  Compute:
    $$
    \begin{align*}
    \boldsymbol{\hat{\theta}}^{(r)}=\arg\max_{\boldsymbol{\theta}}\mathcal{Q}(\boldsymbol{\theta},\boldsymbol{\hat{\theta}}^{(r-1)})
    \end{align*}
    $$
    where $\boldsymbol{\hat{\theta}}^{(r)}=(\boldsymbol{\hat{\pi}}^{(r)},\boldsymbol{\hat{\mu}}^{(r)},\boldsymbol{\hat{\sigma}}^{(r)}),$
    $$
    \begin{align*}
    &\boldsymbol{\hat{\pi}}^{(r)}=(\hat{\pi}_1^{(r)},\dots,\hat{\pi}_G^{(r)})\\[2ex]
    &\boldsymbol{\hat{\mu}}^{(r)}=(\hat{\mu}_1^{(r)},\dots,\hat{\mu}_G^{(r)})\\[2ex]
    &\boldsymbol{\hat{\sigma}}^{(r)}=(\hat{\sigma}_1^{(r)},\dots,\hat{\sigma}_G^{(r)}),
    \end{align*}
    $$ 
    with
    <center>$\hat{\pi}_g^{(r)}=\frac{1}{n}\sum_{i=1}^n\hat{p}_{ig}^{(r-1)},\quad\quad\hat\mu_g^{(r)}=\sum_{i=1}^n\frac{\hat{p}_{ig}^{(r-1)}}{\left(\sum_{j=1}^n\hat{p}_{jg}^{(r-1)}\right)}X_i$</center> 
    <center>$\hat\sigma_g^{(r)}=\sqrt{\sum_{i=1}^n\frac{\hat{p}_{ig}^{(r-1)}}{\left(\sum_{j=1}^n\hat{p}_{jg}^{(r-1)}\right)}\left(X_i-\hat\mu_g^{(r)}\right)^2}$</center>

3. **Check Convergence:**<br> 
   Stop if the value of the maximized log-likelihood function 
   $$
   \mathcal{Q}(\boldsymbol{\hat{\theta}}^{(r)},\boldsymbol{\hat{\theta}}^{(r-1)})
   $$ 
   does not change anymore substantially; i.e., if
   $$
   \mathcal{Q}(\boldsymbol{\hat{\theta}}^{(r)},\boldsymbol{\hat{\theta}}^{(r-1)})
   \approx
   \mathcal{Q}(\boldsymbol{\hat{\theta}}^{(r+1)},\boldsymbol{\hat{\theta}}^{(r-1+1)}),
   $$
   or, equivalently, if
   $$
   \left|\boldsymbol{\hat{\theta}}^{(r+1)} - \boldsymbol{\hat{\theta}}^{(r)}\right|\approx 0.
   $$



## Cluster Analysis {#sec-CA}

The problem of predicting a discrete random variable $Y$ (i.e. the group label) from a possibly multivariate predictor $X$ is called **classification**. 

Consider iid data 
$$
(Y_1,X_1),\dots,(Y_n,X_n)\overset{\text{i.i.d.}}{\sim}(Y,X)
$$
where 

* $X_i\in\mathbb{R}^p$ is a $p$-dimensional vector and 
* $Y_i$ takes values in some finite set $\mathcal{Y}.$  

> **Note:** Above in @sec-TrueViewEM, we used $Z,$ here we use $Y$ to denote the (unknown) group labels.

::: {.callout-note}
# Example 
Predict $Y\in\mathcal{Y}=\{0,1\}$ (e.g. passing the exam ($Y=1$) vs. failing $Y=0$) using the observed predictor values $X\in\mathbb{R}^p$ (e.g. previous gradings, number of hours studied, etc.)
:::

A **classification rule** $h$ is a function 
$$
h: \mathbb{R}^p \to \mathcal{Y}.
$$
That is, when we observe a new $X\in\mathbb{R}^p,$ we predict $Y$ to be $h(X)\in\mathcal{Y}.$

::: {.callout-note}
# (Un-)Supervised Classification

* **Supervised Classification:** If there are learning/training data <span style="color:#f01e2c">**with group-labels**</span>
$$
({\color{red}Y_1},X_1),\dots,({\color{red}Y_n},X_n)
$$
that can be used to estimate $h,$ it's called a **supervised classification** (computer science: supervised learning) problem. 

* **Unsupervised Classification/Cluster Analysis:** If there are learning/training data <span style="color:#f01e2c">**without group-labels**</span>
$$
X_1,\dots,X_n
$$
it's called a **unsupervised classification** (computer science: unsupervised learning) problem or **cluster analysis**.
:::

### Bayes Classifier 

We would like to find a classification rule $h$ that makes accurate
predictions. The most often used quantity to measure the accuracy of classification methods is the **error rate**.

::: {.callout-note appearance="minimal"} 
## 
::: {#def-errorRate}
## Error rate
The **true error rate** of the classifier $h$ is the loss function
$$
L(h) = P(h(X)\neq Y).
$${#eq-ErrorRate} 
The **empirical error rate** is
$$
\hat{L}_n(h)=\frac{1}{n}\sum_{i=1}^n 1_{(h(X_i)\neq Y_i)},
$$
where $1_{(\cdot)}$ denotes the indicator function with $1_{(\texttt{TRUE})}=1$ and $1_{(\texttt{FALSE})}=0.$
:::
:::

We try to find a classifier $h$ that minimizes $L(h)$ and $\hat{L}_n(h),$ respectively. 


Let us focus on the special case of only two groups which can be coded, without loss of generality, as 
$$
Y\in\{0,1\}
$$
For instance, 
$$
Y_i=\left\{\begin{array}{ll}
1&\text{if penguin $i$ belongs to species Chinstrap}\\
0&\text{if penguin $i$ belongs NOT to species Chinstrap}.
\end{array}\right..
$$

The **regression function** (i.e. the conditional mean function) is then given by 
$$
\begin{align*}
m(x)
&:=\mathbb{E}(Y|X=x)\\[2ex]
&=1\cdot P(Y=1|X=x) + 0\cdot P(Y=0|X=x)\\[2ex]
&=P(Y=1|X=x).
\end{align*}
$$
That is, the conditonal mean $m(x)$ is the **posterior probability**, i.e. the probability of $Y=1$ given $X=x.$


From **Bayes' theorem** it follows that 
$$
\begin{align*}
m(x)
&=P(Y=1|X=x)\\[2ex]
&=\frac{P(Y=1) f_{X|Y}(x|Y=1)}{P(Y=0) f_{X|Y}(x|Y=0)+P(Y=1) f_{X|Y}(x|Y=1) },\\[2ex]
&=\frac{\pi_1\; f_{X|Y}(x|Y=1)}{\pi_0\;f_{X|Y}(x|Y=0)+\pi_1\;f_{X|Y}(x|Y=1)}\\[2ex]
&=\frac{\pi_1\; f_{X|Y}(x|Y=1)}{f_{X}(x)},
\end{align*}
$${#eq-RegFun}
where 

1. 
$$
\pi_0= P(Y=0)\quad\text{and}\quad\pi_1  = P(Y=1)
$$ 
denote the **prior probabilities** with $\pi_0 + \pi_1 = 1,$  
2. 
$$
f_{X|Y}(x|Y=0)\quad\text{and}\quad f_{X|Y}(x|Y=1)
$$
denote the **conditional density functions** of $X$ given $Y=0$ and $Y=1,$ respectively, and 
3.  
$$
f_X(x)=\pi_1\;\; f_{X|Y}(x|Y=1) + \pi_0\;\; f_{X|Y}(x|Y=0)
$$ 
denotes the **unconditional density function** of $X.$

**Note:** Here $f$ denotes here some (unknown) density function, not necessarily the Gaussian density or a Gaussian mixture. 



The **Bayes classifier**, $h^\ast,$ classifies data according to the **Bayes classification rule**


::: {.callout-note appearance="minimal"} 
## 
::: {#def-BayesCR}

## Bayes Classification Rule and Decision Boundary
<br>
The **Bayes classification rule** $h^\ast$ is given by
$$
h^\ast(x) = \left\{\begin{array}{ll}
1&\text{if }P(Y=1|X=x)>\frac{1}{2}\\
0&\text{otherwise}.
\end{array}\right.
$$
The **decision boundary** of a classifier $h$ is given by the set 
$$
\mathcal{D}(h)=\{x : P(Y=1|X=x)=P(Y=0|X=x)\}.
$$
:::
:::

Equivalent forms of the Bayes' classification rule:
$$
\begin{align*}
h^\ast(x) 
& = \left\{\begin{array}{ll}
1&\text{if }m(x)>\frac{1}{2}\\
0&\text{otherwise}.
\end{array}\right.\\[2ex]
& = \left\{\begin{array}{ll}
1&\text{if }P(Y=1|X=x)>P(Y=0|X=x)\\
0&\text{otherwise}.
\end{array}\right.\\[2ex]
& = \left\{\begin{array}{ll}
1&\text{if }\pi_1 f_{X|Y}(x|Y=1)>\pi_0f_{X|Y}(x|Y=0)\\
0&\text{otherwise}.
\end{array}\right.
\end{align*}
$$

::: {.callout-note icon=false} 
## 
::: {#thm-BayesOptimal}

## Optimality of the Bayes decision rule
<br>
The Bayes decision rule is optimal. That is, if $h$ is any
other classification rule then 
$$
L(h^\ast)\leq L(h),
$$
where $L(h)=P(h(X)\neq Y)$ denotes the error rate loss function defined in @def-errorRate. 
:::
:::


The Bayes decision rule $h^\ast(x)$ depends on the unknown $m(x)=P(Y=1|X=x)$ and thus cannot be used in practice. However, we can use data to find some approximation to the Bayes decision rule. 

**Very roughly**, there are three main approaches:

1. **Empirical Risk Minimization:** Choose a set of classifiers $\mathcal{H}$ and try to find $\hat{h}\in\mathcal{H}$ such that 
$$
\hat{h}:=\arg\min_{h\in\mathcal{H}}L(h)
$$
**Example:** Random forests, neural nets, etc. 

2. **Regression:** Find an estimate $\hat{m}(x)$ of the regression function $m(x)=\mathbb{E}(Y|X=x)$ in @eq-RegFun and then use 
$$
\hat{h}(x) = \left\{\begin{array}{ll}
1&\text{if }\hat{m}(x)>\frac{1}{2}\\
0&\text{otherwise}.
\end{array}\right.
$$
**Examples:** Linear regression, logistic regression, etc. 

3. **Density Estimation:** Find density and probability estimates $\hat{f}_{X|Y},$ $\hat{\pi}_0=\hat{P}(Y=0),$ and $\hat{\pi}_1=\hat{P}(Y=1)$ and define 
$$
\begin{align*}
\hat{m}(x)
&=\hat{P}(Y=1|X=x)\\[2ex]
&=\frac{\hat{\pi}_1 \hat{f}_{X|Y}(x|Y=1)}{\hat{\pi}_0 \hat{f}_{X|Y}(x|Y=0) + \hat{\pi}_1 \hat{f}_{X|Y}(x|Y=1)}.
\end{align*}
$$
Then use
$$
\hat{h}(x) = \left\{\begin{array}{ll}
1&\text{if }\hat{m}(x)>\frac{1}{2}\\
0&\text{otherwise}.
\end{array}\right.
$$
**Examples:** Linear/quadratic discriminant analysis, naive Bayes, Gaussian mixture distributions, etc.

#### More than two group labels {-}

Of course, we can generalize all this to the case where the discrete random variables $Y$ takes on more than only two group-labels.

Let 
$$
Y\in\{1,\dots,G\}
$$
for any $G>1.$

Then, the (error rate optimal) Bayes classification rule is 
$$
\begin{align*}
h^\ast(x) 
& = \arg\max_{g}P(Y=g|X=x) \\[2ex]
& = \arg\max_{g}\pi_g f_{X|Y}(x|Y=g),\\[2ex]
\end{align*}
$$
where 
$$
P(Y=g|X=x) = \frac{\pi_g f_{X|Y}(x|Y=g)}{\sum_{g=1}^G\pi_g f_{X|Y}(x|Y=g)}
$$
denotes the **posterior probability** of group $g$, and 
$$
\pi_g = P(Y=g)
$$
denotes the **prior probability** of group $g,$ 
and $f_{X|Y}(x|Y=g)$ denotes the conditional density function of $X$ given $Y=g.$



### Synopsis: Penguin Example 

In our penguin example, we use the **density estimation** approach. 

Estimating general densities $f$ is hard --- particularly in multivariate cases. Therefore, one often tries to make certain simplifying assumptions such as $f$ being a Gaussian (mixture) density. 

In our penguin example, we assume that the conditional density function of flipper length $X$ given species $Y=g$ can be modelled reasonably well using a Gaussian density,
$$
f_{X|Y}(x|Y=g) = \varphi(x|\mu_g,\sigma_g) = \frac{1}{\sqrt{2\pi}\sigma_g}\exp\left(-\frac{1}{2}\left(\frac{x-\mu_g}{\sigma_g}\right)^2\right).
$$
which leads to a Gaussian Mixture distribution. 

The unknown parameters $\pi_g,$ $\mu_g,$ and $\sigma_g,$ $g=1,\dots,G,$ are estimated using the **EM algorithm**

**Unsupervised Classification:** Assign the data points $x_i$ to the group $g$ according to the classification rule
$$
\begin{align*}
\hat{h}(x_i) 
%& = \arg\max_{g}P(Y=g|X=x) \\[2ex]
& = \arg\max_{g}\hat{\pi}_g \varphi(x_i|\hat{\mu}_g,\hat{\sigma}_g),\\[2ex]
\end{align*}
$$

@fig-EMGif shows the iterative progress when estimating a Gaussian mixture distribution using the EM algorithm:

* The vertical line shows the decision boundary
* The two Gaussian density functions (dashed lines) show the conditional densities $\varphi(x|\hat{\mu}_g,\hat{\sigma}_g),$ $g=1,2.$
* The orange and green dots show the (unsupervised) classification results  


```{r, include=knitr::is_html_output(), animation.hook="gifski", interval=0.15}
#| label: fig-EMGif
#| fig-cap: Iterative estimation of the Gaussian mixture distribution using the EM alorighm.
#| echo: false
suppressMessages(library("MASS"))
library("mclust")

## Daten:
x <- cbind(penguin_flipper) # Daten [n x d]-Dimensional. 
d <- ncol(x)                # Dimension (d=1: univariat)
n <- nrow(x)                # Stichprobenumfang
G <- 2                      # Anzahl Gruppen

## Weitere Deklarationen:
llk       <- matrix(NA, n, G)
p         <- matrix(NA, n, G)  
loglikOld <- 1e07
tol       <- 1e-06
it        <- 0
check     <- TRUE 


## EM Algorithmus

## 1. Startwerte fÃ¼r pi, mu und sigma:
pi    <- c(.5,.5)              # Naive pi
sigma <- array(diag(d), c(d,d,G)) # Varianz = 1
mu    <- matrix(c(205,225), nrow=1, byrow=TRUE)
#t(MASS::mvrnorm(G, colMeans(x), sigma[,,1]*4) )

while(check){
  
  ## 2.a Expectation-Schritt 
  for(g in 1:G){
    p[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])
  }
  p <- sweep(p, 1, STATS = rowSums(p), FUN = "/")
  
  ## 2.b Maximization-Schritt
  par   <- mclust::covw(x, p, normalize = FALSE)
  mu    <- par$mean
  sigma <- par$S
  pi    <- colMeans(p)
  
  ## 3. PrÃ¼fung der Konvergenz
  for(g in 1:G) {
    llk[,g] <- pi[g] * mclust:::dmvnorm(x, mu[,g], sigma[,,g])
  }
  loglik <- sum(log(rowSums(llk))) # aktueller max. Log-Likelihood Wert
  ##
  diff      <- abs(loglik - loglikOld)/abs(loglik) # Ã„nderungsrate
  loglikOld <- loglik
  it        <- it + 1
  ## Ã„nderungsrate noch groÃŸ genug (> tol)?
  check     <- diff > tol
  
  ## Plot 
  ##
  xxd     <- seq(min(penguin_flipper)-3, max(penguin_flipper)+5, length.out = np)
  ## Mischungs-Dichte
  yyd     <- dnorm(xxd, mu[1,1], sqrt(sigma)[,,1])*pi[1] +
             dnorm(xxd, mu[1,2], sqrt(sigma)[,,2])*pi[2]
  ## Einzel-Dichten
  yyd1    <- dnorm(xxd, mu[1,1], sqrt(sigma)[,,1])*pi[1]
  yyd2    <- dnorm(xxd, mu[1,2], sqrt(sigma)[,,2])*pi[2]

  loc_line <- which.min(diff(sign(yyd1 - yyd2)))
  
  ## Classification
  cl <- as.numeric(
    dnorm(x, mu[1,1], sqrt(sigma)[,,1])*pi[1] < dnorm(x, mu[1,2], sqrt(sigma)[,,2])*pi[2])

  #class
  
  hist(x = penguin_flipper, xlab="Flipper length (mm)", 
       main="Penguins\n(Two Groups)",
       col=gray(.65,.5), border=gray(.35,.5), freq = FALSE, ylim=c(0, 0.04))
  lines(x = xxd, y=yyd, lwd=2, col=gray(.35,.75))
  lines(x = xxd, y=yyd1, lwd=2, col=gray(.35,.75), lty=2)
  lines(x = xxd, y=yyd2, lwd=2, col=gray(.35,.75), lty=2)
  text(x = 175, y=0.035, labels = paste("Iteration k =",it), pos = 4)
  ##
  set.seed(1)
  stripchart(penguin_flipper[cl==0], method = "jitter", jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[1],.5), bg=alpha(col_v[1],.5), cex=1.3, add = TRUE)
  set.seed(1)
  stripchart(penguin_flipper[cl==1], method = "jitter", jitter = .0005, at = .001,
           pch = 21, col=alpha(col_v[2],.5), bg=alpha(col_v[2],.5), cex=1.3, add = TRUE)
  ##
  abline(v=xxd[loc_line-1], lty=3)
}
```

The final estimation result replicates @fig-GMM-plot1.



The average penguin probably doesn't care about our EM Algorithm.

![Penguin research on the limit.](images/penguin_attack.gif){width=80%  #fig-agressivepenguin}



## Exercises {-}


#### Exercise 1. {-} 

(a) Consider 
$$
X_1,\dots,X_n\overset{\text{i.i.d}}{\sim}X,
$$
where $X\sim\text{Bernoulli}(p).$ Write the expressions of the (log) likelihood functions $\mathcal{L}$ and $\ell$. 

(b) Now let
$$
X_1,\dots,X_n\overset{\text{i.i.d}}{\sim}X,
$$
where $X$ is a Bernoulli mixture random variable with parameters $p_g$ and prior probabilities $\pi_g$, $g=1,\dots,G$. Write the expressions of the (log) likelihood functions $\mathcal{L}$ and $\ell$.  


(c) Let 
$$
(Z_{i1},\dots,Z_{iG})\in\{0,1\}^G
$$ 
denote the vector of latent group indicator random variables with 
$$
Z_{i1}+\dots + Z_{iG}=1
$$ 
and 
$$
P(Z_{ig}=1)=\pi_g,\quad g=1,\dots,G. 
$$ 
Thus, the realization $Z_i=(0,1,0,\dots,0)$ means that the $i$th observation comes from the $2$nd Bernoulli distribution $\text{Bernoulli}(p_2)$.</br> 
Write the expressions of the (log) likelihood functions $\tilde{\mathcal{L}}(\mathbf{p},\boldsymbol{\pi};\mathbf{x},\mathbf{Z})$ and $\tilde{\ell}(\mathbf{p},\boldsymbol{\pi};\mathbf{x},\mathbf{Z})$ that take into account the latend group indicator random variables.  


(d) Write down the expression for the posterior probability 
$$
\mathfrak{p}_{ig} = P(Z_{ig}=1 | X_i = x_i).
$$


(e) Derive the conditional expectation of $\tilde\ell,$ given $\mathbf{X}=\mathbf{x},$ 
$$\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde{\ell}(\mathbf{p},\boldsymbol{\pi};\mathbf{x},\mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right).
$$

(f) Maximize $\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde{\ell}(\mathbf{p},\boldsymbol{\pi};\mathbf{x},\mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)$ with respect to $p_g$ for $g=1,\dots,G.$

(g) Maximize $\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde{\ell}(\mathbf{p},\boldsymbol{\pi};\mathbf{x},\mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)$ with respect to $\pi_g$ for $g=1,\dots,G$ such that $\sum_{g=1}^G\pi_g=1.$

(h) Sketch the EM-Algorithm



<!-- 
#### Exercise NEW NEW. {-} 
The famouse minst data set.

```{r}
suppressMessages(library("tidyverse"))
suppressMessages(
mnist_raw <- read_csv("https://pjreddie.com/media/files/mnist_train.csv", col_names = FALSE)
)

pixels_gathered <- mnist_raw %>%  
head(10000)        %>%  # only the first 10000 cases (one case = one handwritten figure picture (28x28 pixels))
rename(label = X1) %>%  # "label = 5" means a handwritten 5 
mutate(instance = row_number()) %>%  
gather(pixel, value, -label, -instance) %>%  
tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%  
mutate(pixel = pixel - 2,         
       x     = pixel %% 28,         
       y     = 28 - pixel %/% 28)


theme_set(theme_light())

pixels_gathered %>%  
filter(instance <= 12) %>%  
ggplot(aes(x, y, fill = value)) +  geom_tile() +  facet_wrap(~ instance + label)
```
 -->



<!-- 
## Solutions {-} 

#### Solutions of Exercise 1. {-} 

##### (a) Likelihood function {-}

The probability mass function of $X\sim\text{Bernoulli}(p)$ is 
$$
f(x)=p^{x}(1-p)^{1-x},\quad\text{with}\quad x\in\{0,1\}.
$$
Thus the likelihood and the log-likelihood functions are
$$
\begin{align*}
\mathcal{L}(p;\mathbf{x})    & = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}\\
\mathcal{\ell}(p;\mathbf{x}) & = \sum_{i=1}^n \ln\left(p^{x_i}(1-p)^{1-x_i}\right),
\end{align*}
$$
where $\mathbf{x}=(x_1,\dots,x_n)$ denotes the vector of observed data points. 


##### (b) Likelihood function for a Bernoulli mixture distribution {-}
The probability mass function of a Bernoulli mixture distribution is 
$$
\begin{align*}
f_G(x)
& =\sum_{g=1}^G \pi_g\; f_g(x)\\[2ex]
& =\sum_{g=1}^G \pi_g\; p_g^{x}(1-p_g)^{1-x}\quad\text{with}\quad x\in\{0,1\}.
\end{align*}
$$
Thus the likelihood function is
$$
\begin{align*}
\mathcal{L}(\mathbf{p},\boldsymbol{\pi};\mathbf{x})    & = \prod_{i=1}^n\left(\sum_{g=1}^G \pi_g\; p_g^{x_i}(1-p_g)^{1-x_i}\right)\\[2ex]
\ell(\mathbf{p},\boldsymbol{\pi};\mathbf{x}) & = \sum_{i=1}^n\ln\left(\sum_{g=1}^G\pi_g\; p_g^{x_i}(1-p_g)^{1-x_i}\right)
\end{align*}
$$
where $\mathbf{p}=(p_1,\dots,p_G)$ and $\boldsymbol{\pi}=(\pi_1,\dots,\pi_G)$.

##### (c) Likelihood function with group indicator random variables {-}

$$
\begin{align*}
\tilde{\mathcal{L}}(\mathbf{p},\boldsymbol{\pi};\mathbf{x}, \mathbf{Z}) 
& = \prod_{i=1}^n \prod_{g=1}^G\left(\pi_g p_g^{x_i}(1-p_g)^{1-x_i}\right)^{Z_{ig}}\\[2ex]
\tilde{\ell}(\mathbf{p},\boldsymbol{\pi};\mathbf{x}, \mathbf{Z})    
& = \sum_{i=1}^n \sum_{g=1}^G Z_{ig}\left(\ln(\pi_g) + \ln\left(p_g^{x_i}(1-p_g)^{1-x_i}\right)\right)
\end{align*}
$$


##### (d) Posterior probability {-}

$$
\begin{align*}
\mathfrak{p}_{ig} = P(Z_{ig}=1 | X_i = x_i) 
&=\frac{ P(Z_{ig}=1) f_{X|Z}(x_i|Z_{ig}=1)}{f_G(x_i)} \\[2ex]
&=\frac{ P(Z_{ig}=1) f_g(x_i)}{f_G(x_i)} \\[2ex]
&=\frac{\pi_g\; p_g^{x_i} (1-p_g)^{1-x_i}}{\sum_{g=1}^G \pi_g\; p_g^{x_i} (1-p_g)^{1-x_i}}
\end{align*}
$$


##### (e) Conditional Expectation of $\tilde\ell$ {-}


$$
\begin{align*}
&\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde\ell(\mathbf{p},\boldsymbol{\pi};\mathbf{x},\mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)\\[2ex] 
& = \sum_{i=1}^n \sum_{g=1}^G \mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(Z_{ig}\big|X_i=x_i\right)\left(\ln(\pi_g) + \ln\left(p_g^{x_i}(1-p_g)^{1-x_i}\right)\right)\\[2ex]
& = \sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \left(\ln(\pi_g) + \ln\left(p_g^{x_i}(1-p_g)^{1-x_i}\right)\right),
\end{align*}
$$
since 
$$
\begin{align*}
&\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(Z_{ig}\big|\mathbf{X}=\mathbf{x}\right)\\[2ex] 
&= \mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(Z_{ig}\big|X_i=x_i\right)\\[2ex] 
&= P(Z_{ig}=1 | X_i = x_i) 
= \mathfrak{p}_{ig}. 
\end{align*}
$$


##### (f) Maximize $\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde\ell(\mathbf{p},\boldsymbol{\pi}|\mathbf{x}, \mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)$ with respect to $p_g:$ {-}

$$
\begin{align*}
&\frac{\partial}{\partial p_g}\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde\ell(\mathbf{p},\boldsymbol{\pi}|\mathbf{x}, \mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)\\[2ex] 
& = \frac{\partial}{\partial p_g}\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \left(\ln(\pi_g) + \ln\left(p_g^{x_i}(1-p_g)^{1-x_i}\right)\right)\\[2ex]
& = \frac{\partial}{\partial p_g}\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \left(\ln(\pi_g) + \left(x_i\ln(p_g) + (1-x_i)\ln(1-p_g)\right)\right)\\[2ex]
%& = \sum_{i=1}^n \mathfrak{p}_{ig} \frac{\partial}{\partial p_g} \left(x_i\ln(p_g) + (1-x_i)\ln(1-p_g)\right)\\[2ex]
& = \sum_{i=1}^n \mathfrak{p}_{ig} \left(\frac{x_i}{p_g} - \frac{(1-x_i)}{(1-p_g)}\right)\\[2ex]
\end{align*}
$$
First order condition 
$$
\begin{align*}
\sum_{i=1}^n \mathfrak{p}_{ig} \left(\frac{x_i}{\hat{p}_g} - \frac{(1-x_i)}{(1-\hat{p}_g)}\right)&\overset{!}{=}0\\[2ex]
\sum_{i=1}^n \mathfrak{p}_{ig} \left(\frac{x_i(1-\hat{p}_g)}{\hat{p}_g(1-\hat{p}_g)} - \frac{(1-x_i)\hat{p}_g}{\hat{p}_g(1-\hat{p}_g)}\right)&=0\\[2ex]
\sum_{i=1}^n \mathfrak{p}_{ig} \left(x_i(1-\hat{p}_g) - (1-x_i)\hat{p}_g\right)&=0\\[2ex]
\sum_{i=1}^n \mathfrak{p}_{ig} \left(x_i - \hat{p}_g \right)&=0\\[2ex]
\hat{p}_g\sum_{i=1}^n \mathfrak{p}_{ig} &= \sum_{i=1}^n \mathfrak{p}_{ig}x_i \\[2ex]
\hat{p}_g & = \frac{\sum_{i=1}^n \mathfrak{p}_{ig}x_i}{\sum_{i=1}^n \mathfrak{p}_{ig}}
\end{align*}
$$


##### (g) Maximize $\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde\ell(\mathbf{p},\boldsymbol{\pi}|\mathbf{x}, \mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)$ with respect to $\pi_g:$ {-}

Note: We only need to focus on the $\pi_g$ terms since the other terms in  
$$
\begin{align*}
&\mathbb{E}_{\mathbf{p},\boldsymbol{\pi}}\left(\tilde\ell(\mathbf{p},\boldsymbol{\pi}|\mathbf{x}, \mathbf{Z})\big|\mathbf{X}=\mathbf{x}\right)\\[2ex] 
& = \sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \left(\ln(\pi_g) + \ln\left(p_g^{x_i}(1-p_g)^{1-x_i}\right)\right)
\end{align*}
$$
are no functions of $\pi_g$.

However, we need to do maximization under the side constraint that $\sum_{g=1}^G\pi_g=1.$ So, we use the method of Lagrange multipliers. 

The Lagrange function is given by
$$
\begin{align*}
L(\boldsymbol{\pi},\lambda)=\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \ln(\pi_g) - \lambda \left(\sum_{g=1}^G\pi_g-1\right). 
\end{align*}
$$
First order condition with respect to $\pi_g$:
$$
\begin{align*}
\frac{\partial}{\partial \pi_g}L(\boldsymbol{\pi},\lambda)=
\sum_{i=1}^n \mathfrak{p}_{ig}\frac{1}{\pi_g}  - \lambda &\\[2ex] 
\sum_{i=1}^n \mathfrak{p}_{ig}\frac{1}{\hat{\pi}_g}  - \lambda  &\overset{!}{=}0 \\[2ex]
\sum_{i=1}^n \mathfrak{p}_{ig}   &=\lambda \hat{\pi}_g\\[2ex]
\hat{\pi}_g & = \frac{\sum_{i=1}^n \mathfrak{p}_{ig}}{\lambda}  \\[2ex]
\end{align*}
$$
Plugging $\hat{\pi}_g = \frac{\sum_{i=1}^n \mathfrak{p}_{ig}}{\lambda}$ into $L(\boldsymbol{\pi},\lambda)$:
$$
\begin{align*}
L(\lambda)&=\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \ln\left(\frac{\sum_{i=1}^n \mathfrak{p}_{ig}}{\lambda}\right) - \lambda \left(\sum_{g=1}^G\frac{\sum_{i=1}^n \mathfrak{p}_{ig}}{\lambda}-1\right) \\[2ex]
&=\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \left(\ln\left(\sum_{i=1}^n \mathfrak{p}_{ig}\right) - \ln\left(\lambda\right)\right) -  \left(\sum_{g=1}^G\sum_{i=1}^n \mathfrak{p}_{ig}-\lambda \right) \\[2ex]
\end{align*}
$$
First order condition with respect to $\lambda$:
$$
\begin{align*}
\frac{\partial}{\partial \lambda} L(\lambda)=
- \frac{1}{\lambda}\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} +1 &\\[2ex] 
- \frac{1}{\hat\lambda}\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} +1 & \overset{!}{=}0\\[2ex]
\hat\lambda & = \sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig} \\[2ex]
\end{align*}
$$
So, we have that 
$$
\begin{align*}
\hat{\pi}_g 
& = \frac{\sum_{i=1}^n \mathfrak{p}_{ig}}{\hat{\lambda}}\\[2ex]
& = \frac{\sum_{i=1}^n \mathfrak{p}_{ig}}{\sum_{i=1}^n \sum_{g=1}^G \mathfrak{p}_{ig}} 
\end{align*}
$$


##### (h) Sketch of the EM-Algorithm {-}

1. Initialization 
    $$
    \hat{p}_1^{(0)},\dots,\hat{p}_G^{(0)}
    $$ 
    $$
    \hat{\pi}_1^{(0)},\dots,\hat{\pi}_G^{(0)}
    $$
    For instance: $\hat{p}_g^{(0)}=0.5$ and $\hat{\pi}_g^{(0)}=\frac{1}{G}$ for all $g=1,\dots,G.$

2. For $r=1,2,\dots$
    * Expectation-Step: 
    $$
    \begin{align*}
    \hat{\mathfrak{p}}_{ig}^{(r-1)} 
    &=\frac{\hat{\pi}_g^{(r-1)}\; \left(\hat{p}_g^{(r-1)}\right)^{x_i} \left(1-\hat{p}_g^{(r-1)}\right)^{1-x_i}}{\sum_{g=1}^G \hat{\pi}_g^{(r-1)}\; \left(\hat{p}_g^{(r-1)}\right)^{x_i} \left(1-\hat{p}_g^{(r-1)}\right)^{1-x_i}}
    \end{align*}
    $$
    * Maximization-Step: 
    $$
    \begin{align*}
    \hat{p}_g^{(r)} & = \frac{\sum_{i=1}^n \hat{\mathfrak{p}}_{ig}^{(r-1)}x_i}{\sum_{i=1}^n \hat{\mathfrak{p}}_{ig}^{(r-1)}}\\[2ex]
    \hat{\pi}_g^{(r)} & = \frac{\sum_{i=1}^n \hat{\mathfrak{p}}_{ig}^{(r-1)}}{\sum_{i=1}^n \sum_{g=1}^G \hat{\mathfrak{p}}_{ig}^{(r-1)}}  \\[2ex]
    \end{align*}
    $$

3. Check convergence (no relevant change of the maximized log-likelihood function). 

-->

## References {-}