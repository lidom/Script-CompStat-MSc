<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ch3_bootstrap</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="Ch3_Bootstrap_files/libs/clipboard/clipboard.min.js"></script>
<script src="Ch3_Bootstrap_files/libs/quarto-html/quarto.js"></script>
<script src="Ch3_Bootstrap_files/libs/quarto-html/popper.min.js"></script>
<script src="Ch3_Bootstrap_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Ch3_Bootstrap_files/libs/quarto-html/anchor.min.js"></script>
<link href="Ch3_Bootstrap_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Ch3_Bootstrap_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Ch3_Bootstrap_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Ch3_Bootstrap_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Ch3_Bootstrap_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="the-bootstrap" class="level1">
<h1>The Bootstrap</h1>
<!-- TO-DO: 
1. Rework this chapter using the overview article of Horowitz
BOOTSTRAP METHODS IN ECONOMETRICS 
2. Remove the fraction estimator parts 
-->
<p>The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.</p>
<p>Some literature:</p>
<ul>
<li><span class="citation" data-cites="Shao_Tu_1996">@Shao_Tu_1996</span>: The Jackknife and Bootstrap</li>
<li><span class="citation" data-cites="Hall_1992">@Hall_1992</span>: The Bootstrap and Edgeworth Expansion</li>
<li><span class="citation" data-cites="Efron_Tibshirani_1994">@Efron_Tibshirani_1994</span>: An Introduction to the Bootstrap</li>
<li><span class="citation" data-cites="Horowitz_2001">@Horowitz_2001</span>: The Bootstrap. In: Handbook of Econometrics</li>
<li><span class="citation" data-cites="Davison_Hinkley_2013">@Davison_Hinkley_2013</span>: Bootstrap Methods and their Applications</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bradley Efron
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap method is attributed to <a href="https://statistics.stanford.edu/people/bradley-efron">Bradley Efron</a>, who received the <em><a href="https://statsandstories.net/methods/2018/9/28/bootstrapping-an-international-prize">International Prize in Statistics</a></em> (the Nobel price of statistics) for his seminal works on the bootstrap method.</p>
</div>
</div>
<section id="sec-Illustraction" class="level2">
<h2 class="anchored" data-anchor-id="sec-Illustraction">Illustration: When are you happy about the Bootstrap?</h2>
<p>Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y.\)</span> These returns <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random with</p>
<ul>
<li><span class="math inline">\(Var(X)=\sigma^2_X\)</span></li>
<li><span class="math inline">\(Var(Y)=\sigma^2_Y\)</span></li>
<li><span class="math inline">\(Cov(X,Y)=\sigma_{XY}\)</span></li>
</ul>
<p>We want to invest a fraction <span class="math inline">\(\alpha\in(0,1)\)</span> in <span class="math inline">\(X\)</span> and invest the remaining <span class="math inline">\(1-\alpha\)</span> in <span class="math inline">\(Y.\)</span></p>
<p>Our aim is to minimize the variance (risk) of our investment, i.e., we want to minimize <span class="math display">\[
Var\left(\alpha X + (1-\alpha)Y\right).
\]</span> One can show that the value <span class="math inline">\(\alpha\)</span> that minimizes this variance is <span id="eq-alpha"><span class="math display">\[
\alpha = \frac{\sigma^2_Y - \sigma_{XY}}{\sigma^2_X + \sigma^2_Y - 2\sigma_{XY}}.
\tag{1}\]</span></span> Using a data set that contains past measurements <span class="math display">\[
((X_1,Y_1),\dots,(X_n,Y_n))
\]</span> for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y,\)</span> we can estimate the unknown <span class="math inline">\(\alpha\)</span> by plugging in estimates of the variances and covariances <span id="eq-alphahat"><span class="math display">\[
\hat\alpha = \frac{\hat\sigma^2_Y - \hat\sigma_{XY}}{\hat\sigma^2_X + \hat\sigma^2_Y - 2\hat\sigma_{XY}}
\tag{2}\]</span></span> with <span class="math display">\[
\begin{align*}
\hat{\sigma}^2_X&amp;=\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2\\
\hat{\sigma}^2_Y&amp;=\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2\\
\hat{\sigma}_{XY}&amp;=\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)\left(Y_i-\bar{Y}\right),
\end{align*}
\]</span> where <span class="math inline">\(\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i\)</span> and <span class="math inline">\(\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i.\)</span></p>
<p>It is natural to wish to quantify the accuracy of our estimator <span class="math display">\[
\hat\alpha\approx \alpha.
\]</span></p>
<p>For instance, to construct a confidence interval we need to know the standard error of the estimator <span class="math inline">\(\hat\alpha\)</span>, <span class="math display">\[
\sqrt{Var(\hat\alpha)} = \operatorname{SE}(\hat\alpha)=?
\]</span> However, computing <span class="math inline">\(\operatorname{SE}(\hat\alpha)\)</span> is here difficult due to the definition of <span class="math inline">\(\hat\alpha\)</span> in <a href="#eq-alphahat">Equation&nbsp;2</a> which contains variance estimates also in the denominator.</p>
<p>In such cases, we are happy to use the Bootstrap which allows us to approximate <span class="math inline">\(\operatorname{SE}(\hat\alpha)\)</span> simply by resampling from the data.</p>
<p>Moreover, specific versions of the Bootstrap are found to be <strong>more accurate</strong> than standard asymptotic theory results.</p>
</section>
<section id="recap-the-empirical-distribution-function" class="level2">
<h2 class="anchored" data-anchor-id="recap-the-empirical-distribution-function">Recap: The Empirical Distribution Function</h2>
<p>The distribution of a real-valued random variable <span class="math inline">\(X\)</span> can be completely described by its distribution function <span class="math display">\[
F(x)=P(X\leq x)\quad \text{for all } x\in\mathbb{R}.
\]</span></p>
<p>For given data, the sample analogue of <span class="math inline">\(F\)</span> is the so-called <strong>empirical distribution function</strong>, which is an important tool of statistical inference.</p>
<p><strong>Data:</strong> i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span> from <span class="math inline">\(X\sim F\)</span></p>
<p>Let <span class="math inline">\(1_{(\cdot)}\)</span> denote the indicator function, i.e., <span class="math display">\[
1_{(x\leq t)}=\left\{
  \begin{array}
  1 &amp; \quad\text{if}\; x\leq t\\
  0 &amp; \quad\text{if}\; x&gt;t.
  \end{array}
  \right.
\]</span></p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-cdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 ((Cumulative) Distribution Function) </strong></span><span class="math display">\[
F(x)=\mathbb{P}(X \leq x).
\]</span></p>
</div>
</div>
</div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-ecdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Empirical distribution function) </strong></span><span class="math display">\[
F_n(x)=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}
\]</span> I.e <span class="math inline">\(F_n(x)\)</span> is the proportion of observations with <span class="math inline">\(X_i\le x,\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
</div>
</div>
</div>
<p><strong>Properties:</strong></p>
<ul>
<li><span class="math inline">\(0\le F_n(x)\le 1\)</span><br></li>
<li><span class="math inline">\(F_n(x)=0,\)</span> if <span class="math inline">\(x&lt;X_{(1)}\)</span>, where <span class="math inline">\(X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}\)</span> denotes the <strong>order-statistic</strong>.<br></li>
<li><span class="math inline">\(F_n(x)=1,\)</span> if <span class="math inline">\(x\ge X_{(n)}\)</span>, where <span class="math inline">\(X_{(n)}\)</span> is largest observation<br></li>
<li><span class="math inline">\(F_n\)</span> is a monotonically increasing step function</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The empirical distribution function <span class="math inline">\(F_n\)</span> is itself is a distribution function
</div>
</div>
<div class="callout-body-container callout-body">
<p>The empirical distribution funciton <span class="math inline">\(F_n\)</span> is the distribution function (<a href="#def-cdf">Definition&nbsp;1</a>) of the <strong>discrete random variable</strong> <span class="math inline">\(X^*,\)</span> where</p>
<ul>
<li><span class="math inline">\(X^*\in\{X_1,\dots,X_n\}\)</span></li>
</ul>
<p>and</p>
<ul>
<li><span class="math inline">\(P(X^*=X_i)=\frac{1}{n}\)</span> for each <span class="math inline">\(i=1,\dots,n.\)</span></li>
</ul>
<p>Thus</p>
<p><span class="math display">\[
F_n(x) = \mathbb{P}\left(X^*\leq x\right)=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}
\]</span></p>
</div>
</div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="exm-ecdfexample" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Computing the empirical distribution function in R) </strong></span><br></p>
<p>Some data:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.20</td>
</tr>
<tr class="even">
<td>2</td>
<td>4.80</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.40</td>
</tr>
<tr class="even">
<td>4</td>
<td>4.60</td>
</tr>
<tr class="odd">
<td>5</td>
<td>6.10</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.40</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.80</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
</tr>
</tbody>
</table>
<p>Corresponding empirical distribution function using <code>R</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">5.20</span>, <span class="fl">4.80</span>, <span class="fl">5.30</span>, <span class="fl">4.60</span>, </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                    <span class="fl">6.10</span>, <span class="fl">5.40</span>, <span class="fl">5.80</span>, <span class="fl">5.50</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>myecdf_fun     <span class="ot">&lt;-</span> <span class="fu">ecdf</span>(observedSample)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(myecdf_fun, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/ecdfPlot-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Note: ecdf() returns a function!</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">myecdf_fun</span>(<span class="fl">5.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.25</code></pre>
</div>
</div>
</div>
</div>
</div>
<p><span class="math inline">\(F_n(x)\)</span> depends on the i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span> and thus is itself a <strong>random function</strong>.</p>
<p>We obtain <span class="math display">\[
nF_n(x)\sim B(n, p=F(x))\quad x\in\mathbb{R}
\]</span></p>
<p>I.e., <span class="math inline">\(nF_n(x)\)</span> has a binomial distribution with parameters <span class="math inline">\(n\)</span> (‚Äúnumber of trials‚Äù) and <span class="math inline">\(p=F(x)\)</span> (‚Äúprobability of success on a single trial‚Äù).</p>
<p>Thus, <span class="math display">\[
\begin{align*}
\mathbb{E}(nF_n(x))&amp; = np = nF(x)\\[2ex]
\Rightarrow \quad \mathbb{E}(F_n(x))&amp; = p = F(x)\\[2ex]
\Rightarrow \quad \operatorname{Bias}(F_n(x))&amp; = \mathbb{E}(F_n(x)) - F(x) =0\\
\end{align*}
\]</span> and <span class="math display">\[
\begin{align*}
Var(nF_n(x))&amp; = np(1-p) = nF(x)(1-F(x))\\[2ex]
\Rightarrow \quad Var(F_n(x))&amp; = \frac{nF(x)(1-F(x))}{n^2}=\frac{F(x)(1-F(x))}{n}
\end{align*}
\]</span> such that <span class="math display">\[
\begin{align*}
\operatorname{MSE}(F_n(x))
&amp; = (\operatorname{Bias}(F_n(x)))^2 + Var(F_n(x))\\[2ex]
&amp; =\frac{F(x)(1-F(x))}{n}.
\end{align*}
\]</span></p>
<p>This allows us to conclude that <span class="math display">\[
\begin{align*}
F_n(x) &amp; \to_{m.s.} F(x)\quad\text{as}\quad n\to\infty\\[2ex]
\Rightarrow \quad F_n(x) &amp; \to_{p} F(x)\quad\text{as}\quad n\to\infty.
\end{align*}
\]</span></p>
<p>That is, <span class="math inline">\(F_n(x)\)</span> is <strong>point-wise</strong> for each <span class="math inline">\(x\in\mathbb{R}\)</span> a weakly consistent estimator of <span class="math inline">\(F(x).\)</span></p>
<p>The Clivenko-Cantelli <a href="#thm-Clivenko-Cantelli">Theorem&nbsp;1</a> states that <span class="math inline">\(F_n\)</span> is even <strong>uniformly</strong> over <span class="math inline">\(\mathbb{R}\)</span> a consistent estimator of <span class="math inline">\(F.\)</span></p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-Clivenko-Cantelli" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Theorem of Glivenko-Cantelli) </strong></span><span class="math display">\[
\begin{align*}
&amp;\quad P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1\\[2ex]
\Leftrightarrow &amp;\quad
\sup_{x\in\mathbb{R}} |F_n(x)-F(x)|\to_{a.s.} 0
\end{align*}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="basic-idea-of-the-bootstrap" class="level2">
<h2 class="anchored" data-anchor-id="basic-idea-of-the-bootstrap">Basic Idea of the Bootstrap</h2>
<p>The basic idea of the bootstrap is to replace random sampling from the true (unknown) population <span class="math inline">\(F\)</span> (infeasible Monte Carlo simulation) by random sampling from the empirical distribution <span class="math inline">\(F_n\)</span> (feasible Monte Carlo simulation).</p>
<p><strong>Sampling from the population distribution <span class="math inline">\(F\)</span> (infeasible Monte Carlo simulation)</strong> <br>The random sample <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}F\)</span> is generated by drawing observations independently and with replacement from the unknown population distribution function <span class="math inline">\(F\)</span>. That is, for each interval <span class="math inline">\([a,b]\)</span> the probability of drawing an observation in <span class="math inline">\([a,b]\)</span> is given by <span class="math display">\[
P(X\in [a,b])=F(b)-F(a).
\]</span> Let <span class="math inline">\(\theta_0\)</span> denote a distribution parameter of <span class="math inline">\(F\)</span> which we want to estimate, and let <span class="math inline">\(\hat\theta_n\)</span> denote an estimator of <span class="math inline">\(\theta_0.\)</span><br> If we would know <span class="math inline">\(F,\)</span> we could generate arbitrarily many realizations of an estimator <span class="math inline">\(\hat{\theta}_n\)</span> <span class="math display">\[
\hat{\theta}_{n,1}, \hat{\theta}_{n,2}, \dots, \hat{\theta}_{n,m}
\]</span> with <span class="math inline">\(m\to\infty\)</span> and do inference about <span class="math inline">\(\theta_0\)</span> using these realizations. Unfortunately, we don‚Äôt know <span class="math inline">\(F,\)</span> thus Monte Carlo inference is infeasible.</p>
<p><strong>The idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:</strong> <br> Instead of random sampling from <span class="math inline">\(F,\)</span> which is infeasible, the bootstrap uses random sampling from the known empirical distribution function <span class="math inline">\(F_n\)</span> to generate arbitrarily many <strong>bootstrap realizations</strong> of an estimator <span class="math inline">\(\hat{\theta}_n\)</span> <span class="math display">\[
\hat{\theta}^*_{n,1}, \hat{\theta}^*_{n,2}, \dots, \hat{\theta}^*_{n,m}
\]</span> with <span class="math inline">\(m\to\infty\)</span> and do inference about <span class="math inline">\(\theta_0\)</span> using these bootstrap realizations.<br> This is justified asymptotically since for large <span class="math inline">\(n,\)</span> the empirical distribution <span class="math inline">\(F_n\)</span> is ‚Äúclose‚Äù to the unknown distribution <span class="math inline">\(F\)</span> (Glivenko-Cantelli <a href="#thm-Clivenko-Cantelli">Theorem&nbsp;1</a>). That is, for <span class="math inline">\(n\rightarrow\infty\)</span> the relative frequency of observations <span class="math inline">\(X_i\)</span> in <span class="math inline">\([a,b]\)</span> converges to <span class="math inline">\(P(X\in [a,b])\)</span><br>
<span class="math display">\[
  \begin{align*}
  \underbrace{\frac{1}{n}\sum_{i=1}^n1_{(X_i\in[a,b])}}_{=F_n(b)-F_n(a)}&amp;\to_p \underbrace{P(X\in [a,b])}_{=F(b)-F(a)}
  \end{align*}
\]</span></p>
</section>
<section id="the-nonparametric-standard-bootstrap" class="level2">
<h2 class="anchored" data-anchor-id="the-nonparametric-standard-bootstrap">The Nonparametric (Standard) Bootstrap</h2>
<p><strong>Setup:</strong></p>
<ul>
<li>i.i.d. sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> where <span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}F.\)</span></li>
<li>The distribution <span class="math inline">\(F\)</span> is depends on an unknown parameter <span class="math inline">\(\theta_0.\)</span></li>
<li>The data <span class="math inline">\(X_1,\dots,X_n\)</span> is used to estimate <span class="math inline">\(\theta_0.\)</span></li>
<li>Thus, the estimator is a function of the random sample <span class="math display">\[
\hat\theta_n\equiv \hat\theta(X_1,\dots,X_n).
\]</span></li>
</ul>
<p>Moreover, for simplicity let us focus on unbiased and <span class="math inline">\(\sqrt{n}\)</span> consistent estimators, i.e.</p>
<ul>
<li><span class="math inline">\(\mathbb{E}\left(\hat\theta_n\right)=\theta_0\)</span></li>
<li><span class="math inline">\(\operatorname{SE}\left(\hat\theta_n\right)=\sqrt{Var\left(\hat\theta_n\right)}=\frac{1}{\sqrt{n}}\cdot\text{constant}\)</span></li>
</ul>
<p><strong>Inference:</strong> In order to provide standard errors, construct confidence intervals, and to perform tests of hypothesis, we are interested in learning the distribution of <span class="math display">\[
\sqrt{n}\left(\hat\theta_n-\theta_0\right)\quad\text{as}\quad n\to\infty,
\]</span> i.e.&nbsp;in learning the limit of <span class="math display">\[
H_{n}(x)=P\left(\sqrt{n}\left(\hat\theta_n-\theta_0\right)\leq x\right)
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Of course, we could do asymptotic statistics. For instance, using the Lindeberg-L√©vy CLT, we may be able to show that that the limit of <span class="math inline">\(H_{n}(x)\)</span> is the Normal distribution.</p>
<p>However, deriving a useful, explicit expression of the asymptotic variance of <span class="math inline">\(\hat{\theta}_n\)</span> can be very hard (see <a href="#sec-Illustraction">Section&nbsp;1.1</a>). Then, one is very happy to use the bootstrap instead of doing painful math.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The Bootstrap Algorithm (Core Part)
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Draw a bootstrap sample:</strong> Generate a new random sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> by drawing observations independently and with replacement from the available sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span><br></li>
<li><strong>Compute bootstrap estimate:</strong> Compute the estimate <span class="math display">\[
\hat\theta^*_n\equiv \hat\theta(X_1^*,\dots,X_n^*)
\]</span></li>
<li><strong>Bootstrap replications:</strong> Repeat Steps 1 and 2 <span class="math inline">\(m\)</span> times (for a large value of <span class="math inline">\(m,\)</span> such as <span class="math inline">\(m=5000\)</span> or <span class="math inline">\(m=10000\)</span>) leading to <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*
\]</span></li>
</ol>
</div>
</div>
<p>By the Clivenko-Cantelli (<a href="#thm-Clivenko-Cantelli">Theorem&nbsp;1</a>) the bootstrap estimators <span class="math display">\[
\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*
\]</span> allow us to approximate the <strong>bootstrap distribution</strong><br>
<span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\hat\theta^*_n-\hat\theta_n\right)\leq x\right|\mathcal{S}_n\right)
\]</span> arbitrarily well, i.e., <span class="math display">\[
\sup_{x}\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\right|\to_{a.s} 0\quad\text{as}\quad m\to\infty,
\]</span> where <span class="math display">\[
H^{Boot}_{n,m}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(\sqrt{n}\left(\hat\theta^*_{n,j}-\hat\theta_n\right)\leq x\right)}
\]</span> denotes the empirical distribution function, which we can use to approximate the bootstrap distribution <span class="math inline">\(H^{Boot}_{n}(x).\)</span></p>
<p>Since we can choose <span class="math inline">\(m\)</span> arbitrarily large, we can effectively ignore the approximation error in <span class="math inline">\(H^{Boot}_{n,m}(x).\)</span> That is, we can treat the bootstrap distribution <span class="math inline">\(H^{Boot}_{n}(x)\)</span> is known.</p>
<p>The crucial question is, however, whether the feasible bootstrap distribution <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\hat\theta^*_n-\hat\theta_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
\]</span> is able to approximate the unknown distribution <span class="math display">\[
H_{n}(x)=P\left(\sqrt{n}\left(\hat\theta_n-\theta_0\right)\leq x\right)
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<section id="bootstrap-consistency" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="bootstrap-consistency">Bootstrap Consistency</h3>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Caution: The bootstrap does <em>not</em> always work
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bootstrap does <strong>not always work</strong>. A necessary condition for the use of the bootstrap is the <strong>consistency of the bootstrap approximation</strong>.</p>
</div>
</div>
<p>The bootstrap is called <strong>consistent</strong> if, for large <span class="math inline">\(n\)</span>, the bootstrap distribution of <span class="math inline">\(\hat{\theta}^*_n -\hat{\theta}_n\)</span> is a good approximation of the underlying distribution of <span class="math inline">\(\hat{\theta}_n-\theta_0\)</span>, i.e. <span class="math display">\[
\text{distribution}(\hat{\theta}^*_n -\hat{\theta}_n\ |{\cal S}_n)\approx
\text{distribution}(\hat{\theta}_n-\theta_0).
\]</span> The following definition states this more precisely.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-BootstrapConsistency" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Bootstrap consistency) </strong></span>Let the limit (as <span class="math inline">\(n\to\infty\)</span>) of <span class="math inline">\(H_n\)</span> be a non-degenerate distribution, then the bootstrap is <strong>consistent</strong> if and only if <span class="math display">\[
\sup_x \Big|\;
   \underbrace{P\Big(\sqrt{n}\big(\hat\theta^*_n-\hat\theta_n\big)\le x \ |{\cal S}_n\Big)}_{H_n^{Boot}(x)}
  -\underbrace{P\Big(\sqrt{n}\big(\hat\theta_n -\theta_0\big)\le x\Big)}_{H_n(x)}
  \Big|\rightarrow_p 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
</div>
</div>
</div>
<p>Luckily, the standard bootstrap works for a large number of statistical and econometrical problems. However, there are <strong>some crucial requirements</strong>:</p>
<ol type="1">
<li>Generation of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).</li>
<li>Typically, the distribution of the estimator <span class="math inline">\(\hat\theta_n-\theta_0\)</span> needs to be asymptotically normal.</li>
</ol>
<p>The standard bootstrap <strong>will usually fail</strong> if one of the above conditions 1 or 2 is violated. For instance,</p>
<ul>
<li>The bootstrap will not work if the i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> does not properly reflect the way how <span class="math inline">\(X_1,\dots,X_n\)</span> are generated in a first place. (For instance, when <span class="math inline">\(X_1,\dots,X_n\)</span> is generated by a time-series process with auto-correlated data.)</li>
<li>The distribution of the estimator <span class="math inline">\(\hat\theta\)</span> is not asymptotically normal. (For instance, in case of extreme value problems.)</li>
</ul>
<p><strong>Note:</strong> In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature (e.g.&nbsp;the block-bootstrap in case of time-series data).</p>
</section>
<section id="example-inference-about-the-population-mean" class="level3">
<h3 class="anchored" data-anchor-id="example-inference-about-the-population-mean">Example: Inference About the Population Mean</h3>
<p><strong>Setup:</strong></p>
<ul>
<li><span class="math inline">\(X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X\)</span> with <span class="math inline">\(X\sim F\)</span></li>
<li>Continuous random variable <span class="math inline">\(X\sim F\)</span></li>
<li>Non-zero, finite variance <span class="math inline">\(0&lt;Var(X)=\sigma_0^2&lt;\infty\)</span></li>
<li>Unknown mean <span class="math inline">\(\mathbb{E}(X)=\mu_0,\)</span> where<br>
<span class="math display">\[
\mu_0 = \int x f(x) dx = \int x d F(x),
\]</span> where <span class="math inline">\(f=F'\)</span> denotes the density function.</li>
<li>Estimator: Empirical mean <span class="math display">\[
\begin{align*}
\bar{X}_n
&amp;\equiv \bar{X}(X_1,\dots,X_n) \\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n X_i \\[2ex]
&amp;=\int x d F_n(x)
\end{align*}
\]</span></li>
</ul>
<p><strong>Inference Problem:</strong> What is the (asymptotic) distribution of <span class="math display">\[
\sqrt{n}\left(\bar{X}_n -\mu_0\right)
\]</span> as <span class="math inline">\(n\to\infty\)</span>?</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Asymptotic Result
</div>
</div>
<div class="callout-body-container callout-body">
<p>This example is so simple that we know (by the Lindeberg-Levy CLT) that <span class="math display">\[
\sqrt{n}\left(\bar{X}_n -\mu_0\right)\to_d\mathcal{N}(0,\sigma_0)\quad\text{as}\quad n\to\infty,
\]</span> i.e., that <span class="math display">\[
%\bar{X}_n\overset{a}{\sim}\mathcal{N}\left(\mu_0,\frac{1}{n}\sigma_0\right).
H_n(x)=P\left(\sqrt{n}\left(\bar{X}_n-\mu_0\right)\leq x\right)\to\Phi_{\sigma_0}(x)\quad\text{as}\quad n\to\infty,
\]</span> for all continuity points <span class="math inline">\(x,\)</span> where <span class="math display">\[
\Phi_{\sigma_0}(x)=\Phi(x/\sigma_0)
\]</span> with <span class="math inline">\(\Phi\)</span> denoting the distribution function of the standard normal distribution.</p>
</div>
</div>
<p>Yes, the asymptotic result is simple here, but can we simply use the Bootstrap to approximate this limit result with an estimation of <span class="math inline">\(\sigma_0\)</span> right away? I.e., is <span class="math display">\[
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\right|\mathcal{S}_n\right)
\]</span> able to approximate <span class="math inline">\(\Phi_{\sigma_0}\)</span>? In the following, we check this empirically.</p>
<p>Now assume that <span class="math inline">\(n=8\)</span> and that the <strong>observed sample</strong> is</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>-0.6</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.4</td>
</tr>
<tr class="even">
<td>4</td>
<td>-0.8</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.6</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.9</td>
</tr>
<tr class="odd">
<td>7</td>
<td>-0.1</td>
</tr>
<tr class="even">
<td>8</td>
<td>0.7</td>
</tr>
</tbody>
</table>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>observedSample <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.6</span>, <span class="fl">1.0</span>,  <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.8</span>, </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                     <span class="fl">1.6</span>, <span class="fl">1.9</span>, <span class="sc">-</span><span class="fl">0.1</span>,  <span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
So the observed sample mean is
<center>
<span class="math inline">\(\bar X_{n,obs} =\)</span> <code>mean(observedSample)</code> <span class="math inline">\(=\)</span> 0.6375
</center>
<p><br></p>
<p><strong>Bootstrap:</strong></p>
<p>The observed sample <span class="math display">\[
{\cal S}_n=\{X_1,\dots,X_n\}
\]</span> is taken as underlying empirical ‚Äúpopulation‚Äù in order to generate the <strong>bootstrap sample</strong> <span class="math inline">\(X_1^*,\dots,X_n^*\)</span>:</p>
<ul>
<li>i.i.d. samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> are generated by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span>.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">## generating a bootstrap sample</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>bootSample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">size    =</span> <span class="fu">length</span>(observedSample), </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">replace =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>The distribution of <span class="math display">\[
\bar X -\mu
\]</span> is approximated by the conditional distribution of <span class="math display">\[
\bar X^* -\bar X,
\]</span> given the original sample <span class="math inline">\({\cal S}_n,\)</span> i.e.&nbsp;more formally <span class="math display">\[
\underbrace{P\left(\bar{X}^*-\bar{X}&lt;\delta|\mathcal{S}_n\right)}_{\text{approximable}}
\approx
\underbrace{P\left(\bar{X}-\mu&lt;\delta\right)}_{\text{unknown}}.
\]</span></li>
</ul>
<p>For the given data with <span class="math inline">\(n=8\)</span> observations, there are <span class="math display">\[
n^n=8^8=16,777,216
\]</span> possible bootstrap samples which are all equally probable.</p>
<p>The conditional distribution function of <span class="math inline">\(\bar{X}^*-\bar{X}\)</span> given <span class="math inline">\(\mathcal{S}_n\)</span> <span class="math display">\[
P\left(\bar{X}^*-\bar{X}&lt;\delta|\mathcal{S}_n\right)
\]</span> can be approximated using a Monte-Carlo simulation. For this, we draw new data <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(F_n,\)</span> i.e., we sample with replacement data points from the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span></p>
<p>Using a large number <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=10000\)</span>) of simulation runs allows us to generate bootstrap estimates <span class="math display">\[
\bar{X}^*_1,\bar{X}^*_2,\dots,\bar{X}^*_m
\]</span></p>
<!-- Simul. | $X_1^*$|  $X_2^*$| $X_3^*$| $X_4^*$|  $X_5^*$|  $X_6^*$|  $X_7^*$|   $X_8^*$ | $\bar X^*-\bar{X}$
----|----:|----:|----:|----:|----:|----:|----:|----:|:----:
1 | 1.9| -0.8| 1.9|  -0.6| 1.4| -0.1| -0.8| 1.0 | -0.15
2 | 0.7| -0.8| -0.8| 1.0 | 1.6| 1.0| -0.1| -0.8 | -0.4125  
3 | -0.1| 1.9| 0.7|  1.0| -0.1| 1.6| 1.0| -0.6 |  0.0375 
4 | 1.4| 1.0| 1.4|  -0.1| 1.9| -0.8| 1.9| 1.0 | 0.325 
5 | 1.0| 0.7| -0.1|  0.7| 1.4| -0.8| 1.0| 1.6 |  0.05
... ||||||||| -->
<p>These bootstrap estimates are then used to approximate the bootstrap distribution <span class="math display">\[
%\overbrace{
  \underbrace{P\left(\bar X^*-\bar X\leq \delta |{\cal S}_n\right)}_{\text{bootstrap distribution}}%}^{=P^*\left(\bar X^*-\bar X\leq \delta\right)}
\approx
\frac{1}{m}\sum_{k=1}^m 1_{( \bar X^*_k-\bar X\leq \delta)},  
\]</span> where this approximation will be <strong>arbitrarily precise</strong> as <span class="math inline">\(m\to\infty.\)</span> (So, we can effectively ignore this type of approximation error.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>n                <span class="ot">&lt;-</span> <span class="fu">length</span>(observedSample)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>Xbar             <span class="ot">&lt;-</span> <span class="fu">mean</span>(observedSample)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>m                <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co"># number of bootstrap samples </span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Xbar_boot        <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">"double"</span>, <span class="at">length =</span> m)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="fu">seq_len</span>(m)){</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a> bootSample          <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x       =</span> observedSample, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">size    =</span> n, </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a> Xbar_boot[k]        <span class="ot">&lt;-</span> <span class="fu">mean</span>(bootSample)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>( Xbar_boot <span class="sc">-</span> Xbar ), </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Approximate Bootstrap Distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To approximate, for instance, the standard error of <span class="math inline">\(\bar{X},\)</span> we can now simply use the <strong>empirical standard deviation</strong> of <span class="math inline">\(\bar{X}^*_k,\)</span> <span class="math inline">\(k=1,\dots,m.\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sd</span>(Xbar_boot), <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.34</code></pre>
</div>
</div>
<!-- to test the null-hypothesis 
<center>
H$_0:$ $\mu = \mu_0$
</center>
against the alternative hypothesis that 
<center>
H$_1:$ $\mu \neq \mu_0$
</center>
we can use the simulated distribution of $\bar{X}^*_k,$ $k=1,\dots,m.$ 

Under H$_0,$ we know that the true (unknown) mean equals the hypothetical mean $\mu = \mu_0,$ and we know that $\bar{X}\approx \mu$ if $n$ is large.   -->
<section id="theory-the-bootstrap-distribution-of-bar-x--barx" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="theory-the-bootstrap-distribution-of-bar-x--barx">Theory: The Bootstrap Distribution of <span class="math inline">\(\bar X^*- \bar{X}\)</span></h4>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Notation <span class="math inline">\(\mathbb{E}^*(\cdot),\)</span> <span class="math inline">\(Var^*(\cdot),\)</span> and <span class="math inline">\(P^*(\cdot)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the bootstrap literature one frequently finds the notation <span class="math display">\[
\mathbb{E}^*(\cdot),\;Var^*(\cdot),\;\text{and}\;P^*(\cdot)
\]</span> to denote the <strong>conditional</strong> expectation <span class="math display">\[
\mathbb{E}^*(\cdot)=\mathbb{E}(\cdot|\mathcal{S}_n),
\]</span> the <strong>conditional</strong> variance <span class="math display">\[
Var^*(\cdot)=Var(\cdot|\mathcal{S}_n),
\]</span> and the <strong>conditional</strong> probability <span class="math display">\[
P^*(\cdot)=P(\cdot|\mathcal{S}_n),
\]</span> given the sample <span class="math inline">\({\cal S}_n.\)</span></p>
</div>
</div>
<p>The bootstrap focuses on the <strong>conditional</strong> distribution of <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> given the observed sample <span class="math inline">\({\cal S}_n=\{X_1,\dots,X_n\}\)</span> and the resulting conditional distribution of <span class="math display">\[
(\bar X^* -\bar X)|\mathcal{S}_n.
\]</span> These conditional distributions are usually called <strong>bootstrap distributions</strong>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
We know the distribution of <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can analyze the bootstrap distribution of <span class="math inline">\(\bar X^* -\bar X\)</span>, since <strong>we <em>know</em> ü§ü the discrete distribution</strong> of the conditional random variables <span class="math display">\[
X_i^*|\mathcal{S}_n,\;i=1,\dots,n,
\]</span> even though, we do <strong>not know</strong> the distribution of <span class="math inline">\(X_i\sim F,\)</span> <span class="math inline">\(i=1,\dots,n.\)</span></p>
<!-- The discrete distribution of the conditional random variables 
$X_i^*|\mathcal{S}_n,$ is 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\}
$$
with  -->
</div>
</div>
<p>For each <span class="math inline">\(i=1,\dots,n\)</span>, the possible values of the discrete random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> are <span class="math display">\[
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\},
\]</span> and each of these values is equally probable <span class="math display">\[
\begin{align*}
P^*(X_i^*=X_1)&amp;= P(X_i^*=X_1|{\cal S}_n) = \frac{1}{n} \\[2ex]
P^*(X_i^*=X_2)&amp;= P(X_i^*=X_2|{\cal S}_n) = \frac{1}{n} \\[2ex]
&amp;\vdots\\[2ex]
P^*(X_i^*=X_n)&amp;= P(X_i^*=X_n|{\cal S}_n) = \frac{1}{n}.
\end{align*}
\]</span></p>
<p>Thus, <strong>we know the whole distribution</strong> of the (conditional) discrete random variable <span class="math inline">\(X_i^*|\mathcal{S}_n\)</span> and, therefore, can compute, for instance, easily its conditional mean and its variance.</p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(X_i^*\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{E}^*(X_i^*)
&amp;=\mathbb{E}(X_i^*|{\cal S}_n)\\[2ex]
&amp;=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_n\\[2ex]
&amp;=\bar X
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(X_i^*\)</span> is <span class="math display">\[
\begin{align*}
Var^*(X_i^*)
&amp;=Var(X_i^*|{\cal S}_n)\\[2ex]
&amp;=\mathbb{E}((X_i^* - \mathbb{E}(X_i^*|{\cal S}_n))^2|{\cal S}_n)\\[2ex]
&amp;=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2\\[2ex]
&amp;=\hat\sigma^2
\end{align*}
\]</span></p></li>
</ul>
<p>That is, in the bootstrap sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> the ‚Äúpopulation‚Äù mean and the ‚Äúpopulation‚Äù variance are equal to the empirical mean, <span class="math inline">\(\bar{X},\)</span> and the empirical variance, <span class="math inline">\(\hat{\sigma}^2,\)</span> of the original sample <span class="math inline">\(X_1,\dots,X_n.\)</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bootstrap-World vs Population-World
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>bootstrap distribution</strong> of <span class="math display">\[
\hat\theta^*_n-\hat\theta_n
\]</span> is used to approximate the unknown distribution of <span class="math display">\[
\hat\theta_n-\theta_0
\]</span></p>
<p><strong>Note:</strong> For the bootstrap distribution <span class="math inline">\(\hat\theta_n\)</span> is a ‚Äúpopulation parameter‚Äù.</p>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
General case: Conditional moments of transformed <span class="math inline">\(g(X_i^*)\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any (measurable) function <span class="math inline">\(g\)</span> we have <span class="math display">\[
\mathbb{E}^*(g(X_i^*))=\mathbb{E}(g(X_i^*)|\mathcal{S}_n)=\frac{1}{n}\sum_{i=1}^n g(X_i).
\]</span> For instance, <span class="math inline">\(g(X_i)=1_{(X_i\leq \delta)}.\)</span></p>
</div>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Caution: Conditioning on <span class="math inline">\(\mathcal{S}_n\)</span> in important!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Conditioning on the observed sample <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> is very important.</p>
<p>The unconditional distribution of <span class="math inline">\(X_i^*\)</span> is equal to the <strong>unknown distribution</strong> <span class="math inline">\(F\)</span> of <span class="math inline">\(X_i.\)</span> This can be seen from the following derivation: <span class="math display">\[
\begin{align*}
P(X_i^*\leq \delta)
&amp;= P(1_{(X_i^*\leq \delta)}=1) \\[2ex]
&amp;= P(1_{(X_i^*\leq \delta)}=1) \cdot 1 + P(1_{(X_i^*\leq \delta)}=0) \cdot 0\\[2ex]
&amp;= E\left(1_{\left(X_i^*\leq \delta\right)}\right)\\[2ex]
&amp;= E\left[E\left(1_{\left(X_i^*\leq \delta\right)}|\mathcal{S}_n\right)\right]\\[2ex]
&amp;= E\left[\frac{1}{n}\sum_{i=1}^n 1_{\left(X_i\leq \delta\right)}\right]\\[2ex]
&amp;= \frac{n}{n}E\left[1_{\left(X_i\leq \delta\right)}\right]\\[2ex]
&amp;= P\left(X_i\leq \delta\right)=F(\delta)
\end{align*}
\]</span></p>
</div>
</div>
<p><strong>Now consider the bootstrap distribution of <span class="math inline">\(\bar X^*\)</span></strong></p>
<!-- We know the distribution of the i.i.d. random variables 
$$
X_i^*|\mathcal{S}_n, \quad i=1,\dots,n,
$$
it is straight forward to derive the asymptotic distribution of 
$$
(\bar X^*-\bar{X})|\mathcal{S}_n
$$ 
using the central limit theorem.  -->
<p>Firstly, let us derive the conditional mean and variance of <span class="math display">\[
\bar X^* = \frac{1}{n}\sum_{i=1}^nX_i^*.
\]</span></p>
<ul>
<li><p>The conditional mean of <span class="math inline">\(\bar X^*\)</span> is <span class="math display">\[
\begin{align*}
\mathbb{E}^*(\bar X^*)
&amp;=\mathbb{E}(\bar X^*|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^n\mathbb{E}(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n}\sum_{i=1}^n \bar X\\
&amp;=\frac{n}{n}\bar X \\
&amp;=\bar X
\end{align*}
\]</span></p></li>
<li><p>The conditional variance of <span class="math inline">\(\bar X^*\)</span> is <span class="math display">\[
\begin{align*}
Var^*(\bar X^*)
&amp;=Var(\bar X^*|{\cal S}_n)\\
&amp;=\frac{1}{n^2}\sum_{i=1}^n Var(X_i^*|{\cal S}_n)\\
&amp;=\frac{1}{n^2}\sum_{i=1}^n \hat\sigma^2\\
&amp;=\frac{n}{n^2}\hat\sigma^2\\
&amp;=\frac{1}{n}\hat\sigma^2,
\end{align*}
\]</span> where <span class="math inline">\(\hat{\sigma}=\sqrt{\frac{1}{n}\sum_{i=1}^n\left(X_i - \bar{X}\right)^2}.\)</span></p></li>
</ul>
<!-- Next, we can check whether we can apply the classical Lindeberg-L√©vy CLT. 

::: {.callout-tip}

## Good news: We can apply the CLT to $\bar X^*|\mathcal{S}_n$

Conditionally on ${\cal S}_n=\{X_1,\dots,X_n\}$,  

* the random variables $X_1^*,\dots,X_n^*$ are i.i.d.
* with mean $\mathbb{E}^*(X_i^*)=\bar X$ 
* and variance $Var^*(X^*)=\hat\sigma^2$

Thus, we can apply the [central limit theorem (Lindeberg-L√©vy)](https://www.statlect.com/asymptotic-theory/central-limit-theorem) to the appropriately standardized bootstrap sample mean $\hat{X}^*$ conditionally on ${\cal S}_n$
$$
\left(\left.\frac{\sqrt{n}(\bar X^* - \bar X)}{\hat\sigma}\right|{\cal S}_n\right)
$$
::: -->
<p>An appropriate central limit theorem argument implies that <span class="math display">\[
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>Moreover, <span class="math inline">\(\hat\sigma^2\)</span> is a consistent estimator of <span class="math inline">\(\sigma^2,\)</span> and thus asymptotically <span class="math inline">\(\hat\sigma^2\)</span> may be replaced by <span class="math inline">\(\sigma\)</span>. Therefore, <span class="math display">\[
\begin{align*}
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X^* -\bar X)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
\]</span></p>
<p>On the other hand, by the CLT, we also have that <span class="math display">\[
\begin{align*}
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right)
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X - \mu)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
\]</span></p>
<p>This means that the bootstrap is <strong>consistent</strong>, since the bootstrap distribution of <span class="math display">\[
\sqrt{n}(\bar X^* -\bar X)|{\cal S}_n
\]</span> asymptotically <span class="math inline">\((n\rightarrow\infty)\)</span> coincides with the distribution of <span class="math display">\[
\sqrt{n}(\bar X-\mu).
\]</span> In other words, for large <span class="math inline">\(n\)</span>, <span class="math display">\[
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
\]</span></p>
<p>This bootstrap consistency result justifies using the bootstrap distribution <span class="math display">\[
P(\bar{X}^*-\bar{X}\leq \delta|\mathcal{S}_n) \approx
\frac{1}{m}\sum_{k=1}^m 1_{( \bar X^*_k-\bar X\leq \delta)},  
\]</span> which we can approximate (arbitrary precise as <span class="math inline">\(m\to\infty\)</span>) using the bootstrap realizations <span class="math display">\[
\bar{X}^*_1,\;\bar{X}^*_2, \dots, \bar{X}^*_m.
\]</span></p>
<!-- 
### Example: Inference about a Population Proportion

**Setup:** 

* **Data:** i.i.d. random sample $X_1,\dots,X_n,$ where $X_i\in\{0,1\}$ is dichotomous and $P(X_i=1)=p$, $P(X_i=0)=1-p$. 
* **Estimator:** Let 
$$
S=\sum_{i=1}^n 1_{(X_i = 1)}
$$ 
denote the number of $X_i$ which are equal to $1.$ Then, the  maximum likelihood estimate of $p$ is 
$$
\hat p=\frac{1}{n}S.
$$
* **Inference Problem:** What is the distribution of 
$$
(\hat{p} - p)?
$$



::: {.callout-note}

## Recall Asymptotics:

* $n\hat p=S\sim \mathcal{Binom}(n,p)$
* As $n\rightarrow\infty,$ the central limit theorem implies that
$$
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
$$
Thus for $n$ large, the distributions of $\sqrt{n}(\hat p -p)$ and $\hat p -p$ can be approximated by $\mathcal{N}(0,p(1-p))$ and $\mathcal{N}(0,p(1-p)/n)$, respectively.

::: 

**Bootstrap Approach:**

* Random sample $X_1^*,\dots,X_n^*$  generated by drawing observations
independently and with replacement from
$$
{\cal S}_n:=\{X_1,\dots,X_n\}.
$$ 
* Let 
$$
S^*=\sum_{i=1}^n 1_{(X_i^* = 1)}
$$  
denote the number of $X_i^*$ which are equal to $1.$
* Bootstrap estimate of $p$: 
$$
\hat p^*=\frac{1}{n}S^*
$$

The bootstrap now tries to approximate the true distribution of $\hat p - p$ by the **conditional** distribution of $(\hat p^*-\hat p)|\mathcal{S}_n$ given the observed sample ${\cal S}_n,$ where the latter can be approximated arbitrarily well $(m\to\infty)$ using the bootstrap estimators 
$$
p^*_1,p^*_2,\dots,p^*_m;
$$
namely by
$$
P\left(\hat{p}^* - \hat{p} \leq \delta|\mathcal{S}_n\right)\approx \frac{1}{m}\sum_{k=1}^m 1_{(\hat{p}^*_k - \hat{p} \leq\delta )}. 
$$

The bootstrap is called **consistent** if asymptotically $(n\rightarrow \infty)$ the conditional distribution of $(\hat p^*-\hat p)|{\cal S}_n$  coincides with the true distribution of $\hat p - p.$ (Note: a proper scaling is required!)

**The distribution of $X_i^*|\mathcal{S}_n$**

The conditional random variable $X_i^*|\mathcal{S}_n$ is a binary random variable 
$$
X_i^*|\mathcal{S}_n\in\{0,1\}.
$$
Since $X_i^*$ is drawn independently and with replacement from $\mathcal{S}_n,$ we obtain for each $i=1,\dots,n,$
$$
\begin{align*}
& P^*(X_i^*=1)=P(X_i^*=1|{\cal S}_n)=\hat p, \\[2ex]  
& P^*(X_i^*=0)=P(X_i^*=0|{\cal S}_n)=1-\hat p.
\end{align*}
$$
Thus, $X_i^*|{\cal S}_n$ is a Bernoulli distributed random variable with parameter $p=\hat{p}$
$$
X_i^*|{\cal S}_n \sim\mathcal{Bern}(p=\hat p), \quad i=1,\dots,n.\\[5ex]
$$


**The distribution of $\hat{p}^*|\mathcal{S}_n$**

The above implies that $n \hat{p}^*|{\cal S}_n$ has a Binomial distribution with parameters $n$ and $p=\hat{p},$  
$$
\underbrace{n \hat{p}_i^*}_{=S^*}|{\cal S}_n \sim\mathcal{Binom}(n, p=\hat p), \quad i=1,\dots,n.
$$

Therefore,
$$
\begin{align*}
\mathbb{E}^*(n \hat p^*)
&=\mathbb{E}(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p}\\[2ex]
\Rightarrow \mathbb{E}^*(\hat p^*) & = \hat{p}
\end{align*}
$$
and 
$$
\begin{align*}
Var^*(n \hat p^*)
&=Var(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p} (1- \hat{p})\\[2ex]
\Rightarrow Var^*(\hat p^*) & = \frac{\hat{p}(1-\hat{p})}{n}
\end{align*}
$$

An appropriate central limit theorem argument implies that 
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$

Moreover, $\hat p$ is a consistent estimator of $p,$ and thus 
$$
\hat p(1-\hat p)\rightarrow_p p(1-p),\quad n\rightarrow\infty.
$$ 
Therefore, $\hat p(1-\hat p)$ can be replaced asymptotically by $p(1-p)$, and
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right)\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$
So, we can conclude that, 
$$
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0
$$
as $n\rightarrow\infty,$ where $\Phi$ denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large $n$
$$
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx 
\text{distribution}(\sqrt{n}(\hat p -p))%\approx N(0,p(1-p))
$$
and therefore also
$$
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx 
\text{distribution}(\hat p -p).%\approx N(0,p(1-p)/n)
$$
-->
</section>
</section>
<section id="confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Recall: The traditional (non-bootstrap) approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if</p>
<ul>
<li><span class="math inline">\(\theta\in\mathbb{R}\)</span> and</li>
<li><span class="math inline">\(\sqrt{n}(\hat\theta-\theta)\rightarrow_d\mathcal{N}(0,v^2)\)</span> as <span class="math inline">\(n\to\infty,\)</span></li>
</ul>
<p>then one traditionally tries to determine an approximation <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span> (i.e.&nbsp;the standard error of <span class="math inline">\(\hat\theta\)</span>) from the data. An approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval is then given by <span class="math display">\[
\left[
\hat{\theta}-z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}},
\hat{\theta}+z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}}
\right]
\]</span></p>
<p>In some cases it is, however, very difficult to obtain approximations <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span>. Statistical inference is then usually based on the bootstrap.</p>
</div>
</div>
<p>In contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates <span class="math inline">\(\hat v\)</span> of <span class="math inline">\(v\)</span> are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed <strong>more precise</strong> than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)</p>
<section id="the-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-bootstrap-approach">The Bootstrap Approach</h4>
<!-- General approach: Basic bootstrap $(1-\alpha)\times 100\%$ confidence interval -->
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> i.i.d. random sample <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> with <span class="math inline">\(X_i\sim F\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>, where the distribution <span class="math inline">\(F\)</span> depends on the unknown parameter (vector) <span class="math inline">\(\theta.\)</span></li>
<li><strong>Problem:</strong> Construct a confidence interval for <span class="math inline">\(\theta.\)</span></li>
</ul>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Assumption: Bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the following, we will assume that the bootstrap is consistent; i.e.&nbsp;that <span class="math display">\[
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^* -\hat{\theta})|{\cal S}_n)
&amp;\approx
\text{distribution}(\sqrt{n}(\hat{\theta}-\theta))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*-\hat{\theta})|{\cal S}_n
&amp;\overset{d}{\approx} \sqrt{n}(\hat{\theta}-\theta)
\end{align*}
\]</span> if <span class="math inline">\(n\)</span> is sufficiently large.</p>
<p>Caution: This is not always the case and in cases of doubt one needs to show this property.</p>
</div>
</div>
<p><strong>Derivation of the nonparametric bootstrap confidence intervals:</strong></p>
<ul>
<li><p>We can generate <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\theta_k^*\equiv\hat\theta(X_{1k}^*,\dots,X_{nk}^*),\quad k=1,\dots,m,
\]</span> by drawing bootstrap samples <span class="math inline">\(X_{1k}^*,\dots,X_{nk}^*\)</span> independently and with replacement from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}.\)</span></p></li>
<li><p>The <span class="math inline">\(m\)</span> bootstrap estimates allow us to approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> quantile <span class="math inline">\(\hat t_\frac{\alpha}{2}\)</span> and the <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantile <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> of the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}.\)</span> This can be done with negligible approximation error (for <span class="math inline">\(m\)</span> large) using the empirical quantiles <span id="eq-empiricalQuantile"><span class="math display">\[
\hat t_{p}=\left\{
\begin{array}{ll}
\hat\theta^*_{(\lfloor mp\rfloor+1)},         &amp; mp \text{ not a whole number}\\
(\hat\theta^*_{(mp)}+\hat\theta^*_{(mp+1)})/2,&amp; mp \text{ a whole number}
\end{array}\right.
\tag{3}\]</span></span> for <span class="math inline">\(p=\frac{\alpha}{2}\)</span> or <span class="math inline">\(p=1-\frac{\alpha}{2},\)</span> where <span class="math inline">\(\hat\theta_{(i)}^*\)</span> denotes the order statistic <span class="math display">\[
\hat\theta_{(1)}^* \leq \hat\theta_{(2)}^*\leq \dots\leq \hat\theta_{(m)}^*,
\]</span> and <span class="math inline">\(\lfloor mp\rfloor\)</span> denotes the greatest whole number less than or equal to <span class="math inline">\(mp\)</span> (e.g.&nbsp;<span class="math inline">\(\lfloor 4.9\rfloor = 4\)</span>).</p></li>
</ul>
<p>Then <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat t_\frac{\alpha}{2} \leq \hat{\theta}^* \leq \hat t_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &amp; P^*\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}^*-\hat{\theta} \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &amp; P^*\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)
\approx 1-\alpha,
\end{align*}
\]</span> where the approximation becomes arbitrarily precise for <span class="math inline">\(m\to\infty.\)</span> Here, <span class="math inline">\(P^*\)</span> denotes probabilities with respect to the conditional distribution of <span class="math inline">\(\hat{\theta}^*\)</span> given <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span>.</p>
<p>Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> <span class="math display">\[
{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}-\theta)}.
\]</span> Therefore, for large <span class="math inline">\(n,\)</span> <span class="math display">\[
\begin{align*}
&amp;P\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{blue}\sqrt{n}(\hat{\theta}-\theta)}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)\approx 1-\alpha\\[2ex]
\Rightarrow &amp;P\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}-\theta \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &amp;P\left(\hat{\theta}-(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\le \theta\le \hat{\theta}-
(\hat t_\frac{\alpha}{2}-\hat{\theta})\right)\approx 1-\alpha\\[2ex]
\Rightarrow &amp;P\left(2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}\le \theta\le 2\hat{\theta}-
\hat t_\frac{\alpha}{2}\right)\approx 1-\alpha.
\end{align*}
\]</span></p>
<p>Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) <strong>bootstrap confidence interval</strong> is given by <span id="eq-NPBootCI"><span class="math display">\[
\left[2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}, 2\hat{\theta}-\hat t_\frac{\alpha}{2}\right],
\tag{4}\]</span></span> where <span class="math inline">\(\hat t_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> are the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles of the bootstrap distribution approximated by the empirical quantiles of the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math inline">\(\hat{\theta}^*_1, \hat{\theta}^*_2,\dots, \hat{\theta}^*_m.\)</span></p>
</section>
<section id="example-confidence-intervals-for-the-population-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-confidence-intervals-for-the-population-mean">Example: Confidence Intervals for the Population Mean</h4>
<p><strong>Setup:</strong></p>
<ul>
<li><strong>Data:</strong> Let <span class="math inline">\(X_1,\dots,X_n\)</span> denote an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2.\)</span> <!-- * In the following $F$ will denote the corresponding distribution function; i.e., $X_i\sim F$ for all $i=1,\dots,n.$ --></li>
<li><strong>Estimator:</strong> <span class="math inline">\(\bar X=\frac{1}{n} \sum_{i=1}^n X_i\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span></li>
<li><strong>Inference Problem:</strong> Construct a confidence interval for <span class="math inline">\(\mu.\)</span></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Recall: The traditional (non-bootstrap) approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional, non-bootstrap approach for constructing a <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval:</p>
<ul>
<li>By the CLT: <span class="math inline">\(\bar X\overset{a}{\sim} \mathcal{N}(\mu,\frac{\sigma^2}{n})\)</span> for large <span class="math inline">\(n\)</span></li>
<li>Estimation of <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2\)</span></li>
<li>This implies: <span class="math inline">\(\sqrt{n}((\bar X -\mu)/s)\overset{a}{\sim} t_{n-1}\)</span>, and hence <span class="math display">\[
\begin{align*}
&amp;P\left(-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\le \bar X -\mu\le t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\right)\approx 1-\alpha\\
\Rightarrow
&amp;P\left(\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\le \mu\le
      \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}
\right)\approx 1-\alpha
\end{align*}
\]</span></li>
<li><span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval: <span class="math display">\[
\left[\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}},
    \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}}\right],
\]</span> where <span class="math inline">\(t_{n-1,1-\frac{\alpha}{2}}\)</span> denotes the <span class="math inline">\(1-\frac{\alpha}{2}\)</span>-quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This traditional construction relies on the assumption that <span class="math inline">\(\bar X\)</span> is exactly normal distributed, also for small <span class="math inline">\(n,\)</span> which requires that the random sample <span class="math inline">\(X_1,\dots,X_n\)</span> is i.i.d. <em>normally</em> distributed.</p>
<p>If the underlying distribution is <strong>not normal</strong>, then this normal distribution holds <em>approximately</em> if the sample size <span class="math inline">\(n\)</span> is sufficiently large (central limit theorem), i.e., <span class="math display">\[
\bar X \overset{a}{\sim}\mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right).
\]</span> In this case the constructed confidence interval is an <em>approximate</em> <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval.</p>
</div>
</div>
</div>
</div>
</section>
<section id="the-nonparametric-standard-bootstrap-approach" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-nonparametric-standard-bootstrap-approach">The Nonparametric (Standard) Bootstrap Approach</h4>
<p>The bootstrap offers an alternative method for constructing approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals. We already know that the bootstrap is consistent in this situation.</p>
<p><strong>Construction of the nonparametric (standard) bootstrap confidence interval:</strong></p>
<ul>
<li>Draw <span class="math inline">\(m\)</span> bootstrap samples (e.g.&nbsp;<span class="math inline">\(m=10,000\)</span>) and calculate the corresponding estimates <span class="math inline">\(\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m\)</span>.</li>
<li>Compute the empirical quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2}}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2}}\)</span> from <span class="math inline">\(\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m\)</span></li>
<li>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) nonparametric bootstrap confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;4</a>: <span class="math display">\[
\left[2\bar X-\hat t_{1-\frac{\alpha}{2}},
    2\bar X-\hat t_\frac{\alpha}{2}\right]
\]</span></li>
</ul>
</section>
</section>
</section>
<section id="pivot-statistics-and-the-bootstrap-t-method" class="level2">
<h2 class="anchored" data-anchor-id="pivot-statistics-and-the-bootstrap-t-method">Pivot Statistics and the Bootstrap-<span class="math inline">\(t\)</span> Method</h2>
<p>In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-<span class="math inline">\(t\)</span> method (one also speaks of the ‚Äústudentized bootstrap‚Äù). The construction relies on so-called pivotal statistics.</p>
<p>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be an i.i.d. random sample and assume that the distribution of <span class="math inline">\(X\)</span> depends on an unknown parameter (or parameter vector) <span class="math inline">\(\theta\)</span>.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-pivotal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Asymptotically pivotal statistics) </strong></span><em>A statistic <span class="math display">\[
T_n\equiv T(X_1,\dots,X_n)
\]</span> is called (exact) <strong>pivotal</strong>, if the distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter. A statistic <span class="math inline">\(T_n\)</span> is called <strong>asymptotically pivotal</strong>, if the asymptotic distribution of <span class="math inline">\(T_n\)</span> does not depend on any unknown population parameter.</em></p>
</div>
</div>
</div>
<p><em>Exact</em> pivotal statistics are rare and not available in most statistical or econometric applications. It is, however, often possible to construct an <em>asymptotically pivotal</em> statistic. Assume that an estimator <span class="math inline">\(\hat{\theta}\)</span> satisfies <span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta)\rightarrow_d\mathcal{N}(0,v^2),
\]</span> where <span class="math inline">\(v^2\)</span> denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator <span class="math display">\[
\hat v_n^2\equiv \hat v_n(X_1,\dots,X_n)^2
\]</span> of <span class="math inline">\(v\)</span> such that <span class="math display">\[
\hat v_n^2 \rightarrow_p v^2.
\]</span> Then, of course, also <span class="math inline">\(\hat v_n\rightarrow_p v\)</span>, and <span class="math display">\[
T_n:= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
\]</span> This means that <span class="math display">\[
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
\]</span> is <strong>asymptotically pivotal</strong>.</p>
<section id="example-barx-is-asymptotically-pivotal" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-barx-is-asymptotically-pivotal">Example: <span class="math inline">\(\bar{X}\)</span> is Asymptotically Pivotal</h4>
<p>Let <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\}\)</span> be a i.i.d. random sample with <span class="math inline">\(X_i\sim X\)</span> for all <span class="math inline">\(i=1,\dots,n,\)</span> with mean <span class="math inline">\(\mathbb{E}(X)=\mu\)</span>, variance <span class="math inline">\(Var(X)=\sigma^2&gt;0\)</span>, and <span class="math inline">\(\mathbb{E}(|X|^4)=\beta&lt;\infty\)</span>.</p>
<p>If <span class="math inline">\(X\)</span> is normally distributed, we obtain <span class="math display">\[
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{s}\sim t_{n-1}
\]</span> with <span class="math inline">\(s^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2\)</span>, where <span class="math inline">\(t_{n-1}\)</span> denotes the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. We can conclude that <span class="math inline">\(T_n\)</span> is pivotal.</p>
<p>If <span class="math inline">\(X\)</span> is <em>not</em> normally distributed, the central limit theorem implies that <span class="math display">\[
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{s}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span> In this case <span class="math inline">\(T_n\)</span> is an asymptotically pivotal statistics.</p>
</section>
<section id="bootstrap-t-consistency" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-t-consistency">Bootstrap-<span class="math inline">\(t\)</span> Consistency</h4>
<p>The general idea of the bootstrap-<span class="math inline">\(t\)</span> method relies on approximating the unknown distribution of <span class="math display">\[
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
\]</span> by the approximable (via bootstrap resampling) conditional distribution of <span class="math display">\[
T_n^*=\sqrt{n}\frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*},
\]</span> given <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> where the variance estimate <span class="math inline">\(v_n^*\)</span> is computed from the bootstrap sample <span class="math inline">\(X_1^*,\dots,X_n^*,\)</span> i.e. <span class="math display">\[
\hat v_n^*=v_n(X_1^*,\dots,X_n^*).
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Good news: Bootstrap-<span class="math inline">\(t\)</span> consistency follows if the standard nonparametric bootstrap is consistent
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the standard nonparametric bootstrap is consistent, i.e.&nbsp;if the conditional distribution of <span class="math inline">\(\sqrt{n}(\hat{\theta}^*-\hat{\theta})|\mathcal{S}_n\)</span>, given <span class="math inline">\(\mathcal{S}_n\)</span>, yields a consistent estimate of <span class="math inline">\(\mathcal{N}(0,v^2)\)</span>, then also the bootstrap-<span class="math inline">\(t\)</span> method is consistent. That is, then the conditional distribution of <span class="math inline">\(T_n^*|\mathcal{S}_n\)</span>, given <span class="math inline">\(\mathcal{S}_n\)</span>, provides a consistent estimate of the asymptotic distribution of <span class="math inline">\(T_n\rightarrow_d \mathcal{N}(0,1)\)</span> such that <span class="math display">\[
\sup_\delta \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0,
\]</span> where <span class="math inline">\(\Phi\)</span> denotes the distribution function of the standard normal distribution.</p>
</div>
</div>
</section>
<section id="bootstrap-t-confidence-interval" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-t-confidence-interval">Bootstrap-t Confidence Interval</h3>
<p>Let <span class="math inline">\({\cal S}_n:=\{X_1,\dots,X_n\}\)</span> be an i.i.d. random sample from <span class="math inline">\(X\sim F\)</span> with unknown parameter (vector) <span class="math inline">\(\theta.\)</span></p>
<p>Assume that the bootstrap is consistent and that the estimator <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is asymptotically normal.</p>
<p>Furthermore, suppose that a <strong>consistent</strong> estimator <span class="math display">\[
\hat\sigma^2\equiv \hat\sigma^2(X_1,\dots,X_n)
\]</span> of the variance of the estimator <span class="math inline">\(\hat{\theta}\)</span> is available. (A typicall connection between <span class="math inline">\(\hat\sigma^2\)</span> and <span class="math inline">\(\hat{v}^2\)</span> is <span class="math inline">\(\hat\sigma^2=\hat{v}^2/n\)</span>.)</p>
<p>Thus, <span class="math inline">\(\hat\sigma\)</span> denotes the estimator of the standard error of <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p><strong>Derivation of the bootstrap-<span class="math inline">\(t\)</span> confidence interval:</strong></p>
<ul>
<li><p>Based on an i.i.d. re-sample <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\(\mathcal{S}_n=\{X_1,\dots,X_n\},\)</span> calculate the bootstrap estimates <span class="math display">\[
\hat{\theta}^*\equiv \hat{\theta}^*(X_1^*,\dots,X_n^*)
\]</span> and <span class="math display">\[
\hat\sigma^*\equiv \hat\sigma^*(X_1^*,\dots,X_n^*)
\]</span> and the bootstrap statistic <span class="math display">\[
\begin{align*}
T^*&amp;=T^*(X_1^*,\dots,X_n^*)\\
&amp;=\frac{\hat{\theta}^*-\hat{\theta}}{\hat\sigma^*}.
\end{align*}
\]</span> Repeating this yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) many bootstrap statistics <span class="math display">\[
T_1^*,T_2^*, \dots, T_m^*
\]</span> which allow us to approximate the bootstrap distribution of <span class="math inline">\(T^*=\frac{\hat{\theta}^*-\hat{\theta}}{\hat \sigma^*},\)</span> conditionally on <span class="math inline">\(\mathcal{S}_n,\)</span> arbitrarily precise as <span class="math inline">\(m\to\infty.\)</span></p></li>
<li><p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2}}\)</span> of the bootstrap distribution of <span class="math display">\[
\left.\frac{\hat{\theta}^*-\hat{\theta}}{\hat \sigma^*}\right|\mathcal{S}_n
\]</span> using the empirical quantiles based on <span class="math inline">\(T_1^*,T_2^*, \dots, T_m^*\)</span> (see <a href="#eq-empiricalQuantile">Equation&nbsp;3</a>).</p></li>
</ul>
<p>This implies, for large <span class="math inline">\(m,\)</span> <span class="math display">\[
\begin{align*}
&amp;P^*\left(\hat \tau_\frac{\alpha}{2}\leq {\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat \sigma^*}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha
\end{align*}
\]</span> Due to the assumed consistency of the bootstrap, we have that for large <span class="math inline">\(n\)</span> that <span class="math display">\[
\left.{\color{red}\frac{\hat{\theta}^*-\hat{\theta}}{\hat \sigma^*}}\right|\mathcal{S}_n\overset{d}{\approx}
{\color{blue}\frac{\hat{\theta}-\theta}{\hat \sigma}}.
\]</span> Therefore, for lage <span class="math inline">\(n,\)</span> <span class="math display">\[
\begin{align*}
&amp; P\left(\hat \tau_\frac{\alpha}{2}\leq {\color{blue}\frac{\hat{\theta}-\theta}{\hat \sigma}} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P\left(-\hat \sigma \hat \tau_{1-\frac{\alpha}{2}}\leq \theta-\hat{\theta} \leq -\hat \sigma\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha\\
\Rightarrow &amp; P\left(\hat{\theta}-\hat \sigma \hat \tau_{1-\frac{\alpha}{2}}\leq \theta \leq \hat{\theta} -\hat \sigma\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha
\end{align*}
\]</span> Thus, the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) bootstrap-<span class="math inline">\(t\)</span> confidence interval is given by <span id="eq-Boot_tCI"><span class="math display">\[
\left[\hat{\theta}-\hat \tau_{1-\frac{\alpha}{2}}\hat \sigma,\;
      \hat{\theta}-\hat \tau_{  \frac{\alpha}{2}}\hat \sigma\right]
\tag{5}\]</span></span></p>
<p>Note that the above confidence interval gets tight (i.e.&nbsp;its width converges to zero) as <span class="math inline">\(n\to\infty\)</span> since <span class="math inline">\(\hat{\sigma}\)</span> converges to zero proportionally to <span class="math inline">\(1/\sqrt{n}\)</span>.</p>
<section id="example-bootstrap-t-confidence-interval-for-the-mean" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="example-bootstrap-t-confidence-interval-for-the-mean">Example: Bootstrap-<span class="math inline">\(t\)</span> Confidence Interval for the Mean</h4>
<p>Here <span class="math inline">\(\hat\theta = \bar{X}\)</span> and the estimator of the variance of <span class="math inline">\(\bar{X}\)</span> is <span class="math inline">\(\hat\sigma^2=s^2/n\)</span>, where <span class="math inline">\(s^2\)</span> denotes the sample variance <span class="math display">\[
s^2=\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2.
\]</span></p>
<p><strong>Algorithm:</strong></p>
<ul>
<li>Draw i.i.d. random samples <span class="math inline">\(X_1^*,\dots,X_n^*\)</span> from <span class="math inline">\({\cal S}_n\)</span> and calculate <span class="math inline">\(\bar X^*\)</span> as well as <span class="math inline">\(s^*=\sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*)^2}\)</span> to generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) bootstrap realizations <span class="math display">\[
\frac{\bar X^*_1-\bar X}{s^*_1/\sqrt{n}},\dots,\frac{\bar X^*_m-\bar X}{s^*_m/\sqrt{n}}
\]</span></li>
<li>Determine <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_\frac{\alpha}{2}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2}}\)</span> from <span class="math display">\[
\frac{\bar X^*_1-\bar X}{s^*_1/\sqrt{n}},\dots,\frac{\bar X^*_m-\bar X}{s^*_m/\sqrt{n}}\]</span> using <a href="#eq-empiricalQuantile">Equation&nbsp;3</a>.</li>
<li>This yields the <span class="math inline">\(1-\alpha\)</span> confidence interval (using <a href="#eq-Boot_tCI">Equation&nbsp;5</a>): <span class="math display">\[
\left[\bar X-\hat \tau_{1-\frac{\alpha}{2}}\frac{s}{\sqrt{n}},
    \bar X-\hat \tau_{\frac{\alpha}{2}}  \frac{s}{\sqrt{n}}\right],
\]</span> where <span class="math inline">\(s\)</span> is computed from the original sample, i.e., <span class="math display">\[
s=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2}.
\]</span></li>
</ul>
</section>
</section>
<section id="accuracy-of-the-bootstrap-t-method" class="level3">
<h3 class="anchored" data-anchor-id="accuracy-of-the-bootstrap-t-method">Accuracy of the Bootstrap-<span class="math inline">\(t\)</span> method</h3>
<p>Usually, the bootstrap-<span class="math inline">\(t\)</span> provides a <strong>gain in accuracy</strong> over the standard nonparametric bootstrap. The reason is that the approximation of the law of <span class="math inline">\(T_n\)</span> by the bootstrap law of <span class="math display">\[
\left.\frac{\sqrt{n}(\hat{\theta}^*-\hat{\theta})}{v^*_n}\right|\mathcal{S}_n
\]</span> is more direct and hence more accurate (<span class="math inline">\(v^*_n\)</span> depends on the bootstrap sample ‚Äî not the original sample) than by the bootstrap law of <span class="math display">\[
\left.\sqrt{n}(\hat{\theta}^*-\hat{\theta})\right|\mathcal{S}_n.
\]</span></p>
<p>The use of pivotal statistics and the corresponding construction of bootstrap-<span class="math inline">\(t\)</span> confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-<span class="math inline">\(t\)</span> methods are <strong>second order accurate</strong>.</p>
<p>Consider generally <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence intervals of the form <span class="math inline">\([L_n,U_n]\)</span> of <span class="math inline">\(\theta\)</span>. The lower, <span class="math inline">\(L_n\)</span>, and upper bounds, <span class="math inline">\(U_n\)</span>, of such intervals are determined from the data and are thus random, <span class="math display">\[
L_n\equiv L(X_1,\dots,X_n)
\]</span> <span class="math display">\[
U_n\equiv U(X_1,\dots,X_n)
\]</span> and their accuracy depends on the particular procedure applied (e.g.&nbsp;nonparametric bootstrap vs.&nbsp;bootstrap-<span class="math inline">\(t\)</span>).</p>
<ul>
<li>(Symmetric) confidence intervals are said to be <strong>first-order accurate</strong> if there exist some constants <span class="math inline">\(c_1,c_2&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta&lt;L_n)-\frac{\alpha}{2}\right|\le \frac{c_1}{\sqrt{n}}\\
\left|P(\theta&gt;U_n)-\frac{\alpha}{2}\right|\le \frac{c_2}{\sqrt{n}}
\end{align*}
\]</span></li>
<li>(Symmetric) confidence intervals are said to be <strong>second-order accurate</strong> if there exist some constants <span class="math inline">\(c_3,c_4&lt;\infty\)</span> such that for sufficiently large <span class="math inline">\(n\)</span> <span class="math display">\[
\begin{align*}
\left|P(\theta&lt;L_n)-\frac{\alpha}{2}\right|\le \frac{c_3}{n}\\
\left|P(\theta&gt;U_n)-\frac{\alpha}{2}\right|\le \frac{c_4}{n}
\end{align*}
\]</span></li>
</ul>
<p>If the distribution of <span class="math inline">\(\hat\theta\)</span> is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that</p>
<ul>
<li>Standard confidence intervals based on asymptotic approximations are <strong>first-order</strong> accurate.</li>
<li>Nonparametric (standard) boostrap confidence intervals are <strong>first-order</strong> accurate.</li>
<li>Bootstrap-<span class="math inline">\(t\)</span> confidence intervals are <strong>second-order</strong> accurate.</li>
</ul>
<p>The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to <em>much</em> better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Proofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field.</p>
</div>
</div>
</section>
</section>
<section id="regression-analysis-bootstrapping-pairs" class="level2">
<h2 class="anchored" data-anchor-id="regression-analysis-bootstrapping-pairs">Regression Analysis: Bootstrapping Pairs</h2>
<p>Consider the linear regression model <span class="math display">\[
Y_i=X_i^T\beta+ \varepsilon_i,\quad  i=1,\dots,n,
\]</span> where <span class="math inline">\(Y_i\in\mathbb{R}\)</span> denotes the response (or ‚Äúdependent‚Äù) variable and <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
\]</span> denotes the vector of predictor variables. In the following, we differentiate between a <strong>random design</strong> and a <strong>fixed design</strong>.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-RandomFixedDesign" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 (Random and fixed design) </strong></span><br></p>
<p><em><strong>Random Design:</strong> <span class="math display">\[
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
\]</span> are i.i.d. random variables and <span class="math inline">\(\mathbb{E}(\varepsilon_i|X_i)=0\)</span> with either</em></p>
<ul>
<li><em><strong>homoscedastic</strong> errors: <span class="math inline">\(\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, for a constant <span class="math inline">\(\sigma^2&lt;\infty\)</span> or</em></li>
<li><em><strong>heteroscedastic</strong> errors: <span class="math inline">\(\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2(X_i)&lt;\infty\)</span>, <span class="math inline">\(i=1,\dots,n.\)</span></em></li>
</ul>
<p><em><strong>Fixed Design:</strong> <span class="math display">\[
X_1, X_2, \dots, X_n
\]</span> are deterministic vectors in <span class="math inline">\(\mathbb{R}^p\)</span> and <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. random variables with zero mean <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0\)</span> and <strong>homoscedastic errors</strong> <span class="math inline">\(\mathbb{E}(\varepsilon_i^2)=\sigma^2\)</span> for all <span class="math inline">\(i=1,\dots,n.\)</span></em></p>
</div>
</div>
</div>
<p>The least squares estimator <span class="math inline">\(\hat\beta\in\mathbb{R}^p\)</span> is given by <span class="math display">\[
\begin{align*}
\hat\beta
&amp;=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i\\
&amp;=\beta+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i.
\end{align*}
\]</span></p>
<section id="bootstrapping-pairs-bootstrap-under-random-design" class="level3">
<h3 class="anchored" data-anchor-id="bootstrapping-pairs-bootstrap-under-random-design">Bootstrapping Pairs: Bootstrap under Random Design</h3>
<p>Under the random design, we additionally assume that there exists a positive definite (thus invertible) matrix <span class="math inline">\(M\)</span> <span class="math display">\[
M=\mathbb{E}(X_iX_i^T)
\]</span> and a positive semi-definite matrix <span class="math inline">\(Q\)</span> such that <span class="math display">\[
Q=\mathbb{E}(\varepsilon_i^2X_iX_i^T)=\mathbb{E}(\sigma^2(X_i)X_iX_i^T)
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>For homoscedastic errors we have <span class="math display">\[
\begin{align*}
Q
&amp;=\mathbb{E}(\sigma^2(X_i)X_iX_i^T)\\
&amp;=\sigma^2\mathbb{E}(X_iX_i^T)\, =\sigma^2 M.
\end{align*}
\]</span></p>
</div>
</div>
<p>The law of large numbers, the continuous mapping theorem, Slutsky‚Äôs theorem, and the central limit theorem (see your econometrics lecture) implies that <span class="math display">\[
\sqrt{n}(\hat\beta-\beta)\rightarrow_d\mathcal{N}(0,M^{-1}QM^{-1}),\quad n\to\infty.
\]</span></p>
<p>Bootstrapping regression estimates <span class="math inline">\(\hat\beta\)</span> is straightforward under a <strong>random design</strong> (<a href="#def-RandomFixedDesign">Definition&nbsp;5</a>).</p>
<p>Under a random design, <span class="math inline">\((Y_i,X_i)\)</span> are i.i.d. and one may apply the nonparametric bootstrap in order to approximate the distribution of the estimation errors <span class="math display">\[
\hat\beta-\beta.
\]</span> In the literature this procedure is usually called <strong>bootstrapping pairs</strong>, namely, <span class="math inline">\((Y_i, X_i)\)</span>-pairs.</p>
<p><strong>Algorithm:</strong></p>
<ul>
<li>Original data: i.i.d. sample <span class="math inline">\({\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}\)</span></li>
<li>Generate bootstrap samples <span class="math display">\[
(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
\]</span> by drawing observations independently and with replacement from <span class="math inline">\({\cal S}_n.\)</span></li>
<li>Each bootstrap sample <span class="math inline">\((Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)\)</span> leads to a bootstrap realization of the least squares estimator <span class="math display">\[
\hat\beta^*=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
\]</span></li>
</ul>
<p>It can be shown that bootstrapping pairs is <strong>consistent</strong>; i.e.&nbsp;that for large <span class="math inline">\(n\)</span> <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)\approx\mathcal{N}(0,M^{-1}QM^{-1})
\]</span></p>
<section id="confidence-intervals-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="confidence-intervals-1">Confidence Intervals</h4>
<p>This allows to construct basic bootstrap confidence intervals for the <span class="math inline">\(j\)</span>th regression coefficient <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>:</p>
<ul>
<li><p>Generate <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) bootstrap realizations <span class="math display">\[
\hat{\beta}_{j1}^*,\dots,\hat\beta_{jm}^*
\]</span></p></li>
<li><p>Determine the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2},j}\)</span><br>
from the bootstrap realizations <span class="math inline">\(\hat{\beta}_{j1}^*,\dots,\hat\beta_{jm}^*\)</span> using <a href="#eq-empiricalQuantile">Equation&nbsp;3</a>.</p></li>
<li><p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;4</a>: <span class="math display">\[
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j},
    2\hat\beta_j-\hat t_{\frac{\alpha}{2},j}\right]
\]</span></p></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>This basic nonparametric bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are <strong>heteroscedastic</strong>. This is not true for the standard (non-bootstrap) confidence intervals intervals provided by standard software packages.</p>
</div>
</div>
</section>
</section>
</section>
<section id="regression-analysis-residual-bootstrap" class="level2">
<h2 class="anchored" data-anchor-id="regression-analysis-residual-bootstrap">Regression Analysis: Residual bootstrap</h2>
<p>If the sample <span class="math display">\[
(Y_1,X_1),\dots,(Y_n,X_n)
\]</span> is <strong>not</strong> an i.i.d. sample, the bootstrapping pairs procedure proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for <strong>fixed designs</strong> and also generally not in time-series regression contexts. However, if error terms are <strong>homoscedastic</strong>, then it is possible to rely on the <strong>residual bootstrap</strong>.</p>
<p>In the following we will formally assume a regression model <span class="math display">\[
Y_i=X_i^T\beta+ \varepsilon_i, \quad i=1,\dots,n,
\]</span> with <span class="math display">\[
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
\]</span> under <strong>fixed design</strong> (<a href="#def-RandomFixedDesign">Definition&nbsp;5</a>), i.e., where <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are i.i.d. with zero mean <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0\)</span> and <strong>homoscedastic</strong> errors <span class="math display">\[
\mathbb{E}(\varepsilon_i^2)=\sigma^2.
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Applicability of the Residual Bootstrap
</div>
</div>
<div class="callout-body-container callout-body">
<p>Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs or stochastic, but correlated <span class="math inline">\(X\)</span>-variables (time-series). In these cases all arguments are meant conditionally on the given <span class="math inline">\(X_1,\dots,X_n\)</span>. The above assumptions on the error terms then of course have to be satisfied conditionally on <span class="math inline">\(X_1,\dots,X_n\)</span>.</p>
</div>
</div>
<p>The idea of the residual bootstrap is very simple: The model implies that the error terms <span class="math display">\[
\varepsilon_1,\dots,\varepsilon_n
\]</span> are i.i.d which suggests a bootstrap based on resampling the error terms.</p>
<p>These errors are, of course, unobserved, but they can be approximated by the corresponding residuals <span class="math display">\[
\hat \varepsilon_i:=Y_i-X_i^T\hat\beta, \quad i=1,\dots,n,
\]</span> where again <span class="math display">\[
\hat\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
\]</span> denotes the least squares estimator.</p>
<p>It is well known that <span class="math display">\[
\hat\sigma^2:= \frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^2
\]</span> provides an unbiased, consistent estimator of the error variance <span class="math inline">\(\sigma^2\)</span>. That is, <span class="math display">\[
\mathbb{E}(\hat\sigma^2)=\sigma^2 \quad \text{and}\quad \hat\sigma^2\rightarrow_p \sigma^2.
\]</span></p>
<section id="the-residual-bootstrap-algorithm" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-residual-bootstrap-algorithm">The Residual Bootstrap Algorithm</h4>
<p>Based on the original data <span class="math inline">\((Y_i,X_i)\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, and the least squares estimate <span class="math inline">\(\hat\beta\)</span>, calculate the residuals <span class="math inline">\(\hat\varepsilon_1,\dots,\hat \varepsilon_n\)</span>.</p>
<ol type="1">
<li>Generate random bootstrap samples <span class="math inline">\(\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*\)</span> of residuals by drawing observations independently and with replacement from <span class="math display">\[
{\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}.
\]</span></li>
<li>Calculate new depend variables <span class="math display">\[
Y_i^*=X_i^T\hat\beta+\hat\varepsilon_i^*,\quad i=1,\dots,n
\]</span></li>
<li>Bootstrap estimators <span class="math inline">\(\hat\beta^*\)</span> are determined by least squares estimation from the data <span class="math inline">\((Y_1^*,X_1),\dots,(Y_n^*,X_n)\)</span>: <span class="math display">\[
\hat\beta^*=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
\]</span></li>
</ol>
<p>Repeating Steps 1-3 <span class="math inline">\(m\)</span> many times yields <span class="math inline">\(m\)</span> (e.g.&nbsp;<span class="math inline">\(m=100,000\)</span>) bootstrap estimators <span class="math display">\[
\hat\beta^*_1,\hat\beta^*_2,\dots,\hat\beta^*_m
\]</span> which allow us to approximate the bootstrap distribution <span class="math inline">\(\hat\beta^*-\hat\beta|\mathcal{S}_n\)</span> arbitrarily well as <span class="math inline">\(m\to\infty.\)</span></p>
</section>
<section id="motivating-the-residual-bootstrap" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="motivating-the-residual-bootstrap">Motivating the Residual Bootstrap</h4>
<p>It is not difficult to understand why the residual bootstrap generally works for <em>homoscedastic</em> (!) errors. We have <span class="math display">\[
\hat\beta-\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
\]</span> and for large <span class="math inline">\(n\)</span> the distribution of <span class="math inline">\(\sqrt{n}(\hat\beta-\beta)\)</span> is approximately normal with mean 0 and covariance matrix <span class="math inline">\(\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\)</span> <span class="math display">\[
\sqrt{n}(\hat\beta-\beta)\to_d\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)
\]</span></p>
<p>On the other hand (the bootstrap world), we have construction <span class="math display">\[
\hat\beta^*-\hat\beta
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*
\]</span> Conditional on <span class="math inline">\({\cal S}_n,\)</span> the bootstrap error terms are i.i.d with <span class="math display">\[
\mathbb{E}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
\]</span> and <span class="math display">\[
Var(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2.
\]</span> An appropriate central limit theorem argument implies that <span class="math display">\[
\left.\sqrt{n}(\hat\beta^*-\hat\beta)\right|\mathcal{S}_n\to_d\mathcal{N}\left(0,\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right),
\]</span> for <span class="math inline">\(n\to\infty.\)</span></p>
<p>Since<br>
<span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2\rightarrow_p \sigma^2
\]</span> as <span class="math inline">\(n\rightarrow\infty\)</span>, the bootstrap is consistent. That is, for large <span class="math inline">\(n\)</span>, we have approximately <span class="math display">\[
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)
\approx\underbrace{\text{distribution}(\sqrt{n}(\hat\beta-\beta))}_{\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)}
\]</span></p>
</section>
<section id="bootstrap-confidence-intervals-for-the-regression-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-confidence-intervals-for-the-regression-coefficients">Bootstrap confidence intervals for the regression coefficients</h3>
<section id="nonparametric-bootstrap-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="nonparametric-bootstrap-confidence-intervals">Nonparametric bootstrap confidence intervals</h4>
<p>Basic <strong>nonparametric bootstrap</strong> confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat t_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat t_{1-\frac{\alpha}{2},j}\)</span> of the bootstrap distribution of <span class="math inline">\(\hat\beta_j^*\)</span> using the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles (see <a href="#eq-empiricalQuantile">Equation&nbsp;3</a>) based on the <span class="math inline">\(m\)</span> bootstrap estimates <span class="math display">\[
\hat\beta_{j1}^*,\hat\beta_{j2}^*, \dots, \hat\beta_{jm}^*.
\]</span> <!-- This approximation step is with arbitrary accuracy as $m\to\infty.$ --></p>
<p>Compute the approximate <span class="math inline">\((1-\alpha)\times 100\%\)</span> (symmetric) nonparametric bootstrap confidence interval as in <a href="#eq-NPBootCI">Equation&nbsp;4</a>: <span class="math display">\[
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j},
      2\hat\beta_j-\hat t_{ \frac{\alpha}{2},j }\right]
\]</span></p>
</section>
<section id="bootstrap-t-confidence-intervals" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bootstrap-t-confidence-intervals">Bootstrap-<span class="math inline">\(t\)</span> confidence intervals</h4>
<p>Bootstrap-<span class="math inline">\(t\)</span> confidence intervals for the regression coefficients <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p,\)</span> can be constructed as following:</p>
<p>Let <span class="math inline">\(\gamma_{jj}\)</span> denote the <span class="math inline">\(j\)</span>-th diagonal element of the matrix <span class="math inline">\((\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}\)</span>, i.e., <span class="math display">\[
\gamma_{jj}:=\left[\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right]_{jj}.
\]</span> Then <span class="math display">\[
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}
\]</span> with <span class="math display">\[
\hat{\sigma}=\sqrt{\frac{1}{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2}
\]</span> is an asymptotically pivotal statistic, <span class="math display">\[
\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
\]</span></p>
<p>A bootstrap-<span class="math inline">\(t\)</span> interval for <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,\dots,p\)</span>, can thus be constructed as follows:</p>
<p>Approximate the <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles <span class="math inline">\(\hat \tau_{\frac{\alpha}{2},j}\)</span> and <span class="math inline">\(\hat \tau_{1-\frac{\alpha}{2},j}\)</span> of the bootstrap distribution of <span class="math display">\[
T^*=\frac{\hat\beta_j^*-\hat\beta_j}{\hat\sigma^* \sqrt{\gamma_{jj}}}
\]</span> with <span class="math display">\[
\hat\sigma^{*2}:=\frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^{*2},
\]</span> using the empirical <span class="math inline">\(\frac{\alpha}{2}\)</span> and <span class="math inline">\(1-\frac{\alpha}{2}\)</span> quantiles (see <a href="#eq-empiricalQuantile">Equation&nbsp;3</a>) based on the <span class="math inline">\(m\)</span> bootstrap realizations <span class="math display">\[
T^*_1,T_2^*,\dots, T_m^*.
\]</span></p>
<p>Compute the <span class="math inline">\((1-\alpha)\times 100\%\)</span> bootstrap-<span class="math inline">\(t\)</span> confidence interval as in <a href="#eq-Boot_tCI">Equation&nbsp;5</a>: <span class="math display">\[
\left[
  \hat\beta_j-\hat \tau_{1-\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}},\;
  \hat\beta_j-\hat \tau_{\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}}
\right],
\]</span> where <span class="math inline">\(\hat{\sigma}=\sqrt{\frac{1}{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2}.\)</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are many more bootstrap procedures. In case of heteroscedastic errors, for instance, there‚Äôs also the ‚ÄúWild Bootstrap.‚Äù</p>
<p>For high-dimensional problems (<span class="math inline">\(p\)</span> as large as <span class="math inline">\(n\)</span> or larger), one can use (under certain regularity assumptions) the ‚ÄúMultiplier Bootstrap‚Äù.</p>
</div>
</div>
</section>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<section id="exercise-1." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-1.">Exercise 1.</h4>
<p>Consider the empirical distribution function <span class="math display">\[
F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
\]</span> for a random sample <span class="math display">\[
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} F.
\]</span></p>
<ol type="a">
<li><p>Derive the exact distribution of <span class="math inline">\(nF_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span></p></li>
<li><p>Derive the asymptotic distribution of <span class="math inline">\(F_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span></p></li>
<li><p>Show that <span class="math inline">\(F_n(x)\)</span> is a point-wise (weakly) consistent estimator of <span class="math inline">\(F(x)\)</span> for each given <span class="math inline">\(x\in\mathbb{R}\)</span>.</p></li>
</ol>
</section>
<section id="exercise-2." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-2.">Exercise 2.</h4>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Exercise 1 shows that the empirical distribution function is a <strong>point-wise</strong> consistent estimator for each given <span class="math inline">\(x\in\mathbb{R}.\)</span> However, point-wise consistency generally does not imply <strong>uniformly</strong> consistency for all <span class="math inline">\(x\in\mathbb{R},\)</span> and therefore the Clivenko-Cantelli (<a href="#thm-Clivenko-Cantelli">Theorem&nbsp;1</a>) is so famous.</p>
<p>This exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.</p>
</div>
</div>
<p>Point-wise convergence of a function <span class="math inline">\(g_n(x),\)</span> i.e., <span class="math display">\[
|g_n(x) - g(x)|\to 0
\]</span> for each <span class="math inline">\(x\in\mathcal{X}\subset\mathbb{R}\)</span> as <span class="math inline">\(n\to\infty\)</span> generally does not imply uniform convergence, i.e., <span class="math display">\[
\sup_{x\in\mathcal{X}}|g_n(x) - g(x)|\to 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Show this by providing an example for <span class="math inline">\(g_n\)</span> which converges point-wise, but not uniformly for <span class="math inline">\(x\in\mathcal{X}\)</span>.</p>
<!-- 
http://personal.psu.edu/drh20/asymp/fall2002/lectures/ln03.pdf 
-->
</section>
<section id="exercise-3." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-3.">Exercise 3.</h4>
<p>Consider the following setup:</p>
<ul>
<li>iid data <span class="math inline">\(X_1,\dots,X_n\)</span> with <span class="math inline">\(X_i\sim F\)</span></li>
<li><span class="math inline">\(\mathbb{E}(X_i)=\mu\)</span></li>
<li><span class="math inline">\(Var(X_i)=\sigma^2&lt;\infty\)</span></li>
<li>Estimator: <span class="math inline">\(\bar{X}_n=n^{-1}\sum_{i=1}^nX_i\)</span></li>
</ul>
<ol type="a">
<li>Derive the classic confidence interval for <span class="math inline">\(\mu\)</span> using the asymptotic normality of the estimator <span class="math inline">\(\bar{X}.\)</span> Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of <span class="math inline">\(n=20\)</span> and,</li>
</ol>
<ul>
<li>Part 1: For <span class="math inline">\(F\)</span> being the normal distribution with <span class="math inline">\(\mu=1\)</span> and standard deviation <span class="math inline">\(\sigma=2\)</span>, and</li>
<li>Part 2: For <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom.</li>
</ul>
<ol start="2" type="a">
<li><p>Reconsider the case of <span class="math inline">\(n=20\)</span> and <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.</p></li>
<li><p>Reconsider the case of <span class="math inline">\(n=20\)</span> and <span class="math inline">\(F\)</span> being the <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-<span class="math inline">\(t\)</span> confidence interval.</p></li>
</ol>
</section>
<section id="exercise-4." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exercise-4.">Exercise 4.</h4>
<!-- Computational Statistics, James E. Gentle,  Exercise 13.1. -->
<p>Let <span class="math inline">\(\mathcal{S}_n = \{Y_1 , \dots, Y_n\}\)</span> be a random sample from a population with mean <span class="math inline">\(\mu\)</span>, variance <span class="math inline">\(\sigma^2,\)</span> and distribution function <span class="math inline">\(F.\)</span> Let <span class="math inline">\(F_n\)</span> be the empirical distribution function. Let <span class="math inline">\(\bar{Y}\)</span> be the sample mean for <span class="math inline">\(\mathcal{S}_n.\)</span> Let <span class="math inline">\(\mathcal{S}^*_n = \{Y_1^‚àó,\dots, Y_n^‚àó\}\)</span> be a random sample taken independently and with replacement from <span class="math inline">\(\mathcal{S}_n.\)</span> Let <span class="math inline">\(\bar{Y}^*\)</span> be the sample mean for <span class="math inline">\(\mathcal{S}^*_n.\)</span></p>
<ol type="a">
<li><p>Show that <span class="math display">\[
\mathbb{E}^*(\bar{Y}^*) = \bar{Y}
\]</span></p></li>
<li><p>Show that <span class="math display">\[
\mathbb{E}(\bar{Y}^*) = \mu
\]</span></p></li>
</ol>
<!-- 
#### Exercise 5. {-}
Computational Statistics, James E. Gentle,  Exercise 13.6. 
-->
<!-- {{< include Ch3_Solutions.qmd >}} -->
</section>
</section>
<section id="solutions" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="solutions">Solutions</h2>
<section id="solutions-of-exercise-1." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solutions-of-exercise-1.">Solutions of Exercise 1.</h4>
<section id="a" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a">(a)</h5>
<p>The exact point-wise distribution of <span class="math inline">\(nF_n(x)\)</span> for a given <span class="math inline">\(x\in\mathbb{R}.\)</span><br>
<span class="math display">\[
\begin{align*}
F_n(x)
&amp; = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\\
\Rightarrow nF_n(x)
&amp; = \sum_{i=1}^n 1_{(X_i\leq x)} \sim \mathcal{Binom}\left(n,p=F(x)\right),
\end{align*}
\]</span> since <span class="math inline">\(1_{(X_i\leq x)}\)</span> is a Bernoulli random variable with parameter <span class="math display">\[
\begin{align*}
p
&amp; = P(1_{(X_i\leq x)} = 1)
&amp; = P(X_i \leq x)
&amp; = F(x).
\end{align*}
\]</span></p>
</section>
<section id="b" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="b">(b)</h5>
<!-- The asymptotic  point-wise distribution of $F_n(x)$ for a given $x\in\mathbb{R}.$   -->
<p>From (a), we have that <span class="math display">\[
\begin{align*}
\mathbb{E}(nF_n(x)) &amp;= nF(x)\\
\Leftrightarrow\quad  \mathbb{E}(F_n(x)) &amp;= F(x)
\end{align*}
\]</span> and that <span class="math display">\[
\begin{align*}
Var(nF_n(x)) &amp;= nF(x)(1-F(x))\\
\Leftrightarrow \quad Var(F_n(x)) &amp;= \frac{F(x)(1-F(x))}{n}.
\end{align*}
\]</span></p>
<p>Moreover, since <span class="math inline">\(F_n(x) = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\)</span> is an average over i.i.d. random variables <span class="math inline">\(1_{(X_1\leq x)},\dots,1_{(X_n\leq x)},\)</span> the standard CLT implies <span class="math display">\[
\frac{F_n(x)-F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}}\to_d\mathcal{N}(0,1).
\]</span> Or with a slight abuse of notation: <span class="math display">\[
F_n(x)\overset{a}{\sim}\mathcal{N}\left(F(x),\frac{F(x)(1-F(x))}{n}\right).
\]</span></p>
</section>
<section id="c" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="c">(c)</h5>
<p>The mean squared error between <span class="math inline">\(F_n(x)\)</span> and <span class="math inline">\(F(x)\)</span> is given by <span class="math display">\[
\begin{align*}
\operatorname{MSE}(F_n(x))
&amp;= \mathbb{E}\left((F_n(x)-F(x))^2\right)\\[2ex]
&amp;= Var(F_n(x)) + \left(\mathbb{E}(F_n(x))-F(x)\right)^2.
\end{align*}
\]</span> It follows from our previous results that for each <span class="math inline">\(x\in\mathbb{R}\)</span> <span class="math display">\[
Var(F_n(x)) = \frac{F(x)(1-F(x))}{n} \to 0
\]</span> as <span class="math inline">\(n\to\infty,\)</span> and that <span class="math display">\[
\mathbb{E}(F_n(x)) -F(x) = 0
\]</span> for all <span class="math inline">\(n.\)</span> Therefore, <span class="math display">\[
\operatorname{MSE}(F_n(x)) = Var(F_n(x)) \to 0
\]</span> as <span class="math inline">\(n\to\infty.\)</span> Thus we can conclude that <span class="math inline">\(F_n(x)\)</span> converges in the mean-square sense to <span class="math inline">\(F(x)\)</span> for each <span class="math inline">\(x\in\mathbb{R},\)</span> <span class="math display">\[
F_n(x)\to_{ms} F(x)
\]</span> as <span class="math inline">\(n\to\infty.\)</span></p>
<p>Since convergence in the mean square sense implies convergence in probability, we also have that for each <span class="math inline">\(x\in\mathbb{R}\)</span> <span class="math display">\[
F_n(x)\to_{p} F(x)
\]</span> as <span class="math inline">\(n\to\infty\)</span> which shows that <span class="math inline">\(F_n(x)\)</span> is weakly consistent for <span class="math inline">\(F(x)\)</span> for each <span class="math inline">\(x\in\mathbb{R}.\)</span></p>
</section>
</section>
<section id="solutions-of-exercise-2." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solutions-of-exercise-2.">Solutions of Exercise 2.</h4>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Another, equivalent way to define uniform convergence:</p>
<p><span class="math inline">\(g_n(\cdot)\)</span> converges <strong>uniformly</strong> to <span class="math inline">\(g(\cdot)\)</span> if for every <span class="math inline">\(\varepsilon&gt;0,\)</span> there exists an <span class="math inline">\(N\)</span> such that <span class="math display">\[
|g_n(x) - g(x)| &lt; \varepsilon
\]</span> for all <span class="math inline">\(n\geq N\)</span> and <strong>for all</strong> <span class="math inline">\(x\in\mathcal{X}.\)</span></p>
<p>I.e., <span class="math inline">\(g_n(\cdot)\)</span> converges <strong>uniformly</strong> to <span class="math inline">\(g(\cdot)\)</span> if it is possible to draw an <span class="math inline">\(\varepsilon\)</span>-band around the graph of <span class="math inline">\(g(x)\)</span> that contains <strong>all of the graphs</strong> of <span class="math inline">\(g_n(x)\)</span> for large enough <span class="math inline">\(n.\)</span></p>
</div>
</div>
<p><strong>Example 1:</strong> <span class="math inline">\(\mathcal{X}=\mathbb{R}\)</span><br> The function <span class="math display">\[
g_n(x) = x\left(1+\frac{1}{n}\right)
\]</span> converges point-wise to <span class="math display">\[
g(x)=x,
\]</span> since <span class="math display">\[
|g_n(x)-g(x)|=\frac{|x|}{n}%\to 0\quad \text{as}\quad n\to\infty.
\]</span> converges to zero as <span class="math inline">\(n\to\infty\)</span> for each given <span class="math inline">\(x\in\mathcal{X}.\)</span></p>
<p>However, <span class="math inline">\(g_n\)</span> does not converge uniformly to <span class="math inline">\(g\)</span> since <span class="math display">\[
\sup_{x\in\mathbb{R}}|g_n(x)-g(x)|=\sup_{x\in\mathbb{R}}\frac{|x|}{n}=\infty\neq 0
\]</span> for each <span class="math inline">\(n.\)</span></p>
<p>Note that for a small <span class="math inline">\(\varepsilon&gt; 0,\)</span> an <span class="math inline">\(\varepsilon\)</span>-band around <span class="math inline">\(g(x) = x\)</span> fails to capture the graphs of <span class="math inline">\(g_n(x)=x(1+1/n).\)</span></p>
<p><strong>Example 2:</strong> <span class="math inline">\(\mathcal{X}=(0,1)\)</span><br> The function <span class="math display">\[
g_n(x) = x^n
\]</span> converges point-wise to <span class="math display">\[
g(x)=0,
\]</span> since <span class="math display">\[
|g_n(x)-g(x)|=x^n
\]</span> converges to zero as <span class="math inline">\(n\to\infty\)</span> for each given <span class="math inline">\(x\in(0,1).\)</span></p>
<p>However, <span class="math inline">\(g_n\)</span> does not converge uniformly to <span class="math inline">\(g\)</span> since <span class="math display">\[
\sup_{x\in(0,1)}|g_n(x)-g(x)|=\sup_{x\in(0,1)}x^n=1\neq 0
\]</span> for each <span class="math inline">\(n.\)</span></p>
<p>Note that for a small <span class="math inline">\(\varepsilon&gt; 0,\)</span> an <span class="math inline">\(\varepsilon\)</span>-band around <span class="math inline">\(g(x) = 0\)</span> fails to capture the graphs of <span class="math inline">\(g_n(x)=x^n.\)</span></p>
</section>
<section id="solutions-of-exercise-3." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solutions-of-exercise-3.">Solutions of Exercise 3.</h4>
<p>Link to the video: <a href="https://www.dropbox.com/s/w6pdr98s04hyk4t/Ch3_Ex3.mp4?dl=0">HERE</a></p>
<section id="a-part-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a-part-1">(a) Part 1:</h5>
<p>Setup:</p>
<ul>
<li>iid data <span class="math inline">\(X_1,\dots,X_n\)</span> with <span class="math inline">\(X_i\sim F\)</span></li>
<li><span class="math inline">\(\mathbb{E}(X_i)=\mu\)</span></li>
<li><span class="math inline">\(Var(X_i)=\sigma^2&lt;\infty\)</span></li>
<li>Estimator: <span class="math inline">\(\bar{X}_n=n^{-1}\sum_{i=1}^nX_i\)</span></li>
</ul>
<p>If <span class="math inline">\(F\)</span> is a normal distribution:</p>
<p><span class="math display">\[
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\sim \mathcal{N}(0,1)\quad\text{for all}\;n.
\end{array}
\]</span></p>
<p>For non-normal distributions <span class="math inline">\(F\)</span> we have by the classic CLT: <span class="math display">\[
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
\]</span></p>
<p>Usually, we do not know <span class="math inline">\(\sigma\)</span> and have to estimate this parameter using a consistent estimator such as <span class="math inline">\(s^2=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2\)</span>, where <span class="math inline">\(s\to_p\sigma\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>Then by Slusky‚Äôs Theorem (allows to combine ‚Äú<span class="math inline">\(\to_d\)</span>‚Äù and ‚Äú<span class="math inline">\(\to_p\)</span>‚Äù statements) we have that: <span class="math display">\[
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{s}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
\]</span></p>
<p>The <strong>classic confidence interval</strong> is then based on the above (asymptotic) normality result: <span class="math display">\[
\operatorname{CI}_{\operatorname{classic},n}=\left[\bar{X}_n\,-\,z_{1-\alpha/2}\frac{s}{\sqrt{n}},\bar{X}_n\,+\,z_{1-\alpha/2}\frac{s}{\sqrt{n}}\right],
\]</span> where <span class="math inline">\(z_{1-\alpha/2}\)</span> is the <span class="math inline">\((1-\alpha/2)\)</span>-quantile of the standard normal distribution. Alternatively, one can apply a ‚Äúsmall-sample correction‚Äù by using the <span class="math inline">\((1-\alpha/2)\)</span>-quantile <span class="math inline">\(t_{n-1, 1-\alpha/2}\)</span> of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p>From the above arguments it follows that: <span class="math display">\[
P\left(\mu\in \operatorname{CI}_{\operatorname{classic},n}\right)\to 1-\alpha\quad\text{as}\quad n\to\infty.
\]</span></p>
<p>Let us consider the finite-<span class="math inline">\(n\)</span> (with <span class="math inline">\(n=20\)</span>) performance of the classic confidence interval for the case where <span class="math inline">\(F\)</span> is a <strong>normal distribution</strong> with mean <span class="math inline">\(\mu=1\)</span> and standard deviation <span class="math inline">\(\sigma=2\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="do">##  Setup:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span> <span class="co"># Sample Size</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>mean  <span class="ot">&lt;-</span>    <span class="dv">1</span> <span class="co"># Mean</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>sdev  <span class="ot">&lt;-</span>    <span class="dv">2</span> <span class="co"># Standard Deviation</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>CI.lo.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>CI.up.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  X.sample     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n=</span>n, <span class="at">mean =</span> mean, <span class="at">sd =</span> sdev) </span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimates:</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  X.bar.MC     <span class="ot">&lt;-</span> <span class="fu">mean</span>(X.sample)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>  sd.hat.MC    <span class="ot">&lt;-</span> <span class="fu">sd</span>(X.sample)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Classic CIs:</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>  CI.lo.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>  CI.up.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.lo.vec[b] &lt;- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.up.vec[b] &lt;- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.lo.vec <span class="sc">&lt;=</span> mean  <span class="sc">&amp;</span>  mean <span class="sc">&lt;=</span> CI.up.vec</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.lo.vec, CI.up.vec), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Classic 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Normal DGP)"</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="a-part-2-classic-confidence-interval-n20-and-x_isim-chi2_1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a-part-2-classic-confidence-interval-n20-and-x_isim-chi2_1">(a) Part 2: Classic Confidence Interval (<span class="math inline">\(n=20\)</span> and <span class="math inline">\(X_i\sim \chi^2_1\)</span>)</h5>
<p>Now, we consider the finite-<span class="math inline">\(n\)</span> performance of the classic confidence interval under the same setup as above, but for the case where <span class="math inline">\(F\)</span> is a <strong>non-normal distribution</strong>, namely, a <span class="math inline">\(\chi^2_1\)</span>-distribution with <span class="math inline">\(1\)</span> degree of freedom:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>CI.lo.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>CI.up.vec  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>  X.sample     <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimates:</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  X.bar.MC     <span class="ot">&lt;-</span> <span class="fu">mean</span>(X.sample)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>  sd.hat.MC    <span class="ot">&lt;-</span> <span class="fu">sd</span>(X.sample)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Classic CIs:</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>  CI.lo.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>  CI.up.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="at">p =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span>(sd.hat.MC<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.lo.vec[b] &lt;- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>  <span class="co">#CI.up.vec[b] &lt;- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.lo.vec <span class="sc">&lt;=</span> mean  <span class="sc">&amp;</span>  mean <span class="sc">&lt;=</span> CI.up.vec</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.lo.vec, CI.up.vec), <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Classic 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Non-Normal DGP)"</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="b-standard-bootstrap-confidence-interval-n20-and-x_isim-chi2_1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="b-standard-bootstrap-confidence-interval-n20-and-x_isim-chi2_1">(b) Standard bootstrap confidence interval (<span class="math inline">\(n=20\)</span> and <span class="math inline">\(X_i\sim \chi^2_1\)</span>)</h5>
<p>Let‚Äôs generate an iid random sample <span class="math inline">\(S_n\)</span> with <span class="math inline">\(X_i\sim\chi^2_1\)</span> and the corresponding estimate <span class="math inline">\(\bar X_n\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="do">## IID random sample:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>S_n  <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Empirical mean:</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>(X.bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6737282</code></pre>
</div>
</div>
<p>The <strong>standard bootstrap confidence interval</strong> is given by (see lecture script): <span class="math display">\[
\left[2\bar{X}_n - \hat{t}_{1-\alpha/2}, 2\bar{X}_n - \hat{t}_{\alpha/2}\right],
\]</span> where <span class="math inline">\(\hat{t}_{\alpha/2}\)</span> and <span class="math inline">\(\hat{t}_{1-\alpha/2}\)</span> denote the <span class="math inline">\((\alpha/2)\)</span> and <span class="math inline">\((1-\alpha/2)\)</span>-quantiles of the conditional distribution of <span class="math inline">\(\bar{X}_n^\ast\)</span> given <span class="math inline">\(\mathcal{S}_n=\left\{X_1,\dots,X_n\right\}\)</span>, i.e., of the <strong>bootrap distribution</strong> of <span class="math inline">\(\bar{X}_n^\ast\)</span>.</p>
<p>In the following we approximate the bootstrap distribution of <span class="math inline">\(\bar{X}_n^\ast\)</span> using <span class="math inline">\(m=1500\)</span> boostrap resamplings, compute the quantiles <span class="math inline">\(\hat{t}_{\alpha/2}\)</span> and <span class="math inline">\(\hat{t}_{1-\alpha/2}\)</span>, and plot all of this:</p>
<div class="cell" data-fig.margin="true">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Bootstr-Setup:</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>alpha            <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">15</span><span class="co">#1500</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate bootstap samples:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>Bootstr.Samples  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Boostrap draws of \bar{X}_n^*:</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>X.bar.bootstr.vec <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Quantile of the bootstr.-distribution of \bar{X}_n^*:</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>t<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>t<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="do">## plot</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(X.bar.bootstr.vec), <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Bootstr.-Distr. of "</span>,<span class="fu">bar</span>(X)[n]<span class="sc">^</span>{<span class="st">" *"</span>})))</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">c</span>(t<span class="fl">.1</span>,t<span class="fl">.2</span>),<span class="at">col=</span><span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="432"></p>
</div>
</div>
<p>Using our preparatory work above, the standard bootstrap confidence interval can be computed as following:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Basic Bootstrap Confidence Interval:</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bootstr.lo <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar <span class="sc">-</span> t<span class="fl">.1</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bootstr.up <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar <span class="sc">-</span> t<span class="fl">.2</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Re-labeling of otherwise false names:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Basic.Bootstr.lo, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"2.5%"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Basic.Bootstr.up, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"97.5%"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%     97.5% 
0.4269145 0.9802301 </code></pre>
</div>
</div>
<p>Now, we can investigate the finite-<span class="math inline">\(n\)</span> performance of the standard bootstrap confidence interval:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>mean  <span class="ot">&lt;-</span>   df</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">15</span><span class="co">#1500</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Setup:</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bstr.lo.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>CI.Basic.Bstr.up.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  S_n.MC        <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimate:</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>  X.bar.MC      <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n.MC)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>  <span class="do">## </span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples.MC  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    Bootstr.Samples.MC[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n.MC, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>  X.bar.bootstr.MC.vec <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples.MC, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>  <span class="do">## (1-alpha/2)-quantile:</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>  t.<span class="fl">1.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.MC.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>  t.<span class="fl">2.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(X.bar.bootstr.MC.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Basic Bootstrap CIs:</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>  CI.Basic.Bstr.lo.vec[b] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar.MC <span class="sc">-</span> t.<span class="fl">1.</span>MC</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>  CI.Basic.Bstr.up.vec[b] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>X.bar.MC <span class="sc">-</span> t.<span class="fl">2.</span>MC</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.Basic.Bstr.lo.vec<span class="sc">&lt;=</span>mean <span class="sc">&amp;</span> mean<span class="sc">&lt;=</span>CI.Basic.Bstr.up.vec</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), </span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Basic Bootrap 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Non-Normal DGP)"</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Basic.Bstr.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Basic.Bstr.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Basic.Bstr.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Basic.Bstr.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="c-bootstrap-t-confidence-interval-n20-and-x_isim-chi2_1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="c-bootstrap-t-confidence-interval-n20-and-x_isim-chi2_1">(c) Bootstrap-<span class="math inline">\(t\)</span> confidence interval (<span class="math inline">\(n=20\)</span> and <span class="math inline">\(X_i\sim \chi^2_1\)</span>)</h5>
<p>The bootstrap-t confidence interval is given by (see lecture script): <span class="math display">\[
\left[\bar{X}_n-\hat{\tau}_{1-\alpha/2}\hat\sigma,  \bar{X}_n-\hat{\tau}_{\alpha/2}\hat\sigma\right],
\]</span> where <span class="math inline">\(\hat\sigma=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2\)</span>, and where <span class="math inline">\(\hat{\tau}_{\alpha/2}\)</span> and <span class="math inline">\(\hat{\tau}_{1-\alpha/2}\)</span> denote the <span class="math inline">\((\alpha/2)\)</span> and the <span class="math inline">\((1-\alpha/2)\)</span>-quantiles of the bootstrap distribution of: <span class="math display">\[
\frac{\bar{X}_n^\ast-\bar{X}_n}{\hat\sigma^\ast}.
\]</span></p>
<p>In the following we approximate the bootstrap distribution of <span class="math inline">\((\bar{X}_n^\ast-\bar{X}_n)/\hat\sigma^\ast\)</span>, compute the quantiles <span class="math inline">\(\hat{\tau}_{\alpha/2}\)</span> and <span class="math inline">\(\hat{\tau}_{1-\alpha/2}\)</span>, and plot all of this:</p>
<div class="cell" data-fig.margin="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="do">## IID random sample:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>S_n  <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Empirical mean and sd:</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>X.bar   <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>sd.hat  <span class="ot">&lt;-</span> <span class="fu">sd</span>(S_n)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Bootstr-Setup:</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>alpha            <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">15</span><span class="co">#1500</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="do">## Generate bootstap samples:</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>Bootstr.Samples  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Compute boostrap draws of (\bar{X}_n^*-\bar{X}_n)/\hat{\sigma}^\ast:</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>X.bar.bootstr.vec    <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>sd.bootstr.vec       <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>Bootstr.t.sample.vec <span class="ot">&lt;-</span> (X.bar.bootstr.vec <span class="sc">-</span> X.bar)<span class="sc">/</span>sd.bootstr.vec</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="do">## Quantile of the bootstr.-distribution of \bar{X}_n^*:</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>tau<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.sample.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>tau<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.sample.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="do">## plot</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(Bootstr.t.sample.vec), <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>,</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Bootstr.-t-Distr. of "</span>,</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>          (<span class="fu">bar</span>(X)[n]<span class="sc">^</span>{<span class="st">" *"</span>}<span class="sc">-</span><span class="fu">bar</span>(X)[n])<span class="sc">/</span><span class="fu">hat</span>(sigma)<span class="sc">^</span>{<span class="st">"*"</span>})))</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">c</span>(tau<span class="fl">.1</span>,tau<span class="fl">.2</span>),<span class="at">col=</span><span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="432"></p>
</div>
</div>
<p>Using our preparatory work above, the basic bootstrap confidence interval can be computed as following:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Basic Bootstrap Confidence Interval:</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.lo <span class="ot">&lt;-</span> X.bar <span class="sc">-</span> tau<span class="fl">.1</span> <span class="sc">*</span> sd.hat</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.up <span class="ot">&lt;-</span> X.bar <span class="sc">-</span> tau<span class="fl">.2</span> <span class="sc">*</span> sd.hat</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Re-labeling of otherwise false names:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Bstr.t.lo, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"2.5%"</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="fu">attr</span>(CI.Bstr.t.up, <span class="st">"names"</span>) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"97.5%"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="do">##</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(CI.Bstr.t.lo, CI.Bstr.t.up)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%     97.5% 
0.4536211 1.4388635 </code></pre>
</div>
</div>
<p>Let us investigate the finite-<span class="math inline">\(n\)</span> performance of the bootstrap-t confidence interval:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Setup:</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span>   <span class="dv">20</span>  <span class="co"># Sample Size</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span>    <span class="dv">1</span>  <span class="co"># (=&gt; mean==1)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>mean  <span class="ot">&lt;-</span>   df</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="co"># Level</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>n.Bootsrap.draws <span class="ot">&lt;-</span> <span class="dv">15</span><span class="co">#1500</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Setup:</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>B          <span class="ot">&lt;-</span> <span class="dv">1500</span> <span class="co"># MC repetitions</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.lo.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>CI.Bstr.t.up.vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, B)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="do">## MC-Simulation:</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Data Generating Process:</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>  S_n.MC        <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(n, <span class="at">df=</span>df)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Estimates:</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>  X.bar.MC      <span class="ot">&lt;-</span> <span class="fu">mean</span>(S_n.MC)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>  sd.MC         <span class="ot">&lt;-</span> <span class="fu">sd</span>(S_n.MC)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>  <span class="do">## </span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>  Bootstr.Samples.MC  <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>n.Bootsrap.draws)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.Bootsrap.draws){</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    Bootstr.Samples.MC[,j] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x=</span>S_n.MC, <span class="at">size=</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>  X.bar.bootstr.MC.vec <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples.MC, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> mean)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>  sd.bootstr.MC.vec    <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> Bootstr.Samples.MC, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> sd)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Make it a "Bootstrap-t" sample:</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>  Bootstr.t.MC.vec <span class="ot">&lt;-</span> (X.bar.bootstr.MC.vec <span class="sc">-</span> X.bar.MC)<span class="sc">/</span>sd.bootstr.MC.vec</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>  <span class="do">## (1-alpha/2)-quantile:</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>  tau.<span class="fl">1.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.MC.vec, <span class="at">probs =</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>  tau.<span class="fl">2.</span>MC <span class="ot">&lt;-</span> <span class="fu">quantile</span>(Bootstr.t.MC.vec, <span class="at">probs =</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Basic Bootstrap CIs:</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>  CI.Bstr.t.lo.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> tau.<span class="fl">1.</span>MC <span class="sc">*</span> sd.MC</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>  CI.Bstr.t.up.vec[b] <span class="ot">&lt;-</span> X.bar.MC <span class="sc">-</span> tau.<span class="fl">2.</span>MC <span class="sc">*</span> sd.MC</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="do">## How often does the classic CI cover the true mean?</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>CI.checks      <span class="ot">&lt;-</span> CI.Bstr.t.lo.vec<span class="sc">&lt;=</span>mean <span class="sc">&amp;</span> mean<span class="sc">&lt;=</span>CI.Bstr.t.up.vec</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>freq.non.cover <span class="ot">&lt;-</span> <span class="fu">length</span>(CI.checks[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>])<span class="sc">/</span>B</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="do">## Plot</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">range</span>(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), </span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">1</span>,B), <span class="at">type=</span><span class="st">"n"</span>, </span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"MC Repetitions"</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>, </span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Bootrap-t 95% Confidence Intervals</span><span class="sc">\n</span><span class="st">(Non-Normal DGP)"</span>)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="fu">c</span>(<span class="dv">1</span>), <span class="at">labels =</span><span class="st">"True Mean = 1"</span>)</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>); <span class="fu">box</span>()</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">text=</span><span class="fu">paste0</span>(<span class="st">"(Freq. of Non-Covering CIs: "</span>,</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>      <span class="fu">round</span>(freq.non.cover,<span class="at">digits =</span> <span class="dv">2</span>),<span class="st">")"</span>), <span class="at">line =</span> <span class="fl">2.5</span>)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="do">## Covering CIs:</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Bstr.t.lo.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Bstr.t.up.vec[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">TRUE</span>], </span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">1</span>, <span class="at">col=</span><span class="st">"black"</span>)</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="do">## Non-Covering CIs:</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(<span class="at">x0=</span>CI.Bstr.t.lo.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>       <span class="at">x1=</span>CI.Bstr.t.up.vec[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>       <span class="at">y0=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], <span class="at">y1=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>B)[CI.checks<span class="sc">==</span><span class="cn">FALSE</span>], </span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>       <span class="at">angle=</span><span class="dv">90</span>, <span class="at">code =</span> <span class="dv">3</span>, <span class="at">length =</span> .<span class="dv">05</span>, <span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mean,<span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Ch3_Bootstrap_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>
<section id="solutions-of-exercise-4." class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="solutions-of-exercise-4.">Solutions of Exercise 4.</h4>
<p>Link to the video: <a href="https://www.dropbox.com/s/upsl5hggr0jcrgb/Ch3_Ex4.mp4?dl=0">HERE</a></p>
<section id="a-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="a-1">(a)</h5>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}^*(\bar{Y}^*)
&amp; = \mathbb{E}\left(\left.\bar{Y}^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \mathbb{E}\left(\left.\frac{1}{n}\sum_{i=1}^n Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(\left.Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \mathbb{E}\left(\left.Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
&amp; = \sum_{i=1}^n \frac{1}{n} Y_i
= \bar{Y}
\end{align*}
\]</span> since <span class="math inline">\((Y_i^*|\mathcal{S}_n)\in\{Y_1,\dots,Y_n\}\)</span> and <span class="math inline">\(P(Y_j^*=Y_i|\mathcal{S}_n)=\frac{1}{n}\)</span> for each <span class="math inline">\(i,j\in 1,\dots,n.\)</span></p>
</section>
<section id="b-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="b-1">(b)</h5>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(\bar{Y}^*)
&amp; = \mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n Y_i^*\right)\\[2ex]
&amp; = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(Y_i^*\right)\\[2ex]
&amp; = \mathbb{E}\left(Y_i^*\right)\\[2ex]
&amp; = \mu
\end{align*}
\]</span> since <span class="math inline">\(Y_i^*\sim Y_i\sim F.\)</span></p>
</section>
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>