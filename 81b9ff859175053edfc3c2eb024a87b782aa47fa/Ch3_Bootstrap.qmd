<!-- LTeX: language=en-US -->
# The Bootstrap 


<!-- TO-DO: 
1. Rework this chapter using the overview article of Horowitz
BOOTSTRAP METHODS IN ECONOMETRICS 
2. Remove the fraction estimator parts 
-->

The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations (asymptotic statistics) leading, for example, to the construction of (approximate) confidence intervals. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.


Some literature:

* @Shao_Tu_1996: The Jackknife and Bootstrap
* @Davison_Hinkley_2013: Bootstrap Methods and their Applications
* @Efron_Tibshirani_1994: An Introduction to the Bootstrap
* @Hall_1992: The Bootstrap and Edgeworth Expansion
* @Horowitz_2001: The Bootstrap. In: Handbook of Econometrics
* @Mammen_1992_Book: When Does Bootstrap Work: Asymptotic Results and Simulations



::: {.callout-tip}
## Bradley Efron

The bootstrap method is attributed to [Bradley Efron](https://statistics.stanford.edu/people/bradley-efron), who received the *[International Prize in Statistics](https://statsandstories.net/methods/2018/9/28/bootstrapping-an-international-prize)* (the Nobel price of statistics) for his seminal works on the bootstrap method. 
:::



## Illustration: When are you happy about the Bootstrap? {#sec-Illustration}

Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of $X$ and $Y.$ These returns $X$ and $Y$ are random with 

* $Var(X)=\sigma^2_X$
* $Var(Y)=\sigma^2_Y$ 
* $Cov(X,Y)=\sigma_{XY}$

We want to invest a fraction $\alpha\in(0,1)$ in $X$ and invest the remaining $1-\alpha$ in $Y.$

Our aim is to minimize the variance (risk) of our investment, i.e., we want to minimize
$$
Var\left(\alpha X + (1-\alpha)Y\right).
$$ 
One can show that the value $\alpha$ that minimizes this variance is
$$
\alpha = \frac{\sigma^2_Y - \sigma_{XY}}{\sigma^2_X + \sigma^2_Y - 2\sigma_{XY}}.
$${#eq-alpha}
Using a data set that contains past measurements 
$$
((X_1,Y_1),\dots,(X_n,Y_n))
$$
for $X$ and $Y,$ we can estimate the unknown $\alpha$ by plugging in estimates of the variances and covariances
$$
\hat\alpha_n = \frac{\hat\sigma^2_{Y,n} - \hat\sigma_{XY,n}}{\hat\sigma^2_{X,n} + \hat\sigma^2_{Y,n} - 2\hat\sigma_{XY,n}}
$${#eq-alphahat}
with 
$$
\begin{align*}
\hat{\sigma}^2_{X,n}&=\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2\\ 
\hat{\sigma}^2_{Y,n}&=\frac{1}{n}\sum_{i=1}^n\left(Y_i-\bar{Y}\right)^2\\ 
\hat{\sigma}_{XY,n}&=\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}\right)\left(Y_i-\bar{Y}\right),
\end{align*}
$$
where 
$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ and 
$\bar{Y}=\frac{1}{n}\sum_{i=1}^nY_i.$

It is natural to wish to quantify the accuracy of our estimator 
$$
\hat\alpha_n\approx \alpha.
$$ 

For instance, to construct a confidence interval we need to know the standard error of the estimator $\hat\alpha$, 
$$
\sqrt{Var(\hat\alpha_n)} = \operatorname{SE}(\hat\alpha_n)=?
$$
However, deriving an explicit expression for $\operatorname{SE}(\hat\alpha)$ is difficult here due to the definition of $\hat\alpha_n$ in @eq-alphahat which contains variance estimators also in the denominator. 

::: {.callout-note}
# Conclusion: Why Bootstrap? 
In cases as described above, we are happy to use the **Basic Bootstrap Method** (@sec-BasicBootstrap) which allows estimating $\operatorname{SE}(\hat\alpha)$ by resampling from the data observed; i.e. without the need of an explicit formula of a consistent estimator of $\operatorname{SE}(\hat\alpha).$ The **Basic Bootstrap Method** is found to be as accurate as the standard asymptotic normality results which, however, require an explicit formula of an estimator of $\operatorname{SE}(\hat\alpha)$ to become useful.


If we have a consistent estimator for the $\operatorname{SE}(\hat\alpha),$ then we can make use of this estimator by applying the **Bootstrap-$\mathbf{t}$ Method** (@sec-BootT). The Bootstrap-$t$ Method is found to be **more accurate** than the standard asymptotic normality results. 
:::


## Recap: The Empirical Distribution Function

The distribution of a real-valued random variable $X$ can be completely described by its (cumulative) distribution function

::: {.callout-note appearance="minimal"} 
##
::: {#def-cdf}
# (Cumulative) Distribution Function (CDF)
$$
F(x)=P(X \leq x)\quad\text{for all}\quad x\in\mathbb{R}.
$$
:::
:::


The sample analogue of $F$ is the so-called **empirical distribution function**, which is an important tool of statistical inference.


Let  
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}\sim X
$$
denote a real-valued random sample with $X\sim F,$ and let $1_{(\cdot)}$ denote the indicator function, i.e., 
$$
\begin{align*}
1_{(\text{TRUE})} &=1\quad\text{and}\quad 1_{(\text{FALSE})}=0.
\end{align*}
$$  



::: {.callout-note appearance="minimal"} 
##
::: {#def-ecdf}

# Empirical (Cumulative) Distribution Function (ECDF)

$$
F_n(x)=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\quad\text{for all}\quad x\in\mathbb{R}.
$$
I.e $F_n(x)$ is the proportion of observations with $X_i\le x,$ $i=1,\dots,n.$
:::
:::

**Properties of the ECDF:**

$F_n$ is a **monotonically increasing right-continuous step function** that is bounded between zero and one,
$$
0\le F_n(x)\le 1,
$$
where 
$$
F_n(x)=\left\{
  \begin{array}{ll}
  0&\text{ if }x  < X_{(1)}\\
  1&\text{ if }x\ge X_{(n)}\\
  \end{array}
\right.
$$
where 
$$
X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}
$$ 
denotes the **order-statistic**. 


$F_n$ is itself a **distribution function according to @def-cdf**; namely, the distribution function of the **discrete random variable** $X^*,$ where  

$$
X^*\in\{X_1,\dots,X_n\}
$$ 
and 
$$
P(X^*=X_i)=\frac{1}{n}\quad\text{for each}\quad i=1,\dots,n.
$$
Thus
$$
\begin{align*}
F_n(x) 
&=\frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\\[2ex]
&= P\left(X^*\leq x\right).
\end{align*}
$$



::: {.callout-note appearance="minimal"} 
##
::: {#exm-ecdfexample}
# Computing the empirical distribution function $F_n$ in `R` 

<br>

Some data, i.e. an observed realization of a random sample $X_1,\dots,X_n\overset{\text{i.i.d}}{\sim}F:$

| $i$  | $X_i$| 
|------|------|
| 1    | 5.20 |
| 2    | 4.80 |
| 3    | 5.30 |
| 4    | 4.60 |
| 5    | 6.10 |
| 6    | 5.40 |
| 7    | 5.80 |
| 8    | 5.50 |


Corresponding empirical distribution function using `R`:

```{r, ecdfPlot}

observedSample <- c(5.20, 4.80, 5.30, 4.60, 
                    6.10, 5.40, 5.80, 5.50)

myecdf_fun     <- ecdf(observedSample)

plot(myecdf_fun, main="")
```

The `R` function `ecdf()` returns a function that gives the values of $F_n(x):$
```{r, ecdfEval}
## Note: ecdf() returns a function that can be evaluated! 
myecdf_fun(5.0)
```


Sampling from the empirical distribution function $F_n$ is equivalent to resampling (with replacement and with equal probabilities) data points from the observed data `observedSample`. 

```{r}
n        <- length(observedSample)

resample <- sample(observedSample, 
                   size    = n, 
                   replace = TRUE)
resample
```


:::
:::

#### **Statistical Properties of $F_n$** {-}

$F_n(x)$ depends on the i.i.d. random sample $X_1,\dots,X_n$ and thus is itself a **random function**. 

We obtain
$$
nF_n(x)\sim B(n, p=F(x))\quad\text{for each}\quad x\in\mathbb{R}
$${#eq-ecdfDistr}
 
I.e., $nF_n(x)$ has a binomial distribution with parameters: 

* $n$ ("number of trials") 
* $p=F(x)$ ("probability of success on a single trial").

> **Note:** The result in @eq-ecdfDistr holds for any $F.$ Therefore, $nF_n,$ and thus also $F_n,$ is called **distribution free** 

@eq-ecdfDistr implies that
$$
\begin{align*}
\mathbb{E}(nF_n(x))& = np = nF(x)\\[2ex]
\Rightarrow \quad \mathbb{E}(F_n(x))& = p = F(x)\\[2ex]
\Rightarrow \quad \operatorname{Bias}(F_n(x))& = \mathbb{E}(F_n(x)) - F(x) =0\\
\end{align*}
$$
and 
$$
\begin{align*}
Var(nF_n(x))& = np(1-p) = nF(x)(1-F(x))\\[2ex]
\Rightarrow \quad Var(F_n(x))& = \frac{nF(x)(1-F(x))}{n^2}=\frac{F(x)(1-F(x))}{n}.
\end{align*}
$$
Therefore,
$$
\begin{align*}
\operatorname{MSE}(F_n(x))
& = (\operatorname{Bias}(F_n(x)))^2 + Var(F_n(x))\\[2ex]
& =\frac{F(x)(1-F(x))}{n}.
\end{align*}
$$

This allows us to conclude that 
$$
\begin{align*}
F_n(x) & \to_{m.s.} F(x)\quad\text{as}\quad n\to\infty\\[2ex]
\Rightarrow \quad F_n(x) & \to_{p} F(x)\quad\text{as}\quad n\to\infty
\end{align*}
$$
for each $x\in\mathbb{R}.$

That is, $F_n(x)$ is a **pointwise consistent** estimator of $F(x)$ for each $x\in\mathbb{R}.$  

The Clivenko-Cantelli @thm-Clivenko-Cantelli states that $F_n$ is even an **uniformly consistent** estimator of $F.$

::: {.callout-note appearance="minimal"} 
## 
::: {#thm-Clivenko-Cantelli}

# Theorem of Glivenko-Cantelli

</br>

Let $X_1,\dots,X_n\overset{\text{i.i.d.}}\sim X$ denote a real-valued random sample with $X\sim F.$ Then
$$
\begin{align*}
&\quad P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1\\[2ex]
\Leftrightarrow &\quad 
\sup_{x\in\mathbb{R}} |F_n(x)-F(x)|\to_{a.s.} 0.
\end{align*}
$$
:::
:::


### Basic Idea of the Bootstrap {-}

The basic idea of the bootstrap is to replace random sampling from the true (unknown) population $F$ (infeasible Monte Carlo simulation) by random sampling from the empirical distribution $F_n$ (feasible Monte Carlo simulation). 


**Sampling from the population distribution $F$ (infeasible Monte Carlo simulation)** </br>The random sample $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X$ with $X\sim F$ is generated by drawing observations independently and with replacement from the unknown population distribution function $F$. That is, for each interval $[a,b]$ the probability of drawing an observation in $[a,b]$ is given by 
$$
P(X\in [a,b])=F(b)-F(a).
$$
Let $\theta_0$ denote a distribution parameter of $F$ which we want to estimate, and let $\hat\theta_n$ denote an estimator of $\theta_0.$</br>
If we would know $F,$ we could generate arbitrarily many realizations of the estimator $\hat{\theta}_n$ 
$$
\hat{\theta}_{n,1}, \hat{\theta}_{n,2}, \dots, \hat{\theta}_{n,m}
$$
with $m\to\infty$ and do inference about $\theta_0$ using these realizations. Unfortunately, we don't know $F,$ thus Monte Carlo inference is infeasible.  

**The idea of the bootstrap is to make the infeasible Monte Carlo inference feasible:** </br> 
Instead of random sampling from $F,$ which is infeasible (as we don't know $F$), the bootstrap uses random sampling from the known empirical distribution function $F_n$ to generate arbitrarily many **bootstrap realizations** of the estimator $\hat{\theta}_n$ 
$$
\hat{\theta}^*_{n,1}, \hat{\theta}^*_{n,2}, \dots, \hat{\theta}^*_{n,m}
$$
with $m\to\infty$ and do inference about $\theta_0$ using these bootstrap realizations.</br> 
This is justified asymptotically since for large $n,$ the empirical distribution $F_n$ is "close" to the unknown distribution $F$ (Glivenko-Cantelli @thm-Clivenko-Cantelli). That is, for $n\rightarrow\infty$ the relative frequency of observations $X_i$ in $[a,b]$ converges to $P(X\in [a,b])$   
$$
  \begin{align*}
  \underbrace{\frac{1}{n}\sum_{i=1}^n1_{(X_i\in[a,b])}}_{=F_n(b)-F_n(a)}&\to_p \underbrace{P(X\in [a,b])}_{=F(b)-F(a)}
  \end{align*}
$$



## The Basic Bootstrap Method {#sec-BasicBootstrap}


The basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption. The basic bootstrap method is  often also called:

* (Standard) Nonparametric Bootstrap Method or
* Nonparametric Percentile Bootstrap Method




**Setup:**

* i.i.d. sample $\mathcal{S}_n=\{X_1,\dots,X_n\},$ where $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X$ with real valued $X\sim F.$ 
* The distribution $F$ is depends on an unknown parameter $\theta_0.$
* The data $X_1,\dots,X_n$ is used to estimate $\theta_0\in\mathbb{R}.$
* Thus, the estimator is a function of the random sample 
$$
  \hat\theta_n\equiv \hat\theta(X_1,\dots,X_n).
$$
* Moreover, for simplicity let us focus on **unbiased** and $\boldsymbol{\sqrt{n}}$**-consistent** estimators, i.e.

  * $\mathbb{E}\left(\hat\theta_n\right)=\theta_0$
  * $\operatorname{SE}\left(\hat\theta_n\right)=\sqrt{Var\left(\hat\theta_n\right)}=\frac{1}{\sqrt{n}}\cdot\text{constant}$

**Inference:** In order to provide (approximate for $n\to\infty$) standard errors, construct confidence intervals, and to perform tests of hypothesis, we need to know the **distribution** of 
$$
\sqrt{n}\left(\hat\theta_n-\theta_0\right)\quad\text{as}\quad n\to\infty;
$$
i.e. we need to know the limit of the distribution function
$$
H_{n}(x)=P\left(\sqrt{n}\left(\hat\theta_n-\theta_0\right)\leq x\right)\quad\text{as}\quad n\to\infty.
$$


We could use asymptotic statistics to derive this limit. For instance, using the Lindeberg-Lévy CLT, we may be able to show that the limit of $H_{n}(x)$ is the distribution function of the Normal distribution with mean zero and asymptotic variance $\lim_{n\to\infty}n\cdot Var\big(\hat\theta_n\big).$ 

However, deriving a useful, explicit expression of the asymptotic variance $\lim_{n\to\infty}n\cdot Var\big(\hat\theta_n\big)$ can be *very* hard (see @sec-Illustration). Then, one is happy to use the bootstrap instead of doing painful math. Moreover, specific version of the Bootstrap can be even more accurate then a standard asymptotic Normality result. 


::: {.callout-note}
# The Core Part of the Bootstrap Algorithm
1. **Draw a bootstrap sample:** Generate a new random sample 
$$
X_1^*,\dots,X_n^*
$$ 
by drawing observations **independently and with replacement** from the available sample $\mathcal{S}_n=\{X_1,\dots,X_n\}.$</br>
2. **Compute bootstrap estimate:** Compute the estimate 
$$
\hat\theta^*_n\equiv \hat\theta(X_1^*,\dots,X_n^*)
$$
3. **Bootstrap replications:** Repeat Steps 1 and 2 $m$ times (for a large value of $m,$ such as $m=5000$ or $m=10000$) leading to $m$ bootstrap estimates 
$$
\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*
$$
:::

By the Clivenko-Cantelli (@thm-Clivenko-Cantelli) the bootstrap estimates 
$$
\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*
$$ 
allow us to approximate the **bootstrap distribution**  
$$
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\hat\theta^*_n-\hat\theta_n\right)\leq x\right|\mathcal{S}_n\right)
$$ 
arbitrarily well, i.e., 
$$
\sup_{x\in\mathbb{R}}\left|H^{Boot}_{n,m}(x)-H^{Boot}_{n}(x)\right|\to_{a.s} 0\quad\text{as}\quad m\to\infty,
$$ 
where 
$$
H^{Boot}_{n,m}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(\sqrt{n}\left(\hat\theta^*_{n,j}-\hat\theta_n\right)\leq x\right)}
$$ 
denotes the **empirical distribution function** based on the $\hat\theta_{n,1}^*,\hat\theta_{n,2}^*,\dots,\hat\theta_{n,m}^*$ centered by $\hat{\theta}_n$ and scaled by $\sqrt{n}.$

Since we can choose $m$ arbitrarily large, we can effectively ignore the approximation error between $H^{Boot}_{n,m}(x)$ and $H^{Boot}_{n}(x).$ That is, we can (and will do so) treat the bootstrap distribution $H^{Boot}_{n}(x)$ **as known.** 🤓  

The crucial question is, however, whether the (effectively known) bootstrap distribution 
$$
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\hat\theta^*_n-\hat\theta_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
$$ 
is able to approximate the unknown distribution 
$$
H_{n}(x)=P\left(\sqrt{n}\left(\hat\theta_n-\theta_0\right)\leq x\right)
$$
as $n\to\infty.$ This is a basic requirement called **bootstrap consistency**. If a bootstrap method is inconsistent, you shall not use it in practice. 


### Bootstrap Consistency {-}

The bootstrap does **not always work**. A necessary condition for the use of the bootstrap is the **consistency of the bootstrap approximation**.

The bootstrap is called **consistent** if, for large $n$, the bootstrap distribution of $\sqrt{n}\big(\hat{\theta}^*_n -\hat{\theta}_n\big)|\mathcal{S}_n$ is a good approximation of the distribution of $\sqrt{n}\big(\hat{\theta}_n-\theta_0\big);$ i.e., if
$$
\underbrace{\text{distribution}\left(\sqrt{n}\big(\hat{\theta}^*_n -\hat{\theta}_n\big)\ |{\cal S}_n\right)}_{H_n^{Boot}}\approx 
\underbrace{\text{distribution}\left(\sqrt{n}\big(\hat{\theta}_n-\theta_0\big)\right)}_{H_n}.
$$
for large $n.$

The following definition states this more precisely. 

::: {.callout-note appearance="minimal"} 
## 
::: {#def-BootstrapConsistency}
## Bootstrap Consistency 
</br>
Let the limit (as $n\to\infty$) of $H_n$ be a non-degenerate distribution. Then the bootstrap is **consistent** if and only if
$$
\sup_{x\in\mathbb{R}} \Big|\;
\underbrace{P\Big(\sqrt{n}\big(\hat\theta^*_n-\hat\theta_n\big)\le x \ |{\cal S}_n\Big)}_{H_n^{Boot}(x)} 
  -\underbrace{P\Big(\sqrt{n}\big(\hat\theta_n -\theta_0\big)\le x\Big)}_{H_n(x)}
  \Big|\rightarrow_p 0
$$
as $n\to\infty.$
:::
:::

Luckily, the standard bootstrap is consistent in a large number of statistical problems. Typically, the bootstrap is consistent if the following two requirements hold:

1. Generation of the bootstrap sample must reflect appropriately the way in which the original sample has been generated. That is, 
   * if the original sample was generated by i.i.d. sampling, then also the bootstrap samples need to be generated by i.i.d. sampling. 
   * if the original sample was generated by cluster sampling, then also the bootstrap samples need to be generated by cluster sampling. 
2. Typically, the distribution of 
$$
\sqrt{n}\left(\hat\theta_n-\theta_0\right)
$$ 
needs to be asymptotically normal. 

The standard bootstrap **will usually fail** if one of the above conditions is violated. For instance, ...

* the  bootstrap will not work if the i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $\mathcal{S}_n=\{X_1,\dots,X_n\}$ does not properly reflect the way how $X_1,\dots,X_n$ are generated in the first place. (For instance, when $X_1,\dots,X_n$ is generated by a time-series process with auto-correlated data, but the bootstrap samples are generated by i.i.d. sampling from $\mathcal{S}_n$) 
* the  bootstrap will not work if the distribution of 
$$
\sqrt{n}\left(\hat\theta_n-\theta_0\right)
$$ 
is not asymptotically normal. (For instance, in case of extreme value problems.)

**Note:** In order to deal with more complex sampling schemes alternative bootstrap procedures have been proposed in the literature (e.g. the block-bootstrap in case of time-series data).

### Example: Inference About the Population Mean 

**Setup:**

* $X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim}X$ with $X\sim F$ 
* Continuous random variable $X\sim F$ 
* Non-zero, finite variance $0<Var(X)=\sigma_0^2<\infty$ 
* Unknown mean $\mathbb{E}(X)=\mu_0,$ where  
$$
\mu_0 = \int x f(x) dx = \int x d F(x),
$$ 
where $f=F'$ denotes the density function.
* Estimator: Empirical mean
$$
\begin{align*}
\bar{X}_n
&\equiv \bar{X}(X_1,\dots,X_n) \\[2ex]
&=\frac{1}{n}\sum_{i=1}^n X_i \\[2ex]
&=\int x d F_n(x)
\end{align*}
$$

**Inference Problem:** What is the (asymptotic) distribution of 
$$
\sqrt{n}\left(\bar{X}_n -\mu_0\right)
$$
as $n\to\infty$? 


::: {.callout-note}
# Recall: Inference using Classic Asymptotic Statistics 
This example is so simple that we know (by the Lindeberg-Lévy CLT) that 
$$
\sqrt{n}\left(\bar{X}_n -\mu_0\right)\to_d\mathcal{N}\left(0,\sigma_0^2\right)\quad\text{as}\quad n\to\infty,
$$
i.e., that
$$
%\bar{X}_n\overset{a}{\sim}\mathcal{N}\left(\mu_0,\frac{1}{n}\sigma_0\right).
H_n(x)=P\left(\sqrt{n}\left(\bar{X}_n-\mu_0\right)\leq x\right)\to\Phi_{\sigma_0}(x)\quad\text{as}\quad n\to\infty,
$$
for all continuity points $x,$ where 
$$
\Phi_{\sigma_0}(x)=\Phi\left(\frac{x}{\sigma_0}\right)
$$ 
with $\Phi$ denoting the distribution function of the standard normal distribution, i.e.
$$
\Phi_{\sigma_0}(x)
=\Phi\left(\frac{x}{\sigma_0}\right)
=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x/\sigma_0}\exp\left(-\frac{1}{2}z^2\right)\,dz.
$$ 
:::

Yes, the asymptotic result is simple here (boring 🥱), but can we alternatively use the Bootstrap to approximate this limit result? I.e., is 
$$
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
$$ 
able to approximate $\Phi_{\sigma_0}(x)$ for all $x\in\mathbb{R}$? 

Before we answer this question theoretically (see @sec-Theory1 and @sec-Theory2BootstrapConsist), we check it empirically.  

#### Practice: Empirical Consideration of the Bootstrap distribution {#sec-PracticeXbar}

In this chapter we investigate the distribution of 
$$
\sqrt{n}\left.\left(\bar X^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n
$$
i.e., the bootstrap distribution 
$$
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right)
$$ 
empirically using artificial data. 

> **Question to be checked:** Is $H^{Boot}_{n}(x)$ able to approximate the asymptotic distribution $\Phi_{\sigma_0}(x)$?  

Let us consider the following **observed sample** $\mathcal{S}_n=\{X_1,\dots,X_n\}$ with a rather smallish sample size of $n=8$ shown in @tbl-ChiSqSample. The data was generated by drawing from a $\chi^2$-distribution with $\operatorname{df}=2.$ That is, $Var(X)=\sigma_0^2=2\cdot \operatorname{df}=4.$ 

> The bootstrap is justified asymptotically $(n\to\infty).$ Choosing a smallish data size of $n=8$ is done out of curiosity. The approximation of $\Phi_{\sigma_0}(x)$ by $H^{Boot}_{n}(x)$ will become better for larger sample sizes. 


```{r, echo = FALSE}
set.seed(123)
observedSample <- rchisq(n = 8, df = 2)
# round(observedSample, 2)
```

| $i$  | $X_i$| 
|------|------|
| 1    | 0.36 |
| 2    | 3.39 |
| 3    | 3.24 |
| 4    | 4.90 |
| 5    | 1.76 |
| 6    | 5.33 |
| 7    | 7.77 |
| 8    | 1.93 |

: Observed realization of the random sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$ with sample size $n=8$ drawn from a $\chi^2$-distribution with $\operatorname{df}=2.${#tbl-ChiSqSample}

```{r}
observedSample <- c(0.36, 3.39, 3.24, 4.90, 
                    1.76, 5.33, 7.77, 1.93)
```

So the observed sample mean is 
<center>
$\bar X_{n,obs} =$ `mean(observedSample)` $=$ `r mean(observedSample)`
</center>

<br>

**Bootstrap:**

The observed sample 
$$
{\cal S}_n=\{X_1,\dots,X_n\}
$$ 
is taken as underlying empirical "population" in order to generate the i.i.d. **bootstrap sample** 
$$
X_1^*,\dots,X_n^*
$$

These i.i.d. samples $X_1^*,\dots,X_n^*$ are generated by drawing   observations independently and with replacement from ${\cal S}_n=\{X_1,\dots,X_n\}.$ 

Each realization of the bootstrap sample leads to a new realization of the bootstrap estimator $\bar{X}^*_n$ as demonstrated in the following `R` code:

```{r}
## generating one realization of the bootstrap sample
bootSample <- sample(x       = observedSample, 
                     size    = length(observedSample), 
                     replace = TRUE)

## computing the realization of the bootstrap estimator
mean(bootSample)
```

We can now approximate the bootstrap distribution
$$
H^{Boot}_n(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n\right)
$$
using the empirical distribution function 
$$
H^{Boot}_{n,m}(x)=\frac{1}{m}\sum_{j=1}^m 1_{\left(\sqrt{n}\left(\bar{X}^*_{n,j}-\bar{X}_n\right)\leq x\right)}
$$
based on the bootstrap estimators 
$$
\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m}
$$
generated using the bootstrap algorithm 

1. Generate bootstrap sample 
2. Compute bootstrap estimator
3. Repeat Steps 1 and 2 $m$ times 

with a (very) large $m.$ The following `R` code demonstrates this: 

```{r}
n                <- length(observedSample)
Xbar             <- mean(observedSample)

m                <- 10000 # number of bootstrap samples 
Xbar_boot        <- vector(mode = "double", length = m)

## Bootstrap algorithm
for(k in seq_len(m)){
 bootSample          <- sample(x       = observedSample, 
                               size    = n, 
                               replace = TRUE)
 Xbar_boot[k]        <- mean(bootSample)
}

plot(ecdf( sqrt(n) * (Xbar_boot - Xbar) ), 
     xlab = "", ylab = "", 
     main = "Bootstrap Distribution vs Normal Limit Distribution")
curve(pnorm(x, mean = 0, sd = sqrt(4)), col = "red", add = TRUE)     
legend("topleft", 
       legend = c("Bootstrap Distribution", 
                  "Normal Limit Distribution with\nMean = 0 and Variance = 4"), 
      col = c("black", "red"), lty = c(1,1))
```



**Note:** To plot the Normal limit distribution we need to make use of our knowledge that $X_i\overset{\text{i.i.d.}}{\sim}\chi^2_{(\operatorname{df}=2)}$ which implies that we know the **usually unknown** asymptotic variance of the estimator $\bar{X}_n:$ 
$$
nVar(\bar{X}_n)=Var(\sqrt{n}(\bar{X}_n-\mu_0))=\sigma_0^2=2\cdot\operatorname{df}=4, 
$$ 
for each $n=1,2,\dots,$ thus also $\lim_{n\to\infty}nVar(\bar{X}_n)=4.$

Usually, however, we do not know the value of the asymptotic variance, but need an estimator for this quantity. (Which can be hard to derive.) 

By contrast to the asymptotic normality result from applying the CLT, the bootstrap distribution gives us the <span style="color:#FF5733">**complete**</span> **distribution** without having to know the asymptotic variance. 

That is, to estimate the usually unknown value of the asymptotic variance 
$$
\lim_{n\to\infty}nVar(\bar{X}_n)=\sigma_0^2
$$
(here $\sigma_0^2=4$), we can simply use the **empirical variance** of the bootstrap estimators $\bar{X}^*_{n,1},\dots,\bar{X}^*_{n,m}$ multiplied by $n,$ i.e.
$$
n\;\cdot\;\underbrace{\frac{1}{m}\sum_{j=1}^m \left(\bar{X}^*_{n,j} - \left(\frac{1}{m}\sum_{j=1}^m\bar{X}^*_{n,j}\right)\right)^2}_{\text{estimator of the usually unknown }Var(\bar{X}_n)}
$$


as done in the following `R`-code:
```{r}
round(n * var(Xbar_boot), 2)
```



<!-- 
:::{.callout-note}
For the given data with $n=8$ observations, there are 
$$
n^n=8^8=16,777,216
$$ 
possible bootstrap samples which are all equally probable. 
:::
-->

#### Theory (Part 1): Mean and Variance of the Bootstrap distribution  {#sec-Theory1}

In this chapter we begin with the theoretical consideration of the Bootstrap distribution
$$
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right).
$$ 
We begin with focusing on the mean and the variance of $H^{Boot}_{n}.$ 


::: {.callout-tip}
## Notation $\mathbb{E}^*(\cdot),$ $Var^*(\cdot),$ and $P^*(\cdot)$
In the bootstrap literature one frequently finds the notation 
$$
\mathbb{E}^*(\cdot),\;Var^*(\cdot),\;\text{and}\;P^*(\cdot)
$$ 
to denote the **conditional** expectation
$$
\mathbb{E}^*(\cdot)=\mathbb{E}(\cdot|\mathcal{S}_n),
$$
the **conditional** variance 
$$
Var^*(\cdot)=Var(\cdot|\mathcal{S}_n),
$$ 
and the **conditional** probability 
$$
P^*(\cdot)=P(\cdot|\mathcal{S}_n),
$$ 
given the sample ${\cal S}_n.$
:::


The bootstrap focuses on the **bootstrap distribution**, i.e. on the conditional distribution of 
$$
\sqrt{n}(\bar X^*_n -\bar X_n)|\mathcal{S}_n.
$$ 



::: {.callout-tip}
## We know the distribution of $X_i^*|\mathcal{S}_n$

We can analyze the bootstrap distribution of $\sqrt{n}(\bar X^*_n -\bar X_n)|\mathcal{S}_n,$ since **we *know* 🤟 the discrete distribution** of the conditional random variables 
$$
X_i^*|\mathcal{S}_n,\;i=1,\dots,n,
$$ 
even though, we do **not know** the distribution of $X_i\sim F,$ $i=1,\dots,n.$

<!-- The discrete distribution of the conditional random variables 
$X_i^*|\mathcal{S}_n,$ is 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\}
$$
with  -->
:::


For each $i=1,\dots,n$, the possible values of the discrete random variable $X_i^*|\mathcal{S}_n$ are 
$$
X_i^*|\mathcal{S}_n\in\{X_1,\dots,X_n\},
$$ 
and each of these values is equally probable: 
$$
\begin{align*}
P^*(X_i^*=X_1)&= P(X_i^*=X_1|{\cal S}_n) = \frac{1}{n} \\[2ex]
P^*(X_i^*=X_2)&= P(X_i^*=X_2|{\cal S}_n) = \frac{1}{n} \\[2ex]
&\vdots\\[2ex] 
P^*(X_i^*=X_n)&= P(X_i^*=X_n|{\cal S}_n) = \frac{1}{n}.
\end{align*}
$$

Thus, **we know the whole distribution** of the discrete conditional random variable $X_i^*|\mathcal{S}_n$ and, therefore, can compute, for instance, easily its conditional mean and its variance.

* The conditional mean of $X_i^*$ given ${\cal S}_n$ is
$$
\begin{align*}
\mathbb{E}^*(X_i^*)
&=\mathbb{E}(X_i^*|{\cal S}_n)\\[2ex]
&=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_n\\[2ex]
&=\bar X_n.
\end{align*}
$$
I.e., the empirical mean $\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i$ of the original sample $X_1,\dots,X_n$ is the "population" mean of the bootstrap sample $X^*_1,\dots,X^*_n.$

* The conditional variance of $X_i^*$ given ${\cal S}_n$ is
$$
\begin{align*}
Var^*(X_i^*)
&=Var(X_i^*|{\cal S}_n)\\[2ex] 
&=\mathbb{E}\left((X_i^* - \mathbb{E}(X_i^*|{\cal S}_n))^2|{\cal S}_n\right)\\[2ex] 
&=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2\\[2ex] 
&=\hat\sigma^2_0.
\end{align*}
$$
I.e., the empirical variance $\hat\sigma^2_{0}=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2$ of the original sample $X_1,\dots,X_n$ is the "population" variance of the bootstrap sample $X^*_1,\dots,X^*_n.$


::: {.callout-tip}
## General case: Conditional moments of transformed $g(X_i^*)$

For any (measurable) function $g$ we have 
$$
\mathbb{E}^*(g(X_i^*))=\mathbb{E}(g(X_i^*)|\mathcal{S}_n)=\frac{1}{n}\sum_{i=1}^n g(X_i).
$$
For instance, $g(X_i)=1_{(X_i\leq x)}.$
:::

::: {.callout-important}

## Caution: Conditioning on $\mathcal{S}_n$ in important!

Conditioning on the observed sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$ is very important. 

The unconditional distribution of $X_i^*$ is equal to the **unknown distribution** $F.$ This can be seen from the following derivation: 
$$
\begin{align*}
P(X_i^*\leq x) 
&= P(1_{(X_i^*\leq x)}=1) \\[2ex]
&= P(1_{(X_i^*\leq x)}=1) \cdot 1 + P(1_{(X_i^*\leq x)}=0) \cdot 0\\[2ex]
&= \mathbb{E}\left(1_{\left(X_i^*\leq x\right)}\right)\\[2ex]
&= \mathbb{E}\left({\color{blue}\mathbb{E}\left(1_{\left(X_i^*\leq x\right)}|\mathcal{S}_n\right)}\right)\\[2ex]
&= \mathbb{E}\left({\color{blue}\frac{1}{n}\sum_{i=1}^n 1_{\left(X_i\leq x\right)}}\right)\quad[\text{{\color{blue}from our derivations above}}]\\[2ex]
&= \frac{n}{n}\mathbb{E}\left(1_{\left(X_i\leq x\right)}\right)\\[2ex]
&= P(1_{(X_i\leq x)}=1) \cdot 1 + P(1_{(X_i\leq x)}=0) \cdot 0\\[2ex]
&= P\left(X_i\leq x\right)=F(x)
\end{align*}
$$
:::


**Now we can consider the mean and the variance of:** 
$$
\sqrt{n}\left.\left(\bar X^*_n-\bar{X}_n\right)\;\right|\;\mathcal{S}_n.
$$


* The conditional mean of $\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)$ given $\mathcal{S}_n$ is
$$
\begin{align*}
\mathbb{E}^*\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\right)
&=\mathbb{E}\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)|{\cal S}_n\right)\\[2ex]
&=\sqrt{n}\,\mathbb{E}\left(\left(\bar X^*_n-\bar{X}_n\right)|{\cal S}_n\right)\\[2ex]
&=\sqrt{n}\left(\mathbb{E}\left(\bar X^*_n|{\cal S}_n\right)- \mathbb{E}\left(\bar{X}_n|{\cal S}_n\right)\right)\\[2ex]
&=\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n{\color{red}\mathbb{E}\left(X^*_i|{\cal S}_n\right)}- \frac{1}{n}\sum_{i=1}^n{\color{blue}\mathbb{E}\left(X_i|{\cal S}_n\right)}\right)\\[2ex]
&=\sqrt{n}\left(\frac{n}{n}{\color{red}\bar{X}_n} - \frac{1}{n}\sum_{i=1}^n{\color{blue}X_i}\right)\\[2ex]
&=\sqrt{n}\left(\bar{X}_n - \bar{X}_n\right)\\[2ex]
&= 0.
\end{align*}
$$

* The conditional variance of $\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)$ given $\mathcal{S}_n$ is
$$
\begin{align*}
Var^*\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)\right)
&=Var\left(\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)|{\cal S}_n\right)\\[2ex]
&=n\,Var\left(\big(\bar X^*_n-\bar{X}_n\big)|{\cal S}_n\right)\\[2ex]
&=n\,Var\big(\bar X^*_n|{\cal S}_n\big)\quad[\text{cond.~on $\mathcal{S}_n,$ $\bar{X}_n$ is a constant}]\\[2ex]
&=n\,Var\Big(\frac{1}{n}\sum_{i=1}^n X_i^*\Big|{\cal S}_n\Big)\\
&=n\,\frac{1}{n^2}\sum_{i=1}^n Var\big(X_i^*|{\cal S}_n\big)\\
&=n\,\frac{n}{n^2} Var\big(X_i^*|{\cal S}_n\big)\\
&=Var\big(X_i^*|{\cal S}_n\big)\\[2ex]
&=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X}_n)^2\quad[\text{derived above}]\\[2ex]
&=\hat\sigma^2_0,
\end{align*}
$$
where 
$$
\hat\sigma^2_0\to_p \sigma_0^2\quad\text{as}\quad n\to\infty.
$$

::: {.callout-note}
Thus, we know now that for large $n$ ($n\to\infty$)  **mean (zero) and the variance** ($\sigma_0^2$) of the bootstrap distribution of 
$$
\sqrt{n}\left(\bar X^*_n-\bar{X}_n\right)|\mathcal{S}_n
$$ 
matches the **mean (zero) and the variance** ($\sigma_0^2$) of the limit distribution $\Phi_{\sigma_0}.$

Bootstrap consistency, however, addresses the total distribution---not only the first two moments. 
::: 


#### Theory (Part 2): Bootstrap Consistency {#sec-Theory2BootstrapConsist}

In this chapter we continue our theoretical consideration of the Bootstrap distribution
$$
H^{Boot}_{n}(x)=P\left(\left.\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x\;\right|\;\mathcal{S}_n\right),
$$
but consider now the total distribution---not only mean and variance. 


::: {.callout-note  appearance="minimal"}
#
::: {#def-CharacFun}
## Characteristic Function
Let $X\in\mathbb{R}$ be a random variable and let $\mathcal{i}=\sqrt{-1}$ be the imaginary unit. Then the function $\psi_X:\mathbb{R}\to\mathbb{C}$ defined by
$$
\psi_X(t) = \mathbb{E}(\exp(\mathcal{i}tX))
$$
is called **the characteristic function** of $X.$
:::
:::


::: {.callout-note}
# Characteristic Function: Some useful facts
The characteristic function ... 

* ... uniquely determines its associated probability distribution. 
* ... can be used to easily derive (all) the moments of a random variable by
$$
\mathbb{E}(X^n) = \mathcal{i}^n \left[\frac{d^n}{d t^n}\psi_X(t)\right]_{t=0} 
$$
* ... is often used to prove that two distributions are equal.



* The characteristic function of $\Phi_{\sigma_0}$ is
$$
\begin{align*}
\psi_{\Phi_{\sigma_0}}(t)
&=\exp\left(-\frac{1}{2}\sigma_0^2\,t^2\right)\\[2ex]
&=\lim_{n\to\infty}\left(1-\frac{1}{n}\,\frac{1}{2}\,\sigma_0^2 \, t^2\right)^n
\end{align*}
$${#eq-CharacNormal}

* The characteristic function of $\sum_{i=1}^nW_i,$ where $W_1,\dots,W_n$ are i.i.d., is
$$
\psi_{\sum_{i=1}^nW_i}(t)=\left(\psi_{W_1}(t)\right)^n
$${#eq-CharacFctSum}

* Let $W$ be a random variable with $\mathbb{E}(W)=0$ and $Var(W)=\sigma_W^2.$ Then, we have that (see Equation (26.11) in @Billingsley_1995) 
$$
\psi_W(t)=1-\frac{1}{2}\sigma_W^2 \, t^2 + \lambda(t),
$${#eq-CharacFctBillingsley}
where $|\lambda(t)|\leq |t^2|\,\mathbb{E}\left(\min(|t|\,|W|^3, W^2)\right).$
:::


> The following can be found in Example 3.1 in @Shao_Tu_1996

It follows from  the Lindeberg-Lévy CLT that 
$$
H_n(x)=P\left(\sqrt{n}\left(\bar{X}_n-\mu_0\right)\leq x\right)\to\Phi_{\sigma_0}(x)=\Phi\left(\frac{x}{\sigma_0}\right)\quad\text{as}\quad n\to\infty,
$$
for all $x\in\mathbb{R}.$ This result can be proven by showing that the characteristic function of $H_n$ tends to that of $\Phi_{\sigma_0}.$ 

To see this, rewrite
$$
\begin{align*}
\sqrt{n}\left(\bar{X}_n-\mu_0\right)
& = \sum_{i=1}^n\frac{X_i-\mu_0}{\sqrt{n}}\\[2ex]
& = \sum_{i=1}^n W_{i,n}
\end{align*}
$$
where 

* $W_{1,n},\dots,W_{n,n}$ are i.i.d. with
* $\mathbb{E}(W_{i,n})=0$ and 
* $Var(W_{i,n})=\frac{1}{n}\sigma_0^2.$

Therefore, by @eq-CharacFctSum together with @eq-CharacFctBillingsley
$$
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}_n-\mu_0\right)}(t)
&=\psi_{\sum_{i=1}^n W_{i,n}}(t)\\[2ex] 
&=\left(\psi_{W_{1,n}}(t)\right)^n\\[2ex] 
&=\left(1-\frac{1}{2}\,\frac{1}{n}\sigma_0^2 \, t^2 + \lambda_n(t)\right)^n,
\end{align*}
$$
where 
$$
\begin{align*}
|\lambda_n(t)|
&\leq |t^2|\,\mathbb{E}\left(\min\big(|t|\,\left|W_{1,n}\right|^3, \left|W_{1,n}\right|^2\big)\right)\\[2ex]
&= |t^2|\,\mathbb{E}\left(\min\big(|t|\,n^{-3/2}\left|X_1-\mu_0\right|^3, n^{-1}\left|X_1-\mu_0\right|^2\big)\right).
\end{align*}
$$
That is, 
$$
n|\lambda_n(t)|\to 0\quad\text{as}\quad n\to\infty,
$$
which means that $|\lambda_n(t)|\to 0$ faster than $n^{-1}$ for any fixed $t$ (and thus also for any $t$ in the neighborhood around zero, which is all we need). 

That is, for any fixed $t,$
$$
\begin{align*}
\lambda_n(t) & = o(n^{-1})\quad (\Leftrightarrow n|\lambda_n(t)|\to 0\quad\text{as}\quad n\to\infty)\\[2ex] 
\frac{1}{2}\,\frac{1}{n}\sigma_0^2 & = O(n^{-1})
\end{align*}
$$

Thus, by @eq-CharacNormal
$$
\begin{align*}
\lim_{n\to\infty}\psi_{\sqrt{n}\left(\bar{X}_n-\mu_0\right)}(t) 
&= \lim_{n\to\infty}\psi_{\sum_{i=1}^n W_{i,n}}(t)\\[2ex] 
&= \lim_{n\to\infty}\left(1-\frac{1}{n}\,\frac{1}{2}\,\sigma_0^2 \, t^2 + \lambda_n(t)\right)^n\\[2ex]
&= \lim_{n\to\infty}\left(1-\frac{1}{n}\,\frac{1}{2}\,\sigma_0^2 \, t^2\right)^n\\[2ex]
&=\exp\left(-\frac{1}{2}\sigma_0^2\,t^2\right)\\[2ex]
&=\psi_{\Phi_{\sigma_0}}(t)
\end{align*}
$$


OK, we have shown that $H_n$ tends to $\Phi_{\sigma_0}$ by showing that the characteristic function of $H_n$ tends to that of $\Phi_{\sigma_0};$ **i.e. we have shown the Lindeberg-Lévy CLT.**

To show **bootstrap consistency** we need to show that $H_n^{Boot}$ also tends to $\Phi_{\sigma_0}.$ To do so, we can mimic the above proof, by showing that the characteristic function of $H_n^{Boot}$ tends to that of $\Phi_{\sigma_0}.$

Rewrite
$$
\begin{align*}
\sqrt{n}\left(\bar{X}^*_n- \bar{X}_n\right)|\mathcal{S}_n
& = \sum_{i=1}^n\frac{X^*_i- \bar{X}_n}{\sqrt{n}}|\mathcal{S}_n\\[2ex]
& = \sum_{i=1}^n W^*_{i,n}|\mathcal{S}_n
\end{align*}
$$
where 

* $W^*_{1,n}|\mathcal{S}_n,\dots,W^*_{n,n}|\mathcal{S}_n$ is i.i.d. with
* $\mathbb{E}(W^*_{1,n}|\mathcal{S}_n)=0$ and 
* $Var(W^*_{1,n}|\mathcal{S}_n)=\frac{1}{n}\hat{\sigma}_n^2=\frac{1}{n}\left(\frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}_n\right)^2\right)$

Therefore, by @eq-CharacFctSum together with @eq-CharacFctBillingsley
$$
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)|\mathcal{S}_n}(t)
&=\psi_{\sum_{i=1}^n W_{i,n}|\mathcal{S}_n}(t)\\[2ex] 
&=\left(\psi_{W_{1,n}|\mathcal{S}_n}(t)\right)^n\\[2ex] 
&=\left(1-\frac{1}{2}\,\frac{1}{n}{\color{darkgreen}\hat{\sigma}_n^2} \, t^2 + {\color{red}\lambda_n^*(t)}\right)^n,
\end{align*}
$$
where 
$$
\begin{align*}
|\lambda_n^*(t)|
&\leq |t^2|\,{\color{blue}\mathbb{E}^*}\left(\min\big(|t|\,n^{-3/2}\left|X^*_1-\bar{X}_n\right|^3, n^{-1}\left|X_1^* - \bar{X}_n\right|^2\big)\right)\\[2ex]
&= |t^2|\,{\color{blue}\frac{1}{n}\sum_{i=1}^n}\left(\min\big(|t|\,n^{-3/2}\left|X_i-\bar{X}_n\right|^3, n^{-1}\left|X_i - \bar{X}_n\right|^2\big)\right).
\end{align*}
$$
By the Marcinkiewicz strong law of large numbers, we obtain that 
$$
n{\color{red}|\lambda^*_n(t)|}\to_{a.s.} 0\quad\text{as}\quad n\to\infty, 
$$
i.e., ${\color{red}|\lambda^*_n(t)|}\to_{a.s.} 0$ faster than $n^{-1}.$ 


Moreover, 
$$
{\color{darkgreen}\hat\sigma_n^2} = \frac{1}{n}\sum_{i=1}^n\left(X_i-\bar{X}_n\right)^2\to_{a.s.}\sigma_0^2
$$

Thus, we have that (using @eq-CharacNormal)
$$
\begin{align*}
\psi_{\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)|\mathcal{S}_n}(t)
\to_{a.s.}&
\lim_{n\to\infty}\left(1-\frac{1}{2}\,\frac{1}{n}\sigma_0^2 \, t^2\right)^n\\[2ex]
&=\exp\left(-\frac{1}{2}\sigma_0^2\,t^2\right)\\[2ex]
&=\psi_{\Phi_{\sigma_0}}(t). 
\end{align*}
$$
This implies that the limit ($n\to\infty$) of $H_n^{Boot}$ is $\Phi_{\sigma_0}$ almost surely. 

Hence we have shown that the basic bootstrap is consistent for doing inference about $\mu_0$ using $\bar{X}_n.$


<!-- 
An appropriate central limit theorem argument implies that
$$
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
$$
Moreover, $\hat\sigma^2$ is a consistent estimator of $\sigma^2,$ and thus asymptotically $\hat\sigma^2$ may be replaced by $\sigma$. Therefore, 
$$
\begin{align*}
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X^* -\bar X)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
$$
On the other hand, by the CLT, we also have that 
$$
\begin{align*}
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right) 
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty\\[2ex]
\Rightarrow\left(\left.\sqrt{n}(\bar X - \mu)\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,\sigma^2),\quad n\to\infty.
\end{align*}
$$
This means that the bootstrap is **consistent**, since the bootstrap distribution of 
$$
\sqrt{n}(\bar X^* -\bar X)|{\cal S}_n
$$ 
asymptotically $(n\rightarrow\infty)$ coincides with the distribution of 
$$
\sqrt{n}(\bar X-\mu).
$$
In other words, for large $n$,
$$
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
$$ 
-->


This bootstrap consistency result justifies using the bootstrap distribution 
$$
\begin{align*}
H_n^{Boot}(x)
&=P\left(\sqrt{n}\left(\bar{X}^*_n-\bar{X}_n\right)\leq x|\mathcal{S}_n\right)\\[2ex] 
&\approx
\frac{1}{m}\sum_{j=1}^m 1_{\left(\sqrt{n}\left(\bar X^*_{n,j}-\bar X_n\right)\leq x\right)}=H_{n,m}^{Boot}(x),  
\end{align*}
$$ 
of $\bar{X}_n^\ast$ for doing inference about $\mu_0.$ 


In the following section, we show how to build a confidence interval using the bootstrap distribution of an estimator $\hat\theta_n.$ 



<!-- 
### Example: Inference about a Population Proportion

**Setup:** 

* **Data:** i.i.d. random sample $X_1,\dots,X_n,$ where $X_i\in\{0,1\}$ is dichotomous and $P(X_i=1)=p$, $P(X_i=0)=1-p$. 
* **Estimator:** Let 
$$
S=\sum_{i=1}^n 1_{(X_i = 1)}
$$ 
denote the number of $X_i$ which are equal to $1.$ Then, the  maximum likelihood estimate of $p$ is 
$$
\hat p=\frac{1}{n}S.
$$
* **Inference Problem:** What is the distribution of 
$$
(\hat{p} - p)?
$$



::: {.callout-note}

## Recall Asymptotics:

* $n\hat p=S\sim \mathcal{Binom}(n,p)$
* As $n\rightarrow\infty,$ the central limit theorem implies that
$$
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
$$
Thus for $n$ large, the distributions of $\sqrt{n}(\hat p -p)$ and $\hat p -p$ can be approximated by $\mathcal{N}(0,p(1-p))$ and $\mathcal{N}(0,p(1-p)/n)$, respectively.

::: 

**Bootstrap Approach:**

* Random sample $X_1^*,\dots,X_n^*$  generated by drawing observations
independently and with replacement from
$$
{\cal S}_n:=\{X_1,\dots,X_n\}.
$$ 
* Let 
$$
S^*=\sum_{i=1}^n 1_{(X_i^* = 1)}
$$  
denote the number of $X_i^*$ which are equal to $1.$
* Bootstrap estimate of $p$: 
$$
\hat p^*=\frac{1}{n}S^*
$$

The bootstrap now tries to approximate the true distribution of $\hat p - p$ by the **conditional** distribution of $(\hat p^*-\hat p)|\mathcal{S}_n$ given the observed sample ${\cal S}_n,$ where the latter can be approximated arbitrarily well $(m\to\infty)$ using the bootstrap estimators 
$$
p^*_1,p^*_2,\dots,p^*_m;
$$
namely by
$$
P\left(\hat{p}^* - \hat{p} \leq \delta|\mathcal{S}_n\right)\approx \frac{1}{m}\sum_{k=1}^m 1_{(\hat{p}^*_k - \hat{p} \leq\delta )}. 
$$

The bootstrap is called **consistent** if asymptotically $(n\rightarrow \infty)$ the conditional distribution of $(\hat p^*-\hat p)|{\cal S}_n$  coincides with the true distribution of $\hat p - p.$ (Note: a proper scaling is required!)

**The distribution of $X_i^*|\mathcal{S}_n$**

The conditional random variable $X_i^*|\mathcal{S}_n$ is a binary random variable 
$$
X_i^*|\mathcal{S}_n\in\{0,1\}.
$$
Since $X_i^*$ is drawn independently and with replacement from $\mathcal{S}_n,$ we obtain for each $i=1,\dots,n,$
$$
\begin{align*}
& P^*(X_i^*=1)=P(X_i^*=1|{\cal S}_n)=\hat p, \\[2ex]  
& P^*(X_i^*=0)=P(X_i^*=0|{\cal S}_n)=1-\hat p.
\end{align*}
$$
Thus, $X_i^*|{\cal S}_n$ is a Bernoulli distributed random variable with parameter $p=\hat{p}$
$$
X_i^*|{\cal S}_n \sim\mathcal{Bern}(p=\hat p), \quad i=1,\dots,n.\\[5ex]
$$


**The distribution of $\hat{p}^*|\mathcal{S}_n$**

The above implies that $n \hat{p}^*|{\cal S}_n$ has a Binomial distribution with parameters $n$ and $p=\hat{p},$  
$$
\underbrace{n \hat{p}_i^*}_{=S^*}|{\cal S}_n \sim\mathcal{Binom}(n, p=\hat p), \quad i=1,\dots,n.
$$

Therefore,
$$
\begin{align*}
\mathbb{E}^*(n \hat p^*)
&=\mathbb{E}(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p}\\[2ex]
\Rightarrow \mathbb{E}^*(\hat p^*) & = \hat{p}
\end{align*}
$$
and 
$$
\begin{align*}
Var^*(n \hat p^*)
&=Var(n \hat p^*|\ {\cal S}_n)\\[2ex]
& = n \hat{p} (1- \hat{p})\\[2ex]
\Rightarrow Var^*(\hat p^*) & = \frac{\hat{p}(1-\hat{p})}{n}
\end{align*}
$$

An appropriate central limit theorem argument implies that 
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$

Moreover, $\hat p$ is a consistent estimator of $p,$ and thus 
$$
\hat p(1-\hat p)\rightarrow_p p(1-p),\quad n\rightarrow\infty.
$$ 
Therefore, $\hat p(1-\hat p)$ can be replaced asymptotically by $p(1-p)$, and
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right)\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$
So, we can conclude that, 
$$
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0
$$
as $n\rightarrow\infty,$ where $\Phi$ denotes the distribution function of the standard normal distribution. This means that the bootstrap is consistent, i.e. for large $n$
$$
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx 
\text{distribution}(\sqrt{n}(\hat p -p))%\approx N(0,p(1-p))
$$
and therefore also
$$
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx 
\text{distribution}(\hat p -p).%\approx N(0,p(1-p)/n)
$$
-->



### The Basic Bootstrap Confidence Interval 

::: {.callout-note}

## Recall: Confidence Interval for $\theta_0$ using Classic Asymptotic Statistics

**Setup:**

* $\theta_0\in\mathbb{R}$ and 
* $\sqrt{n}(\hat\theta_n-\theta_0)\rightarrow_d\mathcal{N}(0,v_0^2)$ as $n\to\infty,$ 
* $\hat{v}_n\to_{p} v_0$ as $n\to\infty$

An approximate $(1-\alpha)\times 100\%$ confidence interval is then given by
$$
\left[
 \hat{\theta}_n - z_{1-\frac{\alpha}{2}}\frac{\hat v_n}{\sqrt{n}},
 \hat{\theta}_n + z_{1-\frac{\alpha}{2}}\frac{\hat v_n}{\sqrt{n}}
\right],
$$
where $z_{1-\frac{\alpha}{2}}$ denotes the $(1-\alpha)/2$ quantile of the standard Normal distribution $(z_{0.975}=1.96).$ This confidence interval is approximate, since it is only asymptotically justified, and, thus, is generally not exact in finite samples. 
::: 

In some cases it is, however, very difficult to obtain approximations $\hat v_n$ of $v_0$ (see @sec-Illustration). Statistical inference is then usually based on the **bootstrap confidence intervals**.

In many situations it can be shown that bootstrap confidence intervals (or tests) are even **more precise** than asymptotic normality based confidence intervals. (This applies to the bootstrap-$t$ method discussed in @sec-BootT.)


#### Algorithm of the Basic Bootstrap Confidence Interval for $\theta_0:$ {-}

**Setup:**

* **Data:** i.i.d. random sample 
$$
{\cal S}_n:=\{X_1,\dots,X_n\}
$$ 
with $X_i\overset{\text{i.i.d.}}{\sim} F$ for all $i=1,\dots,n.$
* The parameter of interest $\theta_0\in\mathbb{R}$ is an parameter of $F$. 
* $\hat{\theta}_n$ denotes the estimator of $\theta_0\in\mathbb{R}.$ 
* **Problem:** Construct a confidence interval for $\theta_0\in\mathbb{R}.$

::: {.callout-warning}

## Assumption: Bootstrap is Consistent

In the following, we will assume that the bootstrap is consistent; i.e. that
$$
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^*_n -\hat{\theta}_n)|{\cal S}_n)
&\approx 
\text{distribution}(\sqrt{n}(\hat{\theta}_n-\theta_0))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)|{\cal S}_n
&\overset{d}{\approx} \sqrt{n}(\hat{\theta}_n -\theta_0)
\end{align*}
$$
if $n$ is sufficiently large. 

Caution: This is not always the case and in cases of doubt one needs to show this property. 

**Good to know:** Theorem 1 in @Mammen_1992_Book shows that the basic bootstrap is consistent if $\sqrt{n}(\hat{\theta}_n-\theta_0)\to_d\mathcal{N}(0,v_0^2).$
:::

**Algorithm (3 Steps):**

1. Generate $m$ bootstrap estimates  
$$
\hat\theta_{n,1}^*,\dots,\hat\theta_{n,m}^*
$$
by repeatedly ($m$ times) drawing bootstrap samples $X_{1}^*,\dots,X_{n}^*$ independently and with replacement from $\mathcal{S}_n=\{X_1,\dots,X_n\}$ and computing $\hat{\theta}^\ast_{n,j}\equiv \hat{\theta}^\ast_{j}(X_{1}^*,\dots,X_{n}^*),$ $j=1,\dots,m.$


2. Use the $m$ bootstrap estimates $\hat\theta_{n,1}^*,\dots,\hat\theta_{n,m}^*$ to approximate the $\frac{\alpha}{2}$ and the $1-\frac{\alpha}{2}$ quantiles of the conditional distribution of $\hat{\theta}^*$ given ${\cal S}_n.$ This can be done with negligible approximation error (for $m$ large) using the empirical quantiles 
$$
\hat q^*_{n,p}=\left\{
  \begin{array}{ll}
  \hat\theta^*_{n,(\lfloor mp\rfloor+1)},         & mp \text{ not a whole number}\\
  (\hat\theta^*_{n,(mp)}+\hat\theta^*_{n,(mp+1)})/2,& mp \text{ a whole number}
\end{array}\right.
$${#eq-empiricalQuantile}
for $p=\frac{\alpha}{2}$ or $p=1-\frac{\alpha}{2},$ where $\hat\theta_{n,(j)}^*$ denotes the $j$th order statistic 
$$
\hat\theta_{n,(1)}^* \leq \hat\theta_{n,(2)}^*\leq \dots\leq \hat\theta_{n,(m)}^*,
$$
and $\lfloor mp\rfloor$ denotes the greatest whole number less than or equal to $mp$ (e.g. $\lfloor 4.9\rfloor = 4$).

3. The approximate $(1-\alpha)\times 100\%$  **basic bootstrap confidence interval** is then given by 
$$
\left[2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}, 2\hat{\theta}_n-\hat q^*_{n,\frac{\alpha}{2}}\right],
$${#eq-NPBootCI}
where $\hat{\theta}_n$ is computed from the original sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$ and the empirical quantiles $\hat q^*_{n,1-\frac{\alpha}{2}}$ and $\hat q^*_{n,\frac{\alpha}{2}}$ are computed from the bootstrap estimates $\hat\theta_{n,1}^*,\dots,\hat\theta_{n,m}^*.$


::: {.callout-note}
The quantiles $\hat q^*_{n,p}$ are those of the distribution 
$$
G_{n,m}^{Boot}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(\hat{\theta}^*_{n,j}\leq x\right)}.
$$ 
However, we'll treat the quantiles $\hat q^*_{n,p}$ as quantiles of the distribution
$$
G_{n}^{Boot}(x)=P\left(\hat{\theta}^*_{n}\leq x\,\big|\,\mathcal{S}_n\right),
$$
since for large $m$ ($m\to\infty$) the difference between $G_{n,m}^{Boot}$ and $G_{n}^{Boot}$ is negligible (Glivenko-Cantelli @thm-Clivenko-Cantelli) and we can choose $m$ to be large. 
:::

**Justifying the Basic Bootstrap CI (@eq-NPBootCI) for $\theta_0$:** 

The following three approximate statements $(\approx (1-\alpha))$ are exact for $m\to\infty:$
$$
\begin{align*}
&P^*\left(\hat q^*_{n,\frac{\alpha}{2}} \leq \hat{\theta}^*_n \leq \hat q^*_{n,1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P^*\left(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n \leq\hat{\theta}^*_n -\hat{\theta}_n \leq \hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n\right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P^*\left(
\sqrt{n}(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n)\leq{\color{red}\sqrt{n}(\hat{\theta}_n^*-\hat{\theta}_n)}\leq \sqrt{n}(\hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n)\right)
\approx 1-\alpha
\end{align*}
$$

Due to the assumed consistency of the bootstrap, we have that for large $n$
$$
{\color{red}\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}_n-\theta_0)}.
$$ 
Therefore, for large $n$ and large $m,$
$$
\begin{align*}
&P\left(
\sqrt{n}(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n)\leq{\color{blue}\sqrt{n}(\hat{\theta}_n-\theta_0)}\leq \sqrt{n}(\hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n)\right)\approx 1-\alpha\\[2ex]
\Rightarrow &P\left(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n\leq\hat{\theta}_n-\theta_0 \leq \hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n\right)
\approx 1-\alpha\\[2ex]
\Rightarrow &P\left(\hat q^*_{n,\frac{\alpha}{2}}-2\hat{\theta}_n\leq-\theta_0 \leq \hat q^*_{n,1-\frac{\alpha}{2}}-2\hat{\theta}_n\right)
\approx 1-\alpha\\[2ex]
%\Rightarrow &P\left(\hat{\theta}_n-(\hat q^*_{n,1-\frac{\alpha}{2}}-\hat{\theta}_n)\le \theta_0\le \hat{\theta}_n-(\hat q^*_{n,\frac{\alpha}{2}}-\hat{\theta}_n)\right)\approx 1-\alpha\\[2ex]
\Rightarrow &P\left(2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}\le \theta_0\le 2\hat{\theta}_n-
 \hat q^*_{n,\frac{\alpha}{2}}\right)\approx 1-\alpha.
\end{align*}
$$
This demonstrates that the **basic bootstrap confidence interval** in @eq-NPBootCI
$$
\left[2\hat{\theta}_n-\hat q^*_{n,1-\frac{\alpha}{2}}, 2\hat{\theta}_n-\hat q^*_{n,\frac{\alpha}{2}}\right],
$$
is indeed an asymptotically valid (i.e. approximate) $(1-\alpha)\times 100\%$ confidence interval.


#### Example: Basic Bootstrap Confidence Interval for the Population Mean {-}

**Setup:**

* **Data:** Let $X_1,\dots,X_n$ denote an i.i.d. random sample from $X\sim F$ with mean $\mu_0$ and variance $\sigma^2_0.$
* **Estimator:** $\bar X_n = \frac{1}{n} \sum_{i=1}^n X_i$ is an unbiased estimator of $\mu_0.$
* **Inference Problem:** Construct a confidence interval for $\mu_0.$


::: {.callout-note}

## Recall: Confidence Interval for $\mu_0$ using Classic Asymptotic Statistics

**Setup:**

* By the CLT: $\sqrt{n}(\bar X_n - \mu_0)\to_d\mathcal{N}(0,\sigma^2_0)$ as $n\to\infty$
* Estimation of $\sigma^2_0$: $\hat{\sigma}^2_n=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X_n)^2,$ where $\hat{\sigma}^2_n\to_p\sigma^2_0$ as $n\to\infty.$
* This implies: $\sqrt{n}((\bar X_n -\mu_0)/\hat{\sigma}_n)\to_d\mathcal{N}(0,1)$ as $n\to\infty$

Let $z_{\alpha/2}$ and $z_{1-\alpha/2}$ denote the $\alpha/2$ and the $(1-\alpha/2)$-quantile of $\mathcal{N}(0,1).$ Since $z_{\alpha/2} = -z_{1-\alpha/2},$ we have that 
$$
\begin{align*}
&P\left(-z_{1-\frac{\alpha}{2}}\le \frac{\sqrt{n}(\bar X_n -\mu_0)}{\hat{\sigma}_n}\le z_{1-\frac{\alpha}{2}}\right)\approx 1-\alpha\\[2ex]
\Rightarrow\quad
&P\left(-z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}\le \bar X_n -\mu_0\le z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}\right)\approx 1-\alpha\\[2ex]
\Rightarrow\quad 
&P\left(\bar X_n -z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}\le \mu_0\le 
        \bar X_n +z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_n}{\sqrt{n}}
  \right)\approx 1-\alpha
\end{align*}
$$

* Approximate $(1-\alpha)\times 100\%$ confidence interval: 
$$
\left[\bar X_n -z_{1-\frac{\alpha}{2}}\left(\frac{\hat{\sigma}_n}{\sqrt{n}}\right),
      \bar X_n +z_{1-\frac{\alpha}{2}}\left(\frac{\hat{\sigma}_n}{\sqrt{n}}\right)\right]
$$
:::



**Algorithm of the basic bootstrap confidence interval for $\mu_0$:**

The bootstrap offers an alternative method for constructing approximate $(1-\alpha)\times 100\%$ confidence intervals. We already know that the bootstrap is consistent in this situation (see @sec-Theory2BootstrapConsist). 

1. Draw $m$ bootstrap samples (e.g. $m=10,000$) and calculate the corresponding estimates 
$$
\bar X^*_{n,1},\bar X^*_{n,2},\dots,\bar X^*_{n,m}.
$$

2. Compute the empirical quantiles $\hat q^*_{n,\frac{\alpha}{2}}$ and $\hat q^*_{n,1-\frac{\alpha}{2}}$ from $\bar X^*_{n,1},\bar X^*_{n,2},\dots,\bar X^*_{n,m}.$ 

3. Compute the approximate $(1-\alpha)\times 100\%$ basic bootstrap confidence interval according to @eq-NPBootCI:
$$
\left[2\bar X_n -\hat q^*_{n,1-\frac{\alpha}{2}}, 
      2\bar X_n -\hat q^*_{n,\frac{\alpha}{2}}\right],
$$
where $\bar{X}_n$ is computed from the original sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$ and the empirical quantiles $\hat q^*_{n,1-\frac{\alpha}{2}}$ and $\hat q^*_{n,\frac{\alpha}{2}}$ are computed from the bootstrap estimates $\bar{X}_{n,1}^*,\dots,\bar{X}_{n,m}^*.$


## The Bootstrap-$t$ Method {#sec-BootT}

The basic bootstrap method is a nonparametric bootstrap method since the method does not require a specific distributional (i.e. parametric) assumption.


In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-$t$ method (one also speaks of the "studentized bootstrap"), which is also a nonparametric bootstrap method. The construction relies on so-called **(asymptotically) pivotal statistics**.

Let $X_1,\dots,X_n$ be an i.i.d. random sample and assume that the distribution of $X$ depends on an unknown parameter (or parameter vector) $\theta$.

::: {.callout-note appearance="minimal"} 
## 
::: {#def-pivotal}

## (Asymptotically) Pivotal Statistics

</br>

A statistic (i.e. a function of the random sample)
$$
T_n\equiv T(X_1,\dots,X_n)
$$ 
is called **exact pivotal**, if the distribution of $T_n$ does not depend on any unknown population parameter. 

A statistic $T_n$ is called **asymptotically pivotal**, if the asymptotic distribution of $T_n$ does not depend on any unknown population parameter.
:::
::: 


**Exact pivotal** statistics are rare and not available in most statistical or econometric applications. 


It is, however, often possible to construct an ***asymptotically pivotal*** statistic. Consider, for instance, an asymptotically normal $\sqrt{n}$-consistent estimator $\hat{\theta}_n$ of $\theta_0,$ i.e.
$$
\sqrt{n}(\hat{\theta}_n-\theta_0)\rightarrow_d\mathcal{N}(0,v_0^2),
$$
where $v^2$ denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a **consistent** estimator of $v_0^2$
$$
\hat v_n^2 \rightarrow_p v_0^2\quad\text{as}\quad n\to\infty,
$$
which implies (Continuous Mapping Theorem) that also 
$$
\hat v_n \rightarrow_p v_0\quad\text{as}\quad n\to\infty.
$$
Then, 
$$
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
$$ 
is **asymptotically pivotal**, since
$$
T_n = \sqrt{n}\frac{(\hat{\theta}_n-\theta_0)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$


#### Example: $\bar{X}_n$ {-}

Let $\mathcal{S}_n=\{X_1,\dots,X_n\}$ be a i.i.d. random sample with $X_i\sim X$ for all $i=1,\dots,n,$ with mean $\mathbb{E}(X)=\mu_0$, variance $0<Var(X)=\sigma_0^2<\infty$, and $\mathbb{E}(|X|^4)=\beta<\infty$. 
 
* If $X$ is **normally distributed**, we obtain
$$
T_n=\frac{\sqrt{n}(\bar X_n-\mu_0)}{s_n}\sim t_{n-1}\quad\text{for any}\quad n=2,3,\dots
$$
with $s_n^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X_n)^2$, where $t_{n-1}$ denotes the $t$-distribution with $n-1$ degrees of freedom. We can conclude that $T_n$ is **exact pivotal**.

* If $X$ is ***not* normally distributed**, the central limit theorem implies that
$$
T_n=\frac{\sqrt{n}(\bar X_n-\mu_0)}{s_n}\rightarrow_d\mathcal{N}(0,1),\quad\text{as}\quad n\to\infty.
$$
In this case $T_n$ is an **asymptotically pivotal statistic**.


#### Bootstrap-$t$ Consistency {-}

The general idea of the bootstrap-$t$ method relies on approximating the unknown distribution of 
$$
T_n = \sqrt{n}\frac{(\hat{\theta}_n-\theta_0)}{\hat v_n}
$$ 
by the approximable (via bootstrap resampling) conditional distribution of
$$
T_n^*\big|\mathcal{S}_n =\sqrt{n}\frac{(\hat{\theta}_n^*-\hat{\theta}_n)}{\hat v_n^*}\Big|\mathcal{S}_n,
$$ 
given $\mathcal{S}_n=\{X_1,\dots,X_n\},$ where the standard deviation estimate $\hat{v}_n^*$ is computed from the bootstrap sample $X_1^*,\dots,X_n^*,$ i.e.
$$
\hat v_n^*\equiv \hat{v}(X_1^*,\dots,X_n^*).
$$
 


::: {.callout-tip}
## Good news: Bootstrap-$t$ consistency follows if the basic bootstrap is consistent 

If the basic bootstrap is consistent and if the variance estimator $\hat{v}_n^2$ is consistent, then also the bootstrap-$t$ method is consistent. 

<!-- , i.e. if the conditional distribution of $\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)|\mathcal{S}_n$, given $\mathcal{S}_n$, yields a consistent estimate of $\mathcal{N}(0,v^2)$, then also the bootstrap-$t$ method is consistent. That is, then the conditional distribution of $T_n^*|\mathcal{S}_n$, given $\mathcal{S}_n$, provides a consistent estimate of the asymptotic distribution of $T_n\rightarrow_d \mathcal{N}(0,1)$ such that
$$
\sup_{x\in\mathbb{R}} \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*_n-\hat{\theta}_n)}{\hat v_n^*}\le x \;\right|\;{\cal S}_n\right)-\Phi(x)\right|\rightarrow_p 0,\quad\text{as}\quad n\to\infty,
$$
where $\Phi$ denotes the distribution function of the standard normal distribution.  -->
:::


### The Bootstrap-$t$ Confidence Interval 

**Setup:**

* Let ${\cal S}_n:=\{X_1,\dots,X_n\}$ be an i.i.d. random sample from $X\sim F$ with unknown parameter of interest $\theta_0\in\mathbb{R}.$ 
* Let $\hat{\theta}_n$ be a $\sqrt{n}$-consistent, asymptotically normal estimator of $\theta_0,$ i.e.
$$
\sqrt{n}\left(\hat{\theta}_n - \theta_0\right)\to_d\mathcal{N}(0,v_0^2)\quad\text{as}\quad n\to\infty
$$
* Assume that the **bootstrap is consistent**.
* Let $\hat{v}_n^2$ denote a **consistent** estimator of the asymptotic variance $v_0^2=\lim_{n\to\infty}Var\left(\sqrt{n}\left(\hat{\theta}_n-\theta_0\right)\right)$ i.e. 
$$
\hat v^2_n\equiv \hat v^2(X_1,\dots,X_n)
$$ 
such that 
$$
\begin{align*}
\hat v^2_n &\to_p v_0^2\quad\text{as}\quad n\to\infty
\end{align*}
$$
and
$$
\begin{align*}
\hat v_n   &\to_p v_0\quad\text{as}\quad n\to\infty.
\end{align*}
$$  


#### Algorithm of the Bootstrap-$t$ Confidence Interval for $\theta_0$: {-}

**Algorithm (3 Steps):**

1. Based on an i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $\mathcal{S}_n=\{X_1,\dots,X_n\},$ calculate the bootstrap estimates 
$$
\hat{\theta}^*_n\equiv \hat{\theta}^*(X_1^*,\dots,X_n^*)
$$ 
and
$$
\hat v^*_n\equiv \hat v^*(X_1^*,\dots,X_n^*)
$$ 
and the bootstrap statistic 
$$
\begin{align*}
T_n^*&=\sqrt{n}\frac{\hat{\theta}^*_n-\hat{\theta}_n}{\hat v^*_n}.
\end{align*}
$$ 
Repeating this yields $m$ (e.g. $m=100,000$) many bootstrap estimates 
$$
T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*.
$$

2. Compute the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat q^*_{n,\frac{\alpha}{2}}$ and $\hat q^*_{n,1-\frac{\alpha}{2}}$ of the bootstrap estimates $T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*$ (see @eq-empiricalQuantile). 

3. Compute the approximate $(1-\alpha)\times 100\%$ bootstrap-$t$ confidence interval  
$$
\left[\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{\hat v_n}{\sqrt{n}}\right),\;
      \hat{\theta}_n - \hat q^*_{n,  \frac{\alpha}{2}}   \left(\frac{\hat v_n}{\sqrt{n}}\right)\right],
$${#eq-Boot_tCI}
where $\hat\theta_n$ and $\hat v_n$ are the estimates of $\theta_0$ and $v_0$ based on the original sample $\mathcal{S}_n=\left\{X_1,\dots,X_n\right\},$ and where the empirical quantiles $\hat q^*_{n,1-\frac{\alpha}{2}}$ and $\hat q^*_{n,\frac{\alpha}{2}}$ are computed from the bootstrap estimates $T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*.$



**Justifying the Bootstrap-$t$ CI (@eq-Boot_tCI) for $\theta_0$:**

The bootstrap estimates 
$$
T_{n,1}^*,T_{n,2}^*, \dots, T_{n,m}^*
$$
yield the empirical bootstrap distribution
$$
H_{n,m}^{Boot}(x)=\frac{1}{m}\sum_{j=1}^m1_{\left(T_{n,j}^*\;\leq\; x\right)}
$$ 
which approximates the bootstrap distribution 
$$
H_{n}^{Boot}(x)=P\left(\left.T_{n}^*\leq x\;\right|\;\mathcal{S}_n\right)
$$ 
arbitrarily precise as $m\to\infty$ (Glivenko-Cantelli @thm-Clivenko-Cantelli).

Thus, the empirical bootstrap quantiles $\hat q^*_{n,\frac{\alpha}{2}}$ and $\hat q^*_{n,1-\frac{\alpha}{2}}$ of $H_{n,m}^{Boot}$ are indeed consistent ($m\to\infty$) for the quantiles $\hat q_{n,\frac{\alpha}{2}}$ and $\hat q_{n,1-\frac{\alpha}{2}}$ of the bootstrap distribution $H_{n}^{Boot}.$ This implies, for large $m,$
$$
P^*\left(\hat q^*_{n,\frac{\alpha}{2}}\leq {\color{red}\sqrt{n}\frac{\hat{\theta}^*_n-\hat{\theta}_n}{\hat v^*_n}} \leq \hat q^*_{n,1-\frac{\alpha}{2}}\right)
\approx 1-\alpha.
$$


Moreover, due to the assumed consistencies of the bootstrap and of the estimator $\hat v_n,$ we have that for large $n$ that 
$$
\left.{\color{red}\sqrt{n}\frac{\hat{\theta}^*-\hat{\theta}}{\hat v_n^*}}\right|\mathcal{S}_n\overset{d}{\approx}
{\color{blue}\sqrt{n}\frac{\hat{\theta}_n-\theta_0}{v_0}}.
$$
Therefore, for large $n$ and large $m,$ 
$$
\begin{align*}
& P\left(\hat q^*_{n,\frac{\alpha}{2}}\leq {\color{blue}\sqrt{n}\frac{\hat{\theta}_n-\theta_0}{v_0}} \leq \hat q^*_{n,1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P\left(\hat q^*_{n,\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right)\leq  \hat{\theta}_n-\theta_0 \leq \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right) \right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P\left(- \hat{\theta}_n + \hat q^*_{n,\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right) \leq -\theta_0 \leq - \hat{\theta}_n + \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right)\right)
\approx 1-\alpha\\[2ex]
\Rightarrow & P\left(\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}}\left(\frac{v_0}{\sqrt{n}}\right)\leq \theta_0 \leq \hat{\theta}_n - \hat q^*_{n,\frac{\alpha}{2}} \left(\frac{v_0}{\sqrt{n}}\right) \right)
\approx 1-\alpha
\end{align*}
$$
Thus, the approximate $(1-\alpha)\times 100\%$ bootstrap-$t$ confidence interval (@eq-Boot_tCI) 
$$
\left[\hat{\theta}_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{\hat v_n}{\sqrt{n}}\right),\;
      \hat{\theta}_n - \hat q^*_{n,  \frac{\alpha}{2}}   \left(\frac{\hat v_n}{\sqrt{n}}\right)\right],
$$
is indeed an asymptotic (i.e. approximate) $(1-\alpha)\times 100\%$ CI.  




#### Example: Bootstrap-$t$ Confidence Interval for the Mean {-}

Here $\hat\theta_n = \bar{X}_n$ and the estimator of the asymptotic variance of $\bar{X}_n$ is $s^2\approx \lim_{n\to\infty}n Var(\bar{X}_n)=\sigma_0^2$, where $s^2$ denotes the sample variance 
$$
s_n^2=\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar{X}_n\right)^2.
$$

**Algorithm:**

* Draw i.i.d. random samples $X_1^*,\dots,X_n^*$ from ${\cal S}_n$ and calculate
$\bar X^*$ as well as $s_n^*=\sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*_n)^2}$ to generate $m$ (e.g. $m=100,000$) bootstrap realizations
$$
\sqrt{n}\frac{\bar X^*_{n,1}-\bar X_n}{s^*_{n,1}},\dots,\sqrt{n}\frac{\bar X^*_{n,m}-\bar X_n}{s^*_{n,m}}
$$ 
* Determine $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat q^*_{n,\frac{\alpha}{2}}$ and $\hat q^*_{n,1-\frac{\alpha}{2}}$ from 
$$
\sqrt{n}\frac{\bar X^*_{n,1}-\bar X_n}{s^*_{n,1}},\dots,\sqrt{n}\frac{\bar X^*_{n,m}-\bar X_n}{s^*_{n,m}}
$$
using @eq-empiricalQuantile.
* This yields the $(1-\alpha)\times 100 \%$ confidence interval (using @eq-Boot_tCI):
$$
\left[\bar X_n - \hat q^*_{n,1-\frac{\alpha}{2}} \left(\frac{s_n}{\sqrt{n}}\right),
      \bar X_n - \hat q^*_{n,  \frac{\alpha}{2}} \left(\frac{s_n}{\sqrt{n}}\right)\right],
$$
where $\bar X_n$ and $s_n$ are computed from the original sample, i.e., 
$$
s_n=\sqrt{\frac{1}{n-1}\sum_{i=1}^n\left(X_i-\bar{X}\right)^2},  
$$
and where the empirical quantiles $\hat q^*_{n,1-\frac{\alpha}{2}}$ and $\hat q^*_{n,\frac{\alpha}{2}}$ are computed from the bootstrap estimates $\sqrt{n}\frac{\bar X^*_{n,j}-\bar X_n}{s^*_{n,j}},$ $j=1,\dots,m.$


### Accuracy of the Bootstrap-$t$ method 

Usually, the bootstrap-$t$ provides a **gain in accuracy** over the basic bootstrap. The reason is that the approximation of the law of $T_n$ by the bootstrap law of 
$$
\left.\frac{\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)}{\hat{v}^*_n}\;\right|\;\mathcal{S}_n
$$ 
is more direct and hence more accurate ($\hat{v}^*_n$ depends also on the bootstrap sample---not on the original sample) than by the bootstrap law of 
$$
\left.\sqrt{n}(\hat{\theta}^*_n-\hat{\theta}_n)\;\right|\;\mathcal{S}_n.
$$


The use of pivotal statistics and the corresponding construction of bootstrap-$t$ confidence intervals is motivated by theoretical results which show that under mild conditions the bootstrap-$t$ methods are  **second order accurate**.

Consider generally $(1-\alpha)\times 100\%$ confidence intervals of the form 
$$
[L_n,U_n]
$$ 
of $\theta$. The lower, $L_n$, and upper bounds, $U_n$, of such intervals are determined from the data and are thus random, 
$$
L_n\equiv L(X_1,\dots,X_n)
$$
$$
U_n\equiv U(X_1,\dots,X_n)
$$ 
and their accuracy depends on the particular procedure applied (e.g. basic bootstrap vs. bootstrap-$t$).


* Two-sided $(1-\alpha)\cdot 100\%$ confidence intervals $[L_n,U_n]$ are said to be **first-order accurate** if there exist some constant $c<\infty$ such that for sufficiently large $n$
$$
\begin{align*}
\left|P(\theta_0\not\in [L_n,U_n])-\alpha\right|\le \frac{c}{\sqrt{n}}
\end{align*}
$$
* Two-sided $(1-\alpha)\cdot 100\%$ confidence intervals $[L_n,U_n]$ are said to be **second-order accurate** if there exist some constant $c<\infty$ such that for sufficiently large $n$
$$
\begin{align*}
\left|P(\theta_0\not\in[L_n,U_n])-\alpha\right|\le \frac{c}{n}
\end{align*}
$$


If the distribution of $\hat\theta_n$ is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that

* Standard confidence intervals based on asymptotic normality approximations are **first-order** accurate. 
* Basic bootstrap confidence intervals are **first-order** accurate.
* Bootstrap-$t$ confidence intervals are **second-order** accurate.

The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to *much* better approximations. 

::: {.callout-note}
Proofs required for this type of results is technically difficult since Edgeworth expansions are involved. The investigation of the accuracy of the bootstrap estimators is still an active field (see, for instance, @koike2024high). 
:::



## Bootstrap and Linear Regression Analysis

In this chapter we consider two different bootstrap resampling procedures that can be applied in linear regression analysis. @sec-bootPairs considers the **Bootstrapping Pairs** algorithm and @sec-bootResid considers the **Residual Bootstrap** algorithm. We begin with outlining the general setup. 


Consider the linear regression model 
$$
Y_i=X_i^T\beta_0 + \varepsilon_i,\quad  i=1,\dots,n,
$$
where $Y_i\in\mathbb{R}$ denotes the response (or "dependent") variable and 
$$
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p
$$
denotes the vector of predictor variables. In the following, we differentiate between a **random design** and a **fixed design**. 

::: {.callout-note appearance="minimal"} 
## 
::: {#def-RandomFixedDesign}

## Random and fixed design 
<br>

**Random Design:**
$$
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
$$
are i.i.d. random variables with $\mathbb{E}(\varepsilon_i|X_i)=0,$ $M=\mathbb{E}(X_iX_i^T)$ non-singular, and with either

* **homoskedastic** errors: $\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2_0$, $i=1,\dots,n$, for a constant $\sigma^2<\infty$ or 
* **heteroskedastic** errors: $\mathbb{E}(\varepsilon_i^2|X_i)=\sigma^2_0(X_i)<\infty$, $i=1,\dots,n.$

**Fixed Design:** 
$$
X_1, X_2, \dots, X_n
$$
are deterministic vectors in $\mathbb{R}^p$ and $\varepsilon_1,\dots,\varepsilon_n$ are i.i.d. random variables with zero mean, $\mathbb{E}(\varepsilon_i)=0,$ and **homoskedastic errors**, $\mathbb{E}(\varepsilon_i^2)=\sigma^2_0,$ for all $i=1,\dots,n.$
:::
:::

The least squares estimator $\hat\beta_n\in\mathbb{R}^p$ is given by
$$
\begin{align*}
\hat\beta_n 
&=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i.
\end{align*}
$$

Using that $Y_i=X_i^\top\beta_0+\varepsilon_i,$ one can derive that
$$
\begin{align*}
\hat\beta_n 
&=\beta_0+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i.
\end{align*}
$$



### Bootstrap under Random Design: Bootstrapping Pairs {#sec-bootPairs}

Under a random design (@def-RandomFixedDesign), we assume that there exists a non-singular (thus invertible) matrix $M$ 
$$
M=\mathbb{E}(X_iX_i^T).
$$
This implies that the following matrix $Q$ is also non-singular:
$$
\begin{align*}
Q 
&=\mathbb{E}(\varepsilon_i^2X_iX_i^T)\\[2ex]
&=\mathbb{E}(\mathbb{E}(\varepsilon_i^2X_iX_i^T|X_i))\\[2ex]
&=\mathbb{E}(\sigma^2_0(X_i)X_iX_i^T\mathbb{E}(1|X_i))\\[2ex] 
&=\mathbb{E}(\sigma^2_0(X_i)X_iX_i^T)
\end{align*}
$$

::: {.callout-tip}
In case of homoskedastic errors, we have that 
$$
\begin{align*}
Q
&=\mathbb{E}(\sigma^2_0(X_i)X_iX_i^T)\\
&=\sigma^2_0\;\mathbb{E}(X_iX_i^T)\\[2ex] 
&=\sigma^2_0\;M.
\end{align*}
$$
:::

The law of large numbers, the continuous mapping theorem, Slutsky's theorem, and the central limit theorem (see econometrics lecture) implies that
$$
\sqrt{n}(\hat\beta_n-\beta_0)\rightarrow_d\mathcal{N}_p(0,M^{-1}QM^{-1}),\quad n\to\infty,
$$
where $\mathcal{N}_p(0,M^{-1}QM^{-1})$ denotes the $p$-dimensional normal distribution with $(p\times 1)$-dimensional mean $0$ and $(p\times p)$-dimensional variance-covariance matrix $M^{-1}QM^{-1}.$  


The idea of bootstraping pairs is very simple: The procedure builds upon the assumption that 
$$
(Y_1,X_1), (Y_2,X_2), \dots, (Y_n,X_n)
$$
are i.i.d. which suggests a bootstrap based on resampling the pairs $(Y_i,X_i),$ $i=1,\dots,n.$ 

**Bootstraping Pairs Algorithm:**

1. Generate bootstrap samples
  $$
  (Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
  $$ 
  by drawing observations independently and with replacement from ${\cal S}_n.$
2. Bootstrap estimators $\hat\beta^*_n$ are determined by least squares estimation from the data $(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*):$
$$
 \hat\beta^*_n=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
$$

Repeating Steps 1-2 $m$-many times yields $m$ (e.g. $m=10,000$) bootstrap estimates
$$
\hat\beta^*_{n,1},\dots,\hat\beta^*_{n,m}
$$
which allow us to approximate the bootstrap distribution $\hat\beta^*_n-\hat\beta_n|\mathcal{S}_n$ arbitrarily well as $m\to\infty.$
 

It can be shown that bootstrapping pairs is **consistent**; i.e. that for large $n$
$$
\text{distribution}(\sqrt{n}(\hat\beta^*_n-\hat\beta_n) |{\cal S}_n)\approx\mathcal{N}_p(0,M^{-1}QM^{-1})
$$


### Bootstrap under Fixed Design: The Residual Bootstrap {#sec-bootResid}

If the sample 
$$
(Y_1,X_1),\dots,(Y_n,X_n)
$$ 
is **not** an i.i.d. sample, the bootstrapping pairs procedure (@sec-bootPairs) proposed above will generally not be consistent. Therefore, bootstrapping pairs is not necessarily applicable for **fixed designs** and also generally not in time-series regression contexts. However, if error terms are **homoskedastic**, then it is possible to rely on the **residual bootstrap**.


In the following we will formally assume a regression model
$$
Y_i=X_i^T\beta_0 + \varepsilon_i, \quad i=1,\dots,n,
$$
with 
$$
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
$$
under **fixed design** (@def-RandomFixedDesign), where 
$$
\varepsilon_1,\dots,\varepsilon_n
$$ 
are i.i.d. with zero mean 
$$
\mathbb{E}(\varepsilon_i)=0
$$ 
and **homoskedastic** errors
$$
\mathbb{E}(\varepsilon_i^2)=\sigma^2_0.
$$ 


::: {.callout-tip}

## Applicability of the Residual Bootstrap under Random Designs

Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random designs---even when the $X$-variables are correlated (e.g. time-series). 

In such cases, the following arguments are meant *conditionally* on the observed predictors $X_1,\dots,X_n$. 

The above assumptions on the error terms then, of course, also have to be satisfied *conditionally* on $X_1,\dots,X_n.$
:::

The idea of the residual bootstrap is very simple: The procedure builds upon the assumption that the error terms 
$$
\varepsilon_1,\dots,\varepsilon_n
$$ 
are i.i.d which suggests a bootstrap based on **resampling the error terms**. 

These errors are, of course, unobserved, but they can be approximated by the corresponding residuals
$$
\hat \varepsilon_i=Y_i-X_i^T\hat\beta_n, \quad i=1,\dots,n,
$$
where 
$$
\hat\beta_n=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
$$ 
denotes the least squares estimator based on the original sample $\mathcal{S}_n$. 

It is well known that
$$
\hat\sigma^2_n= \frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i^2
$$ 
provides a consistent estimator of the error variance $\sigma^2$. That is,
$$
\hat\sigma^2_n\rightarrow_p \sigma_0^2
$$
as $n\to\infty.$


**Residual Bootstrap Algorithm:**

Based on the original data $(Y_i,X_i)$, $i=1,\dots,n$, and the least squares estimate $\hat\beta_n$, calculate the residuals $\hat\varepsilon_1,\dots,\hat \varepsilon_n$.

1. Generate random bootstrap samples $\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*$ of residuals by drawing observations independently and with replacement
from 
$$
{\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}.
$$
2. Calculate new depend variables 
$$
Y_i^*=X_i^T\hat\beta_n + \hat\varepsilon_i^*,\quad i=1,\dots,n
$$
3. Bootstrap estimators $\hat\beta^*_n$ are determined by least squares estimation from the data $(Y_1^*,X_1),\dots,(Y_n^*,X_n)$:
$$
\hat\beta^*_n = \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
$$

Repeating Steps 1-3 $m$ many times yields $m$ (e.g. $m=10,000$) bootstrap estimates
$$
\hat\beta^*_{n,1},\hat\beta^*_{n,2},\dots,\hat\beta^*_{n,m}
$$
which allow us to approximate the bootstrap distribution $\hat\beta^*_n-\hat\beta_n|\mathcal{S}_n$ arbitrarily well as $m\to\infty.$


#### Motivating the Residual Bootstrap {-}

It is not difficult to understand why the residual bootstrap generally works for *homoskedastic* (!) errors. We have
$$
\hat\beta_n-\beta_0=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
$$
and for large $n$ we have that
$$
\sqrt{n}(\hat\beta_n - \beta_0)\to_d\mathcal{N}_p(0,\sigma^2_0 M^{-1}),
$$ 
where $\mathcal{N}_p(0,\sigma^2 M^{-1})$ denotes the $p$ dimensional normal distribution with $(p\times 1)$ mean $0$ and $(p\times p)$ variance-covariance matrix $\sigma^2_0 M^{-1}.$

On the other hand (the bootstrap world), we have the construction
$$
\hat\beta^*_n - \hat\beta_n
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*.
$$
Conditionally on ${\cal S}_n,$ the bootstrap error terms $\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*$ are i.i.d with
$$
\mathbb{E}(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
$$
and
$$
Var(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 = \hat\sigma^2_n,
$$
where $\hat\sigma^2_n\to\sigma^2_0$ as $n\to\infty.$ 

An appropriate central limit theorem argument implies that 
$$
\left.\sqrt{n}(\hat\beta^*_n-\hat\beta_n)\right|\mathcal{S}_n\to_d\mathcal{N}_p\left(0,\sigma^2_0\, M^{-1}\right),
$$
as $n\to\infty,$ assuming that $\frac{1}{n}\sum_{i=1}^n X_iX_i^T\to M$ as $n\to\infty$ with $M$ being invertible. 

That is, for large $n$, we have that
$$
\text{distribution}(\sqrt{n}(\hat\beta^*_n-\hat\beta_n) |{\cal S}_n)
\approx\underbrace{\text{distribution}(\sqrt{n}(\hat\beta_n-\beta_0))}_{\mathcal{N}_p\left(0,\sigma^2_0\, M^{-1}\right)}
$$



### Bootstrap under Fixed Design: The Wild Bootstrap {#sec-bootWild} 


The wild bootstrap is a method for generating bootstrap samples that do not consist of resampling the original data (bootstrapping pairs in @sec-bootPairs) or residuals (bootstrapping residuals in @sec-bootResid). Rather, the wild bootstrap combines the data with random variables drawn from a known distribution to form a bootstrap sample. 

The wild bootstrap provides a way to deal with issues such as **heteroskedasticity of unknown form in fixed-design** regression models or random-design models in which one conditions on the predictors. 

In the following we will formally assume a regression model
$$
Y_i=X_i^T\beta_0 + \varepsilon_i, \quad i=1,\dots,n,
$$
with 
$$
X_i:=(\underbrace{X_{i1}}_{=1},X_{i2},\ldots,X_{ip})^T\in\mathbb{R}^p,
$$
where the $X_i$'s are fixed in repeated samples (**fixed design**), and where the error terms 
$$
\varepsilon_1,\dots,\varepsilon_n
$$ 
are independent across $i=1,\dots,n,$ with   
$$
\mathbb{E}(\varepsilon_i)=0\quad\text{for all}\quad i=1,\dots,n,
$$ 
and possibly **heteroskedastic** variances ($\sigma^2_{0,i}\neq \sigma^2_{0,j}$ for $i\neq j=1,\dots,n$)
$$
0<\mathbb{E}(\varepsilon_i^2)=\sigma^2_{0,i}<\infty\quad\text{for all}\quad i=1,\dots,n.
$$ 


That is, the data generating process is here independent across $i=1,\dots,n,$ but not necessarily identically distributed across $i=1,\dots,n.$


::: {.callout-tip}

## Applicability of the Wild Bootstrap under Random Designs

Though we will formally rely on a fixed design assumption, the wild bootstrap (as the residual bootstrap) is also applicable for random designs. 

In random designs, the following arguments are meant *conditionally* on the observed predictors $X_1,\dots,X_n$. 

The above assumptions on the error terms then, of course, also have to be satisfied *conditionally* on $X_1,\dots,X_n.$
:::



As the residual bootstrap (@sec-bootResid), the wild bootstrap uses the $X_i$'s from the original data. I.e., the $X_i$'s are not resampled. The wild bootstrap generates bootstrap samples 
$$
\{(Y_1^\ast,X_1),\dots,(Y_n^\ast,X_n)\}
$$
from 
$$
Y_i^\ast=X_i^\top\hat\beta_n + \varepsilon^\ast_i,\quad i=1,\dots,n,
$$
where the $\varepsilon_i^\ast$'s are generated by either of the following two methods:

1. Let 
$$
\hat\varepsilon_i=Y_i - X_i^\top\hat\beta_n,\quad i=1,\dots,n,
$$  
be the OLS residuals. For each $i=1,\dots,n,$ let $F_i$ be the $i$-specific distribution of a discrete two-point random variable 
$$
W_i\in\left\{\left(1-\sqrt{5}\right)\hat\varepsilon_i,\;\left(1+\sqrt{5}\right)\frac{\hat\varepsilon_i}{2}\right\}
$$ 
with 
$$
\begin{align*}
P\left(W_i = \left(1-\sqrt{5}\right)\hat\varepsilon_i\right) &= \frac{1+\sqrt{5}}{2\sqrt{5}}\\[2ex]
P\left(W_i = \left(1+\sqrt{5}\right)\frac{\hat\varepsilon_i}{2}\right)&=1 - \frac{1+\sqrt{5}}{2\sqrt{5}}.
\end{align*}
$$
Under this construction, we have that 
$$
\begin{align*}
\mathbb{E}(W_i)  &=0\\[2ex]
\mathbb{E}(W_i^2)&=\hat\varepsilon_i^2\\[2ex]
\mathbb{E}(W_i^3)&=\hat\varepsilon_i^3.
\end{align*}
$$
The wild bootstrap uses 
$$
\varepsilon_i^\ast = W_i\quad\text{for each}\quad i=1,\dots,n
$$
to generate the bootstrap samples. @Mammen_1993 provides a detailed discussion of the properties of this method.
2. The second method is an example of the **multiplier bootstrap**, meaning that the $\varepsilon_i^\ast$'s are multiples of transformations of the residuals $\hat{\varepsilon}_i$ and independent random variables. Specifically, let $U_i,$ $i=1,\dots,n,$ be real valued random variables that are independent from each other and independent of the residuals $\hat{\varepsilon}_i.$ Let $\mathbb{E}(U_i)=0$ and $\mathbb{E}(U_i^2)=1$ for all $i=1,\dots,n.$ One possibility would be $U_i\sim\mathcal{N}(0,1).$ Let $f(\hat{\varepsilon}_i)$ be a transformation of the residuals, where $f(\hat{\varepsilon}_i)=\hat{\varepsilon}_i$ is one possibility. The multiplier bootstrap uses 
$$
\varepsilon_i^\ast = U_i\,f(\hat{\varepsilon}_i)\quad\text{for each}\quad i=1,\dots,n
$$
to generate the bootstrap samples. @Davidson_and_Flachaire_2008 discuss properties of this method. 


Repeatedly generating new bootstrap errors 
$$
\varepsilon^\ast_1,\dots,\varepsilon^\ast_n,
$$ 
allows us to repeatedly generate new bootstrap samples 
$$
\{(\underbrace{X_1^\top\hat\beta_n + \varepsilon^\ast_1}_{=Y_1^\ast},X_1),\dots,(\underbrace{X_n^\top\hat\beta_n + \varepsilon^\ast_n}_{=Y_n^\ast},X_n)\},
$$
which allows us to repeatedly generate new bootstrap realizations 
$$
\hat\beta_n^\ast = \left(X^\top X\right)^{-1}X^\top Y^\ast. 
$$



### Bootstrap Confidence Intervals for the $j$th Component of the Regression Coefficient $\beta_{0,j}$

This chapter introduces two confidence intervals. The first uses the basic bootstrap method (@sec-BasicBootstrap); the second uses the bootstrap-$t$ method (@sec-BootT). 

Both confidence intervals can be constructed either via bootstrapping pairs (@sec-bootPairs) or via bootstrapping residuals (@sec-bootResid). 

::: {.callout-tip}
While the bootstrap confidence intervals based on bootstrapping pairs (@sec-bootPairs) are heteroskedasticity robust, the bootstrap confidence intervals based on bootstrapping residuals are only valid for homoskedastic errors.    
:::

#### Basic Bootstrap Confidence Intervals for $\beta_{0,j}$ 

Let $\beta_{0,j}\in\mathbb{R}$, $j=1,\dots,p$, denote the $j$th component of $\beta_0\in\mathbb{R}^p,$ and let $\hat{\beta}_{j,n}\in\mathbb{R}$ denote the $j$th component of the estimator $\hat\beta_n\in\mathbb{R}^p.$

The basic bootstrap confidence interval for $\beta_{0,j}\in\mathbb{R}$ can be constructed as following:

1. Use either bootstrapping pairs (@sec-bootPairs) or bootstrapping residuals (@sec-bootResid) or the wild bootstrap (@sec-bootWild) to generate $m$ (e.g. $m=10,000$) bootstrap realizations
$$
\hat{\beta}_{j,n,1}^\ast,\dots,\hat\beta_{j,n,m}^\ast.
$$


2. Determine the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat q^\ast_{n,\frac{\alpha}{2},j}$ and $\hat q^\ast_{n,1-\frac{\alpha}{2},j}$ from the bootstrap realizations $\hat{\beta}_{j,n,1}^\ast,\dots,\hat\beta_{j,n,m}^\ast$ using @eq-empiricalQuantile.

3. Compute the approximate $(1-\alpha)\times 100\%$ confidence interval as in @eq-NPBootCI:
$$
\left[2\hat\beta_{nj}-\hat q^\ast_{n,1-\frac{\alpha}{2},j}, 
      2\hat\beta_{nj}-\hat q^\ast_{n,\frac{\alpha}{2},j}\right],
$$
where $\hat\beta_{nj}$ denotes the $j$th component of $\hat\beta_{n}$ computed from the original sample $\mathcal{S}_n,$ and where the empirical quantiles $\hat q^\ast_{n,1-\frac{\alpha}{2},j}$ and $\hat q^\ast_{n,\frac{\alpha}{2},j}$ are computed from the bootstrap estimates $\hat{\beta}_{j,n,1}^\ast,\dots,\hat\beta_{j,n,m}^\ast.$

::: {.callout-note}

## Remark 

This basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval for **homoskedastic errors,** but also for **heteroskedastic** errors. In case of heteroskedastic errors, one needs to use an appropriate boostrap such as the pairs boostrap (random design) or the wild bootstrap (fixed design).

Note that standard confidence intervals usually provided by statistical software packages are for homoskedastic errors. For instance, the `confint(object)` function in `R` for an `object` returned by the `lm()` function uses the standard error formula for **homoskedastic** errors. 
::: 


<!-- 
#### Basic Bootstrap Confidence Intervals {-}

Basic **nonparametric bootstrap** confidence intervals for the regression coefficients $\beta_j$, $j=1,\dots,p,$ can be constructed as following: 

1. Generate $m$ bootstrap estimates
$$
\hat\beta_{n,1,j}^*,\hat\beta_{n,2,j}^*, \dots, \hat\beta_{n,m,j}^*.
$$

2. Compute the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat q_{n,\frac{\alpha}{2},j}$ and $\hat q_{n,1-\frac{\alpha}{2},j}$ (see @eq-empiricalQuantile) of the $m$ bootstrap estimates $\hat\beta_{n,1,j}^*,\hat\beta_{n,2,j}^*, \dots, \hat\beta_{n,m,j}^*.$

3. Compute the approximate $(1-\alpha)\times 100\%$ basic (nonparametric) bootstrap confidence interval as in @eq-NPBootCI:
$$
\left[2\hat\beta_{n,j}-\hat q_{n,1-\frac{\alpha}{2},j}, 
      2\hat\beta_{n,j}-\hat q_{n, \frac{\alpha}{2},j }\right],
$$
where $\hat\beta_{n,j}$ denotes the $j$th component of the estimate $\hat\beta_n\in\mathbb{R}^p$ computed from the orginal sample $\mathcal{S}_n.$ 
-->


#### Bootstrap-$t$ Confidence Intervals for $\beta_{0,j}$ 

Bootstrap-$t$ confidence intervals for the regression coefficients $\beta_{0,j}$, $j=1,\dots,p,$ can be constructed as following: 


Consider the statistic 
$$
T_n = \frac{\hat\beta_{j,n} -\beta_{0,j}}{\widehat{\operatorname{SE}}(\hat\beta_{j,n})},
$$
where 

* $\beta_{0,j}$ denotes the $j$th element of $\beta_0\in\mathbb{R}^p,$ 
* $\hat\beta_{j,n}$ denotes the $j$th element of the OLS estimator $\hat\beta_n\in\mathbb{R}^p.$ 

In the case of homoskedastic error terms
$$
\begin{align*}
\widehat{\operatorname{SE}}(\hat\beta_{j,n})
&=\frac{\hat{\sigma}_n\sqrt{\hat{\gamma}_{jj,n}}}{\sqrt{n}},
\end{align*}
$$
where 
$$
\hat{\sigma}_n=\sqrt{\frac{1}{n}\sum_{i=1}^n\hat{\varepsilon}_i^2}
$$
and where 
$$
\hat{\gamma}_{jj,n}
=\left[\widehat{M}_n^{-1}\right]_{jj}
=\left[\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^\top\right)^{-1}\right]_{jj}
$$
denotes the $j$-th diagonal element of the $(p\times p)$-dimensional matrix $\widehat{M}_n^{-1}=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}.$ 

In the case of heteroskedastic errors, one can use 
$$
\widehat{\operatorname{SE}}(\hat\beta_{j,n})
=\frac{\sqrt{\left[\widehat{M}_n^{-1}\widehat{Q}_n\widehat{M}_n^{-1}\right]_{jj}}}{\sqrt{n}}
$$
where 

* $\widehat{M}_n^{-1}=(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$
* $\widehat{Q}_n=\widehat{\mathbb{E}}(\varepsilon_i^2X_iX_i^\top)$ denotes a Heteroskedasticity Consistent (HC) estimators of $\mathbb{E}(\varepsilon_i^2X_iX_i^\top);$ e.g. the HC2-estimator  $\widehat{\mathbb{E}}(\varepsilon_i^2X_iX_i^\top)=\frac{1}{n-p}\sum_{i=1}^n\hat\varepsilon_i^2X_iX_i^\top.$

Note that $T_n$ is an asymptotically pivotal statistic; i.e.,
$$
T_n= \frac{(\hat\beta_{n,j}-\beta_{0,j})}{\widehat{\operatorname{SE}}(\hat\beta_{j,n})}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
$$ 

A bootstrap-$t$ interval for $\beta_{0,j}$, $j=1,\dots,p$, can thus be constructed as follows:

1. Use
* bootstrapping pairs (@sec-bootPairs) for random designs,
* bootstrapping residuals (@sec-bootResid) for fixed designs and homoskedastic errors, or 
* wild bootstrap (@sec-bootWild) for fixed desgins and heteroskedastic errors,
to generate $m$ (e.g. $m=10,000$) bootstrap realizations
$$
T^*_{n,1},T_{n,2}^*,\dots, T_{n,m}^*,
$$
with
$$
T^*_{n,k}=\frac{\hat\beta_{n,j}^*-\hat\beta_{0,j}}{\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})},\quad k=1,\dots,m,
$$
where 

   * $\hat\beta_{0,j}$ is computed from the original sample
   * $\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})$ is an appropriate estimate of the standard error (see above)
   * the residual components in $\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})$ are resampled
   * the $X$-components in $\widehat{\operatorname{SE}}(\hat\beta^*_{j,n})$ are resampled only in random designs, but kept fix in fixed desgins. 

2. Compute the empirical $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat q^\ast_{n,\frac{\alpha}{2},j}$ and $\hat q^\ast_{n,1-\frac{\alpha}{2},j}$ (see @eq-empiricalQuantile) from the bootstrap estimates $T^*_{n,1},T_{n,2}^*,\dots, T_{n,m}^*.$


3. Compute the $(1-\alpha)\times 100\%$ bootstrap-$t$ confidence interval as in @eq-Boot_tCI:
$$
\left[
  \hat\beta_{j,n}-\hat q^\ast_{1-\frac{\alpha}{2},n,j}\;\left(\widehat{\operatorname{SE}}(\hat\beta_{j,n})\right),\; 
  \hat\beta_{j,n}-\hat q^\ast_{\frac{\alpha}{2},n,j}\;\left(\widehat{\operatorname{SE}}(\hat\beta_{j,n})\right)
\right],
$$
where $\hat\beta_{n,j}$ and $\widehat{\operatorname{SE}}(\hat\beta_{j,n})$ are computed from the original sample $\mathcal{S}_n=((Y_1,X_1),\dots,(Y_n,X_n)),$ and where the empirical quantiles $\hat q^*_{n,1-\frac{\alpha}{2}}$ and $\hat q^*_{n,\frac{\alpha}{2}}$ are computed from the bootstrap realizations $T^*_{n,1},T_{n,2}^*,\dots, T_{n,m}^*.$


::: {.callout-note}

## Remark 
In case of **heteroskedastic errors,** one needs to use an appropriate bootstrap version (e.g. bootstrapping pairs (random desgin) or wild bootstrap (fixed desgin)) and an appropriate (heteroskedasticity robust) version of $\widehat{\operatorname{SE}}(\hat{\beta}_{j,n}^\ast).$
::: 

### Statistical Hypothesis Testing 

In the following, we consider a fixed design, where one can use the **residual bootstrap** (@sec-bootResid) or the **wild bootstrap** (@sec-bootWild). 


Suppose we want to test the hypothesis 
$$
\begin{align*}
H_0                    &: \beta_{0,j}    = 0\\[2ex] 
\text{against}\quad H_1&: \beta_{0,j} \neq 0
\end{align*}
$$
using the test statistic 
$$
T_n = \frac{\hat\beta_{j,n} - 0}{\widehat{\operatorname{SE}}(\hat\beta_{j,n})},
$$
where 

* $\beta_{0,j}$ denotes the $j$th element of $\beta_0\in\mathbb{R}^p,$ 
* $\hat\beta_{j,n}$ denotes the $j$th element of the OLS estimator $\hat\beta_n\in\mathbb{R}^p.$ 
* $\widehat{\operatorname{SE}}(\hat\beta_{j,n})$ is an appropriate estimate of the standard error (see above)

The $p$-value is given by 
$$
\begin{align*}
&p_{obs} = \\[2ex]
&2\,\min\left\{P(T_n \geq T_{n,obs}|H_0\;\text{is true}),\;P(T_n \leq T_{n,obs}|H_0\;\text{is true})\right\}
\end{align*}
$$
where $T_{n,obs}$ is the value of the test statistic computed from the original sample 
$$
\mathcal{S}_n=\left\{(Y_1,X_1),\dots,(Y_n,X_n)\right\}.
$$

To conduct the test using the bootstrap, we have to estimate $p_{obs}$ by the bootstrap algorithm. 

Central question: **How to generate bootstrap samples under $H_0$?**

To estimate $\beta_0\in\mathbb{R}^p$ under $H_0,$ we need to estimate all elements in $\beta_0$ that are not specified/fixed by $H_0$ leaving the other elements at their $H_0$-values.  

Let $\beta^{H_0}_0\in\mathbb{R}^{(p-1)}$ denote the parameter vector that contains all elements of $\beta_0\in\mathbb{R}^p$ that are not specified by $H_0.$ 
<!-- 
Under $H_0,$ we can rewrite the regression model as 
$$
\begin{align*}
\overbrace{Y_i - \beta_{0,j}^0 X_j}^{=\tilde{Y}_i} = \tilde{X}_i^\top \beta^{H_0}_0 + \varepsilon_i,
\end{align*}
$$
where $\tilde{X}_i\in\mathbb{R}^{(p-1)}$ denotes the predictor vector with the $j$th element removed.  
-->
The estimator of $\beta^{H_0}_0\in\mathbb{R}^{(p-1)}$ is then given by
$$
\underset{((p-1)\times 1)}{\hat{\beta}^{H_0}_{n}}=\left(\tilde{X}^\top\tilde{X}\right)^{-1}\tilde{X}^\top Y
$$
where the $(n\times (p-1))$-matrix $\tilde{X}$ is the matrix $X$ with the $j$th column removed. 

Using $\hat{\beta}^{H_0}_{n},$ we can compute the $(n\times 1)$-vector of residuals under $H_0$ as
$$
\left(\begin{matrix}\hat{\varepsilon}^{H_0}_1\\ \vdots\\\hat{\varepsilon}^{H_0}_n\end{matrix}\right)=\hat{\varepsilon}^{H_0}=Y-\tilde{X}\hat{\beta}^{H_0}_{n}.
$$

**Bootstrap algorithm:** 

1.1 **Residual Bootstrap:** Draw independently and with replacement $n$ values from 
$$
\{\hat{\varepsilon}^{H_0}_1, \dots, \hat{\varepsilon}^{H_0}_n\}
$$ 
to generate bootstrap realizations under $H_0$
$$
\{\hat{\varepsilon}^{H_0\ast}_1, \dots, \hat{\varepsilon}^{H_0\ast}_n\}.
$$ 
These allow us to generate the bootstrap samples under $H_0,$
$$
\left\{(Y_1^{H_0\ast},X_1),\dots,(Y_n^{H_0\ast},X_n)\right\},
$$
where 
$$
Y_i^{H_0\ast} = \tilde{X}_i^\top \hat{\beta}^{H_0}_{n} + \hat{\varepsilon}^{H_0\ast}_i,\quad i=1,\dots,n.
$$

1.2 **Wild Bootstrap:** Use  
$$
\{\hat{\varepsilon}^{H_0}_1, \dots, \hat{\varepsilon}^{H_0}_n\}
$$ 
to generate wild bootstrap errors under $H_0$
$$
\varepsilon^{H_0\ast}_i=\left\{
  \begin{array}{ll}
  (1-\sqrt{5})\hat{\varepsilon}^{H_0}_i&\text{with propability }(1+\sqrt{5})/2\sqrt{5}\\
  (1+\sqrt{5})\hat{\varepsilon}^{H_0}_i/2&\text{with propability }1-(1+\sqrt{5})/2\sqrt{5}
  \end{array}
\right.
$$
These allow us to generate the bootstrap samples under $H_0,$
$$
\left\{(Y_1^{H_0\ast},X_1),\dots,(Y_n^{H_0\ast},X_n)\right\},
$$
where 
$$
Y_i^{H_0\ast} = \tilde{X}_i^\top \hat{\beta}^{H_0}_{n} + \varepsilon^{H_0\ast}_i,\quad i=1,\dots,n.
$$
2. Based on the bootstrap sample 
$$
\left\{(Y_1^{H_0\ast},X_1),\dots,(Y_n^{H_0\ast},X_n)\right\}
$$ 
we can compute the bootstrap realization of the OLS estimator under $H_0,$
$$
\hat{\beta}^{\ast}_{n}=\left(X^\top X\right)^{-1} X^\top Y^{H_0\ast},
$$
which allows us to generate the corresponding realization of the test statistic 
$$
T_n^{\ast}
$$ 
under $H_0.$

3. Repeating Steps 1-2 leads to $m$-many (e.g. $m=10,000$) bootstrap realizations of the test statistic  
$$
T_{n,1}^{\ast},\dots,T_{n,m}^{\ast}
$$ 
under $H_0.$


To estimate the unknown $p_{obs},$ we can use now the following estimator
$$
\begin{align*}
&\hat p_{obs} = \\[2ex]
&=2\,\min\left\{\hat{P}(T_n \geq T_{n,obs}|H_0\;\text{is true}),\;\hat{P}(T_n \leq T_{n,obs}|H_0\;\text{is true})\right\}\\[2ex]
&=2\,\min\left\{
  \frac{1}{m}\sum_{j=1}^m 1_{\left(T_{n,j}^{\ast} \geq T_{n,obs}\right)},\;
  \frac{1}{m}\sum_{j=1}^m 1_{\left(T_{n,j}^{\ast} \leq T_{n,obs}\right)}
  \right\}
\end{align*}
$$

::: {.callout-tip}
In case of heteroskedasticity, the wild bootstrap and a corresponding formula for $\widehat{\operatorname{SE}}(\hat{\beta}_j)$ has to be used.
:::

## Exercises {-}


#### Exercise 1. {-} 

Consider the empirical distribution function 
$$
F_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{(X_i\leq x)}
$$
for a random sample 
$$
X_1,\dots,X_n\overset{\text{i.i.d.}}{\sim} F.
$$

(a) Derive the exact distribution of $nF_n(x)$ for a given $x\in\mathbb{R}.$ 

(b) Derive the asymptotic distribution of $F_n(x)$ for a given $x\in\mathbb{R}.$  

(c) Show that $F_n(x)$ is a point-wise (weakly) consistent estimator of $F(x)$ for each given $x\in\mathbb{R}$.


#### Exercise 2. {-} 

::: {.callout-tip}
Exercise 1 shows that the empirical distribution function is a **point-wise** consistent estimator for each given $x\in\mathbb{R}.$ However, point-wise consistency generally does not imply **uniformly** consistency for all $x\in\mathbb{R},$ and therefore the Clivenko-Cantelli (@thm-Clivenko-Cantelli) is so famous.  

This exercise is intended to show that that point-wise convergence does generally not imply uniform convergence.
:::  

Point-wise convergence of a function $g_n(x),$ i.e.,
$$
|g_n(x) - g(x)|\to 0
$$ 
for each $x\in\mathcal{X}\subset\mathbb{R}$ as $n\to\infty$ generally does not imply uniform convergence, i.e., 
$$
\sup_{x\in\mathcal{X}}|g_n(x) - g(x)|\to 0
$$ 
as $n\to\infty.$ 

Show this by providing an example for $g_n$ which converges point-wise, but not uniformly for $x\in\mathcal{X}$. 

<!-- 
http://personal.psu.edu/drh20/asymp/fall2002/lectures/ln03.pdf 
-->


#### Exercise 3. {-} 

Consider the following setup:

*  iid data $X_1,\dots,X_n$ with $X_i\sim F$
*  $\mathbb{E}(X_i)=\mu$
*  $Var(X_i)=\sigma^2<\infty$
*  Estimator: $\bar{X}_n=n^{-1}\sum_{i=1}^nX_i$

(a) Derive the classic confidence interval for $\mu$ using the asymptotic normality of the estimator $\bar{X}.$ Use a Monte Carlo simulation to approximate the coverage probability of this confidence interval for a sample size of $n=20$ and, 

* Part 1: For $F$ being the normal distribution with $\mu=1$ and standard deviation $\sigma=2$, and 
* Part 2: For $F$ being the $\chi^2_1$-distribution with $1$ degree of freedom. 

(b) Reconsider the case of $n=20$ and $F$ being the $\chi^2_1$-distribution with $1$ degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the standard nonparametric bootstrap confidence interval.  

(c) Reconsider the case of $n=20$ and $F$ being the $\chi^2_1$-distribution with $1$ degree of freedom. Use a Monte Carlo simulation to approximate the coverage probability of the bootstrap-$t$ confidence interval.


#### Exercise 4. {-}

<!-- Computational Statistics, James E. Gentle,  Exercise 13.1. -->

Let $\mathcal{S}_n = \{Y_1 , \dots, Y_n\}$ be a random sample from a population with mean $\mu$, variance $\sigma^2,$ and distribution function $F.$ Let $F_n$ be the empirical distribution function. Let $\bar{Y}$ be the sample mean for $\mathcal{S}_n.$ Let $\mathcal{S}^*_n = \{Y_1^∗,\dots, Y_n^∗\}$ be a random sample taken independently and with replacement from $\mathcal{S}_n.$ Let $\bar{Y}^*$ be the sample mean for $\mathcal{S}^*_n.$


(a) Show that 
$$
\mathbb{E}^*(\bar{Y}^*) = \bar{Y}
$$

(b) Show that 
$$
\mathbb{E}(\bar{Y}^*) = \mu
$$


<!-- 
#### Exercise 5. {-}
Computational Statistics, James E. Gentle,  Exercise 13.6. 
-->
<!-- {{< include Ch3_Solutions.qmd >}} -->


<!--
## Solutions {-}

#### Solutions of Exercise 1. {-} 

##### (a) {-}

The exact point-wise distribution of $nF_n(x)$ for a given $x\in\mathbb{R}.$  
$$
\begin{align*}
F_n(x) 
& = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}\\
\Rightarrow nF_n(x) 
& = \sum_{i=1}^n 1_{(X_i\leq x)} \sim \mathcal{Binom}\left(n,p=F(x)\right),
\end{align*}
$$
since $1_{(X_i\leq x)}$ is a Bernoulli random variable with parameter 
$$
\begin{align*}
p 
& = P(1_{(X_i\leq x)} = 1)\\[2ex]
& = P(X_i \leq x)\\[2ex] 
& = F(x).
\end{align*}
$$
Note that this holds for any distribution of $X_i.$ Therefore, one says that $nF_n(x)$ is **distribution free.**

##### (b) {-}

From (a), we have that (using the standard mean and variance expressions for Binomial distributed random variables):
$$
\begin{align*}
\mathbb{E}(nF_n(x)) &= nF(x)\\[2ex] 
\Leftrightarrow\quad  \mathbb{E}(F_n(x)) &= F(x)
\end{align*}
$$
and that 
$$
\begin{align*}
Var(nF_n(x)) &= nF(x)(1-F(x))\\[2ex]
\Leftrightarrow \quad Var(F_n(x)) &= \frac{F(x)(1-F(x))}{n}.
\end{align*}
$$

Moreover, since $F_n(x)  = \frac{1}{n} \sum_{i=1}^n 1_{(X_i\leq x)}$ is an average over i.i.d. random variables $1_{(X_1\leq x)},\dots,1_{(X_n\leq x)},$ the standard CLT (Lindeberg-Lévy) implies that 
$$
\frac{F_n(x)-F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}}\to_d\mathcal{N}(0,1)
$$
as $n\to\infty.$ Or with a slight abuse of notation: 
$$
F_n(x)\overset{a}{\sim}\mathcal{N}\left(F(x),\frac{F(x)(1-F(x))}{n}\right).
$$


##### (c) {-}


The mean squared error between $F_n(x)$ and $F(x)$ is given by
$$
\begin{align*}
\operatorname{MSE}(F_n(x)) 
&= \mathbb{E}\left((F_n(x)-F(x))^2\right)\\[2ex]
&= Var(F_n(x)) + \left(\mathbb{E}(F_n(x))-F(x)\right)^2.
\end{align*}
$$
It follows from our previous results that for each $x\in\mathbb{R}$
$$
Var(F_n(x)) = \frac{F(x)(1-F(x))}{n} \to 0 
$$
as $n\to\infty,$ and that 
$$
\mathbb{E}(F_n(x)) -F(x) = 0 
$$
for all $n.$ Therefore, 
$$
\operatorname{MSE}(F_n(x)) = Var(F_n(x)) \to 0
$$
as $n\to\infty.$ 

Thus we can conclude that $F_n(x)$ converges in the mean-square sense to $F(x)$ for each $x\in\mathbb{R},$ 
$$
F_n(x)\to_{ms} F(x)
$$
as $n\to\infty.$ 

Since convergence in the mean square sense implies convergence in probability, we also have that for each $x\in\mathbb{R}$
$$
F_n(x)\to_{p} F(x)
$$
as $n\to\infty$ which shows that $F_n(x)$ is weakly consistent for $F(x)$ for each $x\in\mathbb{R}.$



#### Solutions of Exercise 2. {-} 

::: {.callout-tip}
Another, equivalent way to define uniform convergence: 

$g_n(\cdot)$ converges **uniformly** to $g(\cdot)$ if for every $\varepsilon>0,$ there exists an $N$ such that 
$$
|g_n(x) - g(x)| < \varepsilon 
$$ 
for all $n\geq N$ and **for all** $x\in\mathcal{X},$ where $\mathcal{X}$ denotes the domain of the functions $g_n$ and $g.$ 


I.e., $g_n(\cdot)$ converges **uniformly** to $g(\cdot)$ if it is possible to draw an $\varepsilon$-band around the graph of $g(x)$ that contains **all of the graphs** of $g_n(x)$ for large enough $n\geq N.$
::: 

**Example 1:** $\mathcal{X}=\mathbb{R}$<br>
The function 
$$
g_n(x) = x\left(1+\frac{1}{n}\right)
$$ 
converges point-wise (for each given $x\in\mathbb{R}$) to 
$$
g(x)=x,
$$ 
since 
$$
|g_n(x)-g(x)|=\frac{|x|}{n}\to 0\quad \text{as}\quad n\to\infty.
$$
for each given $x\in\mathcal{X}.$

However, $g_n$ does not converge uniformly to $g$ since
$$
\sup_{x\in\mathbb{R}}|g_n(x)-g(x)|=\sup_{x\in\mathbb{R}}\frac{|x|}{n}=\infty\neq 0
$$
for each $n.$ 

Note that for a small $\varepsilon> 0,$ an $\varepsilon$-band around $g(x) = x$ fails to capture the graphs of $g_n(x)=x(1+1/n)$ with $n\geq N,$ since for any $N$ and $n^\ast\geq N$ we have that $g_{n^\ast}(x)=x(1+1/n^\ast)\to\infty$ as $x\to\infty.$ 


**Example 2:** $\mathcal{X}=(0,1)$<br>
The function 
$$
g_n(x) = x^n
$$ 
converges point-wise (for each given $x\in(0,1)$) to 
$$
g(x)=0,
$$ 
since 
$$
|g_n(x)-g(x)|=x^n\to 0\quad\text{as}\quad n\to\infty 
$$
for each given $x\in(0,1).$

However, $g_n$ does not converge uniformly to $g$ since
$$
\sup_{x\in(0,1)}|g_n(x)-g(x)|=\sup_{x\in(0,1)}x^n=1\neq 0
$$
for each $n.$ 

Note that for a small $0<\varepsilon<1,$ an $\varepsilon$-band around $g(x) = 0$ fails to capture the graphs of $g_n(x)=x^n,$ with $n\geq N,$ since for any $N$ and $n^\ast\geq N$ we have that $g_{n^\ast}(x)=x^{n^\ast}\to 1$ as $x\to 1.$ 



#### Solutions of Exercise 3. {-}

##### (a) Part 1: {-}

Setup:

*  iid data $X_1,\dots,X_n$ with $X_i\sim F$
*  $\mathbb{E}(X_i)=\mu$
*  $Var(X_i)=\sigma^2<\infty$
*  Estimator: $\bar{X}_n=n^{-1}\sum_{i=1}^nX_i$

If $F$ is a normal distribution:


$$
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\sim \mathcal{N}(0,1)\quad\text{exactly for all}\;n.
\end{array}
$$

For non-normal distributions $F$ we have by the classic CLT:
$$
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{\sigma}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
$$

Usually, we do not know $\sigma$ and have to estimate this parameter using a consistent estimator such as $s^2=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2$, where $s\to_p\sigma$ as $n\to\infty$.


Then by Slusky's Theorem (allows to combine $\to_d$ and $\to_p$-statements) we have that: 
$$
\begin{array}{rlc}
\sqrt{n}\left(\frac{\bar{X}_n-\mu}{s}\right)\to_d \mathcal{N}(0,1)\quad\text{as}\;n\to\infty.
\end{array}
$$


The **classic confidence interval** is then based on the above (asymptotic) normality result:
$$
\operatorname{CI}_{\operatorname{classic},n}=\left[\bar{X}_n\,-\,z_{1-\alpha/2}\frac{s}{\sqrt{n}},\bar{X}_n\,+\,z_{1-\alpha/2}\frac{s}{\sqrt{n}}\right],
$$
where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$-quantile of the standard normal distribution. Alternatively, one can apply a "small-sample correction" by using the $(1-\alpha/2)$-quantile $t_{n-1, 1-\alpha/2}$ of the $t$-distribution with $n-1$ degrees of freedom.  


From the above arguments it follows that:
$$
P\left(\mu\in \operatorname{CI}_{\operatorname{classic},n}\right)\to 1-\alpha\quad\text{as}\quad n\to\infty.
$$

Let us consider the finite-$n$ (with $n=20$) performance of the classic confidence interval for the case where $F$ is a **normal distribution** with mean $\mu=1$ and standard deviation $\sigma=2$:
```{r}
##  Setup:
n     <-   20 # Sample Size
mean  <-    1 # Mean
sdev  <-    2 # Standard Deviation
alpha <- 0.05 # Level

set.seed(123)
B          <- 1500 # MC repetitions
CI.lo.vec  <- rep(NA, B)
CI.up.vec  <- rep(NA, B)
  
## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  X.sample     <- rnorm(n=n, mean = mean, sd = sdev) 
  ## Estimates:
  X.bar.MC     <- mean(X.sample)
  sd.hat.MC    <- sd(X.sample)
  ## Classic CIs:
  
  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))
  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))

  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Classic 95% Confidence Intervals\n(Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==TRUE], 
       x1=CI.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==FALSE], 
       x1=CI.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


##### (a) Part 2: Classic Confidence Interval ($n=20$ and $X_i\sim \chi^2_1$) {-}


Now, we consider the  finite-$n$ performance of the classic confidence interval under the same setup as above, but for the case where $F$ is a **non-normal distribution**, namely, a $\chi^2_1$-distribution with $1$ degree of freedom:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)
alpha <- 0.05 # Level

set.seed(123)
B          <- 1500 # MC repetitions
CI.lo.vec  <- rep(NA, B)
CI.up.vec  <- rep(NA, B)
  
## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  X.sample     <- rchisq(n, df=df)
  ## Estimates:
  X.bar.MC     <- mean(X.sample)
  sd.hat.MC    <- sd(X.sample)
  ## Classic CIs:
  
  CI.lo.vec[b] <- X.bar.MC - qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))
  CI.up.vec[b] <- X.bar.MC + qnorm(p = 1-alpha/2)*(sd.hat.MC/sqrt(n))
  
  #CI.lo.vec[b] <- X.bar.MC - qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
  #CI.up.vec[b] <- X.bar.MC + qt(p = 1-alpha/2, df=n-1)*(sd.hat.MC/sqrt(n))
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.lo.vec <= mean  &  mean <= CI.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.lo.vec, CI.up.vec), ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Classic 95% Confidence Intervals\n(Non-Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==TRUE], 
       x1=CI.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.lo.vec[CI.checks==FALSE], 
       x1=CI.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


##### (b) Basic Bootstrap Confidence Interval ($n=20$ and $X_i\sim \chi^2_1$) {-}


Let's generate an iid random sample $S_n$ with $X_i\sim\chi^2_1$ and the corresponding estimate $\bar X_n$:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)

## IID random sample:
set.seed(123)
S_n  <- rchisq(n, df=df)

## Empirical mean:
(X.bar <- mean(S_n))
```


The **standard bootstrap confidence interval** is given by (see lecture script):
$$
\left[2\bar{X}_n - \hat{q}^\ast_{n,1-\alpha/2}, 2\bar{X}_n - \hat{q}^\ast_{n,\alpha/2}\right],
$$
where $\bar{X}_n$ denotes the estimate computed from the original sample, and $\hat{q}^\ast_{\alpha/2}$ and $\hat{q}^\ast_{1-\alpha/2}$ denote the $(\alpha/2)$ and $(1-\alpha/2)$-quantiles of the conditional distribution of $\bar{X}_n^\ast$ given $\mathcal{S}_n=\left\{X_1,\dots,X_n\right\}.$ 

In the following we first generate the $m$ bootstrap realizations 
$$
\bar{X}_{n,1}^\ast,\dots,\bar{X}_{n,m}^\ast,
$$ 
compute their quantiles $\hat{q}^\ast_{n,\alpha/2}$ and $\hat{q}^\ast_{n,1-\alpha/2},$ and plot all of this:

```{r, fig.margin = TRUE,fig.width=4.5, fig.height=3.5}
## Bootstr-Setup:
alpha            <- 0.05
n.Bootsrap.draws <- 1500

## Generate bootstap samples:
Bootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)

for(j in 1:n.Bootsrap.draws){
  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)
}
## Boostrap draws of \bar{X}_n^*:
X.bar.bootstr.vec <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)

## Quantile of the bootstr.-distribution of \bar{X}_n^*:
q.1 <- quantile(X.bar.bootstr.vec, probs = 1-alpha/2)
q.2 <- quantile(X.bar.bootstr.vec, probs = alpha/2)
## plot
plot(ecdf(X.bar.bootstr.vec), xlab="", ylab="",
     main=expression(paste("Bootstr.-Distr. of ",bar(X)[n]^{" *"})))
abline(v=c(q.1,q.2),col="red")
```

Using our preparatory work above, the basic bootstrap confidence interval can be computed as following:

```{r}
## Basic Bootstrap Confidence Interval:
CI.Basic.Bootstr.lo <- 2*X.bar - q.1
CI.Basic.Bootstr.up <- 2*X.bar - q.2

## Re-labeling of otherwise false names:
attr(CI.Basic.Bootstr.lo, "names") <- c("2.5%")
attr(CI.Basic.Bootstr.up, "names") <- c("97.5%")
##
c(CI.Basic.Bootstr.lo, CI.Basic.Bootstr.up)
```


Now, we can investigate the finite-$n$ performance of the standard bootstrap confidence interval:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)
mean  <-   df
alpha <- 0.05 # Level
n.Bootsrap.draws <- 1500

## MC-Setup:
set.seed(123)
B          <- 1500 # MC repetitions
CI.Basic.Bstr.lo.vec <- rep(NA, B)
CI.Basic.Bstr.up.vec <- rep(NA, B)

## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  S_n.MC        <- rchisq(n, df=df)
  ## Estimate:
  X.bar.MC      <- mean(S_n.MC)
  ## 
  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)
  for(j in 1:n.Bootsrap.draws){
    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)
  }
  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)
  ## (1-alpha/2)-quantile:
  q.1.MC <- quantile(X.bar.bootstr.MC.vec, probs = 1-alpha/2)
  q.2.MC <- quantile(X.bar.bootstr.MC.vec, probs = alpha/2)
  ## Basic Bootstrap CIs:
  CI.Basic.Bstr.lo.vec[b] <- 2*X.bar.MC - q.1.MC
  CI.Basic.Bstr.up.vec[b] <- 2*X.bar.MC - q.2.MC
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.Basic.Bstr.lo.vec<=mean & mean<=CI.Basic.Bstr.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.Basic.Bstr.lo.vec, CI.Basic.Bstr.up.vec), 
     ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Basic Bootrap 95% Confidence Intervals\n(Non-Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==TRUE], 
       x1=CI.Basic.Bstr.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.Basic.Bstr.lo.vec[CI.checks==FALSE], 
       x1=CI.Basic.Bstr.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


##### (c) Bootstrap-$t$ Confidence Interval ($n=20$ and $X_i\sim \chi^2_1$) {-}


The bootstrap-$t$ confidence interval is given by (see lecture script):
$$
\left[
  \bar{X}_n-\hat{q}^\ast_{n,1-\alpha/2}\left(\frac{s_n}{\sqrt{n}}\right),  
  \bar{X}_n-\hat{q}^\ast_{n,\alpha/2}  \left(\frac{s_n}{\sqrt{n}}\right)
\right],
$$
where $\bar{X}_n$ and $s_n=(n-1)^{-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2$ are computed from the original sample, and where $\hat{q}^\ast_{n,\alpha/2}$ and $\hat{q}^\ast_{n,1-\alpha/2}$ denote the empirical $(\alpha/2)$ and the $(1-\alpha/2)$-quantiles compute from the bootstrap estimates: 
$$
\sqrt{n}\frac{\bar{X}_{n,j}^\ast-\bar{X}_n}{s_{n,j}^\ast}\quad j=1,\dots,m.
$$

In the following we first generate the $m$ bootstrap realizations 
$$
\sqrt{n}\frac{\bar{X}_{n,j}^\ast-\bar{X}_n}{s_{n,j}^\ast}\quad j=1,\dots,m,
$$
compute their quantiles $\hat{q}^\ast_{n,\alpha/2}$ and $\hat{q}^\ast_{n,1-\alpha/2}$, and plot all of this:

```{r, fig.margin = TRUE, fig.width=4.5, fig.height=3.5}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)

## IID random sample:
set.seed(123)
S_n  <- rchisq(n, df=df)

## Empirical mean and sd:
X.bar   <- mean(S_n)
sd.hat  <- sd(S_n)

## Bootstr-Setup:
alpha            <- 0.05
n.Bootsrap.draws <- 1500

## Generate bootstap samples:
Bootstr.Samples  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)

for(j in 1:n.Bootsrap.draws){
  Bootstr.Samples[,j] <- sample(x=S_n, size=n, replace = TRUE)
}
## Compute boostrap draws of (\bar{X}_n^*-\bar{X}_n)/\hat{\sigma}^\ast:
X.bar.bootstr.vec    <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = mean)
sd.bootstr.vec       <- apply(X = Bootstr.Samples, MARGIN = 2, FUN = sd)
##
Bootstr.t.sample.vec <- sqrt(n)*(X.bar.bootstr.vec - X.bar)/sd.bootstr.vec
## Quantile of the bootstr.-distribution of \bar{X}_n^*:
q.1 <- quantile(Bootstr.t.sample.vec, probs = 1-alpha/2)
q.2 <- quantile(Bootstr.t.sample.vec, probs = alpha/2)
## plot
plot(ecdf(Bootstr.t.sample.vec), xlab="", ylab="",
     main=expression(paste("Bootstr.-t-Distr. of ",
          sqrt(n)(bar(X)[n]^{" *"}-bar(X)[n])/s[n]^{"*"})))
abline(v=c(q.1,q.2),col="red")
```



Using our preparatory work above, the bootstrap-$t$ confidence interval can be computed as following:
```{r}
## Basic Bootstrap Confidence Interval:
CI.Bstr.t.lo <- X.bar - q.1 * sd.hat/sqrt(n)
CI.Bstr.t.up <- X.bar - q.2 * sd.hat/sqrt(n)

## Re-labeling of otherwise false names:
attr(CI.Bstr.t.lo, "names") <- c("2.5%")
attr(CI.Bstr.t.up, "names") <- c("97.5%")
##
c(CI.Bstr.t.lo, CI.Bstr.t.up)
```



Let us investigate the finite-$n$ performance of the bootstrap-t confidence interval:
```{r}
## Setup:
n     <-   20  # Sample Size
df    <-    1  # (=> mean==1)
mean  <-   df
alpha <- 0.05 # Level
n.Bootsrap.draws <- 1500

## MC-Setup:
set.seed(123)
B          <- 1500 # MC repetitions
CI.Bstr.t.lo.vec <- rep(NA, B)
CI.Bstr.t.up.vec <- rep(NA, B)

## MC-Simulation:
for(b in 1:B){
  ## Data Generating Process:
  S_n.MC        <- rchisq(n, df=df)
  ## Estimates:
  X.bar.MC      <- mean(S_n.MC)
  sd.MC         <- sd(S_n.MC)
  ## 
  Bootstr.Samples.MC  <- matrix(NA, nrow=n, ncol=n.Bootsrap.draws)
  for(j in 1:n.Bootsrap.draws){
    Bootstr.Samples.MC[,j] <- sample(x=S_n.MC, size=n, replace = TRUE)
  }
  X.bar.bootstr.MC.vec <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = mean)
  sd.bootstr.MC.vec    <- apply(X = Bootstr.Samples.MC, MARGIN = 2, FUN = sd)
  ## Make it a "Bootstrap-t" sample:
  Bootstr.t.MC.vec     <- sqrt(n)*(X.bar.bootstr.MC.vec - X.bar.MC)/sd.bootstr.MC.vec
  ## (1-alpha/2)-quantile:
  q.1.MC <- quantile(Bootstr.t.MC.vec, probs = 1-alpha/2)
  q.2.MC <- quantile(Bootstr.t.MC.vec, probs = alpha/2)
  ## Basic Bootstrap CIs:
  CI.Bstr.t.lo.vec[b] <- X.bar.MC - q.1.MC * sd.MC/sqrt(n)
  CI.Bstr.t.up.vec[b] <- X.bar.MC - q.2.MC * sd.MC/sqrt(n)
}

## How often does the classic CI cover the true mean?
CI.checks      <- CI.Bstr.t.lo.vec<=mean & mean<=CI.Bstr.t.up.vec
freq.non.cover <- length(CI.checks[CI.checks==FALSE])/B

## Plot
plot(x=0,y=0,xlim=range(CI.Bstr.t.lo.vec, CI.Bstr.t.up.vec), 
     ylim=c(1,B), type="n", 
     ylab="MC Repetitions", xlab="", axes = FALSE, 
     main="Bootrap-t 95% Confidence Intervals\n(Non-Normal DGP)")
axis(1, at=c(1), labels ="True Mean = 1")
axis(2); box()
mtext(side = 1, text=paste0("(Freq. of Non-Covering CIs: ",
      round(freq.non.cover,digits = 2),")"), line = 2.5)
## Covering CIs:
arrows(x0=CI.Bstr.t.lo.vec[CI.checks==TRUE], 
       x1=CI.Bstr.t.up.vec[CI.checks==TRUE], 
       y0=c(1:B)[CI.checks==TRUE], y1=c(1:B)[CI.checks==TRUE], 
       angle=90, code = 3, length = .1, col="black")
## Non-Covering CIs:
arrows(x0=CI.Bstr.t.lo.vec[CI.checks==FALSE], 
       x1=CI.Bstr.t.up.vec[CI.checks==FALSE], 
       y0=c(1:B)[CI.checks==FALSE], y1=c(1:B)[CI.checks==FALSE], 
       angle=90, code = 3, length = .05, col="red")
abline(v=mean,col="blue", lwd=1.5)
```


#### Solutions of Exercise 4. {-}



##### (a)  {-}

$$
\begin{align*}
\mathbb{E}^*(\bar{Y}^*) 
& = \mathbb{E}\left(\left.\bar{Y}^*\right|\mathcal{S}_n\right)\\[2ex]
& = \mathbb{E}\left(\left.\frac{1}{n}\sum_{i=1}^n Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
& = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(\left.Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
& = \mathbb{E}\left(\left.Y_i^*\right|\mathcal{S}_n\right)\\[2ex]
& = \sum_{i=1}^n \frac{1}{n} Y_i
 = \bar{Y}
\end{align*}
$$
since $(Y_i^*|\mathcal{S}_n)\in\{Y_1,\dots,Y_n\}$ and $P(Y_j^*=Y_i|\mathcal{S}_n)=\frac{1}{n}$ for each $i,j\in 1,\dots,n.$


##### (b)  {-}


$$
\begin{align*}
\mathbb{E}(\bar{Y}^*) 
& = \mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n Y_i^*\right)\\[2ex]
& = \frac{1}{n}\sum_{i=1}^n \mathbb{E}\left(Y_i^*\right)\\[2ex]
& = \mathbb{E}\left(Y_i^*\right)\\[2ex]
& = \mu
\end{align*}
$$
since $Y_i^*\sim Y_i\sim F.$ 

-->

## References {-} 