# An Introduction to the Bootstrap 


Some literature:

* Hall, P. (1992): The Bootstrap and Edgeworth Expansion; Springer Verlag
* Shao, J. and Tu, D. (1995): The Jacknife and Bootstrap; Springer Verlag
* Horowitz, J.L. (2001): The Bootstrap. In: Handbook of Econometrics, Volume 5; Elsevier Science B.V.
* Davison, A.C. and Hinkley, D.V. (2005): Bootstrap Methods and their
Applications; Cambridge University Press


## The empirical distribution function


The distribution of a real-valued random variable $X$ can be completely described by its distribution function
$$
F(x)=P(X\leq x)\quad \text{for all } x\in\mathbb{R}.
$$


**Data:** i.i.d. random sample $X_1,\dots,X_n$

For given data, the sample analogue of $F$ is the so-called *empirical distribution function*, which is an important tool of statistical inference.


Let $I(\cdot)$ denote the indicator function, i.e., $I(x\leq t)=1$ if $x\leq t$, and $I(x\leq t)=0$ if $x>t.$


::: {#def-ecdf}

# Empirical distribution function

$$
F_n(x)=\frac{1}{n} \sum_{i=1}^n I(X_i\leq x)
$$
i.e $F_n(x)$ is the proportion of observations with $X_i\le x,$ $i=1,\dots,n.$
:::


**Properties:**

* $0\le F_n(x)\le 1$ 
* $F_n(x)=0$, if $x<X_{(1)}$, where $X_{(1)}\leq X_{(2)}\leq \dots \leq X_{(n)}$ denotes the **order-statistic**; i.e. $X_{(1)}$ is the smallest observation 
* $F(x)=1$, if $x\ge X_{(n)}$, where $X_{(n)}$ is largest observation
* $F_n$ monotonically increasing step function
* Structurally, $F_n$ itself is a distribution function; it is equivalent to the distribution function of a **discrete random variable** $X^*$ with possible values $X^*\in\{X_1,\dots,X_n\}$ and with $P(X^*=X_i)=\frac{1}{n}$ for all $i=1,\dots,n.$


::: {#exm-ecdfexample}

# Empirical distribution function

<br>

Some data:

| $i$  | $X_i$| 
|------|------|
| 1    | 5.20 |
| 2    | 4.80 |
| 3    | 5.40 |
| 4    | 4.60 |
| 5    | 6.10 |
| 6    | 5.40 |
| 7    | 5.80 |
| 8    | 5.50 |


Corresponding empirical distribution function using `R`:

```{r, ecdfPlot}

observedSample <- c(5.20, 4.80, 5.40, 4.60, 
                    6.10, 5.40, 5.80, 5.50)

myecdf_fun     <- ecdf(observedSample)

plot(myecdf_fun, main="")
```
:::


$F_n(x)$ depends on the observed sample and thus is **random**. We obtain

* For every $x\in\mathbb{R}$
$$
nF_n(x)\sim B(n, p=F(x))
$$
I.e., $nF_n(x)$ has a binomial distribution with parameters $n$ and $p=F(x)$.

* $E(F_n(x))=F(x)$
* $Var(F_n(x))=\frac{F(x)(1-F(x))}{n}$



::: {#thm-Clivenk-Cantelli}

# Theorem of Glivenko-Cantelli
$$
P\left(\lim_{n\rightarrow\infty} \sup_{x\in\mathbb{R}} |F_n(x)-F(x)|=0\right)=1
$$

:::




## The bootstrap: Basic idea

The bootstrap is an important tool of modern statistical analysis. It establishes a general framework for simulation-based statistical inference. In simple situations the uncertainty of an estimate may be gauged by analytical calculations leading, for example, to the construction of confidence intervals based on an assumed probability model for the available data. The bootstrap replaces complicated and often inaccurate approximations to biases, variances and other measures of uncertainty by computer simulations.


#### The idea of the bootstrap {-}

* The random sample $X_1,\dots,X_n$ is generated by drawing observations independently and with replacement from the underlying population with distribution function $F$. That is, for each interval $[a,b]$ the probability of drawing an observation in $[a,b]$ is given by 
$$
P(X\in [a,b])=F(b)-F(a).
$$
* For large $n$: The empirical distribution $F_n$ of the sample values is "close" to the unknown distribution $F$ of $X$ in the underlying population. That is, the relative frequency of observations $X_i$ in $[a,b]$ converges to $P(X\in [a,b])$; i.e. for $n\rightarrow\infty$
  $$
  \begin{align*}
  \frac{1}{n}\sum_{i=1}^nI(X_i\in[a,b])&\to_p P(X\in [a,b])\\
  \Leftrightarrow \qquad F_n(b)-F_n(a)&\to_p F(b)-F(a)
  \end{align*}
  $$
* The idea of the bootstrap consists in mimicking the data generating process:
   * Random sampling from the true population (i.e. from $F$) is replaced by random sampling from the **observed data** (i.e. from $F_n$). This is justified by the insight that the empirical distribution $F_n$ of the observed data is "similar" to the true distribution $F$ (@thm-Clivenk-Cantelli).



### Example: Inference about the population mean 


**Setup:**

* **Population Model:** Continuous random variable $X$ with *unknown* mean $\mu$
* **Data:** i.i.d. sample $X_1,\dots,X_n$ from $X$
* **Problem:** What is the distribution of $\bar{X} -\mu$?

Now assume that $n=8$ and that the **observed sample** is

| $i$  | $X_i$| 
|------|------|
| 1    | -0.6 |
| 2    |  1.0 |
| 3    |  1.4 |
| 4    | -0.8 |
| 5    |  1.6 |
| 6    |  1.9 |
| 7    | -0.1 |
| 8    |  0.7 |

```{r}
observedSample <- c(-0.6, 1.0, 1.4, -0.8, 1.6, 1.9, -0.1, 0.7)
```

So the sample mean is 
<center>
$\bar X =$ `mean(observedSample)` $=$ `r mean(observedSample)`
</center>

<br>

**Bootstrap:**

The observed sample ${\cal S}_n=\{X_1,\dots,X_n\}$ is taken as underlying empirical "population" in order to generate **"Bootstrap data"** $X_1^*,\dots,X_n^*$:

* i.i.d. samples $X_1^*,\dots,X_n^*$ are generated by drawing   observations independently and with replacement from ${\cal S}_n=\{X_1,\dots,X_n\}$.

```{r}
## generating a bootstrap sample
bootSample <- sample(x       = observedSample, 
                     size    = length(observedSample), 
                     replace = TRUE)
```
* The distribution of $\bar X -\mu$ is approximated by the conditional distribution of $\bar X^* -\bar X$ given the original sample ${\cal S}_n$
$$
\underbrace{P\left(\bar{X}-\mu<\delta\right)}_{\text{unknown}}\approx 
\underbrace{P\left(\bar{X}^*-\bar{X}<\delta|\mathcal{S}_n\right)}_{\text{approximable}}
$$


For the given data with $n=8$ observations, there are 
$$
n^n=8^8=16,777,216
$$ 
possible bootstrap samples which are all equally probable. 

The conditional distribution function of $\bar{X}^*-\bar{X}$ given $\mathcal{S}_n$ 
$$
P\left(\bar{X}^*-\bar{X}<\delta|\mathcal{S}_n\right)
$$
can be approximated using Monte-Carlo simulations.


For example, $m=5$ simulation runs may lead to the following results:


Simul. | $X_1^*$|  $X_2^*$| $X_3^*$| $X_4^*$|  $X_5^*$|  $X_6^*$|  $X_7^*$|   $X_8^*$ | $\bar X^*-\bar{X}$
----|----:|----:|----:|----:|----:|----:|----:|----:|:----:
1 | 1.9| -0.8| 1.9|  -0.6| 1.4| -0.1| -0.8| 1.0 | `r 0.4875 -mean(observedSample)`
2 | 0.7| -0.8| -0.8| 1.0 | 1.6| 1.0| -0.1| -0.8 | `r 0.225-mean(observedSample)`  
3 | -0.1| 1.9| 0.7|  1.0| -0.1| 1.6| 1.0| -0.6 |  `r 0.675-mean(observedSample)` 
4 | 1.4| 1.0| 1.4|  -0.1| 1.9| -0.8| 1.9| 1.0 | `r 0.9625-mean(observedSample)` 
5 | 1.0| 0.7| -0.1|  0.7| 1.4| -0.8| 1.0| 1.6 |  `r 0.6875-mean(observedSample)`

Of course, $m=5$ simulations will not be enough. But using $m=1000$ or $m=2000$ then 
$$
P\left(\bar X^*-\bar X\leq \delta |{\cal S}_n\right) \approx \frac{\sum_{k=1}^m
 I( \bar X^*_k-\bar X\leq \delta)}{m}
$$
will provide a fairly accurate approximation.

```{r}
n                <- length(observedSample)
Xbar             <- mean(observedSample)
m                <- 2000
bootRealizations <- numeric(m)

for(l in seq_len(m)){
 bootSample          <- sample(x       = observedSample, 
                               size    = n, 
                               replace = TRUE)
 bootXbar            <- mean(bootSample)
 bootRealizations[l] <- bootXbar - Xbar
}

plot(ecdf(bootRealizations), main="")
```



The bootstrap focuses on the **conditional** distribution of $X_1^*,\dots,X_n^*$ given the observed sample ${\cal S}_n=\{X_1,\dots,X_n\}$ and the resulting conditional distribution of
$$
\bar X^* -\bar X.
$$ 
Often these conditional distributions are called **bootstrap distributions**.

In the bootstrap literature one also frequently finds the notation $E^*(\cdot)$, $Var^*(\cdot)$, or $P^*(\cdot)$ to denote **conditional** expectations, variances, or probabilities given the sample ${\cal S}_n.$



#### The bootstrap distribution of $\bar X^* -\bar X$ {-}

In order to analyze the bootstrap distribution of $\bar X^* -\bar X$, let us first study properties of the random variables $X_i^*.$ Some basic properties of the bootstrap distribution of $X_i^*$ are easily
verified:

* For each $i=1,\dots,n$ the possible values of $X_i^*$ are $X_1,\dots,X_n$, and each of these values is equally probable 
$$
\begin{align*}
\frac{1}{n} 
&= P(X_i^*=X_1|{\cal S}_n)\\
&= P(X_i^*=X_2|{\cal S}_n)\\
&=\dots\\ 
&=P(X_i^*=X_n|{\cal S}_n)
\end{align*}
$$

* The conditional mean of $X_i^*$ is
$$
\begin{align*}
E^*(X_i^*)
&:=E(X_i^*|{\cal S}_n)\\
&=\frac{1}{n}X_1+\frac{1}{n}X_2+\dots+\frac{1}{n}X_2\\
&=\bar X
\end{align*}
$$

* The conditional variance of $X_i^*$ is
$$
\begin{align*}
Var^*(X_i^*)
&:=Var(X_i^*|{\cal S}_n)\\ 
&=\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2\\ 
&=\hat\sigma^2
\end{align*}
$$

Now we can consider the bootstrap distribution of $\bar X^* -\bar X.$

As discussed above, conditional on ${\cal S}_n=\{X_1,\dots,X_n\},$ the random variables $X_1^*,\dots,X_n^*$ are i.i.d. random variables with mean 
$E^*(X_i^*)=\bar X$ 
and variance 
$Var^*(X^*)=\hat\sigma^2$. 
This obviously implies that
$$
E^*(\bar X^*)=\bar X\quad \text{and}\quad Var^*( \bar X^*)=\frac{\hat\sigma^2}{n}.
$$

Thus, the central limit theorem (Lindeberg-LÃ©vy) implies that
$$
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\hat\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
$$

Moreover, $\hat\sigma^2$ is a consistent estimator of $\sigma^2,$ and thus asymptotically $\hat\sigma^2$ may be replaced by $\sigma$. Therefore, 
$$
\left(\left.\frac{\sqrt{n}(\bar X^* -\bar X)}{\sigma}\right|{\cal S}_n\right)\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
$$



On the other hand, we also have that 
$$
\left(\frac{\sqrt{n}(\bar X -\mu )}{ \sigma}\right) 
\rightarrow_{d} \mathcal{N}(0,1),\quad n\to\infty.
$$

This means that the bootstrap is **consistent**: The bootstrap distribution of $\sqrt{n}(\bar X^* -\bar X)$ asymptotically coincides with the distribution of $\sqrt{n}(\bar X-\mu)$ as $n\rightarrow
\infty$. In other words, for large $n$,
$$
\text{distribution}(\bar X^* -\bar X|{\cal S}_n)\approx \text{distribution}(\bar X-\mu).
$$

This **bootstrap consistency** justifies using and trusting the approximable bootstrap distribution $P(\bar{X}^*-\bar{X}\leq \delta|\mathcal{S}_n)$ as a tool for doing inference about $\mu.$ 


### Example: Inference about a population proportion

**Setup:** 

* **Data:** i.i.d. random sample $X_1,\dots,X_n,$ where $X_i\in\{0,1\}$ is dichotomous and $P(X_i=1)=p$, $P(X_i=0)=1-p$. 
* **Estimator:** Let $S$ denote the number of $X_i$ which are equal to $1.$ The maximum likelihood estimate of $p$ is $\hat p=S/n.$
* **Problem:** Inference about $p$.

**Recall:** 

* $n\hat p=S\sim B(n,p)$
* As $n\rightarrow\infty$ the central limit theorem implies that
$$
\frac{\sqrt{n}(\hat p -p)}{\sqrt{p(1-p)}}\rightarrow_d \mathcal{N}(0,1)
$$
* Thus for $n$ large, the distributions of $\sqrt{n}(\hat p -p)$ and $\hat p -p$ can be approximated by $\mathcal{N}(0,p(1-p))$ and $\mathcal{N}(0,p(1-p)/n)$, respectively.

**Bootstrap:**

* Random sample $X_1^*,\dots,X_n^*$  generated by drawing observations
independently and with replacement from
$$
{\cal S}_n:=\{X_1,\dots,X_n\}.
$$ 
* Let $S^*$ denote the number of $X_i^*$ which are equal to $1.$
* Bootstrap estimate of $p$: $\hat p^*=S^*/n$


The distribution of $\hat p^*$ depends on the observed sample ${\cal S}_n:=\{X_1,\dots,X_n\}$. A different sample ${\cal S}_n$ will lead to a different distribution. The bootstrap now tries to approximate the true distribution of $\hat p - p$ by the **conditional** distribution of $\hat p^*-\hat p$ given the observed sample ${\cal S}_n$. 


The bootstrap is called **consistent** if asymptotically (i.e. for $n\rightarrow \infty$) the conditional distribution of $\hat p^*-\hat p$  coincides with the true distribution of $\hat p - p.$ (Note: a proper scaling is required!)


We obtain
$$
\begin{align*}
& P^*(X_i^*=1)=P(X_i^*=1|\ {\cal S}_n)=\hat p, \\  
& P^*(X_i^*=0)=P(X_i^*=0|\ {\cal S}_n)=1-\hat p
\end{align*}
$$
and
$$
\begin{align*}
&   E^*(\hat p^*)=E(\hat p^*|\ {\cal S}_n)=\hat p, \\ 
& Var^*(\hat p^*)=E[(\hat p^*-\hat p)^2|\ {\cal S}_n]=\frac{\hat p(1-\hat p)}{n}
\end{align*}
$$

The bootstrap distribution of $n\hat p^*=S^*$ given ${\cal S}_n$ is equal to $B(n,\hat p)$. For large $n$ this means that the bootstrap distribution of 
$$
\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}
$$ 
is approximately standard normal. In other words,
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{\hat p(1-\hat p)}}\right|{\cal S}_n\right) \rightarrow_d \mathcal{N}(0,1)
$$

Moreover, $\hat p$ is a consistent estimator of $p$ and therefore $\hat p(1-\hat p)\mathbb{R}\rightarrow_p p(1-p)$ as $n\rightarrow\infty$. This implies that asymptotically $\hat p(1-\hat p)$ may be replaced by $p(1-p)$, and
$$
\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{ \sqrt{p(1-p)}}\right|{\cal S}_n\right) \mathcal{N}(0,1)
$$
More precisely, as $n\rightarrow\infty$
$$
\sup_\delta \left|
  P\left(\left.\frac{\sqrt{n}(\hat p^* -\hat p)}{\sqrt{p(1-p)}}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0,
$$
where $\Phi$ denotes the distribution function of the standard normal distribution.

This means that the bootstrap is consistent: For large $n$
$$
\text{distribution}(\sqrt{n}(\hat p^* -\hat p)|{\cal S}_n)\approx 
\text{distribution}(\sqrt{n}(\hat p -p))\approx N(0,p(1-p))
$$
as well as
$$
\text{distribution}(\hat p^* -\hat p|{\cal S}_n)\approx 
\text{distribution}(\hat p -p)\approx N(0,p(1-p)/n)
$$



## The nonparametric (standard) bootstrap

**Setup:**

* **Data:** i.i.d. random sample $\mathcal{S}_n=\{X_1,\dots,X_n\}$. 
  * The distribution $X_i\sim F,$ $i=1,\dots,n,$ depends on an unknown parameter (vector) $\boldsymbol{\theta}.$
  * The data $X_1,\dots,X_n$ is used to estimate an element $\theta$ of the parameter vector $\boldsymbol{\theta}.$  
  * Thus, the estimator is a function of the random sample 
  $$
  \hat\theta\equiv \hat\theta(X_1,\dots,X_n).
  $$
* **Inference:** We are interested in evaluating the distribution of $\hat\theta-\theta$
in order to 
  * provide standard errors
  * construct confidence intervals or 
  * perform tests of hypothesis.



**The bootstrap algorithm:**

1. **Draw a bootstrap sample:** Generate a random samples $X_1^*,\dots,X_n^*$ by drawing observations independently and with replacement from the available sample $X_1,\dots,X_n.$
2. **Compute bootstrap estimate:** Compute the estimate 
$$
\hat\theta^*\equiv \hat\theta_k(X_1^*,\dots,X_n^*)
$$
3. **Bootstrap replications:** Repeat steps 1 and 2 $m$ times (e.g. $m=2000$) leading to $m$ values 
$$
\hat\theta_1^*,\hat\theta_2^*,\dots,\hat\theta_m^*
$$
4. The **bootstrap distribution** of $\hat\theta^*-\hat\theta$ is used to approximate the distribution of $\hat\theta-\theta$.



The theoretical justification of the bootstrap is based on asymptotic arguments. In many important applications the bootstrap is able to provide confidence intervals or tests which are more accurate than those based on standard asymptotic approximations.

ðŸ›‘ The bootstrap does **not always work**. A necessary condition for the use of the bootstrap is *consistency of the bootstrap approximation*:

**Consistent Bootstrap:** For large $n$, the bootstrap distribution of $\hat{\theta}^* -\hat{\theta}$ is a good approximation of the underlying distribution of $\hat{\theta}-\theta$, i.e.
$$
\text{distribution}(\hat{\theta}^* -\hat{\theta}\ |{\cal S}_n)\approx 
\text{distribution}(\hat{\theta}-\theta).
$$
More precisely, if for some $\gamma>0$ (usually: $\gamma=1/2$) we have $n^\gamma(\hat{\theta}-\theta)\rightarrow_d Z$ for some random variable $Z$ with a non-degenerate distribution, then the bootstrap is **consistent** if and only if
$$
\sup_\delta \left|
   P\left(n^\gamma(\hat\theta^*-\hat\theta)\le \delta|{\cal S}_n\right) 
  -P\left(n^\gamma(\hat\theta      -\theta)\le \delta\right)
  \right|\rightarrow 0
$$
as $n\to\infty.$


<br>

Luckily, the standard bootstrap "works" for a large number of statistical and econometrical problems. However, there are some crucial requirements:

1. Generation of the bootstrap sample must reflect appropriately the way in which the original sample has been generated (here i.i.d. sampling).
2. The distribution of the estimator $\hat\theta$ needs to be asymptotically normal. 


The standard bootstrap **will usually fail** if one of the above conditions
1 or 2 is violated. For instance, 

* The  bootstrap will not work if the i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $X_1,\dots,X_n$ does not properly reflect the way how the $X_1,\dots,X_n$ are generated when $X_1,\dots,X_n$ is a time-series with auto-correlated data. 
* The distribution of the estimator $\hat\theta$ is not asymptotically normal. (E.g. in case of extreme value problems.)

**Note:** In order to deal with more complex situations alternative bootstrap procedures have been proposed in the literature. (Block-bootstrap in case of time-series data.)




### Confidence intervals


#### The traditional (non-bootstrap) approach {-}

Traditional, non-bootstrap approaches for constructing confidence intervals and statistical hypothesis tests are usually based on asymptotic normal approximations. For example, if 

* $\theta\in\mathbb{R}$ and 
* $\sqrt{n}(\hat\theta-\theta)\rightarrow_d\mathcal{N}(0,v^2)$ as $n\to\infty,$ 

then one traditionally tries to determine an approximation $\hat v$ of $v$ (the standard error of $\hat\theta$) from the data. An approximate $(1-\alpha)\times 100\%$ confidence interval is then given by
$$
\left[
 \hat{\theta}-z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}},
 \hat{\theta}+z_{1-\frac{\alpha}{2}}\frac{\hat v}{\sqrt{n}}
\right]
$$

In some cases it is, however, very difficult to obtain approximations $\hat v$ of $v$. Statistical inference is then usually based on the bootstrap.

In contemporary statistical analysis the bootstrap is frequently used even for standard problems, where estimates $\hat v$ of $v$ are easily constructed. The reason is that in many situations it can be shown that bootstrap confidence intervals or tests are indeed **more precise** than those determined analytically based on asymptotic formulas. (This particularly applies to the bootstrap t-method discussed in the next section.)


#### The bootstrap approach {-}

<!-- General approach: Basic bootstrap $(1-\alpha)\times 100\%$ confidence interval -->

**Setup:**

* **Data:** i.i.d. random sample ${\cal S}_n:=\{X_1,\dots,X_n\}$ with $X_i\sim F$ for all $i=1,\dots,n$, where the distribution $F$ depends on the unknown parameter vector $\boldsymbol{\theta}.$ 
* **Problem:** Construct a confidence interval for $\theta,$ where $\theta$ is an unknown element of $\boldsymbol{\theta}.$ 

In the following, we will assume that the bootstrap is consistent; i.e. that
$$
\begin{align*}
\text{distribution}(\sqrt{n}(\hat{\theta}^* -\hat{\theta})|{\cal S}_n)
&\approx 
\text{distribution}(\sqrt{n}(\hat{\theta}-\theta))\\
\text{short:}\quad\quad\sqrt{n}(\hat{\theta}^*-\hat{\theta})|{\cal S}_n
&\overset{d}{\approx} \sqrt{n}(\hat{\theta}-\theta)
\end{align*}
$$
if $n$ is sufficiently large.


Determine the $\frac{\alpha}{2}$ quantile $\hat t_\frac{\alpha}{2}$ and the $1-\frac{\alpha}{2}$ quantile $\hat t_{1-\frac{\alpha}{2}}$  of the conditional distribution of $\hat{\theta}^*$ given ${\cal S}_n:=\{X_1,\dots,X_n\}$ (i.e. of the "bootstrap distribution"). 


This implies
$$
\begin{align*}
&P^*\left(\hat t_\frac{\alpha}{2} \leq \hat{\theta}^* \leq \hat t_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow & P^*\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}^*-\hat{\theta} \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\
\Rightarrow & P^*\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)
\approx 1-\alpha
\end{align*}
$$
Here, $P^*$  denotes probabilities with respect to the   conditional distribution of  $\hat{\theta}^*$ given ${\cal S}_n:=\{X_1,\dots,X_n\}$.


Due to the assumed consistency of the bootstrap, we have that for large $n$
$$
{\color{red}\sqrt{n}(\hat{\theta}^*-\hat{\theta})}|{\cal S}_n\overset{d}{\approx} {\color{blue}\sqrt{n}(\hat{\theta}-\theta)}.
$$ 
Therefore, for large $n$
$$
\begin{align*}
&P\left(
\sqrt{n}(\hat t_\frac{\alpha}{2}-\hat{\theta})\leq{\color{blue}\sqrt{n}(\hat{\theta}-\theta)}\leq \sqrt{n}(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\right)\approx 1-\alpha\\
\Rightarrow &P\left(\hat t_\frac{\alpha}{2}-\hat{\theta}\leq\hat{\theta}-\theta \leq \hat t_{1-\frac{\alpha}{2}}-\hat{\theta}\right)
\approx 1-\alpha\\
\Rightarrow &P\left(\hat{\theta}-(\hat t_{1-\frac{\alpha}{2}}-\hat{\theta})\le \theta\le \hat{\theta}-
 (\hat t_\frac{\alpha}{2}-\hat{\theta})\right)\approx 1-\alpha\\
\Rightarrow &P\left(2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}\le \theta\le 2\hat{\theta}-
 \hat t_\frac{\alpha}{2}\right)\approx 1-\alpha.
\end{align*}
$$

Thus, the approximate $(1-\alpha)\times 100\%$ (symmetric) bootstrap confidence interval is given by 
$$
\left[2\hat{\theta}-\hat t_{1-\frac{\alpha}{2}}, 2\hat{\theta}-\hat t_\frac{\alpha}{2}\right],
$${#eq-NPBootCI}
where $\hat t_\frac{\alpha}{2}$ and $\hat t_{1-\frac{\alpha}{2}}$ are the  $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles of the bootstrap distribution approximated by the empirical quantiles of the $m$ bootstrap realizations 
$$
\hat{\theta}^*_1, \hat{\theta}^*_2,\dots, \hat{\theta}^*_m.
$$


#### Example: Population mean {-}

**Setup:**

* **Data:** Let $X_1,\dots,X_n$ denote an i.i.d. random sample with mean $\mu$ and variance $\sigma^2$.
  <!-- * In the following $F$ will denote the corresponding distribution function; i.e., $X_i\sim F$ for all $i=1,\dots,n.$ -->
* **Estimator:** $\bar X=\frac{1}{n} \sum_{i=1}^n X_i$ is an unbiased estimator of $\mu$
* **Inference Problem:** Construct a confidence interval for $\mu.$


Traditional, non-bootstrap approach for constructing a $(1-\alpha)\times 100\%$ confidence interval:

* By the CLT: $\bar X\overset{a}{\sim} \mathcal{N}(\mu,\frac{\sigma^2}{n})$ for large $n$
* Estimation of $\sigma^2$: $S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$
* This implies: $\sqrt{n}((\bar X -\mu)/S)\overset{a}{\sim} t_{n-1}$, and hence
$$
\begin{align*}
&P\left(-t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\le \bar X -\mu\le t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\right)\approx 1-\alpha\\
\Rightarrow 
&P\left(\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\le \mu\le 
        \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}
  \right)\approx 1-\alpha
\end{align*}
$$
* $(1-\alpha)\times 100\%$ confidence interval: 
$$
\left[\bar X-t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}},
      \bar X+t_{n-1,1-\frac{\alpha}{2}}\frac{S}{\sqrt{n}}\right]
$$



**Remark:** This traditional construction relies on the assumption that 
$$
\bar X \overset{a}{\sim}\mathcal{N}(\mu,\frac{\sigma^2}{n}).
$$ 
This is exactly true (also for small $n$) if the random sample $X_1,\dots,X_n$ is i.i.d. normally distributed. If the underlying distribution is **not normal**, then this condition is *approximately* fulfilled if the sample size $n$ is sufficiently large (central limit theorem, CLT). In this case the constructed confidence interval is an *approximate* $(1-\alpha)\times 100\%$ confidence interval. 


The bootstrap offers an alternative method for constructing approximate $(1-\alpha)\times 100\%$ confidence intervals. We already know
that the bootstrap is consistent in this situation.

Construction of the nonparametric (i.e. standard) bootstrap confidence interval:

* Draw $m$ bootstrap samples (e.g. $m=2000$) and calculate the corresponding estimates $\bar X^*_1,\bar X^*_2,\dots,\bar X^*_m$.
* Order the resulting estimates: 
$$
\bar X^*_{(1)} \le \bar X^*_{(2)}\le \dots \le \bar X^*_{(m)}
$$
* Compute the empirical quantiles: 
   * $\hat    t_\alpha:=\bar X^*_{([(m+1)\frac{\alpha}{2}])}$ and 
   * $\hat t_{1-\alpha}:=\bar X^*_{([(m+1)(1-\frac{\alpha}{2})])}$
* Approximate $(1-\alpha)\times 100\%$ (symmetric) confidence interval:
$$
\left[2\bar X-\hat t_{1-\frac{\alpha}{2}}, 
      2\bar X-\hat t_\frac{\alpha}{2}\right]
$$




## Pivot statistics and the bootstrap-$t$ method


In many situations it is possible to get more accurate bootstrap confidence intervals by using the bootstrap-$t$ method (one also speaks of "studentized bootstrap confidence intervals"). The construction relies on so-called pivotal statistics.

Let $X_1,\dots,X_n$ be an i.i.d. random sample and assume that the distribution of $X$ depends on an unknown parameter (or parameter vector) $\theta$.

A statistic 
$$
T_n\equiv T(X_1,\dots,X_n)
$$ 
is called "pivotal", if the distribution of $T_n$ does **not depend on any unknown population parameter**.

A statistic $T_n\equiv T(X_1,\dots,X_n)$ is called "asymptotically pivotal", if its *asymptotic* distribution does not depend on any unknown population parameter.


Exact pivotal statistics are rare and not available in most econometric applications. It is, however, often possible to construct an *asymptotically pivotal* statistic. Assume that an estimator $\hat{\theta}$ satisfies
$$
\sqrt{n}(\hat{\theta}-\theta)\rightarrow_d\mathcal{N}(0,v^2),
$$
where $v^2$ denotes the asymptotic variance. Additionally assume that it is possible to use the data in order to construct a consistent estimator
$$
\hat v_n^2\equiv \hat v_n(X_1,\dots,X_n)^2
$$ 
of $v$ such that
$$
\hat v_n^2 \rightarrow_p v^2.
$$
Then, of course, also $\hat v_n\rightarrow_p v$, and
$$
T_n:= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}\rightarrow_d \mathcal{N}(0,1),\quad n\to\infty.
$$
This means that $T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}$ is *asymptotically pivotal*.



#### Example: Asymptotically pivotal statistic {-}

Let $\mathcal{S}_n=\{X_1,\dots,X_n\}$ be a i.i.d. random sample with $X_i\sim X$ for all $i=1,\dots,n,$ with mean $E(X)=\mu$, variance $Var(X)=\sigma^2>0$, and $E(|X|^3)=\beta<\infty$. 
 
If $X$ is normally distributed, we obtain
$$
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{S}\sim t_{n-1}
$$
with $S^2=\frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2$, where $t_{n-1}$ denotes the $t$-distribution with $n-1$ degrees of freedom. We can conclude that $T_n$ is pivotal.

If $X$ is *not* normally distributed, the central limit theorem implies that
$$
T_n:=\frac{\sqrt{n}(\bar X-\mu)}{S}\rightarrow_d\mathcal{N}(0,1),\quad n\to\infty.
$$
In this case $T_n$ is an asymptotically pivotal statistics.


The general idea of bootstrap-$t$ confidence intervals (as well as many bootstrap based tests) now relies on approximating the distribution of 
$$
T_n= \sqrt{n}\frac{(\hat{\theta}-\theta)}{\hat v_n}
$$ 
by the conditional distribution of
$$
T_n^*=\sqrt{n}\frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*},
$$ 
where the variance estimate
$$
\hat v_n^*=v_n(X_1^*,\dots,X_n^*)
$$
is computed from the bootstrap sample $X_1^*,\dots,X_n^*.$ 


If the standard nonparametric bootstrap is consistent, i.e. if the conditional distribution of $\sqrt{n}(\hat{\theta}^*-\hat{\theta})$, given $\mathcal{S}_n$, yields a consistent estimate of $\mathcal{N}(0,v^2)$, then also the bootstrap-$t$ method is consistent. That is, the conditional distribution of $T_n^*$, given $\mathcal{S}_n$, provides a consistent estimate of the asymptotic distribution of $T_n$: $T_n\rightarrow_d N(0,1)$ such that
$$
\sup_\delta \left|P\left(\left.\sqrt{n} \frac{(\hat{\theta}^*-\hat{\theta})}{\hat v_n^*}\le \delta\right|{\cal S}_n\right)-\Phi(\delta)\right|\rightarrow_p 0.
$$

Usually, the bootstrap-$t$ provides a gain in accuracy over the standard nonparametric bootstrap.
As is easily seen when considering the above examples and expansions, the approximation of the law of
$T_n$ by the bootstrap law of $T_n^*$ is even more direct (and hence more accurate) than the approximation
of the law of $\sqrt{n}(\hat{\theta}-\theta)$ by the bootstrap law of  $\sqrt{n}(\hat{\theta}^*-\hat{\theta})$.


### Bootstrap-t confidence interval 

Let ${\cal S}_n:=\{X_1,\dots,X_n\}$ be a i.i.d. random sample and let the distribution $X_i\sim F$, $i=1,\dots,n,$ depend on the unknown parameter (vector) $\theta$. Assume that bootstrap is consistent and that the estimator $\hat{\theta}$ of $\theta$ is asymptotically normal. Furthermore, suppose that a **consistent** estimator 
$$
\hat v\equiv \hat v(X_1,\dots,X_n)
$$ 
of the asymptotic standard deviation $v$ is available.


**The bootstrap-$t$ algorithm:**

* Based on an i.i.d. re-sample $X_1^*,\dots,X_n^*$ from $\mathcal{S}_n=\{X_1,\dots,X_n\},$ calculate bootstrap estimates 
$$
\hat{\theta}^*\quad\text{and}\quad \hat v^*.
$$
* Determine $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat \tau_\frac{\alpha}{2}$ and $\hat \tau_{1-\frac{\alpha}{2}}$ of the bootstrap distribution
of 
$$
\frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*}.
$$

Then
$$
\begin{align*}
&P^*\left(\hat \tau_\frac{\alpha}{2}\leq \frac{\hat{\theta}^*-\hat{\theta}}{\hat v^*} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(\hat \tau_\frac{\alpha}{2}\leq \frac{\hat{\theta}-\theta}{\hat v} \leq \hat \tau_{1-\frac{\alpha}{2}}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta-\hat{\theta} \leq -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha\\
\Rightarrow & P\left(\hat{\theta}-\hat v \hat \tau_{1-\frac{\alpha}{2}}\leq \theta \leq \hat{\theta} -\hat v\hat\tau_\frac{\alpha}{2}\right)
\approx 1-\alpha
\end{align*}
$$
Bootstrap-$t$ interval:
$$
\left[\hat{\theta}-\hat \tau_{1-\frac{\alpha}{2}}\hat v,
      \hat{\theta}-\hat \tau_{  \frac{\alpha}{2}}\hat v\right]
$$


#### Example: Bootstrap-$t$ confidence interval for the mean {-}


**Algorithm:**

* Draw i.i.d. random samples $X_1^*,\dots,X_n^*$ from ${\cal S}_n$ and calculate
$\bar X^*$ as well as $S^{*2}=\frac{1}{n-1}\sum_{i=1}^n (X_i^*-\bar X^*)^2$.
* Determine $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat \tau_\frac{\alpha}{2}$ and $\hat \tau_{1-\frac{\alpha}{2}}$  of the bootstrap distribution
of  $\frac{\bar X^*-\bar X}{S^*}$
* This yields the $1-\alpha$ confidence interval
$$
\left[\bar X-\hat \tau_{1-\frac{\alpha}{2}}S,
      \bar X-\hat \tau_{\frac{\alpha}{2}}S\right]
$$


The use of pivotal statistics and the corresponding construction of bootstrap-$t$ intervals is motivated by theoretical results which show that under mild conditions it is **second order accurate**.

Consider generally $(1-\alpha)\times 100\%$ confidence intervals of the form $[L_n,U_n]$ of $\theta$. The lower, $L_n$, and upper bounds, $U_n$, of such intervals are determined from the data, 
$$
L_n\equiv L(X_1,\dots,X_n)
$$
$$
U_n\equiv U(X_1,\dots,X_n)
$$ 
and their accuracy depends on the particular procedure applied (e.g. nonparametric bootstrap vs. bootstrap-$t$).


* (Symmetric) confidence intervals are said to be **first-order accurate** if there exist some constants $c_1,c_2<\infty$ such that for sufficiently large $n$
$$
\begin{align*}
\left|P(\theta<L_n)-\frac{\alpha}{2}\right|\le \frac{c_1}{\sqrt{n}}\\
\left|P(\theta>U_n)-\frac{\alpha}{2}\right|\le \frac{c_2}{\sqrt{n}}
\end{align*}
$$
* (Symmetric) confidence intervals are said to be **second-order accurate** if there exist some constants $c_3,c_4<\infty$ such that
for sufficiently large $n$
$$
\begin{align*}
\left|P(\theta<L_n)-\frac{\alpha}{2}\right|\le \frac{c_3}{n}\\
\left|P(\theta>U_n)-\frac{\alpha}{2}\right|\le \frac{c_4}{n}
\end{align*}
$$


If the distribution of $\hat\theta$ is asymptotically normal and the bootstrap is consistent, then under some additional regularity conditions it can usually be shown that

* Standard confidence intervals based on asymptotic approximations are first-order accurate. 
* Nonparametric (standard) boostrap confidence intervals are first-order accurate.
* Bootstrap-$t$ confidence intervals are **second-order** accurate.

The difference between first and second-order accuracy is not just a theoretical nicety. In many practically important situations second-order accurate intervals lead to *much* better approximations. If possible, bootstrap confidence intervals as well as tests should thus be based on pivotal statistics.



## Regression Analysis: Bootstrapping pairs


**Problem:** Analyze the influence of a vector of (explanatory) predictor variables 
$$
X:=(X_1,X_2,\ldots,X_p)^T\in\mathbb{R}^p
$$ 
on a response
variable (or "dependent" variable) $Y.$


Bootstrapping regression estimates is straightforward under a *random design* (i.e. a random $X$).

* **Random design:** Data consists of an i.i.d sample
  $$
  (Y_1,X_1),\ldots,(Y_n,X_n)
  $$
  That is, $(Y_i,X_i)\in\mathbb{R}^{(p+1)}$ is a multivariate random variable without degenerated marginal distributions.
* **Model:**
  $$
  \begin{align*}
  Y_i=X_i^T\beta+ \varepsilon_i,\quad  i=1,\dots,n,
  \end{align*}
  $$
  where $E(\varepsilon_i|X_i)=0$ and either
    * $E(\varepsilon_i^2|X_i)=\sigma^2$, $i=1,\dots,n$, for a fixed
      $\sigma^2<\infty$ (homoscedastic errors), or
    * $E(\varepsilon_i^2|X_i)=\sigma^2(X_i)<\infty$, $i=1,\dots,n$ (heteroscedastic errors).
* Additional assumptions required for asymptotic theory
    * There exists a positive definite matrix $M$ 
    $$
    M=E(X_iX_i^T)
    $$
    * There exists a positive semi-definite matrix $Q$ such that
    $$
    Q=E(\varepsilon_i^2X_iX_i^T)=E(\sigma^2(X_i)X_iX_i^T)
    $$
    *Note:* For homoscedastic errors we have $Q=\sigma^2 M$.



The least squares estimator $\hat\beta$ is given by
$$
\begin{align*}
\hat\beta 
&=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i\\
&=\beta+\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
\end{align*}
$$

The law of large numbers, the continuous mapping theorem, Slutsky's theorem, and the central limit theorem (see the econometric lecture) implies that
$$
\sqrt{n}(\hat\beta-\beta)\rightarrow_d\mathcal{N}(0,M^{-1}QM^{-1}),\quad n\to\infty.
$$

Since $(Y_i,X_i)$ are i.i.d. one may apply the nonparametric bootstrap in order to approximate the distribution of 
$$
\hat\beta-\beta.
$$ 
In the literature this procedure is usually called "bootstrapping pairs"; namely, "$Y_i$ and $X_i$" pairs.


* Original data: i.i.d. sample ${\cal S}_n:=\{(Y_1,X_1),\dots,(Y_n,X_n)\}$
* Generate bootstrap samples
  $$
  (Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)
  $$ 
  by drawing observations independently and with replacement from ${\cal S}_n.$
* Each bootstrap sample $(Y_1^*,X_1^*),\dots,(Y_n^*,X_n^*)$ leads to a bootstrap realization of the least squares estimator
$$
 \hat\beta^*=\left(\sum_{i=1}^n X_i^*X_i^{*T}\right)^{-1}\sum_{i=1}^n X_i^*Y_i^*
$$
 

It can be shown that bootstrapping pairs is **consistent**; i.e. that for large $n$
$$
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)\approx\mathcal{N}(0,M^{-1}QM^{-1})
$$

This allows to construct basic bootstrap confidence intervals for the regression coefficients $\beta_j$, $j=1,\dots,p$:

* Determine the $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles 
$\hat t_{\frac{\alpha}{2},j}$ 
and 
$\hat t_{1-\frac{\alpha}{2},j}$  
of the conditional distribution of $\hat\beta_j^*$, given ${\cal S}_n.$
* Approximate $(1-\alpha)\times 100\%$ (symmetric) confidence interval as in @eq-NPBootCI:
$$
\left[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j}, 
      2\hat\beta_j-\hat t_{\frac{\alpha}{2},j}\right]
$$


**Remark:** The basic bootstrap confidence interval provides an asymptotically (first order) accurate confidence interval, even if the errors are **heteroscedastic** (unequal variances)! This is not true for the standard confidence intervals intervals provided by standard software packages.


## Regression Analysis: Residual bootstrap

Bootstrapping pairs is not necessarily applicable for **fixed design** or if the $X_i$-variables are correlated (like in time-series contexts). In this case 
$$
(Y_1,X_1),\dots,(Y_n,X_n)
$$ 
is **not** an i.i.d. sample, and the "bootstrapping pairs" procedure proposed above will generally not be consistent. 

However, if error terms are *homoscedastic*, then it is possible to rely on
the residual bootstrap.



In the following we will formally assume a regression model
$$
Y_i=X_i^T\beta+ \varepsilon_i, \quad i=1,\dots,n,
$$
under *fixed design*, where $\varepsilon_1,\dots,\varepsilon_n$ are i.i.d. zero mean error terms with
$$
E(\varepsilon_i^2)=\sigma^2;
$$ 
i.e., homoscedastic errors.

**Remark:** Though we will formally rely on a fixed design assumption, the residual bootstrap is also applicable for random design or stochastic, but correlated $X$-variables. In these cases all arguments are meant conditionally on the given $X_i$. The  above assumptions on the error terms then of course have to be satisfied conditionally on $X_i$.


The idea of the residual bootstrap is very simple: The model implies that $\varepsilon_1,\dots,\varepsilon_n$ are i.i.d which suggests a bootstrap based on resampling the error terms. 

These errors are unobserved, but they can be approximated by the  corresponding residuals
$$
\hat \varepsilon_i:=Y_i-X_i^T\hat\beta, \quad i=1,\dots,n,
$$
where again 
$$
\hat\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i
$$ 
denotes the least squares estimator. 

It is well known that
$$
\hat\sigma^2:= \frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^2
$$ 
provides an unbiased, consistent estimator of the error variance $\sigma^2$. That is,
$$
E(\hat\sigma^2)=\sigma^2 \quad \text{and } \hat\sigma^2\rightarrow_p \sigma^2
$$


**Residual bootstrap algorithm:**

* Based on the original data $(Y_i,X_i)$, $i=1,\dots,n$, and the least squares estimate $\hat\beta$, calculate the residuals $\hat\varepsilon_1,\dots,\hat \varepsilon_n$.
* Generate random samples $\hat\varepsilon_1^*,\dots,\hat\varepsilon_n^*$ of residuals by drawing observations independently and with replacement
from ${\cal S}_n:=\{\hat\varepsilon_1,\dots,\hat \varepsilon_n\}$.
* Calculate new depend variables 
$$
Y_i^*=X_i^T\hat\beta+\hat\varepsilon_i^*,\quad i=1,\dots,n
$$
* Bootstrap estimators $\hat\beta^*$ are determined by least squares estimation from the data $(Y_1^*,X_1),\dots,(Y_n^*,X_n)$:
$$
\hat\beta^*=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_iY_i^*
$$



It is not difficult to understand why the residual bootstrap generally works (for homoscedastic errors !). We have
$$
\hat\beta-\beta=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\varepsilon_i
$$
and for large $n$ the distribution of $\sqrt{n}(\hat\beta-\beta)$ is approximately normal with mean 0 and covariance matrix $\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}$
$$
\sqrt{n}(\hat\beta-\beta)\to_d\mathcal{N}\left(0,\sigma^2 \left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\right)
$$




By construction
$$
\hat\beta^*-\hat\beta
=\left(\frac{1}{n}\sum_{i=1}^n X_iX_i^T\right)^{-1}\frac{1}{n}\sum_{i=1}^n X_i\hat\varepsilon_i^*
$$
But conditional on ${\cal S}_n$ the bootstrap error terms are i.i.d with
$$
E(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i =0
$$
and
$$
Var(\hat\varepsilon_i^*| {\cal S}_n)=\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2
$$
By central limit arguments limit arguments one will thus obtain that for large $n$ the bootstrap distribution of $\sqrt{n}(\hat\beta^*-\hat\beta)$ is approximately normal with mean zero and covariance matrix  $\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 (\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$. Since  $\frac{1}{n} \sum_{i=1}^n \hat \varepsilon_i^2 \mathbb{R}\rightarrow_P \sigma^2$ as $n\rightarrow\infty$,
the bootstrap is consistent. For large $n$ we have approximately
\begin{align*}
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta) |{\cal S}_n)&\approx
\text{distribution}(\sqrt{n}(\hat\beta^*-\hat\beta))\\
&\approx  N(0,\sigma^2 (\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1})
\end{align*}



<!-- 













Basic bootstrap confidence intervals for the regression coefficients
$\beta_j$, $j=1,\dots,p$:
\begin{itemize}
\item Determine $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat t_{\frac{\alpha}{2},j}$ and $\hat t_{1-\frac{\alpha}{2},j}$  of the conditional distribution
of  $\hat\beta_j^*$.
\item $\rightarrow$ Approximate $1-\alpha$ (symmetric) confidence interval:
$$[2\hat\beta_j-\hat t_{1-\frac{\alpha}{2},j}, 2\hat\beta_j-\hat t_{\frac{\alpha}{2},j}]$$
\end{itemize}

In the given situation one will of course prefer to use bootstrap-t intervals. If $\gamma_{jj}$ denotes
the $j$-th diagonal element of the matrix $(\frac{1}{n}\sum_{i=1}^n X_iX_i^T)^{-1}$, then
$\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}$ is a pivotal statistics, since
$$\frac{\sqrt{n}(\hat\beta_j-\beta_j)}{\hat\sigma\sqrt{\gamma_{jj}}}\mathbb{R}\rightarrow_D N(0,1).$$

A bootstrap-t interval for $\beta_j$, $j=1,\dots,p$, can thus be constructed as follows:

\begin{itemize}
\item Additionally compute $\hat\sigma^{*2}:=\frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^{*2}$, and
 determine $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$ quantiles $\hat \tau_{\frac{\alpha}{2},j}$ and $\hat \tau_{1-\frac{\alpha}{2},j}$  of the bootstrap distribution
of
$$\frac{\hat\beta_j^*-\hat\beta_j}{\hat\sigma^* \sqrt{\gamma_{jj}}}$$
\item This yields the $1-\alpha$ confidence interval
$$[\hat\beta_j-\hat \tau_{1-\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}},
\hat\beta_j-\hat \tau_{\frac{\alpha}{2},j}\hat\sigma \sqrt{\gamma_{jj}}]$$
\end{itemize} 

 -->
